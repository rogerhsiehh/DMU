{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a8173246-62e5-4cbc-8009-5f0919c18057",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yfinance as yf\n",
    "from datetime import datetime, date, timedelta\n",
    "import ta\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "from stable_baselines3 import PPO, A2C\n",
    "from sb3_contrib import RecurrentPPO\n",
    "import optuna\n",
    "import matplotlib.pyplot as plt\n",
    "import gym_anytrading\n",
    "from gym_anytrading.envs import StocksEnv,Actions,Positions\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.callbacks import EvalCallback\n",
    "import pickle\n",
    "import random\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8120e4f-912e-4b28-a4e7-ddbbaafd5f42",
   "metadata": {},
   "source": [
    "# 1. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ba343558-04ec-47ee-a3ab-a0ca7245f053",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Change these variables if necessary\n",
    "window_size = 20 \n",
    "start_date = '2020-01-01' \n",
    "today = datetime.today().date()\n",
    "end_date = today.strftime(\"%Y-%m-%d\") #today is the end\n",
    "ticker = \"NVDA\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "645d7b65-3bf8-4e13-9070-a4e089e1a7bc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/yf/0wv1c8p96x15nmycpl2csp9m0000gn/T/ipykernel_46382/2955599808.py:29: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  data['stoch'].replace([-float('inf'), float('inf')], np.nan, inplace=True)\n",
      "/var/folders/yf/0wv1c8p96x15nmycpl2csp9m0000gn/T/ipykernel_46382/2955599808.py:40: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  data['adi'].replace([np.inf, -np.inf], 0, inplace=True)\n",
      "/var/folders/yf/0wv1c8p96x15nmycpl2csp9m0000gn/T/ipykernel_46382/2955599808.py:58: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  data['williams_r'].replace([-float('inf'), float('inf')], np.nan, inplace=True)\n",
      "/var/folders/yf/0wv1c8p96x15nmycpl2csp9m0000gn/T/ipykernel_46382/2955599808.py:75: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df.drop(['Dividends','Stock Splits'],axis=1, inplace=True)\n"
     ]
    }
   ],
   "source": [
    "SEED = 99\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "\n",
    "\n",
    "stock_data = yf.Ticker(ticker).history(start=start_date, end=end_date, interval='1d')\n",
    "\n",
    "df = stock_data.reset_index()#[['Date','Close']]\n",
    "\n",
    "df.sort_values(by=['Date'], inplace=True)\n",
    "\n",
    "data = df.copy()\n",
    "\n",
    "# Calculate Simple Moving Average (SMA)\n",
    "data['sma'] = ta.trend.sma_indicator(data['Close'], window=10)\n",
    "\n",
    "# Calculate Exponential Moving Average (EMA)\n",
    "data['ema'] = ta.trend.ema_indicator(data['Close'], window=10)\n",
    "\n",
    "# Calculate Moving Average Convergence Divergence (MACD)\n",
    "macd = ta.trend.MACD(data['Close'])\n",
    "data['macd'] = macd.macd()\n",
    "data['macd_signal'] = macd.macd_signal()\n",
    "data['macd_diff'] = macd.macd_diff()\n",
    "\n",
    "# Calculate Stochastic Oscillator\n",
    "stochastic_osc = ta.momentum.StochasticOscillator(data['High'], data['Low'], data['Close'], window=10, smooth_window=3)\n",
    "data['stoch'] = stochastic_osc.stoch()\n",
    "data['stoch'].replace([-float('inf'), float('inf')], np.nan, inplace=True)\n",
    "data['stoch_signal'] = stochastic_osc.stoch_signal()\n",
    "\n",
    "# Calculate Relative Strength Index (RSI)\n",
    "data['rsi'] = ta.momentum.rsi(data['Close'], window=10)\n",
    "\n",
    "# Calculate Average True Range (ATR)\n",
    "data['atr'] = ta.volatility.average_true_range(data['High'], data['Low'], data['Close'], window=10)\n",
    "\n",
    "# Calculate Accumulation Distribution Index (ADI)\n",
    "data['adi'] = ta.volume.acc_dist_index(data['High'], data['Low'], data['Close'], data['Volume'])\n",
    "data['adi'].replace([np.inf, -np.inf], 0, inplace=True)\n",
    "\n",
    "# Calculate Bollinger Bands\n",
    "bollinger = ta.volatility.BollingerBands(data['Close'], window=20, window_dev=2)\n",
    "data['bollinger_hband'] = bollinger.bollinger_hband()\n",
    "data['bollinger_lband'] = bollinger.bollinger_lband()\n",
    "data['bollinger_mavg'] = bollinger.bollinger_mavg()\n",
    "\n",
    "#Commodity Channel Index (CCI)\n",
    "data['cci'] = ta.trend.cci(data['High'], data['Low'], data['Close'], window=20)\n",
    "\n",
    "# Aroon Indicator\n",
    "aroon = ta.trend.AroonIndicator(high=data['High'], low=data['Low'], window=25)\n",
    "data['aroon_up'] = aroon.aroon_up()\n",
    "data['aroon_down'] = aroon.aroon_down()\n",
    "\n",
    "# Williams %R\n",
    "data['williams_r'] = ta.momentum.williams_r(data['High'], data['Low'], data['Close'], lbp=10)\n",
    "data['williams_r'].replace([-float('inf'), float('inf')], np.nan, inplace=True)\n",
    "\n",
    "\n",
    "# Money Flow Index (MFI)\n",
    "data['mfi'] = ta.volume.money_flow_index(data['High'], data['Low'], data['Close'], data['Volume'], window=10)\n",
    "\n",
    "# Rate of Change (ROC)\n",
    "data['roc'] = ta.momentum.roc(data['Close'], window=10)\n",
    "\n",
    "# Chaikin A/D Line\n",
    "data['chaikin_ad'] = ta.volume.ChaikinMoneyFlowIndicator(data['High'], data['Low'], data['Close'], data['Volume'], window=20).chaikin_money_flow()\n",
    "\n",
    "# On-Balance Volume (OBV)\n",
    "data['obv'] = ta.volume.on_balance_volume(data['Close'], data['Volume'])\n",
    "\n",
    "\n",
    "df = data.dropna() \n",
    "df.drop(['Dividends','Stock Splits'],axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "34489cd0-561c-44da-8c48-50f2d32ce3ac",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/yf/0wv1c8p96x15nmycpl2csp9m0000gn/T/ipykernel_46382/3629434742.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_df[columns_to_scale] = scaler.fit_transform(train_df[columns_to_scale])\n",
      "/var/folders/yf/0wv1c8p96x15nmycpl2csp9m0000gn/T/ipykernel_46382/3629434742.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  val_df[columns_to_scale] = scaler.transform(val_df[columns_to_scale])\n",
      "/var/folders/yf/0wv1c8p96x15nmycpl2csp9m0000gn/T/ipykernel_46382/3629434742.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_df[columns_to_scale] = scaler.transform(test_df[columns_to_scale])\n"
     ]
    }
   ],
   "source": [
    "train_df = df[df['Date']<'2023-10-01']\n",
    "val_df = df[(df['Date']>='2023-10-01') & (df['Date']<'2024-10-01')]\n",
    "test_df = df[df['Date']>='2024-10-01']\n",
    "\n",
    "columns_to_scale = [col for col in df.columns if col not in [\"Date\"]]\n",
    "scaler = StandardScaler()\n",
    "train_df[columns_to_scale] = scaler.fit_transform(train_df[columns_to_scale]) \n",
    "val_df[columns_to_scale] = scaler.transform(val_df[columns_to_scale])\n",
    "test_df[columns_to_scale] = scaler.transform(test_df[columns_to_scale])\n",
    "\n",
    "with open('columns_to_scale.pkl', 'wb') as f:\n",
    "    pickle.dump(columns_to_scale, f)\n",
    "    \n",
    "with open('scaler.pkl', 'wb') as f:\n",
    "    pickle.dump(scaler, f)\n",
    "\n",
    "train_df.set_index('Date', inplace=True)\n",
    "val_df.set_index('Date', inplace=True)\n",
    "test_df.set_index('Date', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ae43dfc-35d8-4961-b30a-f1979730ea83",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4e27d3c1-0e2e-4382-a198-20b6ee949389",
   "metadata": {},
   "source": [
    "# 2. Customize Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "85f97672-40e6-49f5-b832-bad7a97f0e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomStocksEnv(StocksEnv):\n",
    "    def __init__(self, df, window_size, frame_bound, render_mode=None):\n",
    "        super().__init__(df, window_size, frame_bound, render_mode)\n",
    "\n",
    "        self.trade_fee_bid_percent = 0.001  # unit\n",
    "        self.trade_fee_ask_percent = 0.001 \n",
    "    \n",
    "    def _process_data(self):\n",
    "        start = self.frame_bound[0] - self.window_size\n",
    "        end = self.frame_bound[1]\n",
    "        prices = self.df.loc[:, 'Close'].to_numpy()[start:end]\n",
    "        signal_features = self.df.drop(['Close'],axis=1).to_numpy()[start:end]\n",
    "    \n",
    "        diff = np.insert(np.diff(prices), 0, 0)\n",
    "        signal_features = np.column_stack((prices, diff, signal_features))\n",
    "    \n",
    "        close_col_idx = self.df.columns.get_loc('Close')\n",
    "        close_mean = scaler.mean_[close_col_idx]\n",
    "        close_scale = scaler.scale_[close_col_idx]\n",
    "        # invert scaling\n",
    "        prices = prices * close_scale + close_mean\n",
    "        \n",
    "        return prices.astype(np.float32), signal_features.astype(np.float32)\n",
    "\n",
    "    def _update_profit(self, action):\n",
    "        trade = False\n",
    "        if (\n",
    "            (action == Actions.Buy.value and self._position == Positions.Short) or\n",
    "            (action == Actions.Sell.value and self._position == Positions.Long)\n",
    "        ):\n",
    "            trade = True\n",
    "\n",
    "        if trade or self._truncated:\n",
    "            current_price = self.prices[self._current_tick]\n",
    "            last_trade_price = self.prices[self._last_trade_tick]\n",
    "\n",
    "            if self._position == Positions.Long:\n",
    "                shares = (self._total_profit * (1 - self.trade_fee_ask_percent)) / last_trade_price\n",
    "                self._total_profit = (shares * (1 - self.trade_fee_bid_percent)) * current_price\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ee5f22-4cd1-4aa5-8e4e-896406a2b560",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df9c14a2-947d-4463-adb4-fb2dee47a790",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f09889c4-5d16-4b13-8add-a9edd7c8f311",
   "metadata": {},
   "source": [
    "# 3. PPO Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb597cfb-a0ba-4464-9632-f239b36891a1",
   "metadata": {},
   "source": [
    "## a. Fine tuning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "775e85ba-22ec-4e3d-98f3-cc82dcced125",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial: optuna.Trial):\n",
    "    \"\"\"\n",
    "    Fine tuning using Bayesian Optimization from optuna\n",
    "    \"\"\"\n",
    "\n",
    "    # Sample hyperparameters from ranges \n",
    "    learning_rate = trial.suggest_float(\"learning_rate\", 1e-5, 1e-2,log=True)\n",
    "    n_steps       = trial.suggest_int(\"n_steps\", 128, 2048, step=128)\n",
    "    gamma         = trial.suggest_float(\"gamma\", 0.90, 0.99, step=0.01)\n",
    "    ent_coef      = trial.suggest_float(\"ent_coef\", 1e-8, 0.1, log=True)\n",
    "    clip_range    = trial.suggest_float(\"clip_range\", 0.1, 0.4, step=0.05)\n",
    "\n",
    "    # Less important hyperparameters:\n",
    "    batch_size       = trial.suggest_categorical(\"batch_size\", [32,64])\n",
    "    gae_lambda       = trial.suggest_float(\"gae_lambda\", 0.8, 0.98)\n",
    "    n_epochs       = trial.suggest_int(\"n_epochs\", 5, 20)\n",
    "\n",
    "\n",
    "    # Create the environment \n",
    "    train_env = CustomStocksEnv(df=train_df, window_size=window_size, frame_bound=(window_size, len(train_df)))\n",
    "    val_env = CustomStocksEnv(df=val_df, window_size=window_size, frame_bound=(window_size, len(val_df)))\n",
    "    val_env = Monitor(val_env)\n",
    "\n",
    "    eval_callback_ft = EvalCallback(\n",
    "        val_env,\n",
    "        best_model_save_path='./logs/best_model_ft/',\n",
    "        log_path='./logs/results_ft/',\n",
    "        eval_freq=500,  # Evaluate every 500 timesteps.\n",
    "        deterministic=True,\n",
    "        render=False,\n",
    "        verbose=0\n",
    "    )\n",
    "    \n",
    "    # Build the PPO model\n",
    "    model = PPO( \n",
    "        policy=\"MlpPolicy\", \n",
    "        env=train_env, \n",
    "        verbose=0, \n",
    "        learning_rate=learning_rate,\n",
    "        n_steps=n_steps,\n",
    "        batch_size=batch_size,\n",
    "        gae_lambda=gae_lambda,\n",
    "        gamma=gamma,\n",
    "        n_epochs=n_epochs, \n",
    "        ent_coef=ent_coef,\n",
    "        clip_range=clip_range\n",
    "    )\n",
    "\n",
    "    #Keep it short - 50,000 steps also make sense\n",
    "    model.learn(total_timesteps=100_000, callback=eval_callback_ft)\n",
    "\n",
    "    model = PPO.load(\"./logs/best_model_ft/best_model.zip\")\n",
    "\n",
    "    \n",
    "    mean_reward, std_reward = evaluate_policy(model, val_env, n_eval_episodes=20, deterministic=False)\n",
    "\n",
    "    # Cleanup the environments\n",
    "    train_env.close()\n",
    "    val_env.close()\n",
    "\n",
    "    return mean_reward\n",
    "\n",
    "def run_optimization():\n",
    "    study = optuna.create_study(direction=\"maximize\")  \n",
    "    study.optimize(objective, n_trials=200)\n",
    "\n",
    "    print(\"Best hyperparameters:\", study.best_params)\n",
    "    print(\"Best value (objective):\", study.best_value)\n",
    "\n",
    "    return study\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "57e94868-7eba-4c34-b392-9f5404f0e22a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-06 01:22:18,275] A new study created in memory with name: no-name-652693b5-0ea0-48c9-88d0-82a441c0334a\n",
      "[I 2025-02-06 01:24:19,274] Trial 0 finished with value: 60.65259669999999 and parameters: {'learning_rate': 0.00018689202902918235, 'n_steps': 896, 'gamma': 0.99, 'ent_coef': 1.1769084927832543e-05, 'clip_range': 0.35, 'batch_size': 32, 'gae_lambda': 0.8489755652611685, 'n_epochs': 16}. Best is trial 0 with value: 60.65259669999999.\n",
      "[I 2025-02-06 01:25:44,454] Trial 1 finished with value: 58.74317255 and parameters: {'learning_rate': 0.00016690521501485124, 'n_steps': 1024, 'gamma': 0.92, 'ent_coef': 1.346044592178081e-08, 'clip_range': 0.4, 'batch_size': 64, 'gae_lambda': 0.9305169301328926, 'n_epochs': 18}. Best is trial 0 with value: 60.65259669999999.\n",
      "[I 2025-02-06 01:27:10,138] Trial 2 finished with value: 61.14758425 and parameters: {'learning_rate': 0.0010268395441309214, 'n_steps': 2048, 'gamma': 0.97, 'ent_coef': 3.984808472802168e-05, 'clip_range': 0.4, 'batch_size': 64, 'gae_lambda': 0.8488661451886598, 'n_epochs': 18}. Best is trial 2 with value: 61.14758425.\n",
      "[I 2025-02-06 01:28:35,022] Trial 3 finished with value: 68.66754789999999 and parameters: {'learning_rate': 0.008119843224604194, 'n_steps': 1920, 'gamma': 0.91, 'ent_coef': 0.04860247034219531, 'clip_range': 0.30000000000000004, 'batch_size': 64, 'gae_lambda': 0.8873723213453154, 'n_epochs': 17}. Best is trial 3 with value: 68.66754789999999.\n",
      "[I 2025-02-06 01:29:36,056] Trial 4 finished with value: 45.48357299999999 and parameters: {'learning_rate': 0.00024591273442387896, 'n_steps': 640, 'gamma': 0.9400000000000001, 'ent_coef': 6.26382446978953e-05, 'clip_range': 0.1, 'batch_size': 64, 'gae_lambda': 0.9711724144732322, 'n_epochs': 7}. Best is trial 3 with value: 68.66754789999999.\n",
      "[I 2025-02-06 01:30:52,710] Trial 5 finished with value: 66.29635884999999 and parameters: {'learning_rate': 0.0006343537619263468, 'n_steps': 128, 'gamma': 0.96, 'ent_coef': 4.283180105963471e-08, 'clip_range': 0.15000000000000002, 'batch_size': 64, 'gae_lambda': 0.8484282722707996, 'n_epochs': 13}. Best is trial 3 with value: 68.66754789999999.\n",
      "[I 2025-02-06 01:32:23,746] Trial 6 finished with value: 51.220361149999995 and parameters: {'learning_rate': 0.00239251393525628, 'n_steps': 1536, 'gamma': 0.96, 'ent_coef': 8.993716826497059e-06, 'clip_range': 0.1, 'batch_size': 64, 'gae_lambda': 0.8058297774942378, 'n_epochs': 20}. Best is trial 3 with value: 68.66754789999999.\n",
      "[I 2025-02-06 01:33:56,886] Trial 7 finished with value: 40.79419895 and parameters: {'learning_rate': 0.0007888338967142716, 'n_steps': 2048, 'gamma': 0.9, 'ent_coef': 2.427555738625077e-06, 'clip_range': 0.15000000000000002, 'batch_size': 32, 'gae_lambda': 0.8486374763707698, 'n_epochs': 11}. Best is trial 3 with value: 68.66754789999999.\n",
      "[I 2025-02-06 01:34:58,835] Trial 8 finished with value: 29.051633900000002 and parameters: {'learning_rate': 7.58488581561512e-05, 'n_steps': 1664, 'gamma': 0.92, 'ent_coef': 0.010711006650993626, 'clip_range': 0.30000000000000004, 'batch_size': 64, 'gae_lambda': 0.9156201931069374, 'n_epochs': 7}. Best is trial 3 with value: 68.66754789999999.\n",
      "[I 2025-02-06 01:35:56,161] Trial 9 finished with value: 60.612368200000006 and parameters: {'learning_rate': 0.00013225533403826709, 'n_steps': 512, 'gamma': 0.92, 'ent_coef': 1.276727181542881e-08, 'clip_range': 0.25, 'batch_size': 64, 'gae_lambda': 0.8981625347925494, 'n_epochs': 5}. Best is trial 3 with value: 68.66754789999999.\n",
      "[I 2025-02-06 01:37:37,102] Trial 10 finished with value: 56.30255095 and parameters: {'learning_rate': 0.009700371962531585, 'n_steps': 1536, 'gamma': 0.9, 'ent_coef': 0.093972458523714, 'clip_range': 0.25, 'batch_size': 32, 'gae_lambda': 0.9616659707805566, 'n_epochs': 13}. Best is trial 3 with value: 68.66754789999999.\n",
      "[I 2025-02-06 01:38:53,857] Trial 11 finished with value: 35.54313355 and parameters: {'learning_rate': 1.198965619426761e-05, 'n_steps': 128, 'gamma': 0.9500000000000001, 'ent_coef': 0.0013591583831912668, 'clip_range': 0.2, 'batch_size': 64, 'gae_lambda': 0.8752744781917432, 'n_epochs': 13}. Best is trial 3 with value: 68.66754789999999.\n",
      "[I 2025-02-06 01:40:16,249] Trial 12 finished with value: 76.1954205 and parameters: {'learning_rate': 0.009817638179109238, 'n_steps': 256, 'gamma': 0.98, 'ent_coef': 5.439681333866025e-07, 'clip_range': 0.2, 'batch_size': 64, 'gae_lambda': 0.8160926179228467, 'n_epochs': 15}. Best is trial 12 with value: 76.1954205.\n",
      "[I 2025-02-06 01:41:38,632] Trial 13 finished with value: 84.59390375000001 and parameters: {'learning_rate': 0.009635827959961221, 'n_steps': 1280, 'gamma': 0.99, 'ent_coef': 4.3721254196190056e-07, 'clip_range': 0.30000000000000004, 'batch_size': 64, 'gae_lambda': 0.8118470054487996, 'n_epochs': 16}. Best is trial 13 with value: 84.59390375000001.\n",
      "[I 2025-02-06 01:43:28,574] Trial 14 finished with value: 57.81392885 and parameters: {'learning_rate': 0.0032491378711311815, 'n_steps': 1280, 'gamma': 0.99, 'ent_coef': 3.149448689159226e-07, 'clip_range': 0.2, 'batch_size': 32, 'gae_lambda': 0.8067117175659211, 'n_epochs': 15}. Best is trial 13 with value: 84.59390375000001.\n",
      "[I 2025-02-06 01:44:37,546] Trial 15 finished with value: 87.38441270000001 and parameters: {'learning_rate': 0.003385997441634187, 'n_steps': 640, 'gamma': 0.98, 'ent_coef': 5.372570048073092e-07, 'clip_range': 0.30000000000000004, 'batch_size': 64, 'gae_lambda': 0.8202696554031311, 'n_epochs': 10}. Best is trial 15 with value: 87.38441270000001.\n",
      "[I 2025-02-06 01:45:46,159] Trial 16 finished with value: 61.62622550000001 and parameters: {'learning_rate': 0.0026407722798479083, 'n_steps': 640, 'gamma': 0.98, 'ent_coef': 1.752867935198303e-07, 'clip_range': 0.30000000000000004, 'batch_size': 64, 'gae_lambda': 0.8268025913933827, 'n_epochs': 10}. Best is trial 15 with value: 87.38441270000001.\n",
      "[I 2025-02-06 01:46:54,087] Trial 17 finished with value: 66.36093565 and parameters: {'learning_rate': 0.004481839128892241, 'n_steps': 1280, 'gamma': 0.98, 'ent_coef': 0.00028557254659298176, 'clip_range': 0.35, 'batch_size': 64, 'gae_lambda': 0.8287479066784191, 'n_epochs': 10}. Best is trial 15 with value: 87.38441270000001.\n",
      "[I 2025-02-06 01:48:17,805] Trial 18 finished with value: 60.62520199999999 and parameters: {'learning_rate': 0.0017439060303439048, 'n_steps': 896, 'gamma': 0.9400000000000001, 'ent_coef': 2.0902200151737643e-06, 'clip_range': 0.35, 'batch_size': 32, 'gae_lambda': 0.8696540391283154, 'n_epochs': 9}. Best is trial 15 with value: 87.38441270000001.\n",
      "[I 2025-02-06 01:49:31,274] Trial 19 finished with value: 43.213675 and parameters: {'learning_rate': 0.00044477783767088203, 'n_steps': 1280, 'gamma': 0.97, 'ent_coef': 1.2255736384358774e-07, 'clip_range': 0.30000000000000004, 'batch_size': 64, 'gae_lambda': 0.8303824111910341, 'n_epochs': 12}. Best is trial 15 with value: 87.38441270000001.\n",
      "[I 2025-02-06 01:51:02,895] Trial 20 finished with value: 78.00796935000002 and parameters: {'learning_rate': 0.004642307091786619, 'n_steps': 512, 'gamma': 0.99, 'ent_coef': 2.007779913630148e-06, 'clip_range': 0.25, 'batch_size': 64, 'gae_lambda': 0.8003425375098673, 'n_epochs': 20}. Best is trial 15 with value: 87.38441270000001.\n",
      "[I 2025-02-06 01:52:36,204] Trial 21 finished with value: 74.55131665 and parameters: {'learning_rate': 0.004810748777181154, 'n_steps': 384, 'gamma': 0.99, 'ent_coef': 1.2881811406209478e-06, 'clip_range': 0.25, 'batch_size': 64, 'gae_lambda': 0.8034795667621824, 'n_epochs': 20}. Best is trial 15 with value: 87.38441270000001.\n",
      "[I 2025-02-06 01:54:05,352] Trial 22 finished with value: 51.376279800000006 and parameters: {'learning_rate': 0.001363800258620752, 'n_steps': 768, 'gamma': 0.97, 'ent_coef': 1.188497553190462e-05, 'clip_range': 0.25, 'batch_size': 64, 'gae_lambda': 0.8211259811010598, 'n_epochs': 19}. Best is trial 15 with value: 87.38441270000001.\n",
      "[I 2025-02-06 01:55:26,269] Trial 23 finished with value: 84.87641155 and parameters: {'learning_rate': 0.006201176550996753, 'n_steps': 384, 'gamma': 0.99, 'ent_coef': 6.743763866148917e-08, 'clip_range': 0.35, 'batch_size': 64, 'gae_lambda': 0.800267030750988, 'n_epochs': 15}. Best is trial 15 with value: 87.38441270000001.\n",
      "[I 2025-02-06 01:56:45,408] Trial 24 finished with value: 78.76226640000002 and parameters: {'learning_rate': 0.006076768068462315, 'n_steps': 1152, 'gamma': 0.98, 'ent_coef': 4.784819044085899e-08, 'clip_range': 0.35, 'batch_size': 64, 'gae_lambda': 0.8386413140858048, 'n_epochs': 15}. Best is trial 15 with value: 87.38441270000001.\n",
      "[I 2025-02-06 01:58:04,728] Trial 25 finished with value: 69.74127404999999 and parameters: {'learning_rate': 0.0020685322738293807, 'n_steps': 384, 'gamma': 0.96, 'ent_coef': 4.7488315168833624e-08, 'clip_range': 0.4, 'batch_size': 64, 'gae_lambda': 0.8666333729445559, 'n_epochs': 14}. Best is trial 15 with value: 87.38441270000001.\n",
      "[I 2025-02-06 02:00:03,342] Trial 26 finished with value: 37.986094099999995 and parameters: {'learning_rate': 2.0463298225508555e-05, 'n_steps': 768, 'gamma': 0.99, 'ent_coef': 5.031539143389872e-07, 'clip_range': 0.30000000000000004, 'batch_size': 32, 'gae_lambda': 0.8135802844263665, 'n_epochs': 17}. Best is trial 15 with value: 87.38441270000001.\n",
      "[I 2025-02-06 02:01:06,629] Trial 27 finished with value: 55.01211270000001 and parameters: {'learning_rate': 0.003274608539687633, 'n_steps': 1024, 'gamma': 0.98, 'ent_coef': 1.155389213295241e-07, 'clip_range': 0.35, 'batch_size': 64, 'gae_lambda': 0.837260046453637, 'n_epochs': 8}. Best is trial 15 with value: 87.38441270000001.\n",
      "[I 2025-02-06 02:02:18,366] Trial 28 finished with value: 37.814607450000004 and parameters: {'learning_rate': 4.507532260578557e-05, 'n_steps': 384, 'gamma': 0.97, 'ent_coef': 0.00017633667251202307, 'clip_range': 0.30000000000000004, 'batch_size': 64, 'gae_lambda': 0.8162807230221903, 'n_epochs': 11}. Best is trial 15 with value: 87.38441270000001.\n",
      "[I 2025-02-06 02:04:10,846] Trial 29 finished with value: 74.87144794999999 and parameters: {'learning_rate': 0.0011740912185816714, 'n_steps': 896, 'gamma': 0.99, 'ent_coef': 1.3971011223482976e-05, 'clip_range': 0.35, 'batch_size': 32, 'gae_lambda': 0.8567826883562466, 'n_epochs': 16}. Best is trial 15 with value: 87.38441270000001.\n",
      "[I 2025-02-06 02:05:27,123] Trial 30 finished with value: 84.58385519999999 and parameters: {'learning_rate': 0.006446313288242675, 'n_steps': 768, 'gamma': 0.9500000000000001, 'ent_coef': 3.780873806800348e-06, 'clip_range': 0.35, 'batch_size': 64, 'gae_lambda': 0.9372393433288937, 'n_epochs': 14}. Best is trial 15 with value: 87.38441270000001.\n",
      "[I 2025-02-06 02:06:44,565] Trial 31 finished with value: 75.80037669999999 and parameters: {'learning_rate': 0.006594141576686287, 'n_steps': 640, 'gamma': 0.9500000000000001, 'ent_coef': 6.387513057699767e-06, 'clip_range': 0.35, 'batch_size': 64, 'gae_lambda': 0.9331224316072527, 'n_epochs': 14}. Best is trial 15 with value: 87.38441270000001.\n",
      "[I 2025-02-06 02:08:08,051] Trial 32 finished with value: 70.1064445 and parameters: {'learning_rate': 0.004334478154086068, 'n_steps': 768, 'gamma': 0.93, 'ent_coef': 7.476413501650549e-07, 'clip_range': 0.4, 'batch_size': 64, 'gae_lambda': 0.9461787038720962, 'n_epochs': 16}. Best is trial 15 with value: 87.38441270000001.\n",
      "[I 2025-02-06 02:09:20,234] Trial 33 finished with value: 68.31328239999999 and parameters: {'learning_rate': 0.006745501810362548, 'n_steps': 1024, 'gamma': 0.99, 'ent_coef': 2.994079202503498e-08, 'clip_range': 0.4, 'batch_size': 64, 'gae_lambda': 0.8913836476874493, 'n_epochs': 12}. Best is trial 15 with value: 87.38441270000001.\n",
      "[I 2025-02-06 02:10:45,210] Trial 34 finished with value: 55.98117889999999 and parameters: {'learning_rate': 0.0039005960939201417, 'n_steps': 512, 'gamma': 0.97, 'ent_coef': 1.6488795485609e-07, 'clip_range': 0.30000000000000004, 'batch_size': 64, 'gae_lambda': 0.911111549397248, 'n_epochs': 17}. Best is trial 15 with value: 87.38441270000001.\n",
      "[I 2025-02-06 02:12:12,194] Trial 35 finished with value: 57.5868019 and parameters: {'learning_rate': 0.00997006067089907, 'n_steps': 896, 'gamma': 0.98, 'ent_coef': 3.958774377813581e-06, 'clip_range': 0.35, 'batch_size': 64, 'gae_lambda': 0.9782277987543767, 'n_epochs': 18}. Best is trial 15 with value: 87.38441270000001.\n",
      "[I 2025-02-06 02:13:30,278] Trial 36 finished with value: 58.51956249999999 and parameters: {'learning_rate': 0.001605317319090854, 'n_steps': 1152, 'gamma': 0.93, 'ent_coef': 2.7893160859057898e-05, 'clip_range': 0.30000000000000004, 'batch_size': 64, 'gae_lambda': 0.9485363893286887, 'n_epochs': 14}. Best is trial 15 with value: 87.38441270000001.\n",
      "[I 2025-02-06 02:15:00,786] Trial 37 finished with value: 105.3122621 and parameters: {'learning_rate': 0.005966992377137779, 'n_steps': 256, 'gamma': 0.96, 'ent_coef': 8.605348972004322e-07, 'clip_range': 0.4, 'batch_size': 64, 'gae_lambda': 0.9141617527087093, 'n_epochs': 17}. Best is trial 37 with value: 105.3122621.\n",
      "[I 2025-02-06 02:16:30,342] Trial 38 finished with value: 99.39672765000002 and parameters: {'learning_rate': 0.002624398149402071, 'n_steps': 256, 'gamma': 0.96, 'ent_coef': 1.1029080556961558e-08, 'clip_range': 0.4, 'batch_size': 64, 'gae_lambda': 0.9112562679259752, 'n_epochs': 18}. Best is trial 37 with value: 105.3122621.\n",
      "[I 2025-02-06 02:17:59,061] Trial 39 finished with value: 71.26591134999998 and parameters: {'learning_rate': 0.000722871351821971, 'n_steps': 256, 'gamma': 0.96, 'ent_coef': 2.3909088686260797e-08, 'clip_range': 0.4, 'batch_size': 64, 'gae_lambda': 0.912267124026712, 'n_epochs': 18}. Best is trial 37 with value: 105.3122621.\n",
      "[I 2025-02-06 02:20:06,357] Trial 40 finished with value: 102.00685845000001 and parameters: {'learning_rate': 0.0004104638635170414, 'n_steps': 256, 'gamma': 0.96, 'ent_coef': 1.5048079294858276e-08, 'clip_range': 0.4, 'batch_size': 32, 'gae_lambda': 0.9015124167814523, 'n_epochs': 19}. Best is trial 37 with value: 105.3122621.\n",
      "[I 2025-02-06 02:22:13,795] Trial 41 finished with value: 92.3965618 and parameters: {'learning_rate': 0.00030954104107474206, 'n_steps': 256, 'gamma': 0.96, 'ent_coef': 1.1717384609766009e-08, 'clip_range': 0.4, 'batch_size': 32, 'gae_lambda': 0.922532864536754, 'n_epochs': 19}. Best is trial 37 with value: 105.3122621.\n",
      "[I 2025-02-06 02:24:23,248] Trial 42 finished with value: 108.88387600000001 and parameters: {'learning_rate': 0.0003144142843474734, 'n_steps': 128, 'gamma': 0.96, 'ent_coef': 1.1718809810098177e-08, 'clip_range': 0.4, 'batch_size': 32, 'gae_lambda': 0.9221658768394616, 'n_epochs': 19}. Best is trial 42 with value: 108.88387600000001.\n",
      "[I 2025-02-06 02:26:32,842] Trial 43 finished with value: 71.6688347 and parameters: {'learning_rate': 0.0002541618360601603, 'n_steps': 128, 'gamma': 0.96, 'ent_coef': 1.1104024630427171e-08, 'clip_range': 0.4, 'batch_size': 32, 'gae_lambda': 0.9214499007744393, 'n_epochs': 19}. Best is trial 42 with value: 108.88387600000001.\n",
      "[I 2025-02-06 02:28:39,704] Trial 44 finished with value: 117.2196115 and parameters: {'learning_rate': 0.00038895903894657874, 'n_steps': 256, 'gamma': 0.9500000000000001, 'ent_coef': 1.9799178339816272e-08, 'clip_range': 0.4, 'batch_size': 32, 'gae_lambda': 0.901928597603968, 'n_epochs': 19}. Best is trial 44 with value: 117.2196115.\n",
      "[I 2025-02-06 02:30:50,253] Trial 45 finished with value: 108.79122545 and parameters: {'learning_rate': 0.00012532630396866937, 'n_steps': 128, 'gamma': 0.9500000000000001, 'ent_coef': 2.4788208983131697e-08, 'clip_range': 0.4, 'batch_size': 32, 'gae_lambda': 0.9014771466117351, 'n_epochs': 19}. Best is trial 44 with value: 117.2196115.\n",
      "[I 2025-02-06 02:33:01,141] Trial 46 finished with value: 89.20493430000002 and parameters: {'learning_rate': 0.00014547423561753916, 'n_steps': 128, 'gamma': 0.9400000000000001, 'ent_coef': 2.2614427352367308e-08, 'clip_range': 0.4, 'batch_size': 32, 'gae_lambda': 0.8999419425055661, 'n_epochs': 19}. Best is trial 44 with value: 117.2196115.\n",
      "[I 2025-02-06 02:35:13,525] Trial 47 finished with value: 52.7722745 and parameters: {'learning_rate': 9.21565102571994e-05, 'n_steps': 128, 'gamma': 0.9500000000000001, 'ent_coef': 7.247454662099891e-08, 'clip_range': 0.4, 'batch_size': 32, 'gae_lambda': 0.8846909645417738, 'n_epochs': 20}. Best is trial 44 with value: 117.2196115.\n",
      "[I 2025-02-06 02:37:16,347] Trial 48 finished with value: 89.27991275 and parameters: {'learning_rate': 0.00041734348182746956, 'n_steps': 256, 'gamma': 0.9500000000000001, 'ent_coef': 2.545586056719467e-08, 'clip_range': 0.4, 'batch_size': 32, 'gae_lambda': 0.8984993735048067, 'n_epochs': 18}. Best is trial 44 with value: 117.2196115.\n",
      "[I 2025-02-06 02:39:15,942] Trial 49 finished with value: 66.12436499999998 and parameters: {'learning_rate': 0.00017474164182649027, 'n_steps': 512, 'gamma': 0.9400000000000001, 'ent_coef': 1.82759048751761e-07, 'clip_range': 0.4, 'batch_size': 32, 'gae_lambda': 0.9039284353461224, 'n_epochs': 17}. Best is trial 44 with value: 117.2196115.\n",
      "[I 2025-02-06 02:41:26,757] Trial 50 finished with value: 45.435851299999996 and parameters: {'learning_rate': 5.635383818434971e-05, 'n_steps': 128, 'gamma': 0.9500000000000001, 'ent_coef': 2.5636678261920225e-07, 'clip_range': 0.4, 'batch_size': 32, 'gae_lambda': 0.8817544308100482, 'n_epochs': 19}. Best is trial 44 with value: 117.2196115.\n",
      "[I 2025-02-06 02:43:31,797] Trial 51 finished with value: 80.65736715 and parameters: {'learning_rate': 0.000506643987973259, 'n_steps': 256, 'gamma': 0.96, 'ent_coef': 1.5882757390283936e-08, 'clip_range': 0.4, 'batch_size': 32, 'gae_lambda': 0.9237098361158984, 'n_epochs': 18}. Best is trial 44 with value: 117.2196115.\n",
      "[I 2025-02-06 02:45:31,479] Trial 52 finished with value: 72.24934010000001 and parameters: {'learning_rate': 0.000904719923493093, 'n_steps': 256, 'gamma': 0.96, 'ent_coef': 6.333804084633016e-08, 'clip_range': 0.4, 'batch_size': 32, 'gae_lambda': 0.9103188167995916, 'n_epochs': 17}. Best is trial 44 with value: 117.2196115.\n",
      "[I 2025-02-06 02:47:44,008] Trial 53 finished with value: 88.47496145 and parameters: {'learning_rate': 0.000248483761503393, 'n_steps': 384, 'gamma': 0.97, 'ent_coef': 1.1858381302565926e-08, 'clip_range': 0.4, 'batch_size': 32, 'gae_lambda': 0.8901587964983648, 'n_epochs': 20}. Best is trial 44 with value: 117.2196115.\n",
      "[I 2025-02-06 02:49:56,096] Trial 54 finished with value: 93.47480584999998 and parameters: {'learning_rate': 0.00012184497823935033, 'n_steps': 128, 'gamma': 0.9500000000000001, 'ent_coef': 3.390057072038305e-08, 'clip_range': 0.35, 'batch_size': 32, 'gae_lambda': 0.9050850658714532, 'n_epochs': 19}. Best is trial 44 with value: 117.2196115.\n",
      "[I 2025-02-06 02:52:01,750] Trial 55 finished with value: 60.4358168 and parameters: {'learning_rate': 0.000592145788245351, 'n_steps': 256, 'gamma': 0.9400000000000001, 'ent_coef': 9.088261401953053e-08, 'clip_range': 0.1, 'batch_size': 32, 'gae_lambda': 0.9178675006599867, 'n_epochs': 18}. Best is trial 44 with value: 117.2196115.\n",
      "[I 2025-02-06 02:54:12,343] Trial 56 finished with value: 58.47526435000001 and parameters: {'learning_rate': 0.0003610153423540268, 'n_steps': 384, 'gamma': 0.96, 'ent_coef': 1.8734501818624136e-08, 'clip_range': 0.15000000000000002, 'batch_size': 32, 'gae_lambda': 0.928385304202506, 'n_epochs': 20}. Best is trial 44 with value: 117.2196115.\n",
      "[I 2025-02-06 02:56:11,682] Trial 57 finished with value: 70.52987825000001 and parameters: {'learning_rate': 0.00019925487983298297, 'n_steps': 512, 'gamma': 0.97, 'ent_coef': 0.008632312588218161, 'clip_range': 0.35, 'batch_size': 32, 'gae_lambda': 0.8927155163622048, 'n_epochs': 17}. Best is trial 44 with value: 117.2196115.\n",
      "[I 2025-02-06 02:58:19,512] Trial 58 finished with value: 37.38755305 and parameters: {'learning_rate': 8.872000639699783e-05, 'n_steps': 1920, 'gamma': 0.9500000000000001, 'ent_coef': 4.2595205890177366e-08, 'clip_range': 0.4, 'batch_size': 32, 'gae_lambda': 0.879402674529387, 'n_epochs': 19}. Best is trial 44 with value: 117.2196115.\n",
      "[I 2025-02-06 02:59:28,665] Trial 59 finished with value: 51.04779165 and parameters: {'learning_rate': 2.7275705477872855e-05, 'n_steps': 128, 'gamma': 0.93, 'ent_coef': 1.0026906500919538e-08, 'clip_range': 0.4, 'batch_size': 32, 'gae_lambda': 0.9393624364698746, 'n_epochs': 5}. Best is trial 44 with value: 117.2196115.\n",
      "[I 2025-02-06 03:01:35,853] Trial 60 finished with value: 61.455248800000014 and parameters: {'learning_rate': 0.00011805932368001579, 'n_steps': 256, 'gamma': 0.96, 'ent_coef': 3.0385964029498943e-07, 'clip_range': 0.35, 'batch_size': 32, 'gae_lambda': 0.906670830670104, 'n_epochs': 18}. Best is trial 44 with value: 117.2196115.\n",
      "[I 2025-02-06 03:03:42,406] Trial 61 finished with value: 81.6137879 and parameters: {'learning_rate': 0.0001237357622380776, 'n_steps': 128, 'gamma': 0.9500000000000001, 'ent_coef': 3.39567928727128e-08, 'clip_range': 0.4, 'batch_size': 32, 'gae_lambda': 0.9038913972566585, 'n_epochs': 19}. Best is trial 44 with value: 117.2196115.\n",
      "[I 2025-02-06 03:05:52,513] Trial 62 finished with value: 96.6930347 and parameters: {'learning_rate': 0.00030357647167503996, 'n_steps': 128, 'gamma': 0.9500000000000001, 'ent_coef': 1.9442370207093208e-08, 'clip_range': 0.35, 'batch_size': 32, 'gae_lambda': 0.8962189349045733, 'n_epochs': 20}. Best is trial 44 with value: 117.2196115.\n",
      "[I 2025-02-06 03:08:01,375] Trial 63 finished with value: 67.95967965 and parameters: {'learning_rate': 0.00034154164717900337, 'n_steps': 384, 'gamma': 0.96, 'ent_coef': 1.7375956328855402e-08, 'clip_range': 0.4, 'batch_size': 32, 'gae_lambda': 0.8952520583919749, 'n_epochs': 20}. Best is trial 44 with value: 117.2196115.\n",
      "[I 2025-02-06 03:10:12,472] Trial 64 finished with value: 99.65818385 and parameters: {'learning_rate': 0.00022207276986040017, 'n_steps': 256, 'gamma': 0.9400000000000001, 'ent_coef': 1.0323395520182325e-07, 'clip_range': 0.4, 'batch_size': 32, 'gae_lambda': 0.9153679257414639, 'n_epochs': 20}. Best is trial 44 with value: 117.2196115.\n",
      "[I 2025-02-06 03:12:14,861] Trial 65 finished with value: 90.147685 and parameters: {'learning_rate': 0.00020210833102187844, 'n_steps': 256, 'gamma': 0.9400000000000001, 'ent_coef': 1.0735421834012577e-07, 'clip_range': 0.4, 'batch_size': 32, 'gae_lambda': 0.9175128338106137, 'n_epochs': 18}. Best is trial 44 with value: 117.2196115.\n",
      "[I 2025-02-06 03:14:08,180] Trial 66 finished with value: 74.51283839999999 and parameters: {'learning_rate': 0.0005184924084040772, 'n_steps': 512, 'gamma': 0.9400000000000001, 'ent_coef': 4.679832304714334e-08, 'clip_range': 0.4, 'batch_size': 32, 'gae_lambda': 0.928596955451983, 'n_epochs': 16}. Best is trial 44 with value: 117.2196115.\n",
      "[I 2025-02-06 03:16:19,272] Trial 67 finished with value: 73.07082745 and parameters: {'learning_rate': 0.0010171020384106783, 'n_steps': 384, 'gamma': 0.91, 'ent_coef': 6.85424688890476e-08, 'clip_range': 0.4, 'batch_size': 32, 'gae_lambda': 0.9141849009567856, 'n_epochs': 20}. Best is trial 44 with value: 117.2196115.\n",
      "[I 2025-02-06 03:18:28,549] Trial 68 finished with value: 56.270315700000005 and parameters: {'learning_rate': 6.426210631399277e-05, 'n_steps': 256, 'gamma': 0.97, 'ent_coef': 7.711670073550144e-07, 'clip_range': 0.35, 'batch_size': 32, 'gae_lambda': 0.951259527773145, 'n_epochs': 19}. Best is trial 44 with value: 117.2196115.\n",
      "[I 2025-02-06 03:20:28,416] Trial 69 finished with value: 52.277166150000006 and parameters: {'learning_rate': 0.00022152078645275164, 'n_steps': 1664, 'gamma': 0.93, 'ent_coef': 1.4197252454828222e-07, 'clip_range': 0.4, 'batch_size': 32, 'gae_lambda': 0.8747004966869194, 'n_epochs': 17}. Best is trial 44 with value: 117.2196115.\n",
      "[I 2025-02-06 03:22:31,126] Trial 70 finished with value: 83.13318255 and parameters: {'learning_rate': 0.0025735979051194524, 'n_steps': 640, 'gamma': 0.96, 'ent_coef': 3.3333252302114046e-08, 'clip_range': 0.35, 'batch_size': 32, 'gae_lambda': 0.8868521452159296, 'n_epochs': 18}. Best is trial 44 with value: 117.2196115.\n",
      "[I 2025-02-06 03:24:42,130] Trial 71 finished with value: 108.92187229999998 and parameters: {'learning_rate': 0.0002996503731976351, 'n_steps': 128, 'gamma': 0.9500000000000001, 'ent_coef': 1.8269864813183284e-08, 'clip_range': 0.4, 'batch_size': 32, 'gae_lambda': 0.8995699753001081, 'n_epochs': 20}. Best is trial 44 with value: 117.2196115.\n",
      "[I 2025-02-06 03:26:48,451] Trial 72 finished with value: 93.80049170000001 and parameters: {'learning_rate': 0.000267204038521812, 'n_steps': 128, 'gamma': 0.9500000000000001, 'ent_coef': 1.593375958990153e-08, 'clip_range': 0.4, 'batch_size': 32, 'gae_lambda': 0.9073367456438133, 'n_epochs': 19}. Best is trial 44 with value: 117.2196115.\n",
      "[I 2025-02-06 03:28:59,429] Trial 73 finished with value: 63.55793124999999 and parameters: {'learning_rate': 0.00039821071484645733, 'n_steps': 256, 'gamma': 0.96, 'ent_coef': 5.746195265310525e-08, 'clip_range': 0.4, 'batch_size': 32, 'gae_lambda': 0.9006629504997951, 'n_epochs': 20}. Best is trial 44 with value: 117.2196115.\n",
      "[I 2025-02-06 03:31:06,452] Trial 74 finished with value: 67.99369185 and parameters: {'learning_rate': 0.0004909930917544787, 'n_steps': 384, 'gamma': 0.9400000000000001, 'ent_coef': 2.6353409955654698e-08, 'clip_range': 0.4, 'batch_size': 32, 'gae_lambda': 0.9175406366875546, 'n_epochs': 19}. Best is trial 44 with value: 117.2196115.\n",
      "[I 2025-02-06 03:33:16,843] Trial 75 finished with value: 97.66462979999999 and parameters: {'learning_rate': 0.0007173470159868222, 'n_steps': 128, 'gamma': 0.9500000000000001, 'ent_coef': 1.0632701382525986e-08, 'clip_range': 0.4, 'batch_size': 32, 'gae_lambda': 0.9228652391461497, 'n_epochs': 20}. Best is trial 44 with value: 117.2196115.\n",
      "[I 2025-02-06 03:35:19,042] Trial 76 finished with value: 63.6277967 and parameters: {'learning_rate': 0.00010397522939907376, 'n_steps': 384, 'gamma': 0.96, 'ent_coef': 4.056463569942147e-08, 'clip_range': 0.4, 'batch_size': 32, 'gae_lambda': 0.9104780005936939, 'n_epochs': 18}. Best is trial 44 with value: 117.2196115.\n",
      "[I 2025-02-06 03:36:20,310] Trial 77 finished with value: 42.516918999999994 and parameters: {'learning_rate': 0.00015458650478493, 'n_steps': 256, 'gamma': 0.9500000000000001, 'ent_coef': 2.262892033783728e-07, 'clip_range': 0.2, 'batch_size': 64, 'gae_lambda': 0.9302883388582078, 'n_epochs': 6}. Best is trial 44 with value: 117.2196115.\n",
      "[I 2025-02-06 03:38:27,254] Trial 78 finished with value: 90.61246860000001 and parameters: {'learning_rate': 0.0001746375407692339, 'n_steps': 128, 'gamma': 0.96, 'ent_coef': 9.535423809418833e-08, 'clip_range': 0.35, 'batch_size': 32, 'gae_lambda': 0.9379050989713631, 'n_epochs': 19}. Best is trial 44 with value: 117.2196115.\n",
      "[I 2025-02-06 03:40:10,765] Trial 79 finished with value: 56.48861055000001 and parameters: {'learning_rate': 0.000280322033521168, 'n_steps': 1408, 'gamma': 0.9400000000000001, 'ent_coef': 1.7202310061044695e-08, 'clip_range': 0.4, 'batch_size': 64, 'gae_lambda': 0.9015672101825988, 'n_epochs': 20}. Best is trial 44 with value: 117.2196115.\n",
      "[I 2025-02-06 03:42:35,809] Trial 80 finished with value: 90.17050780000001 and parameters: {'learning_rate': 0.007909402309656918, 'n_steps': 256, 'gamma': 0.97, 'ent_coef': 1.0329076128854528e-06, 'clip_range': 0.4, 'batch_size': 32, 'gae_lambda': 0.8945145017079524, 'n_epochs': 19}. Best is trial 44 with value: 117.2196115.\n",
      "[I 2025-02-06 03:45:19,840] Trial 81 finished with value: 101.99619635 and parameters: {'learning_rate': 0.0006764984047906072, 'n_steps': 128, 'gamma': 0.9500000000000001, 'ent_coef': 1.2366834585407504e-08, 'clip_range': 0.4, 'batch_size': 32, 'gae_lambda': 0.9249071178756828, 'n_epochs': 20}. Best is trial 44 with value: 117.2196115.\n",
      "[I 2025-02-06 03:48:16,009] Trial 82 finished with value: 91.16510085 and parameters: {'learning_rate': 0.002179265108120773, 'n_steps': 128, 'gamma': 0.9500000000000001, 'ent_coef': 2.4270335566030415e-08, 'clip_range': 0.4, 'batch_size': 32, 'gae_lambda': 0.925855639838703, 'n_epochs': 20}. Best is trial 44 with value: 117.2196115.\n",
      "[I 2025-02-06 03:50:20,994] Trial 83 finished with value: 101.09930135 and parameters: {'learning_rate': 0.0008575089788915769, 'n_steps': 256, 'gamma': 0.9500000000000001, 'ent_coef': 1.6091863432247562e-08, 'clip_range': 0.4, 'batch_size': 32, 'gae_lambda': 0.9196433706709076, 'n_epochs': 18}. Best is trial 44 with value: 117.2196115.\n",
      "[I 2025-02-06 03:52:29,476] Trial 84 finished with value: 86.49804555 and parameters: {'learning_rate': 0.0006042896605259344, 'n_steps': 128, 'gamma': 0.9400000000000001, 'ent_coef': 0.00015420987341435648, 'clip_range': 0.4, 'batch_size': 32, 'gae_lambda': 0.9340578058859054, 'n_epochs': 19}. Best is trial 44 with value: 117.2196115.\n",
      "[I 2025-02-06 03:54:41,681] Trial 85 finished with value: 84.67024639999998 and parameters: {'learning_rate': 0.0008443717776412252, 'n_steps': 256, 'gamma': 0.9500000000000001, 'ent_coef': 5.5929154237893015e-08, 'clip_range': 0.35, 'batch_size': 32, 'gae_lambda': 0.9201736546318079, 'n_epochs': 20}. Best is trial 44 with value: 117.2196115.\n",
      "[I 2025-02-06 03:56:46,081] Trial 86 finished with value: 101.84677570000001 and parameters: {'learning_rate': 0.000376164036310092, 'n_steps': 384, 'gamma': 0.9500000000000001, 'ent_coef': 2.876530962784151e-08, 'clip_range': 0.4, 'batch_size': 32, 'gae_lambda': 0.9448419291654092, 'n_epochs': 18}. Best is trial 44 with value: 117.2196115.\n",
      "[I 2025-02-06 03:58:40,764] Trial 87 finished with value: 82.77576175 and parameters: {'learning_rate': 0.0011510686665158504, 'n_steps': 384, 'gamma': 0.9500000000000001, 'ent_coef': 1.5392426095784693e-08, 'clip_range': 0.4, 'batch_size': 32, 'gae_lambda': 0.9465882579127699, 'n_epochs': 16}. Best is trial 44 with value: 117.2196115.\n",
      "[I 2025-02-06 04:00:42,455] Trial 88 finished with value: 92.11787964999999 and parameters: {'learning_rate': 0.0006848376440951437, 'n_steps': 128, 'gamma': 0.9500000000000001, 'ent_coef': 2.8072865697340463e-08, 'clip_range': 0.35, 'batch_size': 32, 'gae_lambda': 0.9566542381870117, 'n_epochs': 17}. Best is trial 44 with value: 117.2196115.\n",
      "[I 2025-02-06 04:02:45,380] Trial 89 finished with value: 92.3699712 and parameters: {'learning_rate': 0.00041502853808404375, 'n_steps': 512, 'gamma': 0.96, 'ent_coef': 3.825166968858936e-08, 'clip_range': 0.4, 'batch_size': 32, 'gae_lambda': 0.9625877676221414, 'n_epochs': 18}. Best is trial 44 with value: 117.2196115.\n",
      "[I 2025-02-06 04:04:46,685] Trial 90 finished with value: 96.65215875 and parameters: {'learning_rate': 0.0015400366655081315, 'n_steps': 128, 'gamma': 0.9500000000000001, 'ent_coef': 2.164555426350884e-08, 'clip_range': 0.4, 'batch_size': 32, 'gae_lambda': 0.9402643014177487, 'n_epochs': 17}. Best is trial 44 with value: 117.2196115.\n",
      "[I 2025-02-06 04:06:54,093] Trial 91 finished with value: 79.35661595 and parameters: {'learning_rate': 0.00034639455358621976, 'n_steps': 256, 'gamma': 0.9400000000000001, 'ent_coef': 9.199750871445595e-08, 'clip_range': 0.4, 'batch_size': 32, 'gae_lambda': 0.9151659965493767, 'n_epochs': 19}. Best is trial 44 with value: 117.2196115.\n",
      "[I 2025-02-06 04:08:57,461] Trial 92 finished with value: 62.522188099999994 and parameters: {'learning_rate': 0.0005484731392280222, 'n_steps': 384, 'gamma': 0.9400000000000001, 'ent_coef': 1.3391287118962763e-08, 'clip_range': 0.4, 'batch_size': 32, 'gae_lambda': 0.907443693178309, 'n_epochs': 18}. Best is trial 44 with value: 117.2196115.\n",
      "[I 2025-02-06 04:11:09,735] Trial 93 finished with value: 108.27505295 and parameters: {'learning_rate': 0.0004453719773799175, 'n_steps': 256, 'gamma': 0.9500000000000001, 'ent_coef': 4.1392687264973217e-07, 'clip_range': 0.4, 'batch_size': 32, 'gae_lambda': 0.9321269149552456, 'n_epochs': 20}. Best is trial 44 with value: 117.2196115.\n",
      "[I 2025-02-06 04:13:17,748] Trial 94 finished with value: 90.37276829999999 and parameters: {'learning_rate': 0.0004625059927959104, 'n_steps': 256, 'gamma': 0.9500000000000001, 'ent_coef': 1.9673643270960722e-05, 'clip_range': 0.4, 'batch_size': 32, 'gae_lambda': 0.9435971535708669, 'n_epochs': 19}. Best is trial 44 with value: 117.2196115.\n",
      "[I 2025-02-06 04:15:30,359] Trial 95 finished with value: 89.85934499999999 and parameters: {'learning_rate': 0.0006312002974044039, 'n_steps': 128, 'gamma': 0.96, 'ent_coef': 6.144486092712011e-06, 'clip_range': 0.4, 'batch_size': 32, 'gae_lambda': 0.9334789130524023, 'n_epochs': 20}. Best is trial 44 with value: 117.2196115.\n",
      "[I 2025-02-06 04:17:38,760] Trial 96 finished with value: 73.29978595 and parameters: {'learning_rate': 0.0004006239152946408, 'n_steps': 384, 'gamma': 0.9500000000000001, 'ent_coef': 1.7840723616906688e-06, 'clip_range': 0.4, 'batch_size': 32, 'gae_lambda': 0.9258284064334795, 'n_epochs': 19}. Best is trial 44 with value: 117.2196115.\n",
      "[I 2025-02-06 04:19:41,754] Trial 97 finished with value: 86.2664927 and parameters: {'learning_rate': 0.0008143672452132917, 'n_steps': 128, 'gamma': 0.96, 'ent_coef': 0.0012792053369219344, 'clip_range': 0.15000000000000002, 'batch_size': 32, 'gae_lambda': 0.9344609819596148, 'n_epochs': 18}. Best is trial 44 with value: 117.2196115.\n",
      "[I 2025-02-06 04:21:49,225] Trial 98 finished with value: 76.6878104 and parameters: {'learning_rate': 0.00032533716838629935, 'n_steps': 256, 'gamma': 0.9500000000000001, 'ent_coef': 1.055561517793977e-08, 'clip_range': 0.35, 'batch_size': 32, 'gae_lambda': 0.8880077157373413, 'n_epochs': 19}. Best is trial 44 with value: 117.2196115.\n",
      "[I 2025-02-06 04:23:59,671] Trial 99 finished with value: 78.78820170000002 and parameters: {'learning_rate': 0.0003713006199854309, 'n_steps': 384, 'gamma': 0.97, 'ent_coef': 7.15296512967885e-08, 'clip_range': 0.4, 'batch_size': 32, 'gae_lambda': 0.9266205007742998, 'n_epochs': 20}. Best is trial 44 with value: 117.2196115.\n",
      "[I 2025-02-06 04:26:00,931] Trial 100 finished with value: 100.86589814999999 and parameters: {'learning_rate': 0.00022463854635308282, 'n_steps': 512, 'gamma': 0.96, 'ent_coef': 4.057933901838618e-07, 'clip_range': 0.4, 'batch_size': 32, 'gae_lambda': 0.952279764254808, 'n_epochs': 18}. Best is trial 44 with value: 117.2196115.\n",
      "[I 2025-02-06 04:28:02,509] Trial 101 finished with value: 47.29514725 and parameters: {'learning_rate': 0.00028333904122305635, 'n_steps': 512, 'gamma': 0.96, 'ent_coef': 3.1812885542667525e-07, 'clip_range': 0.4, 'batch_size': 32, 'gae_lambda': 0.9654100844847886, 'n_epochs': 18}. Best is trial 44 with value: 117.2196115.\n",
      "[I 2025-02-06 04:30:07,056] Trial 102 finished with value: 91.5101018 and parameters: {'learning_rate': 0.00022762777506648755, 'n_steps': 256, 'gamma': 0.9500000000000001, 'ent_coef': 6.678641681625344e-07, 'clip_range': 0.4, 'batch_size': 32, 'gae_lambda': 0.9427051893004289, 'n_epochs': 18}. Best is trial 44 with value: 117.2196115.\n",
      "[I 2025-02-06 04:32:16,089] Trial 103 finished with value: 69.2232498 and parameters: {'learning_rate': 0.0001889029836908961, 'n_steps': 256, 'gamma': 0.96, 'ent_coef': 1.9756761465214545e-08, 'clip_range': 0.4, 'batch_size': 32, 'gae_lambda': 0.9568107469456428, 'n_epochs': 19}. Best is trial 44 with value: 117.2196115.\n",
      "[I 2025-02-06 04:34:15,416] Trial 104 finished with value: 96.85615779999998 and parameters: {'learning_rate': 0.0004714533698929592, 'n_steps': 128, 'gamma': 0.9500000000000001, 'ent_coef': 3.493779078539069e-07, 'clip_range': 0.4, 'batch_size': 32, 'gae_lambda': 0.9524936446950016, 'n_epochs': 17}. Best is trial 44 with value: 117.2196115.\n",
      "[I 2025-02-06 04:36:27,590] Trial 105 finished with value: 95.2569176 and parameters: {'learning_rate': 0.0001506029670215819, 'n_steps': 384, 'gamma': 0.96, 'ent_coef': 3.432097807411194e-08, 'clip_range': 0.4, 'batch_size': 32, 'gae_lambda': 0.9202783649727843, 'n_epochs': 20}. Best is trial 44 with value: 117.2196115.\n",
      "[I 2025-02-06 04:38:33,720] Trial 106 finished with value: 77.22283409999999 and parameters: {'learning_rate': 0.0005582536336962988, 'n_steps': 640, 'gamma': 0.9500000000000001, 'ent_coef': 1.4607749690067925e-08, 'clip_range': 0.4, 'batch_size': 32, 'gae_lambda': 0.9030075754043572, 'n_epochs': 19}. Best is trial 44 with value: 117.2196115.\n",
      "[I 2025-02-06 04:40:37,975] Trial 107 finished with value: 90.1053361 and parameters: {'learning_rate': 0.00025959495193247423, 'n_steps': 128, 'gamma': 0.96, 'ent_coef': 1.7431554022257446e-07, 'clip_range': 0.4, 'batch_size': 32, 'gae_lambda': 0.9088568550565029, 'n_epochs': 18}. Best is trial 44 with value: 117.2196115.\n",
      "[I 2025-02-06 04:42:03,409] Trial 108 finished with value: 74.0010996 and parameters: {'learning_rate': 0.00030873084061273047, 'n_steps': 256, 'gamma': 0.97, 'ent_coef': 4.644908682642018e-08, 'clip_range': 0.4, 'batch_size': 32, 'gae_lambda': 0.8975887020807725, 'n_epochs': 9}. Best is trial 44 with value: 117.2196115.\n",
      "[I 2025-02-06 04:44:16,356] Trial 109 finished with value: 106.48468070000001 and parameters: {'learning_rate': 0.0004600952305879725, 'n_steps': 384, 'gamma': 0.9500000000000001, 'ent_coef': 2.7874217968767042e-08, 'clip_range': 0.35, 'batch_size': 32, 'gae_lambda': 0.9126315881773287, 'n_epochs': 20}. Best is trial 44 with value: 117.2196115.\n",
      "[I 2025-02-06 04:46:28,750] Trial 110 finished with value: 93.56105389999999 and parameters: {'learning_rate': 0.0011058076994118766, 'n_steps': 128, 'gamma': 0.9500000000000001, 'ent_coef': 4.6617881548965056e-05, 'clip_range': 0.35, 'batch_size': 32, 'gae_lambda': 0.9137793931448565, 'n_epochs': 20}. Best is trial 44 with value: 117.2196115.\n",
      "[I 2025-02-06 04:48:37,401] Trial 111 finished with value: 89.1009533 and parameters: {'learning_rate': 0.0004436581412528353, 'n_steps': 512, 'gamma': 0.9500000000000001, 'ent_coef': 2.5302249649839442e-08, 'clip_range': 0.4, 'batch_size': 32, 'gae_lambda': 0.9309880462685617, 'n_epochs': 19}. Best is trial 44 with value: 117.2196115.\n",
      "[I 2025-02-06 04:50:48,934] Trial 112 finished with value: 88.34946590000001 and parameters: {'learning_rate': 0.00037114656677959015, 'n_steps': 384, 'gamma': 0.9500000000000001, 'ent_coef': 1.4419666390204528e-08, 'clip_range': 0.4, 'batch_size': 32, 'gae_lambda': 0.9192574795425821, 'n_epochs': 20}. Best is trial 44 with value: 117.2196115.\n",
      "[I 2025-02-06 04:52:57,139] Trial 113 finished with value: 85.04235019999999 and parameters: {'learning_rate': 0.0009554061370283805, 'n_steps': 256, 'gamma': 0.96, 'ent_coef': 3.4445380048196725e-06, 'clip_range': 0.4, 'batch_size': 32, 'gae_lambda': 0.8916867540869384, 'n_epochs': 19}. Best is trial 44 with value: 117.2196115.\n",
      "[I 2025-02-06 04:55:09,413] Trial 114 finished with value: 95.0788697 and parameters: {'learning_rate': 0.0007304882973992639, 'n_steps': 256, 'gamma': 0.9500000000000001, 'ent_coef': 4.1578965505369364e-07, 'clip_range': 0.35, 'batch_size': 32, 'gae_lambda': 0.9066696085744473, 'n_epochs': 20}. Best is trial 44 with value: 117.2196115.\n",
      "[I 2025-02-06 04:57:17,171] Trial 115 finished with value: 109.39160419999999 and parameters: {'learning_rate': 0.00021559985321898602, 'n_steps': 384, 'gamma': 0.96, 'ent_coef': 2.094047263554965e-08, 'clip_range': 0.4, 'batch_size': 32, 'gae_lambda': 0.9121903316991575, 'n_epochs': 19}. Best is trial 44 with value: 117.2196115.\n",
      "[I 2025-02-06 04:59:29,535] Trial 116 finished with value: 55.90007295 and parameters: {'learning_rate': 0.0003334765355087073, 'n_steps': 384, 'gamma': 0.9400000000000001, 'ent_coef': 2.0967691723892975e-08, 'clip_range': 0.25, 'batch_size': 32, 'gae_lambda': 0.9119919901900866, 'n_epochs': 20}. Best is trial 44 with value: 117.2196115.\n",
      "[I 2025-02-06 05:01:01,157] Trial 117 finished with value: 104.96329519999999 and parameters: {'learning_rate': 0.001307957319350595, 'n_steps': 128, 'gamma': 0.9500000000000001, 'ent_coef': 3.0739531800930885e-08, 'clip_range': 0.4, 'batch_size': 64, 'gae_lambda': 0.8997187810078258, 'n_epochs': 19}. Best is trial 44 with value: 117.2196115.\n",
      "[I 2025-02-06 05:02:33,203] Trial 118 finished with value: 88.0196008 and parameters: {'learning_rate': 0.005065366473001775, 'n_steps': 128, 'gamma': 0.96, 'ent_coef': 4.929441270049086e-08, 'clip_range': 0.4, 'batch_size': 64, 'gae_lambda': 0.8995455828883812, 'n_epochs': 19}. Best is trial 44 with value: 117.2196115.\n",
      "[I 2025-02-06 05:04:05,909] Trial 119 finished with value: 73.14660354999998 and parameters: {'learning_rate': 0.00013398958349042063, 'n_steps': 128, 'gamma': 0.9500000000000001, 'ent_coef': 3.0220656555688454e-08, 'clip_range': 0.35, 'batch_size': 64, 'gae_lambda': 0.9011735379805418, 'n_epochs': 19}. Best is trial 44 with value: 117.2196115.\n",
      "[I 2025-02-06 05:05:39,019] Trial 120 finished with value: 65.7479599 and parameters: {'learning_rate': 0.0005309285671877661, 'n_steps': 256, 'gamma': 0.97, 'ent_coef': 6.109455598398473e-08, 'clip_range': 0.2, 'batch_size': 64, 'gae_lambda': 0.8799187596719185, 'n_epochs': 20}. Best is trial 44 with value: 117.2196115.\n",
      "[I 2025-02-06 05:07:11,606] Trial 121 finished with value: 103.19279209999999 and parameters: {'learning_rate': 0.0006649560079030009, 'n_steps': 256, 'gamma': 0.9500000000000001, 'ent_coef': 1.8280899020734454e-08, 'clip_range': 0.4, 'batch_size': 64, 'gae_lambda': 0.9037902874704501, 'n_epochs': 19}. Best is trial 44 with value: 117.2196115.\n",
      "[I 2025-02-06 05:08:42,596] Trial 122 finished with value: 101.9173108 and parameters: {'learning_rate': 0.0014157914681057916, 'n_steps': 128, 'gamma': 0.9500000000000001, 'ent_coef': 3.776259975873076e-08, 'clip_range': 0.4, 'batch_size': 64, 'gae_lambda': 0.9045490015577583, 'n_epochs': 19}. Best is trial 44 with value: 117.2196115.\n",
      "[I 2025-02-06 05:10:17,848] Trial 123 finished with value: 100.92102344999999 and parameters: {'learning_rate': 0.002075685403370791, 'n_steps': 128, 'gamma': 0.9500000000000001, 'ent_coef': 3.638444403131499e-08, 'clip_range': 0.4, 'batch_size': 64, 'gae_lambda': 0.9042117503559293, 'n_epochs': 20}. Best is trial 44 with value: 117.2196115.\n",
      "[I 2025-02-06 05:11:51,733] Trial 124 finished with value: 100.45486704999999 and parameters: {'learning_rate': 0.0013937356161664, 'n_steps': 128, 'gamma': 0.9400000000000001, 'ent_coef': 1.0153789794596623e-08, 'clip_range': 0.4, 'batch_size': 64, 'gae_lambda': 0.8950749754740461, 'n_epochs': 19}. Best is trial 44 with value: 117.2196115.\n",
      "[I 2025-02-06 05:13:25,418] Trial 125 finished with value: 95.71752585 and parameters: {'learning_rate': 0.0018251296677007308, 'n_steps': 256, 'gamma': 0.96, 'ent_coef': 2.247947707725214e-08, 'clip_range': 0.4, 'batch_size': 64, 'gae_lambda': 0.9089498567289109, 'n_epochs': 20}. Best is trial 44 with value: 117.2196115.\n",
      "[I 2025-02-06 05:14:57,405] Trial 126 finished with value: 70.73746114999999 and parameters: {'learning_rate': 0.002904887205816003, 'n_steps': 128, 'gamma': 0.9500000000000001, 'ent_coef': 1.3701513411634902e-07, 'clip_range': 0.4, 'batch_size': 64, 'gae_lambda': 0.8843087791570625, 'n_epochs': 19}. Best is trial 44 with value: 117.2196115.\n",
      "[I 2025-02-06 05:16:32,472] Trial 127 finished with value: 102.81074680000002 and parameters: {'learning_rate': 0.0013170344514404969, 'n_steps': 128, 'gamma': 0.9500000000000001, 'ent_coef': 7.559565731847069e-08, 'clip_range': 0.4, 'batch_size': 64, 'gae_lambda': 0.9127741907321486, 'n_epochs': 20}. Best is trial 44 with value: 117.2196115.\n",
      "[I 2025-02-06 05:18:06,588] Trial 128 finished with value: 105.74900565 and parameters: {'learning_rate': 0.0006039494463063472, 'n_steps': 256, 'gamma': 0.96, 'ent_coef': 7.812351513282838e-08, 'clip_range': 0.4, 'batch_size': 64, 'gae_lambda': 0.9153019399192176, 'n_epochs': 20}. Best is trial 44 with value: 117.2196115.\n",
      "[I 2025-02-06 05:19:39,686] Trial 129 finished with value: 106.33825225000001 and parameters: {'learning_rate': 0.000459504087530865, 'n_steps': 256, 'gamma': 0.96, 'ent_coef': 8.327266250154832e-08, 'clip_range': 0.4, 'batch_size': 64, 'gae_lambda': 0.9144616578352299, 'n_epochs': 20}. Best is trial 44 with value: 117.2196115.\n",
      "[I 2025-02-06 05:21:14,405] Trial 130 finished with value: 82.87186495 and parameters: {'learning_rate': 0.000578800791420529, 'n_steps': 256, 'gamma': 0.96, 'ent_coef': 1.1191729486491082e-07, 'clip_range': 0.4, 'batch_size': 64, 'gae_lambda': 0.9151582004808315, 'n_epochs': 20}. Best is trial 44 with value: 117.2196115.\n",
      "[I 2025-02-06 05:22:48,339] Trial 131 finished with value: 78.3462621 and parameters: {'learning_rate': 0.0004356281495597635, 'n_steps': 256, 'gamma': 0.96, 'ent_coef': 8.30708525323826e-08, 'clip_range': 0.4, 'batch_size': 64, 'gae_lambda': 0.911210119323975, 'n_epochs': 20}. Best is trial 44 with value: 117.2196115.\n",
      "[I 2025-02-06 05:24:21,764] Trial 132 finished with value: 85.30441175 and parameters: {'learning_rate': 0.0002843372209363384, 'n_steps': 256, 'gamma': 0.96, 'ent_coef': 2.417578159068543e-07, 'clip_range': 0.4, 'batch_size': 64, 'gae_lambda': 0.898004066368919, 'n_epochs': 20}. Best is trial 44 with value: 117.2196115.\n",
      "[I 2025-02-06 05:25:52,265] Trial 133 finished with value: 88.61562024999999 and parameters: {'learning_rate': 0.003675519543961482, 'n_steps': 384, 'gamma': 0.96, 'ent_coef': 6.520842827647543e-08, 'clip_range': 0.4, 'batch_size': 64, 'gae_lambda': 0.9166690796856612, 'n_epochs': 19}. Best is trial 44 with value: 117.2196115.\n",
      "[I 2025-02-06 05:27:25,848] Trial 134 finished with value: 85.60440845 and parameters: {'learning_rate': 0.00048069225264998395, 'n_steps': 256, 'gamma': 0.96, 'ent_coef': 5.3955193669183584e-08, 'clip_range': 0.4, 'batch_size': 64, 'gae_lambda': 0.9074911902668088, 'n_epochs': 20}. Best is trial 44 with value: 117.2196115.\n",
      "[I 2025-02-06 05:29:00,412] Trial 135 finished with value: 90.07958105 and parameters: {'learning_rate': 0.0006113266083774986, 'n_steps': 128, 'gamma': 0.97, 'ent_coef': 1.6509504272404563e-08, 'clip_range': 0.4, 'batch_size': 64, 'gae_lambda': 0.911679975435839, 'n_epochs': 19}. Best is trial 44 with value: 117.2196115.\n",
      "[I 2025-02-06 05:30:18,961] Trial 136 finished with value: 87.309453 and parameters: {'learning_rate': 0.00024608251146303545, 'n_steps': 256, 'gamma': 0.96, 'ent_coef': 2.684525375530007e-08, 'clip_range': 0.4, 'batch_size': 64, 'gae_lambda': 0.9230769626209813, 'n_epochs': 13}. Best is trial 44 with value: 117.2196115.\n",
      "[I 2025-02-06 05:31:52,717] Trial 137 finished with value: 75.46100064999999 and parameters: {'learning_rate': 0.0007647144084419942, 'n_steps': 384, 'gamma': 0.9500000000000001, 'ent_coef': 1.8669068651807938e-07, 'clip_range': 0.4, 'batch_size': 64, 'gae_lambda': 0.9022301735970836, 'n_epochs': 20}. Best is trial 44 with value: 117.2196115.\n",
      "[I 2025-02-06 05:33:28,347] Trial 138 finished with value: 80.43663384999999 and parameters: {'learning_rate': 0.0003418688800419064, 'n_steps': 128, 'gamma': 0.9500000000000001, 'ent_coef': 1.9709637416080212e-08, 'clip_range': 0.4, 'batch_size': 64, 'gae_lambda': 0.8923155912966729, 'n_epochs': 20}. Best is trial 44 with value: 117.2196115.\n",
      "[I 2025-02-06 05:35:01,082] Trial 139 finished with value: 74.4969051 and parameters: {'learning_rate': 0.00044111840281498314, 'n_steps': 256, 'gamma': 0.96, 'ent_coef': 4.912535292445757e-08, 'clip_range': 0.35, 'batch_size': 64, 'gae_lambda': 0.9144881349911606, 'n_epochs': 19}. Best is trial 44 with value: 117.2196115.\n",
      "[I 2025-02-06 05:36:32,397] Trial 140 finished with value: 67.08215525 and parameters: {'learning_rate': 7.235522253810903e-05, 'n_steps': 128, 'gamma': 0.97, 'ent_coef': 8.087605539586813e-08, 'clip_range': 0.4, 'batch_size': 64, 'gae_lambda': 0.8960636837663808, 'n_epochs': 19}. Best is trial 44 with value: 117.2196115.\n",
      "[I 2025-02-06 05:38:05,589] Trial 141 finished with value: 84.51586485 and parameters: {'learning_rate': 0.0006442353239816339, 'n_steps': 128, 'gamma': 0.9500000000000001, 'ent_coef': 1.3001389178311612e-08, 'clip_range': 0.4, 'batch_size': 64, 'gae_lambda': 0.9231024734522022, 'n_epochs': 20}. Best is trial 44 with value: 117.2196115.\n",
      "[I 2025-02-06 05:39:40,441] Trial 142 finished with value: 87.66871345000001 and parameters: {'learning_rate': 0.0005227366941476115, 'n_steps': 128, 'gamma': 0.9500000000000001, 'ent_coef': 1.0089201861620269e-08, 'clip_range': 0.4, 'batch_size': 64, 'gae_lambda': 0.9271070201884617, 'n_epochs': 20}. Best is trial 44 with value: 117.2196115.\n",
      "[I 2025-02-06 05:41:14,292] Trial 143 finished with value: 92.74020599999999 and parameters: {'learning_rate': 0.00039640003641425886, 'n_steps': 256, 'gamma': 0.9500000000000001, 'ent_coef': 3.4076944880180215e-08, 'clip_range': 0.4, 'batch_size': 64, 'gae_lambda': 0.9060141743045481, 'n_epochs': 20}. Best is trial 44 with value: 117.2196115.\n",
      "[I 2025-02-06 05:42:51,077] Trial 144 finished with value: 62.5077055 and parameters: {'learning_rate': 0.0007097190417760296, 'n_steps': 1920, 'gamma': 0.96, 'ent_coef': 2.0322342192218932e-08, 'clip_range': 0.4, 'batch_size': 64, 'gae_lambda': 0.9181694054362033, 'n_epochs': 19}. Best is trial 44 with value: 117.2196115.\n",
      "[I 2025-02-06 05:44:23,766] Trial 145 finished with value: 92.34403020000002 and parameters: {'learning_rate': 0.0009034389971973043, 'n_steps': 128, 'gamma': 0.9, 'ent_coef': 1.4776205294221297e-08, 'clip_range': 0.4, 'batch_size': 64, 'gae_lambda': 0.9104493972365703, 'n_epochs': 20}. Best is trial 44 with value: 117.2196115.\n",
      "[I 2025-02-06 05:45:54,735] Trial 146 finished with value: 81.55531535 and parameters: {'learning_rate': 0.00020153194475275201, 'n_steps': 384, 'gamma': 0.9500000000000001, 'ent_coef': 2.587758467163999e-08, 'clip_range': 0.4, 'batch_size': 64, 'gae_lambda': 0.8892093857091776, 'n_epochs': 19}. Best is trial 44 with value: 117.2196115.\n",
      "[I 2025-02-06 05:47:26,279] Trial 147 finished with value: 80.43167545 and parameters: {'learning_rate': 0.00010808403723468021, 'n_steps': 256, 'gamma': 0.9400000000000001, 'ent_coef': 0.02513405783749942, 'clip_range': 0.30000000000000004, 'batch_size': 64, 'gae_lambda': 0.9011625107962047, 'n_epochs': 19}. Best is trial 44 with value: 117.2196115.\n",
      "[I 2025-02-06 05:49:38,975] Trial 148 finished with value: 87.60730945 and parameters: {'learning_rate': 0.0012693779291282968, 'n_steps': 128, 'gamma': 0.9500000000000001, 'ent_coef': 4.456862334850083e-08, 'clip_range': 0.4, 'batch_size': 32, 'gae_lambda': 0.9217328883334142, 'n_epochs': 20}. Best is trial 44 with value: 117.2196115.\n",
      "[I 2025-02-06 05:51:50,825] Trial 149 finished with value: 99.5758377 and parameters: {'learning_rate': 0.00031380497985516786, 'n_steps': 256, 'gamma': 0.96, 'ent_coef': 1.352196894613668e-08, 'clip_range': 0.4, 'batch_size': 32, 'gae_lambda': 0.9130520008485944, 'n_epochs': 20}. Best is trial 44 with value: 117.2196115.\n",
      "[I 2025-02-06 05:53:43,452] Trial 150 finished with value: 89.38529685 and parameters: {'learning_rate': 0.001013765447821538, 'n_steps': 128, 'gamma': 0.96, 'ent_coef': 1.8354284795594054e-08, 'clip_range': 0.4, 'batch_size': 32, 'gae_lambda': 0.9080065794348212, 'n_epochs': 15}. Best is trial 44 with value: 117.2196115.\n",
      "[I 2025-02-06 05:55:14,725] Trial 151 finished with value: 108.0634077 and parameters: {'learning_rate': 0.0014775674858495973, 'n_steps': 128, 'gamma': 0.9500000000000001, 'ent_coef': 3.527054057866144e-08, 'clip_range': 0.4, 'batch_size': 64, 'gae_lambda': 0.905606159333677, 'n_epochs': 19}. Best is trial 44 with value: 117.2196115.\n",
      "[I 2025-02-06 05:56:46,338] Trial 152 finished with value: 86.71087644999999 and parameters: {'learning_rate': 0.0019245491970370046, 'n_steps': 128, 'gamma': 0.9500000000000001, 'ent_coef': 3.104107507793392e-08, 'clip_range': 0.4, 'batch_size': 64, 'gae_lambda': 0.9040111184254206, 'n_epochs': 19}. Best is trial 44 with value: 117.2196115.\n",
      "[I 2025-02-06 05:58:16,209] Trial 153 finished with value: 84.2595355 and parameters: {'learning_rate': 0.0004978921993626672, 'n_steps': 128, 'gamma': 0.9500000000000001, 'ent_coef': 1.1762468709035214e-07, 'clip_range': 0.4, 'batch_size': 64, 'gae_lambda': 0.9157129183395066, 'n_epochs': 18}. Best is trial 44 with value: 117.2196115.\n",
      "[I 2025-02-06 05:59:28,562] Trial 154 finished with value: 72.95403610000001 and parameters: {'learning_rate': 0.0011585526646756214, 'n_steps': 256, 'gamma': 0.9500000000000001, 'ent_coef': 2.4049295052549733e-08, 'clip_range': 0.4, 'batch_size': 64, 'gae_lambda': 0.8988224971105403, 'n_epochs': 11}. Best is trial 44 with value: 117.2196115.\n",
      "[I 2025-02-06 06:00:59,204] Trial 155 finished with value: 61.0453611 and parameters: {'learning_rate': 4.79203612206793e-05, 'n_steps': 256, 'gamma': 0.91, 'ent_coef': 4.327823216252233e-08, 'clip_range': 0.4, 'batch_size': 64, 'gae_lambda': 0.9190144000717051, 'n_epochs': 19}. Best is trial 44 with value: 117.2196115.\n",
      "[I 2025-02-06 06:02:30,361] Trial 156 finished with value: 97.4598563 and parameters: {'learning_rate': 0.0017047020038544376, 'n_steps': 1152, 'gamma': 0.9500000000000001, 'ent_coef': 1.0724496152663743e-06, 'clip_range': 0.4, 'batch_size': 64, 'gae_lambda': 0.9296266867960379, 'n_epochs': 20}. Best is trial 44 with value: 117.2196115.\n",
      "[I 2025-02-06 06:04:41,948] Trial 157 finished with value: 70.58495715 and parameters: {'learning_rate': 0.0006365284709998714, 'n_steps': 128, 'gamma': 0.9400000000000001, 'ent_coef': 7.529840995480482e-08, 'clip_range': 0.4, 'batch_size': 32, 'gae_lambda': 0.9252419649059739, 'n_epochs': 20}. Best is trial 44 with value: 117.2196115.\n",
      "[I 2025-02-06 06:06:49,474] Trial 158 finished with value: 69.15599569999999 and parameters: {'learning_rate': 0.00038529173635053606, 'n_steps': 384, 'gamma': 0.96, 'ent_coef': 1.3261095102124709e-08, 'clip_range': 0.4, 'batch_size': 32, 'gae_lambda': 0.9091234483981945, 'n_epochs': 19}. Best is trial 44 with value: 117.2196115.\n",
      "[I 2025-02-06 06:08:17,534] Trial 159 finished with value: 42.15097080000001 and parameters: {'learning_rate': 1.2719113574957683e-05, 'n_steps': 128, 'gamma': 0.9500000000000001, 'ent_coef': 2.957031368959798e-08, 'clip_range': 0.4, 'batch_size': 64, 'gae_lambda': 0.8954674413610003, 'n_epochs': 18}. Best is trial 44 with value: 117.2196115.\n",
      "[I 2025-02-06 06:10:23,891] Trial 160 finished with value: 37.407398050000005 and parameters: {'learning_rate': 0.0004338800482963855, 'n_steps': 1664, 'gamma': 0.96, 'ent_coef': 1.8908004646552174e-08, 'clip_range': 0.4, 'batch_size': 32, 'gae_lambda': 0.9047669046661619, 'n_epochs': 19}. Best is trial 44 with value: 117.2196115.\n",
      "[I 2025-02-06 06:11:57,285] Trial 161 finished with value: 103.73972284999998 and parameters: {'learning_rate': 0.0014655909731639397, 'n_steps': 128, 'gamma': 0.9500000000000001, 'ent_coef': 3.584204618725559e-08, 'clip_range': 0.4, 'batch_size': 64, 'gae_lambda': 0.9034934455183484, 'n_epochs': 19}. Best is trial 44 with value: 117.2196115.\n",
      "[I 2025-02-06 06:13:31,151] Trial 162 finished with value: 81.14984445 and parameters: {'learning_rate': 0.0015572559064838615, 'n_steps': 128, 'gamma': 0.9500000000000001, 'ent_coef': 3.8021749064256056e-08, 'clip_range': 0.4, 'batch_size': 64, 'gae_lambda': 0.9004325898029277, 'n_epochs': 20}. Best is trial 44 with value: 117.2196115.\n",
      "[I 2025-02-06 06:15:03,013] Trial 163 finished with value: 85.36011894999999 and parameters: {'learning_rate': 0.001367512068026971, 'n_steps': 256, 'gamma': 0.9500000000000001, 'ent_coef': 6.438123064878154e-08, 'clip_range': 0.4, 'batch_size': 64, 'gae_lambda': 0.9124789688413805, 'n_epochs': 19}. Best is trial 44 with value: 117.2196115.\n",
      "[I 2025-02-06 06:16:34,361] Trial 164 finished with value: 69.93500659999998 and parameters: {'learning_rate': 0.0008173416877301572, 'n_steps': 256, 'gamma': 0.9500000000000001, 'ent_coef': 2.2627147215416624e-08, 'clip_range': 0.1, 'batch_size': 64, 'gae_lambda': 0.9079794229120075, 'n_epochs': 19}. Best is trial 44 with value: 117.2196115.\n",
      "[I 2025-02-06 06:18:07,905] Trial 165 finished with value: 106.6044551 and parameters: {'learning_rate': 0.0005490379922116228, 'n_steps': 128, 'gamma': 0.9500000000000001, 'ent_coef': 5.436780172187551e-08, 'clip_range': 0.4, 'batch_size': 64, 'gae_lambda': 0.9015502874571786, 'n_epochs': 20}. Best is trial 44 with value: 117.2196115.\n",
      "[I 2025-02-06 06:19:37,287] Trial 166 finished with value: 95.53116829999999 and parameters: {'learning_rate': 0.0005565670083902699, 'n_steps': 128, 'gamma': 0.9500000000000001, 'ent_coef': 5.5780465338613094e-08, 'clip_range': 0.4, 'batch_size': 64, 'gae_lambda': 0.8982806606863211, 'n_epochs': 18}. Best is trial 44 with value: 117.2196115.\n",
      "[I 2025-02-06 06:21:11,696] Trial 167 finished with value: 86.79196725 and parameters: {'learning_rate': 0.0022984241591101047, 'n_steps': 256, 'gamma': 0.96, 'ent_coef': 8.82359314986703e-08, 'clip_range': 0.4, 'batch_size': 64, 'gae_lambda': 0.8929416830991066, 'n_epochs': 20}. Best is trial 44 with value: 117.2196115.\n",
      "[I 2025-02-06 06:22:43,268] Trial 168 finished with value: 100.99533750000003 and parameters: {'learning_rate': 0.0085336341320104, 'n_steps': 128, 'gamma': 0.9400000000000001, 'ent_coef': 1.5290457731243923e-07, 'clip_range': 0.4, 'batch_size': 64, 'gae_lambda': 0.9031304907989739, 'n_epochs': 19}. Best is trial 44 with value: 117.2196115.\n",
      "[I 2025-02-06 06:24:16,714] Trial 169 finished with value: 88.6470979 and parameters: {'learning_rate': 0.0002878232970426891, 'n_steps': 256, 'gamma': 0.9500000000000001, 'ent_coef': 3.947923153632734e-08, 'clip_range': 0.4, 'batch_size': 64, 'gae_lambda': 0.9057536522828932, 'n_epochs': 20}. Best is trial 44 with value: 117.2196115.\n",
      "[I 2025-02-06 06:25:48,489] Trial 170 finished with value: 88.0703662 and parameters: {'learning_rate': 0.0003651510415583802, 'n_steps': 128, 'gamma': 0.96, 'ent_coef': 3.0607291182634346e-08, 'clip_range': 0.4, 'batch_size': 64, 'gae_lambda': 0.9005575485824945, 'n_epochs': 19}. Best is trial 44 with value: 117.2196115.\n",
      "[I 2025-02-06 06:27:39,324] Trial 171 finished with value: 66.69881425000003 and parameters: {'learning_rate': 0.0004858726453584531, 'n_steps': 128, 'gamma': 0.9500000000000001, 'ent_coef': 1.6073903830865343e-08, 'clip_range': 0.4, 'batch_size': 64, 'gae_lambda': 0.9157855700359839, 'n_epochs': 20}. Best is trial 44 with value: 117.2196115.\n",
      "[I 2025-02-06 06:30:12,881] Trial 172 finished with value: 92.33565974999999 and parameters: {'learning_rate': 0.0006785560669219378, 'n_steps': 128, 'gamma': 0.9500000000000001, 'ent_coef': 1.098366632573504e-08, 'clip_range': 0.4, 'batch_size': 32, 'gae_lambda': 0.9125475062934958, 'n_epochs': 20}. Best is trial 44 with value: 117.2196115.\n",
      "[I 2025-02-06 06:32:37,537] Trial 173 finished with value: 69.3620453 and parameters: {'learning_rate': 0.0005682871610955679, 'n_steps': 256, 'gamma': 0.9500000000000001, 'ent_coef': 2.397998417575693e-08, 'clip_range': 0.4, 'batch_size': 32, 'gae_lambda': 0.9212587632416801, 'n_epochs': 20}. Best is trial 44 with value: 117.2196115.\n",
      "[I 2025-02-06 06:34:10,228] Trial 174 finished with value: 93.51500485 and parameters: {'learning_rate': 0.0009589472165118819, 'n_steps': 128, 'gamma': 0.9500000000000001, 'ent_coef': 4.4357862470695615e-08, 'clip_range': 0.25, 'batch_size': 64, 'gae_lambda': 0.9102050646050964, 'n_epochs': 19}. Best is trial 44 with value: 117.2196115.\n",
      "[I 2025-02-06 06:36:17,439] Trial 175 finished with value: 101.33745555 and parameters: {'learning_rate': 0.0004699541695505487, 'n_steps': 128, 'gamma': 0.9500000000000001, 'ent_coef': 1.7195034397386725e-08, 'clip_range': 0.4, 'batch_size': 32, 'gae_lambda': 0.8962779303804905, 'n_epochs': 19}. Best is trial 44 with value: 117.2196115.\n",
      "[I 2025-02-06 06:37:51,108] Trial 176 finished with value: 85.7895159 and parameters: {'learning_rate': 0.0001668899807885681, 'n_steps': 256, 'gamma': 0.96, 'ent_coef': 5.569941026126176e-08, 'clip_range': 0.4, 'batch_size': 64, 'gae_lambda': 0.9063152098068868, 'n_epochs': 20}. Best is trial 44 with value: 117.2196115.\n",
      "[I 2025-02-06 06:40:07,197] Trial 177 finished with value: 91.86076205 and parameters: {'learning_rate': 0.003057534880357, 'n_steps': 128, 'gamma': 0.9500000000000001, 'ent_coef': 3.124639697565641e-08, 'clip_range': 0.4, 'batch_size': 32, 'gae_lambda': 0.917726395533417, 'n_epochs': 20}. Best is trial 44 with value: 117.2196115.\n",
      "[I 2025-02-06 06:41:36,089] Trial 178 finished with value: 70.94161575 and parameters: {'learning_rate': 0.001239788994687375, 'n_steps': 256, 'gamma': 0.96, 'ent_coef': 1.3573850522015493e-08, 'clip_range': 0.4, 'batch_size': 64, 'gae_lambda': 0.9028450206825794, 'n_epochs': 18}. Best is trial 44 with value: 117.2196115.\n",
      "[I 2025-02-06 06:43:48,409] Trial 179 finished with value: 88.52013749999999 and parameters: {'learning_rate': 0.0003415546806087303, 'n_steps': 128, 'gamma': 0.9500000000000001, 'ent_coef': 1.0170380831015666e-08, 'clip_range': 0.4, 'batch_size': 32, 'gae_lambda': 0.9354570019329856, 'n_epochs': 20}. Best is trial 44 with value: 117.2196115.\n",
      "[I 2025-02-06 06:45:19,463] Trial 180 finished with value: 56.939416000000016 and parameters: {'learning_rate': 0.0002521823545729215, 'n_steps': 384, 'gamma': 0.9400000000000001, 'ent_coef': 9.887861700318984e-08, 'clip_range': 0.4, 'batch_size': 64, 'gae_lambda': 0.8897310380517561, 'n_epochs': 19}. Best is trial 44 with value: 117.2196115.\n",
      "[I 2025-02-06 06:46:51,435] Trial 181 finished with value: 83.53012735 and parameters: {'learning_rate': 0.0004176960794954842, 'n_steps': 128, 'gamma': 0.9500000000000001, 'ent_coef': 3.800012419318285e-08, 'clip_range': 0.4, 'batch_size': 64, 'gae_lambda': 0.9044119364995669, 'n_epochs': 19}. Best is trial 44 with value: 117.2196115.\n",
      "[I 2025-02-06 06:48:23,682] Trial 182 finished with value: 77.52407720000001 and parameters: {'learning_rate': 0.0016169935358053954, 'n_steps': 128, 'gamma': 0.9500000000000001, 'ent_coef': 2.2096182955766593e-08, 'clip_range': 0.4, 'batch_size': 64, 'gae_lambda': 0.9095330144480086, 'n_epochs': 19}. Best is trial 44 with value: 117.2196115.\n",
      "[I 2025-02-06 06:49:59,160] Trial 183 finished with value: 84.11552965 and parameters: {'learning_rate': 0.0014128504117323082, 'n_steps': 128, 'gamma': 0.9500000000000001, 'ent_coef': 3.4026922427888275e-08, 'clip_range': 0.4, 'batch_size': 64, 'gae_lambda': 0.9010806444671947, 'n_epochs': 20}. Best is trial 44 with value: 117.2196115.\n",
      "[I 2025-02-06 06:51:30,162] Trial 184 finished with value: 100.56969205 and parameters: {'learning_rate': 0.0010734040392655633, 'n_steps': 256, 'gamma': 0.96, 'ent_coef': 5.421147236475477e-08, 'clip_range': 0.4, 'batch_size': 64, 'gae_lambda': 0.9141296142017588, 'n_epochs': 19}. Best is trial 44 with value: 117.2196115.\n",
      "[I 2025-02-06 06:52:58,721] Trial 185 finished with value: 64.57778999999998 and parameters: {'learning_rate': 0.0007208835141771487, 'n_steps': 256, 'gamma': 0.9500000000000001, 'ent_coef': 2.253269154368258e-08, 'clip_range': 0.4, 'batch_size': 64, 'gae_lambda': 0.8973575029901042, 'n_epochs': 18}. Best is trial 44 with value: 117.2196115.\n",
      "[I 2025-02-06 06:55:08,104] Trial 186 finished with value: 103.83455719999999 and parameters: {'learning_rate': 0.0052115899798174095, 'n_steps': 128, 'gamma': 0.9500000000000001, 'ent_coef': 7.23896966526644e-08, 'clip_range': 0.4, 'batch_size': 32, 'gae_lambda': 0.9065017814413923, 'n_epochs': 19}. Best is trial 44 with value: 117.2196115.\n",
      "[I 2025-02-06 06:57:17,045] Trial 187 finished with value: 57.738950599999995 and parameters: {'learning_rate': 0.006039552640394588, 'n_steps': 2048, 'gamma': 0.9500000000000001, 'ent_coef': 8.218976612200673e-08, 'clip_range': 0.4, 'batch_size': 32, 'gae_lambda': 0.9075315376665906, 'n_epochs': 20}. Best is trial 44 with value: 117.2196115.\n",
      "[I 2025-02-06 06:59:28,929] Trial 188 finished with value: 100.67497999999999 and parameters: {'learning_rate': 0.005632824023423916, 'n_steps': 128, 'gamma': 0.96, 'ent_coef': 1.3606496168774675e-07, 'clip_range': 0.4, 'batch_size': 32, 'gae_lambda': 0.9182836172331861, 'n_epochs': 19}. Best is trial 44 with value: 117.2196115.\n",
      "[I 2025-02-06 07:01:41,619] Trial 189 finished with value: 98.06976384999999 and parameters: {'learning_rate': 0.004445359549253733, 'n_steps': 256, 'gamma': 0.9500000000000001, 'ent_coef': 2.3486528586608312e-07, 'clip_range': 0.4, 'batch_size': 32, 'gae_lambda': 0.9307067558519203, 'n_epochs': 20}. Best is trial 44 with value: 117.2196115.\n",
      "[I 2025-02-06 07:03:53,625] Trial 190 finished with value: 63.7282741 and parameters: {'learning_rate': 0.003884233739633152, 'n_steps': 128, 'gamma': 0.9500000000000001, 'ent_coef': 6.516818592511116e-08, 'clip_range': 0.4, 'batch_size': 32, 'gae_lambda': 0.9241293522317624, 'n_epochs': 20}. Best is trial 44 with value: 117.2196115.\n",
      "[I 2025-02-06 07:06:02,697] Trial 191 finished with value: 104.20997550000001 and parameters: {'learning_rate': 0.005237616838634489, 'n_steps': 128, 'gamma': 0.9500000000000001, 'ent_coef': 4.3068079851856655e-08, 'clip_range': 0.4, 'batch_size': 32, 'gae_lambda': 0.9046622028092466, 'n_epochs': 19}. Best is trial 44 with value: 117.2196115.\n",
      "[I 2025-02-06 07:08:12,597] Trial 192 finished with value: 102.97389515 and parameters: {'learning_rate': 0.004582849655036435, 'n_steps': 128, 'gamma': 0.9500000000000001, 'ent_coef': 1.6949595293161344e-06, 'clip_range': 0.4, 'batch_size': 32, 'gae_lambda': 0.9111035815862483, 'n_epochs': 19}. Best is trial 44 with value: 117.2196115.\n",
      "[I 2025-02-06 07:10:19,688] Trial 193 finished with value: 86.16132545 and parameters: {'learning_rate': 0.005324637030222267, 'n_steps': 768, 'gamma': 0.9500000000000001, 'ent_coef': 4.7344540224146964e-08, 'clip_range': 0.4, 'batch_size': 32, 'gae_lambda': 0.9110715778815833, 'n_epochs': 19}. Best is trial 44 with value: 117.2196115.\n",
      "[I 2025-02-06 07:12:23,052] Trial 194 finished with value: 89.53380605 and parameters: {'learning_rate': 0.008444619377067558, 'n_steps': 128, 'gamma': 0.9500000000000001, 'ent_coef': 2.2786784674080956e-06, 'clip_range': 0.4, 'batch_size': 32, 'gae_lambda': 0.9056182177246648, 'n_epochs': 18}. Best is trial 44 with value: 117.2196115.\n",
      "[I 2025-02-06 07:14:30,464] Trial 195 finished with value: 79.95823200000002 and parameters: {'learning_rate': 0.0075348867901281836, 'n_steps': 896, 'gamma': 0.96, 'ent_coef': 1.5333948212054135e-06, 'clip_range': 0.4, 'batch_size': 32, 'gae_lambda': 0.9001383656354123, 'n_epochs': 19}. Best is trial 44 with value: 117.2196115.\n",
      "[I 2025-02-06 07:16:38,933] Trial 196 finished with value: 86.73576829999999 and parameters: {'learning_rate': 0.006635626951329845, 'n_steps': 128, 'gamma': 0.9400000000000001, 'ent_coef': 1.1762657689524316e-07, 'clip_range': 0.4, 'batch_size': 32, 'gae_lambda': 0.9133688084855396, 'n_epochs': 19}. Best is trial 44 with value: 117.2196115.\n",
      "[I 2025-02-06 07:18:51,592] Trial 197 finished with value: 72.34016455 and parameters: {'learning_rate': 0.004419595755711794, 'n_steps': 256, 'gamma': 0.9500000000000001, 'ent_coef': 1.228738800079524e-06, 'clip_range': 0.4, 'batch_size': 32, 'gae_lambda': 0.9082972836678506, 'n_epochs': 19}. Best is trial 44 with value: 117.2196115.\n",
      "[I 2025-02-06 07:21:00,979] Trial 198 finished with value: 92.82412645000001 and parameters: {'learning_rate': 0.0035048563178297623, 'n_steps': 128, 'gamma': 0.96, 'ent_coef': 7.823422943035421e-07, 'clip_range': 0.4, 'batch_size': 32, 'gae_lambda': 0.893581788109885, 'n_epochs': 19}. Best is trial 44 with value: 117.2196115.\n",
      "[I 2025-02-06 07:23:03,614] Trial 199 finished with value: 74.56138775000001 and parameters: {'learning_rate': 0.007431597508484129, 'n_steps': 256, 'gamma': 0.9500000000000001, 'ent_coef': 6.897098369049739e-06, 'clip_range': 0.4, 'batch_size': 32, 'gae_lambda': 0.9028857678549793, 'n_epochs': 18}. Best is trial 44 with value: 117.2196115.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters: {'learning_rate': 0.00038895903894657874, 'n_steps': 256, 'gamma': 0.9500000000000001, 'ent_coef': 1.9799178339816272e-08, 'clip_range': 0.4, 'batch_size': 32, 'gae_lambda': 0.901928597603968, 'n_epochs': 19}\n",
      "Best value (objective): 117.2196115\n"
     ]
    }
   ],
   "source": [
    "#Run fine tuning\n",
    "study = run_optimization()\n",
    "\n",
    "with open(\"ppo_best_params.pkl\", \"wb\") as f:\n",
    "    pickle.dump(study.best_params, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4d1a805-c19d-48cd-bbab-57820e66ab3f",
   "metadata": {},
   "source": [
    "## b. Training final model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "880d4d28-cf4a-4428-b756-cec7b7b8584c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 6058 |\n",
      "|    iterations      | 1    |\n",
      "|    time_elapsed    | 0    |\n",
      "|    total_timesteps | 256  |\n",
      "-----------------------------\n",
      "Eval num_timesteps=500, episode_reward=-59.83 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 230        |\n",
      "|    mean_reward          | -59.8      |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 500        |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04965801 |\n",
      "|    clip_fraction        | 0.14       |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.654     |\n",
      "|    explained_variance   | -0.347     |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.142     |\n",
      "|    n_updates            | 19         |\n",
      "|    policy_gradient_loss | -0.046     |\n",
      "|    value_loss           | 0.0448     |\n",
      "----------------------------------------\n",
      "New best mean reward!\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 1082 |\n",
      "|    iterations      | 2    |\n",
      "|    time_elapsed    | 0    |\n",
      "|    total_timesteps | 512  |\n",
      "-----------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 1045       |\n",
      "|    iterations           | 3          |\n",
      "|    time_elapsed         | 0          |\n",
      "|    total_timesteps      | 768        |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07367196 |\n",
      "|    clip_fraction        | 0.169      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.635     |\n",
      "|    explained_variance   | -0.145     |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.165     |\n",
      "|    n_updates            | 38         |\n",
      "|    policy_gradient_loss | -0.0889    |\n",
      "|    value_loss           | 0.156      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=1000, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 230         |\n",
      "|    mean_reward          | 0           |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1000        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.076484285 |\n",
      "|    clip_fraction        | 0.187       |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -0.593      |\n",
      "|    explained_variance   | 0.00366     |\n",
      "|    learning_rate        | 0.000389    |\n",
      "|    loss                 | -0.138      |\n",
      "|    n_updates            | 57          |\n",
      "|    policy_gradient_loss | -0.0995     |\n",
      "|    value_loss           | 0.152       |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 20.5     |\n",
      "| time/              |          |\n",
      "|    fps             | 902      |\n",
      "|    iterations      | 4        |\n",
      "|    time_elapsed    | 1        |\n",
      "|    total_timesteps | 1024     |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 889         |\n",
      "|    ep_rew_mean          | 20.5        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 941         |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 1           |\n",
      "|    total_timesteps      | 1280        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.054444104 |\n",
      "|    clip_fraction        | 0.125       |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -0.546      |\n",
      "|    explained_variance   | 0.15        |\n",
      "|    learning_rate        | 0.000389    |\n",
      "|    loss                 | 0.0472      |\n",
      "|    n_updates            | 76          |\n",
      "|    policy_gradient_loss | -0.0582     |\n",
      "|    value_loss           | 0.313       |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=1500, episode_reward=10.19 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 230         |\n",
      "|    mean_reward          | 10.2        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1500        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.053193867 |\n",
      "|    clip_fraction        | 0.116       |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -0.473      |\n",
      "|    explained_variance   | -0.615      |\n",
      "|    learning_rate        | 0.000389    |\n",
      "|    loss                 | -0.174      |\n",
      "|    n_updates            | 95          |\n",
      "|    policy_gradient_loss | -0.0751     |\n",
      "|    value_loss           | 0.0802      |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 20.5     |\n",
      "| time/              |          |\n",
      "|    fps             | 884      |\n",
      "|    iterations      | 6        |\n",
      "|    time_elapsed    | 1        |\n",
      "|    total_timesteps | 1536     |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 35.8       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 909        |\n",
      "|    iterations           | 7          |\n",
      "|    time_elapsed         | 1          |\n",
      "|    total_timesteps      | 1792       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08869153 |\n",
      "|    clip_fraction        | 0.225      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.523     |\n",
      "|    explained_variance   | 0.346      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.122     |\n",
      "|    n_updates            | 114        |\n",
      "|    policy_gradient_loss | -0.12      |\n",
      "|    value_loss           | 0.195      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=2000, episode_reward=39.50 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 230         |\n",
      "|    mean_reward          | 39.5        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 2000        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.093384415 |\n",
      "|    clip_fraction        | 0.204       |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -0.5        |\n",
      "|    explained_variance   | 0.323       |\n",
      "|    learning_rate        | 0.000389    |\n",
      "|    loss                 | -0.144      |\n",
      "|    n_updates            | 133         |\n",
      "|    policy_gradient_loss | -0.108      |\n",
      "|    value_loss           | 0.252       |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 35.8     |\n",
      "| time/              |          |\n",
      "|    fps             | 860      |\n",
      "|    iterations      | 8        |\n",
      "|    time_elapsed    | 2        |\n",
      "|    total_timesteps | 2048     |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 35.8       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 860        |\n",
      "|    iterations           | 9          |\n",
      "|    time_elapsed         | 2          |\n",
      "|    total_timesteps      | 2304       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11873017 |\n",
      "|    clip_fraction        | 0.205      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.467     |\n",
      "|    explained_variance   | -0.797     |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.126     |\n",
      "|    n_updates            | 152        |\n",
      "|    policy_gradient_loss | -0.11      |\n",
      "|    value_loss           | 0.0557     |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=2500, episode_reward=1.51 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 1.51      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 2500      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1070421 |\n",
      "|    clip_fraction        | 0.236     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.453    |\n",
      "|    explained_variance   | 0.44      |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0892   |\n",
      "|    n_updates            | 171       |\n",
      "|    policy_gradient_loss | -0.112    |\n",
      "|    value_loss           | 0.273     |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 35.8     |\n",
      "| time/              |          |\n",
      "|    fps             | 819      |\n",
      "|    iterations      | 10       |\n",
      "|    time_elapsed    | 3        |\n",
      "|    total_timesteps | 2560     |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 38.7       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 812        |\n",
      "|    iterations           | 11         |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 2816       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13835767 |\n",
      "|    clip_fraction        | 0.222      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.408     |\n",
      "|    explained_variance   | 0.283      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.148     |\n",
      "|    n_updates            | 190        |\n",
      "|    policy_gradient_loss | -0.116     |\n",
      "|    value_loss           | 0.104      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=3000, episode_reward=-2.24 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | -2.24     |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 3000      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1569613 |\n",
      "|    clip_fraction        | 0.285     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.484    |\n",
      "|    explained_variance   | 0.629     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.177    |\n",
      "|    n_updates            | 209       |\n",
      "|    policy_gradient_loss | -0.0911   |\n",
      "|    value_loss           | 0.2       |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 38.7     |\n",
      "| time/              |          |\n",
      "|    fps             | 789      |\n",
      "|    iterations      | 12       |\n",
      "|    time_elapsed    | 3        |\n",
      "|    total_timesteps | 3072     |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 38.7      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 804       |\n",
      "|    iterations           | 13        |\n",
      "|    time_elapsed         | 4         |\n",
      "|    total_timesteps      | 3328      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.2703294 |\n",
      "|    clip_fraction        | 0.284     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.39     |\n",
      "|    explained_variance   | 0.12      |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.149    |\n",
      "|    n_updates            | 228       |\n",
      "|    policy_gradient_loss | -0.123    |\n",
      "|    value_loss           | 0.0637    |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=3500, episode_reward=2.71 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 230        |\n",
      "|    mean_reward          | 2.71       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 3500       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07648235 |\n",
      "|    clip_fraction        | 0.176      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.341     |\n",
      "|    explained_variance   | 0.599      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.129     |\n",
      "|    n_updates            | 247        |\n",
      "|    policy_gradient_loss | -0.0961    |\n",
      "|    value_loss           | 0.221      |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 37       |\n",
      "| time/              |          |\n",
      "|    fps             | 795      |\n",
      "|    iterations      | 14       |\n",
      "|    time_elapsed    | 4        |\n",
      "|    total_timesteps | 3584     |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 37        |\n",
      "| time/                   |           |\n",
      "|    fps                  | 800       |\n",
      "|    iterations           | 15        |\n",
      "|    time_elapsed         | 4         |\n",
      "|    total_timesteps      | 3840      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.1102057 |\n",
      "|    clip_fraction        | 0.191     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.396    |\n",
      "|    explained_variance   | 0.448     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.121    |\n",
      "|    n_updates            | 266       |\n",
      "|    policy_gradient_loss | -0.0843   |\n",
      "|    value_loss           | 0.193     |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=4000, episode_reward=31.28 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 230        |\n",
      "|    mean_reward          | 31.3       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 4000       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15818559 |\n",
      "|    clip_fraction        | 0.264      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.424     |\n",
      "|    explained_variance   | -0.938     |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.157     |\n",
      "|    n_updates            | 285        |\n",
      "|    policy_gradient_loss | -0.124     |\n",
      "|    value_loss           | 0.0517     |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 37       |\n",
      "| time/              |          |\n",
      "|    fps             | 787      |\n",
      "|    iterations      | 16       |\n",
      "|    time_elapsed    | 5        |\n",
      "|    total_timesteps | 4096     |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 37         |\n",
      "| time/                   |            |\n",
      "|    fps                  | 799        |\n",
      "|    iterations           | 17         |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 4352       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.27241254 |\n",
      "|    clip_fraction        | 0.282      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.33      |\n",
      "|    explained_variance   | 0.598      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.126     |\n",
      "|    n_updates            | 304        |\n",
      "|    policy_gradient_loss | -0.111     |\n",
      "|    value_loss           | 0.204      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=4500, episode_reward=12.69 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 230        |\n",
      "|    mean_reward          | 12.7       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 4500       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.25654757 |\n",
      "|    clip_fraction        | 0.194      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.259     |\n",
      "|    explained_variance   | 0.52       |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.119     |\n",
      "|    n_updates            | 323        |\n",
      "|    policy_gradient_loss | -0.1       |\n",
      "|    value_loss           | 0.0977     |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 40.7     |\n",
      "| time/              |          |\n",
      "|    fps             | 785      |\n",
      "|    iterations      | 18       |\n",
      "|    time_elapsed    | 5        |\n",
      "|    total_timesteps | 4608     |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 40.7       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 793        |\n",
      "|    iterations           | 19         |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 4864       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.17373058 |\n",
      "|    clip_fraction        | 0.297      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.434     |\n",
      "|    explained_variance   | 0.306      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.129     |\n",
      "|    n_updates            | 342        |\n",
      "|    policy_gradient_loss | -0.107     |\n",
      "|    value_loss           | 0.268      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=5000, episode_reward=-1.05 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 230        |\n",
      "|    mean_reward          | -1.05      |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 5000       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.43769804 |\n",
      "|    clip_fraction        | 0.325      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.328     |\n",
      "|    explained_variance   | 0.667      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0396    |\n",
      "|    n_updates            | 361        |\n",
      "|    policy_gradient_loss | -0.0971    |\n",
      "|    value_loss           | 0.146      |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 40.7     |\n",
      "| time/              |          |\n",
      "|    fps             | 789      |\n",
      "|    iterations      | 20       |\n",
      "|    time_elapsed    | 6        |\n",
      "|    total_timesteps | 5120     |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 45.1       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 800        |\n",
      "|    iterations           | 21         |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 5376       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.18451457 |\n",
      "|    clip_fraction        | 0.241      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.298     |\n",
      "|    explained_variance   | 0.288      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.105     |\n",
      "|    n_updates            | 380        |\n",
      "|    policy_gradient_loss | -0.101     |\n",
      "|    value_loss           | 0.232      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=5500, episode_reward=21.13 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 230        |\n",
      "|    mean_reward          | 21.1       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 5500       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.35528344 |\n",
      "|    clip_fraction        | 0.287      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.388     |\n",
      "|    explained_variance   | 0.657      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0829    |\n",
      "|    n_updates            | 399        |\n",
      "|    policy_gradient_loss | -0.0923    |\n",
      "|    value_loss           | 0.396      |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 45.1     |\n",
      "| time/              |          |\n",
      "|    fps             | 782      |\n",
      "|    iterations      | 22       |\n",
      "|    time_elapsed    | 7        |\n",
      "|    total_timesteps | 5632     |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 45.1      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 794       |\n",
      "|    iterations           | 23        |\n",
      "|    time_elapsed         | 7         |\n",
      "|    total_timesteps      | 5888      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.2510214 |\n",
      "|    clip_fraction        | 0.266     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.368    |\n",
      "|    explained_variance   | -0.778    |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.187    |\n",
      "|    n_updates            | 418       |\n",
      "|    policy_gradient_loss | -0.123    |\n",
      "|    value_loss           | 0.0584    |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=6000, episode_reward=7.25 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 230        |\n",
      "|    mean_reward          | 7.25       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 6000       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.30916822 |\n",
      "|    clip_fraction        | 0.247      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.301     |\n",
      "|    explained_variance   | 0.815      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.147     |\n",
      "|    n_updates            | 437        |\n",
      "|    policy_gradient_loss | -0.105     |\n",
      "|    value_loss           | 0.101      |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 45.1     |\n",
      "| time/              |          |\n",
      "|    fps             | 790      |\n",
      "|    iterations      | 24       |\n",
      "|    time_elapsed    | 7        |\n",
      "|    total_timesteps | 6144     |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 47.4       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 795        |\n",
      "|    iterations           | 25         |\n",
      "|    time_elapsed         | 8          |\n",
      "|    total_timesteps      | 6400       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.46107435 |\n",
      "|    clip_fraction        | 0.304      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.274     |\n",
      "|    explained_variance   | 0.65       |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.16      |\n",
      "|    n_updates            | 456        |\n",
      "|    policy_gradient_loss | -0.127     |\n",
      "|    value_loss           | 0.138      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=6500, episode_reward=30.04 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 30        |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 6500      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.2132301 |\n",
      "|    clip_fraction        | 0.27      |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.416    |\n",
      "|    explained_variance   | -0.672    |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.138    |\n",
      "|    n_updates            | 475       |\n",
      "|    policy_gradient_loss | -0.0867   |\n",
      "|    value_loss           | 0.178     |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 47.4     |\n",
      "| time/              |          |\n",
      "|    fps             | 792      |\n",
      "|    iterations      | 26       |\n",
      "|    time_elapsed    | 8        |\n",
      "|    total_timesteps | 6656     |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 47.4      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 802       |\n",
      "|    iterations           | 27        |\n",
      "|    time_elapsed         | 8         |\n",
      "|    total_timesteps      | 6912      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.2500715 |\n",
      "|    clip_fraction        | 0.253     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.314    |\n",
      "|    explained_variance   | 0.824     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.175    |\n",
      "|    n_updates            | 494       |\n",
      "|    policy_gradient_loss | -0.122    |\n",
      "|    value_loss           | 0.0686    |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=7000, episode_reward=9.00 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 230        |\n",
      "|    mean_reward          | 9          |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 7000       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.17006794 |\n",
      "|    clip_fraction        | 0.214      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.313     |\n",
      "|    explained_variance   | 0.462      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.103     |\n",
      "|    n_updates            | 513        |\n",
      "|    policy_gradient_loss | -0.0929    |\n",
      "|    value_loss           | 0.163      |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 48.1     |\n",
      "| time/              |          |\n",
      "|    fps             | 798      |\n",
      "|    iterations      | 28       |\n",
      "|    time_elapsed    | 8        |\n",
      "|    total_timesteps | 7168     |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 48.1       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 797        |\n",
      "|    iterations           | 29         |\n",
      "|    time_elapsed         | 9          |\n",
      "|    total_timesteps      | 7424       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.23527436 |\n",
      "|    clip_fraction        | 0.235      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.311     |\n",
      "|    explained_variance   | 0.688      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.132     |\n",
      "|    n_updates            | 532        |\n",
      "|    policy_gradient_loss | -0.106     |\n",
      "|    value_loss           | 0.195      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=7500, episode_reward=5.75 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 5.75      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 7500      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.3452406 |\n",
      "|    clip_fraction        | 0.264     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.264    |\n",
      "|    explained_variance   | 0.271     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.112    |\n",
      "|    n_updates            | 551       |\n",
      "|    policy_gradient_loss | -0.0986   |\n",
      "|    value_loss           | 0.0376    |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 48.1     |\n",
      "| time/              |          |\n",
      "|    fps             | 793      |\n",
      "|    iterations      | 30       |\n",
      "|    time_elapsed    | 9        |\n",
      "|    total_timesteps | 7680     |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 48.1       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 800        |\n",
      "|    iterations           | 31         |\n",
      "|    time_elapsed         | 9          |\n",
      "|    total_timesteps      | 7936       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.31296962 |\n",
      "|    clip_fraction        | 0.254      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.278     |\n",
      "|    explained_variance   | 0.722      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.146     |\n",
      "|    n_updates            | 570        |\n",
      "|    policy_gradient_loss | -0.107     |\n",
      "|    value_loss           | 0.113      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=8000, episode_reward=11.59 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "--------------------------------------\n",
      "| eval/                   |          |\n",
      "|    mean_ep_length       | 230      |\n",
      "|    mean_reward          | 11.6     |\n",
      "| time/                   |          |\n",
      "|    total_timesteps      | 8000     |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 0.296304 |\n",
      "|    clip_fraction        | 0.258    |\n",
      "|    clip_range           | 0.4      |\n",
      "|    entropy_loss         | -0.258   |\n",
      "|    explained_variance   | 0.696    |\n",
      "|    learning_rate        | 0.000389 |\n",
      "|    loss                 | -0.0869  |\n",
      "|    n_updates            | 589      |\n",
      "|    policy_gradient_loss | -0.0974  |\n",
      "|    value_loss           | 0.157    |\n",
      "--------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 51.6     |\n",
      "| time/              |          |\n",
      "|    fps             | 797      |\n",
      "|    iterations      | 32       |\n",
      "|    time_elapsed    | 10       |\n",
      "|    total_timesteps | 8192     |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 51.6       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 803        |\n",
      "|    iterations           | 33         |\n",
      "|    time_elapsed         | 10         |\n",
      "|    total_timesteps      | 8448       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.32444766 |\n",
      "|    clip_fraction        | 0.257      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.331     |\n",
      "|    explained_variance   | 0.394      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.142     |\n",
      "|    n_updates            | 608        |\n",
      "|    policy_gradient_loss | -0.0866    |\n",
      "|    value_loss           | 0.145      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=8500, episode_reward=4.39 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 230        |\n",
      "|    mean_reward          | 4.39       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 8500       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.38508213 |\n",
      "|    clip_fraction        | 0.292      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.3       |\n",
      "|    explained_variance   | 0.633      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.157     |\n",
      "|    n_updates            | 627        |\n",
      "|    policy_gradient_loss | -0.132     |\n",
      "|    value_loss           | 0.0962     |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 51.6     |\n",
      "| time/              |          |\n",
      "|    fps             | 796      |\n",
      "|    iterations      | 34       |\n",
      "|    time_elapsed    | 10       |\n",
      "|    total_timesteps | 8704     |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 52.4       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 802        |\n",
      "|    iterations           | 35         |\n",
      "|    time_elapsed         | 11         |\n",
      "|    total_timesteps      | 8960       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.32194018 |\n",
      "|    clip_fraction        | 0.29       |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.265     |\n",
      "|    explained_variance   | 0.323      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.142     |\n",
      "|    n_updates            | 646        |\n",
      "|    policy_gradient_loss | -0.127     |\n",
      "|    value_loss           | 0.117      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=9000, episode_reward=-2.10 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 230        |\n",
      "|    mean_reward          | -2.1       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 9000       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.38904572 |\n",
      "|    clip_fraction        | 0.303      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.285     |\n",
      "|    explained_variance   | 0.769      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.135     |\n",
      "|    n_updates            | 665        |\n",
      "|    policy_gradient_loss | -0.107     |\n",
      "|    value_loss           | 0.0911     |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 52.4     |\n",
      "| time/              |          |\n",
      "|    fps             | 785      |\n",
      "|    iterations      | 36       |\n",
      "|    time_elapsed    | 11       |\n",
      "|    total_timesteps | 9216     |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 52.4       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 789        |\n",
      "|    iterations           | 37         |\n",
      "|    time_elapsed         | 11         |\n",
      "|    total_timesteps      | 9472       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.31834492 |\n",
      "|    clip_fraction        | 0.271      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.262     |\n",
      "|    explained_variance   | -0.632     |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.147     |\n",
      "|    n_updates            | 684        |\n",
      "|    policy_gradient_loss | -0.0995    |\n",
      "|    value_loss           | 0.0518     |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=9500, episode_reward=54.83 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 54.8      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 9500      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.4169852 |\n",
      "|    clip_fraction        | 0.272     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.252    |\n",
      "|    explained_variance   | 0.585     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.12     |\n",
      "|    n_updates            | 703       |\n",
      "|    policy_gradient_loss | -0.11     |\n",
      "|    value_loss           | 0.259     |\n",
      "---------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 52.4     |\n",
      "| time/              |          |\n",
      "|    fps             | 784      |\n",
      "|    iterations      | 38       |\n",
      "|    time_elapsed    | 12       |\n",
      "|    total_timesteps | 9728     |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 54.1      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 789       |\n",
      "|    iterations           | 39        |\n",
      "|    time_elapsed         | 12        |\n",
      "|    total_timesteps      | 9984      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.4388823 |\n",
      "|    clip_fraction        | 0.282     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.224    |\n",
      "|    explained_variance   | 0.714     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.128    |\n",
      "|    n_updates            | 722       |\n",
      "|    policy_gradient_loss | -0.112    |\n",
      "|    value_loss           | 0.151     |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=10000, episode_reward=62.00 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 230        |\n",
      "|    mean_reward          | 62         |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 10000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.22714117 |\n",
      "|    clip_fraction        | 0.233      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.289     |\n",
      "|    explained_variance   | 0.105      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0989    |\n",
      "|    n_updates            | 741        |\n",
      "|    policy_gradient_loss | -0.0953    |\n",
      "|    value_loss           | 0.176      |\n",
      "----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 54.1     |\n",
      "| time/              |          |\n",
      "|    fps             | 786      |\n",
      "|    iterations      | 40       |\n",
      "|    time_elapsed    | 13       |\n",
      "|    total_timesteps | 10240    |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 54.1       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 791        |\n",
      "|    iterations           | 41         |\n",
      "|    time_elapsed         | 13         |\n",
      "|    total_timesteps      | 10496      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.62049115 |\n",
      "|    clip_fraction        | 0.281      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.249     |\n",
      "|    explained_variance   | 0.635      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.111     |\n",
      "|    n_updates            | 760        |\n",
      "|    policy_gradient_loss | -0.104     |\n",
      "|    value_loss           | 0.0954     |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=10500, episode_reward=71.05 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 230        |\n",
      "|    mean_reward          | 71         |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 10500      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.52078545 |\n",
      "|    clip_fraction        | 0.263      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.207     |\n",
      "|    explained_variance   | 0.649      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.128     |\n",
      "|    n_updates            | 779        |\n",
      "|    policy_gradient_loss | -0.1       |\n",
      "|    value_loss           | 0.0882     |\n",
      "----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 55.5     |\n",
      "| time/              |          |\n",
      "|    fps             | 784      |\n",
      "|    iterations      | 42       |\n",
      "|    time_elapsed    | 13       |\n",
      "|    total_timesteps | 10752    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=11000, episode_reward=64.18 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 230        |\n",
      "|    mean_reward          | 64.2       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 11000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.36261612 |\n",
      "|    clip_fraction        | 0.305      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.239     |\n",
      "|    explained_variance   | 0.621      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.124     |\n",
      "|    n_updates            | 798        |\n",
      "|    policy_gradient_loss | -0.0865    |\n",
      "|    value_loss           | 0.356      |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 55.5     |\n",
      "| time/              |          |\n",
      "|    fps             | 781      |\n",
      "|    iterations      | 43       |\n",
      "|    time_elapsed    | 14       |\n",
      "|    total_timesteps | 11008    |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 55.5       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 785        |\n",
      "|    iterations           | 44         |\n",
      "|    time_elapsed         | 14         |\n",
      "|    total_timesteps      | 11264      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.49784437 |\n",
      "|    clip_fraction        | 0.239      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.218     |\n",
      "|    explained_variance   | -0.185     |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.11      |\n",
      "|    n_updates            | 817        |\n",
      "|    policy_gradient_loss | -0.099     |\n",
      "|    value_loss           | 0.077      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=11500, episode_reward=69.59 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 230        |\n",
      "|    mean_reward          | 69.6       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 11500      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.33562875 |\n",
      "|    clip_fraction        | 0.207      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.18      |\n",
      "|    explained_variance   | 0.638      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0587    |\n",
      "|    n_updates            | 836        |\n",
      "|    policy_gradient_loss | -0.0836    |\n",
      "|    value_loss           | 0.173      |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 55.5     |\n",
      "| time/              |          |\n",
      "|    fps             | 780      |\n",
      "|    iterations      | 45       |\n",
      "|    time_elapsed    | 14       |\n",
      "|    total_timesteps | 11520    |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 55.7       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 783        |\n",
      "|    iterations           | 46         |\n",
      "|    time_elapsed         | 15         |\n",
      "|    total_timesteps      | 11776      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.30536476 |\n",
      "|    clip_fraction        | 0.211      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.179     |\n",
      "|    explained_variance   | 0.619      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.102     |\n",
      "|    n_updates            | 855        |\n",
      "|    policy_gradient_loss | -0.0799    |\n",
      "|    value_loss           | 0.317      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=12000, episode_reward=88.80 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 230        |\n",
      "|    mean_reward          | 88.8       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 12000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.38810608 |\n",
      "|    clip_fraction        | 0.275      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.249     |\n",
      "|    explained_variance   | -0.0534    |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.116     |\n",
      "|    n_updates            | 874        |\n",
      "|    policy_gradient_loss | -0.103     |\n",
      "|    value_loss           | 0.0794     |\n",
      "----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 55.7     |\n",
      "| time/              |          |\n",
      "|    fps             | 777      |\n",
      "|    iterations      | 47       |\n",
      "|    time_elapsed    | 15       |\n",
      "|    total_timesteps | 12032    |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 55.7      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 782       |\n",
      "|    iterations           | 48        |\n",
      "|    time_elapsed         | 15        |\n",
      "|    total_timesteps      | 12288     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.5347576 |\n",
      "|    clip_fraction        | 0.303     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.221    |\n",
      "|    explained_variance   | 0.85      |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.186    |\n",
      "|    n_updates            | 893       |\n",
      "|    policy_gradient_loss | -0.121    |\n",
      "|    value_loss           | 0.0955    |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=12500, episode_reward=10.37 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 230        |\n",
      "|    mean_reward          | 10.4       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 12500      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.42819208 |\n",
      "|    clip_fraction        | 0.248      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.203     |\n",
      "|    explained_variance   | 0.537      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.124     |\n",
      "|    n_updates            | 912        |\n",
      "|    policy_gradient_loss | -0.104     |\n",
      "|    value_loss           | 0.153      |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 54.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 774      |\n",
      "|    iterations      | 49       |\n",
      "|    time_elapsed    | 16       |\n",
      "|    total_timesteps | 12544    |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 54.9       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 779        |\n",
      "|    iterations           | 50         |\n",
      "|    time_elapsed         | 16         |\n",
      "|    total_timesteps      | 12800      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.27086887 |\n",
      "|    clip_fraction        | 0.225      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.289     |\n",
      "|    explained_variance   | 0.641      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0298    |\n",
      "|    n_updates            | 931        |\n",
      "|    policy_gradient_loss | -0.075     |\n",
      "|    value_loss           | 0.345      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=13000, episode_reward=41.84 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 41.8      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 13000     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.4635744 |\n",
      "|    clip_fraction        | 0.253     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.212    |\n",
      "|    explained_variance   | 0.371     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.108    |\n",
      "|    n_updates            | 950       |\n",
      "|    policy_gradient_loss | -0.106    |\n",
      "|    value_loss           | 0.0631    |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 54.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 777      |\n",
      "|    iterations      | 51       |\n",
      "|    time_elapsed    | 16       |\n",
      "|    total_timesteps | 13056    |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 54.9       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 782        |\n",
      "|    iterations           | 52         |\n",
      "|    time_elapsed         | 17         |\n",
      "|    total_timesteps      | 13312      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.45221555 |\n",
      "|    clip_fraction        | 0.259      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.203     |\n",
      "|    explained_variance   | 0.728      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0914    |\n",
      "|    n_updates            | 969        |\n",
      "|    policy_gradient_loss | -0.101     |\n",
      "|    value_loss           | 0.148      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=13500, episode_reward=13.63 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 230        |\n",
      "|    mean_reward          | 13.6       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 13500      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.25354746 |\n",
      "|    clip_fraction        | 0.133      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.154     |\n",
      "|    explained_variance   | 0.361      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0273    |\n",
      "|    n_updates            | 988        |\n",
      "|    policy_gradient_loss | -0.046     |\n",
      "|    value_loss           | 0.665      |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 55.7     |\n",
      "| time/              |          |\n",
      "|    fps             | 781      |\n",
      "|    iterations      | 53       |\n",
      "|    time_elapsed    | 17       |\n",
      "|    total_timesteps | 13568    |\n",
      "---------------------------------\n",
      "--------------------------------------\n",
      "| rollout/                |          |\n",
      "|    ep_len_mean          | 889      |\n",
      "|    ep_rew_mean          | 55.7     |\n",
      "| time/                   |          |\n",
      "|    fps                  | 786      |\n",
      "|    iterations           | 54       |\n",
      "|    time_elapsed         | 17       |\n",
      "|    total_timesteps      | 13824    |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 0.580595 |\n",
      "|    clip_fraction        | 0.304    |\n",
      "|    clip_range           | 0.4      |\n",
      "|    entropy_loss         | -0.242   |\n",
      "|    explained_variance   | -0.521   |\n",
      "|    learning_rate        | 0.000389 |\n",
      "|    loss                 | -0.207   |\n",
      "|    n_updates            | 1007     |\n",
      "|    policy_gradient_loss | -0.11    |\n",
      "|    value_loss           | 0.129    |\n",
      "--------------------------------------\n",
      "Eval num_timesteps=14000, episode_reward=48.69 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 230        |\n",
      "|    mean_reward          | 48.7       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 14000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.55200565 |\n",
      "|    clip_fraction        | 0.22       |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.171     |\n",
      "|    explained_variance   | 0.5        |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0621    |\n",
      "|    n_updates            | 1026       |\n",
      "|    policy_gradient_loss | -0.102     |\n",
      "|    value_loss           | 0.225      |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 55.7     |\n",
      "| time/              |          |\n",
      "|    fps             | 784      |\n",
      "|    iterations      | 55       |\n",
      "|    time_elapsed    | 17       |\n",
      "|    total_timesteps | 14080    |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 56         |\n",
      "| time/                   |            |\n",
      "|    fps                  | 784        |\n",
      "|    iterations           | 56         |\n",
      "|    time_elapsed         | 18         |\n",
      "|    total_timesteps      | 14336      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.56469667 |\n",
      "|    clip_fraction        | 0.265      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.186     |\n",
      "|    explained_variance   | 0.477      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.139     |\n",
      "|    n_updates            | 1045       |\n",
      "|    policy_gradient_loss | -0.103     |\n",
      "|    value_loss           | 0.154      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=14500, episode_reward=38.14 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 230        |\n",
      "|    mean_reward          | 38.1       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 14500      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.18159604 |\n",
      "|    clip_fraction        | 0.208      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.218     |\n",
      "|    explained_variance   | 0.489      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0402    |\n",
      "|    n_updates            | 1064       |\n",
      "|    policy_gradient_loss | -0.0549    |\n",
      "|    value_loss           | 0.747      |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 56       |\n",
      "| time/              |          |\n",
      "|    fps             | 783      |\n",
      "|    iterations      | 57       |\n",
      "|    time_elapsed    | 18       |\n",
      "|    total_timesteps | 14592    |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 56         |\n",
      "| time/                   |            |\n",
      "|    fps                  | 788        |\n",
      "|    iterations           | 58         |\n",
      "|    time_elapsed         | 18         |\n",
      "|    total_timesteps      | 14848      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.47644895 |\n",
      "|    clip_fraction        | 0.239      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.162     |\n",
      "|    explained_variance   | 0.232      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.111     |\n",
      "|    n_updates            | 1083       |\n",
      "|    policy_gradient_loss | -0.0913    |\n",
      "|    value_loss           | 0.069      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=15000, episode_reward=19.70 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 19.7      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 15000     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.3948449 |\n",
      "|    clip_fraction        | 0.194     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.147    |\n",
      "|    explained_variance   | 0.723     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.11     |\n",
      "|    n_updates            | 1102      |\n",
      "|    policy_gradient_loss | -0.0664   |\n",
      "|    value_loss           | 0.287     |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 56       |\n",
      "| time/              |          |\n",
      "|    fps             | 785      |\n",
      "|    iterations      | 59       |\n",
      "|    time_elapsed    | 19       |\n",
      "|    total_timesteps | 15104    |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 57.3      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 789       |\n",
      "|    iterations           | 60        |\n",
      "|    time_elapsed         | 19        |\n",
      "|    total_timesteps      | 15360     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.5587884 |\n",
      "|    clip_fraction        | 0.241     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.184    |\n",
      "|    explained_variance   | 0.571     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | 0.00384   |\n",
      "|    n_updates            | 1121      |\n",
      "|    policy_gradient_loss | -0.0833   |\n",
      "|    value_loss           | 0.61      |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=15500, episode_reward=46.13 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 230        |\n",
      "|    mean_reward          | 46.1       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 15500      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.61436796 |\n",
      "|    clip_fraction        | 0.248      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.194     |\n",
      "|    explained_variance   | -0.665     |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.162     |\n",
      "|    n_updates            | 1140       |\n",
      "|    policy_gradient_loss | -0.0839    |\n",
      "|    value_loss           | 0.0787     |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 57.3     |\n",
      "| time/              |          |\n",
      "|    fps             | 788      |\n",
      "|    iterations      | 61       |\n",
      "|    time_elapsed    | 19       |\n",
      "|    total_timesteps | 15616    |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 57.3      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 793       |\n",
      "|    iterations           | 62        |\n",
      "|    time_elapsed         | 20        |\n",
      "|    total_timesteps      | 15872     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.7589702 |\n",
      "|    clip_fraction        | 0.196     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.128    |\n",
      "|    explained_variance   | 0.429     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.088    |\n",
      "|    n_updates            | 1159      |\n",
      "|    policy_gradient_loss | -0.083    |\n",
      "|    value_loss           | 0.206     |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=16000, episode_reward=3.22 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 3.22      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 16000     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.2062517 |\n",
      "|    clip_fraction        | 0.243     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.165    |\n",
      "|    explained_variance   | 0.48      |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.154    |\n",
      "|    n_updates            | 1178      |\n",
      "|    policy_gradient_loss | -0.0876   |\n",
      "|    value_loss           | 0.178     |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 57.4     |\n",
      "| time/              |          |\n",
      "|    fps             | 791      |\n",
      "|    iterations      | 63       |\n",
      "|    time_elapsed    | 20       |\n",
      "|    total_timesteps | 16128    |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 57.4       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 794        |\n",
      "|    iterations           | 64         |\n",
      "|    time_elapsed         | 20         |\n",
      "|    total_timesteps      | 16384      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.46917313 |\n",
      "|    clip_fraction        | 0.228      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.228     |\n",
      "|    explained_variance   | 0.534      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.112     |\n",
      "|    n_updates            | 1197       |\n",
      "|    policy_gradient_loss | -0.0687    |\n",
      "|    value_loss           | 0.255      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=16500, episode_reward=13.15 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 230        |\n",
      "|    mean_reward          | 13.1       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 16500      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.52369374 |\n",
      "|    clip_fraction        | 0.214      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.131     |\n",
      "|    explained_variance   | 0.584      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.118     |\n",
      "|    n_updates            | 1216       |\n",
      "|    policy_gradient_loss | -0.0862    |\n",
      "|    value_loss           | 0.0596     |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 57.4     |\n",
      "| time/              |          |\n",
      "|    fps             | 791      |\n",
      "|    iterations      | 65       |\n",
      "|    time_elapsed    | 21       |\n",
      "|    total_timesteps | 16640    |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 57.9       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 793        |\n",
      "|    iterations           | 66         |\n",
      "|    time_elapsed         | 21         |\n",
      "|    total_timesteps      | 16896      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.47942874 |\n",
      "|    clip_fraction        | 0.266      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.199     |\n",
      "|    explained_variance   | 0.558      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.144     |\n",
      "|    n_updates            | 1235       |\n",
      "|    policy_gradient_loss | -0.106     |\n",
      "|    value_loss           | 0.259      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=17000, episode_reward=15.40 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 15.4      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 17000     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.6041026 |\n",
      "|    clip_fraction        | 0.264     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.191    |\n",
      "|    explained_variance   | 0.743     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.138    |\n",
      "|    n_updates            | 1254      |\n",
      "|    policy_gradient_loss | -0.0913   |\n",
      "|    value_loss           | 0.181     |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 57.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 788      |\n",
      "|    iterations      | 67       |\n",
      "|    time_elapsed    | 21       |\n",
      "|    total_timesteps | 17152    |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 57.9      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 789       |\n",
      "|    iterations           | 68        |\n",
      "|    time_elapsed         | 22        |\n",
      "|    total_timesteps      | 17408     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.5113343 |\n",
      "|    clip_fraction        | 0.257     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.191    |\n",
      "|    explained_variance   | -0.161    |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.116    |\n",
      "|    n_updates            | 1273      |\n",
      "|    policy_gradient_loss | -0.0937   |\n",
      "|    value_loss           | 0.0925    |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=17500, episode_reward=-12.48 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | -12.5     |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 17500     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.8953204 |\n",
      "|    clip_fraction        | 0.208     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.166    |\n",
      "|    explained_variance   | 0.235     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.121    |\n",
      "|    n_updates            | 1292      |\n",
      "|    policy_gradient_loss | -0.0732   |\n",
      "|    value_loss           | 0.618     |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 57.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 787      |\n",
      "|    iterations      | 69       |\n",
      "|    time_elapsed    | 22       |\n",
      "|    total_timesteps | 17664    |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 57.6      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 788       |\n",
      "|    iterations           | 70        |\n",
      "|    time_elapsed         | 22        |\n",
      "|    total_timesteps      | 17920     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.5364358 |\n",
      "|    clip_fraction        | 0.193     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.135    |\n",
      "|    explained_variance   | 0.48      |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.113    |\n",
      "|    n_updates            | 1311      |\n",
      "|    policy_gradient_loss | -0.0744   |\n",
      "|    value_loss           | 0.151     |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=18000, episode_reward=28.36 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 230        |\n",
      "|    mean_reward          | 28.4       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 18000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.22072768 |\n",
      "|    clip_fraction        | 0.139      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.159     |\n",
      "|    explained_variance   | 0.297      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0756    |\n",
      "|    n_updates            | 1330       |\n",
      "|    policy_gradient_loss | -0.0563    |\n",
      "|    value_loss           | 0.231      |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 57.6     |\n",
      "| time/              |          |\n",
      "|    fps             | 785      |\n",
      "|    iterations      | 71       |\n",
      "|    time_elapsed    | 23       |\n",
      "|    total_timesteps | 18176    |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 57.6      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 787       |\n",
      "|    iterations           | 72        |\n",
      "|    time_elapsed         | 23        |\n",
      "|    total_timesteps      | 18432     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.6516649 |\n",
      "|    clip_fraction        | 0.201     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.127    |\n",
      "|    explained_variance   | 0.0735    |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.136    |\n",
      "|    n_updates            | 1349      |\n",
      "|    policy_gradient_loss | -0.0836   |\n",
      "|    value_loss           | 0.0746    |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=18500, episode_reward=-7.64 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | -7.64     |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 18500     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.8294361 |\n",
      "|    clip_fraction        | 0.286     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.161    |\n",
      "|    explained_variance   | 0.388     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.166    |\n",
      "|    n_updates            | 1368      |\n",
      "|    policy_gradient_loss | -0.104    |\n",
      "|    value_loss           | 0.107     |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 57.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 784      |\n",
      "|    iterations      | 73       |\n",
      "|    time_elapsed    | 23       |\n",
      "|    total_timesteps | 18688    |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 57.9       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 787        |\n",
      "|    iterations           | 74         |\n",
      "|    time_elapsed         | 24         |\n",
      "|    total_timesteps      | 18944      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.86325914 |\n",
      "|    clip_fraction        | 0.221      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.151     |\n",
      "|    explained_variance   | 0.396      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0961    |\n",
      "|    n_updates            | 1387       |\n",
      "|    policy_gradient_loss | -0.0764    |\n",
      "|    value_loss           | 0.207      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=19000, episode_reward=53.15 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "--------------------------------------\n",
      "| eval/                   |          |\n",
      "|    mean_ep_length       | 230      |\n",
      "|    mean_reward          | 53.2     |\n",
      "| time/                   |          |\n",
      "|    total_timesteps      | 19000    |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 0.687615 |\n",
      "|    clip_fraction        | 0.233    |\n",
      "|    clip_range           | 0.4      |\n",
      "|    entropy_loss         | -0.131   |\n",
      "|    explained_variance   | -0.195   |\n",
      "|    learning_rate        | 0.000389 |\n",
      "|    loss                 | -0.142   |\n",
      "|    n_updates            | 1406     |\n",
      "|    policy_gradient_loss | -0.0927  |\n",
      "|    value_loss           | 0.0826   |\n",
      "--------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 57.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 782      |\n",
      "|    iterations      | 75       |\n",
      "|    time_elapsed    | 24       |\n",
      "|    total_timesteps | 19200    |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 57.9       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 778        |\n",
      "|    iterations           | 76         |\n",
      "|    time_elapsed         | 25         |\n",
      "|    total_timesteps      | 19456      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.46751738 |\n",
      "|    clip_fraction        | 0.219      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.148     |\n",
      "|    explained_variance   | 0.603      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0948    |\n",
      "|    n_updates            | 1425       |\n",
      "|    policy_gradient_loss | -0.0943    |\n",
      "|    value_loss           | 0.102      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=19500, episode_reward=41.72 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "--------------------------------------\n",
      "| eval/                   |          |\n",
      "|    mean_ep_length       | 230      |\n",
      "|    mean_reward          | 41.7     |\n",
      "| time/                   |          |\n",
      "|    total_timesteps      | 19500    |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 2.440005 |\n",
      "|    clip_fraction        | 0.207    |\n",
      "|    clip_range           | 0.4      |\n",
      "|    entropy_loss         | -0.104   |\n",
      "|    explained_variance   | 0.545    |\n",
      "|    learning_rate        | 0.000389 |\n",
      "|    loss                 | -0.139   |\n",
      "|    n_updates            | 1444     |\n",
      "|    policy_gradient_loss | -0.0817  |\n",
      "|    value_loss           | 0.0903   |\n",
      "--------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 57.6     |\n",
      "| time/              |          |\n",
      "|    fps             | 776      |\n",
      "|    iterations      | 77       |\n",
      "|    time_elapsed    | 25       |\n",
      "|    total_timesteps | 19712    |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 57.6      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 780       |\n",
      "|    iterations           | 78        |\n",
      "|    time_elapsed         | 25        |\n",
      "|    total_timesteps      | 19968     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.5373543 |\n",
      "|    clip_fraction        | 0.219     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.141    |\n",
      "|    explained_variance   | 0.703     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.132    |\n",
      "|    n_updates            | 1463      |\n",
      "|    policy_gradient_loss | -0.0788   |\n",
      "|    value_loss           | 0.146     |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=20000, episode_reward=80.62 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 230        |\n",
      "|    mean_reward          | 80.6       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 20000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.50193226 |\n",
      "|    clip_fraction        | 0.186      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.106     |\n",
      "|    explained_variance   | 0.637      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.115     |\n",
      "|    n_updates            | 1482       |\n",
      "|    policy_gradient_loss | -0.0677    |\n",
      "|    value_loss           | 0.0728     |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 57.6     |\n",
      "| time/              |          |\n",
      "|    fps             | 778      |\n",
      "|    iterations      | 79       |\n",
      "|    time_elapsed    | 25       |\n",
      "|    total_timesteps | 20224    |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 57.4      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 781       |\n",
      "|    iterations           | 80        |\n",
      "|    time_elapsed         | 26        |\n",
      "|    total_timesteps      | 20480     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.4161189 |\n",
      "|    clip_fraction        | 0.202     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.138    |\n",
      "|    explained_variance   | 0.426     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.109    |\n",
      "|    n_updates            | 1501      |\n",
      "|    policy_gradient_loss | -0.0837   |\n",
      "|    value_loss           | 0.0971    |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=20500, episode_reward=-3.63 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | -3.63     |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 20500     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.8659105 |\n",
      "|    clip_fraction        | 0.233     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.161    |\n",
      "|    explained_variance   | 0.592     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0446   |\n",
      "|    n_updates            | 1520      |\n",
      "|    policy_gradient_loss | -0.0689   |\n",
      "|    value_loss           | 0.175     |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 57.4     |\n",
      "| time/              |          |\n",
      "|    fps             | 780      |\n",
      "|    iterations      | 81       |\n",
      "|    time_elapsed    | 26       |\n",
      "|    total_timesteps | 20736    |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 57.4       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 783        |\n",
      "|    iterations           | 82         |\n",
      "|    time_elapsed         | 26         |\n",
      "|    total_timesteps      | 20992      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.58364844 |\n",
      "|    clip_fraction        | 0.22       |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.113     |\n",
      "|    explained_variance   | 0.0183     |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0888    |\n",
      "|    n_updates            | 1539       |\n",
      "|    policy_gradient_loss | -0.0981    |\n",
      "|    value_loss           | 0.0554     |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=21000, episode_reward=-6.20 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | -6.2      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 21000     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.5181951 |\n",
      "|    clip_fraction        | 0.223     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.137    |\n",
      "|    explained_variance   | 0.721     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.12     |\n",
      "|    n_updates            | 1558      |\n",
      "|    policy_gradient_loss | -0.0861   |\n",
      "|    value_loss           | 0.14      |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 57.4     |\n",
      "| time/              |          |\n",
      "|    fps             | 780      |\n",
      "|    iterations      | 83       |\n",
      "|    time_elapsed    | 27       |\n",
      "|    total_timesteps | 21248    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=21500, episode_reward=-9.45 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | -9.45     |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 21500     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.6581222 |\n",
      "|    clip_fraction        | 0.235     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.137    |\n",
      "|    explained_variance   | 0.74      |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.177    |\n",
      "|    n_updates            | 1577      |\n",
      "|    policy_gradient_loss | -0.0953   |\n",
      "|    value_loss           | 0.0813    |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 56.5     |\n",
      "| time/              |          |\n",
      "|    fps             | 780      |\n",
      "|    iterations      | 84       |\n",
      "|    time_elapsed    | 27       |\n",
      "|    total_timesteps | 21504    |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 56.5      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 783       |\n",
      "|    iterations           | 85        |\n",
      "|    time_elapsed         | 27        |\n",
      "|    total_timesteps      | 21760     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.8305664 |\n",
      "|    clip_fraction        | 0.256     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.177    |\n",
      "|    explained_variance   | -0.113    |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.119    |\n",
      "|    n_updates            | 1596      |\n",
      "|    policy_gradient_loss | -0.0822   |\n",
      "|    value_loss           | 0.193     |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=22000, episode_reward=42.41 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 42.4      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 22000     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.0028045 |\n",
      "|    clip_fraction        | 0.214     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.123    |\n",
      "|    explained_variance   | 0.666     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.141    |\n",
      "|    n_updates            | 1615      |\n",
      "|    policy_gradient_loss | -0.0787   |\n",
      "|    value_loss           | 0.0607    |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 56.5     |\n",
      "| time/              |          |\n",
      "|    fps             | 782      |\n",
      "|    iterations      | 86       |\n",
      "|    time_elapsed    | 28       |\n",
      "|    total_timesteps | 22016    |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 56.6      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 785       |\n",
      "|    iterations           | 87        |\n",
      "|    time_elapsed         | 28        |\n",
      "|    total_timesteps      | 22272     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.6098762 |\n",
      "|    clip_fraction        | 0.184     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.133    |\n",
      "|    explained_variance   | 0.389     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0251   |\n",
      "|    n_updates            | 1634      |\n",
      "|    policy_gradient_loss | -0.0677   |\n",
      "|    value_loss           | 0.18      |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=22500, episode_reward=27.18 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "--------------------------------------\n",
      "| eval/                   |          |\n",
      "|    mean_ep_length       | 230      |\n",
      "|    mean_reward          | 27.2     |\n",
      "| time/                   |          |\n",
      "|    total_timesteps      | 22500    |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 1.047905 |\n",
      "|    clip_fraction        | 0.268    |\n",
      "|    clip_range           | 0.4      |\n",
      "|    entropy_loss         | -0.162   |\n",
      "|    explained_variance   | 0.339    |\n",
      "|    learning_rate        | 0.000389 |\n",
      "|    loss                 | -0.0864  |\n",
      "|    n_updates            | 1653     |\n",
      "|    policy_gradient_loss | -0.0886  |\n",
      "|    value_loss           | 0.317    |\n",
      "--------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 56.6     |\n",
      "| time/              |          |\n",
      "|    fps             | 784      |\n",
      "|    iterations      | 88       |\n",
      "|    time_elapsed    | 28       |\n",
      "|    total_timesteps | 22528    |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 56.6       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 788        |\n",
      "|    iterations           | 89         |\n",
      "|    time_elapsed         | 28         |\n",
      "|    total_timesteps      | 22784      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.58704007 |\n",
      "|    clip_fraction        | 0.281      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.147     |\n",
      "|    explained_variance   | 0.377      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0863    |\n",
      "|    n_updates            | 1672       |\n",
      "|    policy_gradient_loss | -0.0911    |\n",
      "|    value_loss           | 0.0645     |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=23000, episode_reward=26.56 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 230        |\n",
      "|    mean_reward          | 26.6       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 23000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.66379297 |\n",
      "|    clip_fraction        | 0.199      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.146     |\n",
      "|    explained_variance   | 0.713      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.136     |\n",
      "|    n_updates            | 1691       |\n",
      "|    policy_gradient_loss | -0.0928    |\n",
      "|    value_loss           | 0.163      |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 56.6     |\n",
      "| time/              |          |\n",
      "|    fps             | 781      |\n",
      "|    iterations      | 90       |\n",
      "|    time_elapsed    | 29       |\n",
      "|    total_timesteps | 23040    |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 56.1       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 784        |\n",
      "|    iterations           | 91         |\n",
      "|    time_elapsed         | 29         |\n",
      "|    total_timesteps      | 23296      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.85413814 |\n",
      "|    clip_fraction        | 0.263      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.128     |\n",
      "|    explained_variance   | 0.711      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.164     |\n",
      "|    n_updates            | 1710       |\n",
      "|    policy_gradient_loss | -0.106     |\n",
      "|    value_loss           | 0.112      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=23500, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 0         |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 23500     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.7080949 |\n",
      "|    clip_fraction        | 0.295     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.153    |\n",
      "|    explained_variance   | 0.424     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.136    |\n",
      "|    n_updates            | 1729      |\n",
      "|    policy_gradient_loss | -0.0968   |\n",
      "|    value_loss           | 0.0951    |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 56.1     |\n",
      "| time/              |          |\n",
      "|    fps             | 783      |\n",
      "|    iterations      | 92       |\n",
      "|    time_elapsed    | 30       |\n",
      "|    total_timesteps | 23552    |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 56.1       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 786        |\n",
      "|    iterations           | 93         |\n",
      "|    time_elapsed         | 30         |\n",
      "|    total_timesteps      | 23808      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.76222306 |\n",
      "|    clip_fraction        | 0.219      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.106     |\n",
      "|    explained_variance   | 0.686      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.107     |\n",
      "|    n_updates            | 1748       |\n",
      "|    policy_gradient_loss | -0.0758    |\n",
      "|    value_loss           | 0.0791     |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=24000, episode_reward=15.81 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 15.8      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 24000     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.5695548 |\n",
      "|    clip_fraction        | 0.233     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.128    |\n",
      "|    explained_variance   | 0.602     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.101    |\n",
      "|    n_updates            | 1767      |\n",
      "|    policy_gradient_loss | -0.102    |\n",
      "|    value_loss           | 0.129     |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 56.4     |\n",
      "| time/              |          |\n",
      "|    fps             | 784      |\n",
      "|    iterations      | 94       |\n",
      "|    time_elapsed    | 30       |\n",
      "|    total_timesteps | 24064    |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 56.4       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 786        |\n",
      "|    iterations           | 95         |\n",
      "|    time_elapsed         | 30         |\n",
      "|    total_timesteps      | 24320      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.81326175 |\n",
      "|    clip_fraction        | 0.249      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.155     |\n",
      "|    explained_variance   | 0.456      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0354    |\n",
      "|    n_updates            | 1786       |\n",
      "|    policy_gradient_loss | -0.064     |\n",
      "|    value_loss           | 0.524      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=24500, episode_reward=-1.70 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "--------------------------------------\n",
      "| eval/                   |          |\n",
      "|    mean_ep_length       | 230      |\n",
      "|    mean_reward          | -1.7     |\n",
      "| time/                   |          |\n",
      "|    total_timesteps      | 24500    |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 0.511568 |\n",
      "|    clip_fraction        | 0.261    |\n",
      "|    clip_range           | 0.4      |\n",
      "|    entropy_loss         | -0.198   |\n",
      "|    explained_variance   | -0.291   |\n",
      "|    learning_rate        | 0.000389 |\n",
      "|    loss                 | -0.129   |\n",
      "|    n_updates            | 1805     |\n",
      "|    policy_gradient_loss | -0.0857  |\n",
      "|    value_loss           | 0.0832   |\n",
      "--------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 56.4     |\n",
      "| time/              |          |\n",
      "|    fps             | 784      |\n",
      "|    iterations      | 96       |\n",
      "|    time_elapsed    | 31       |\n",
      "|    total_timesteps | 24576    |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 56.4       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 781        |\n",
      "|    iterations           | 97         |\n",
      "|    time_elapsed         | 31         |\n",
      "|    total_timesteps      | 24832      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.54931474 |\n",
      "|    clip_fraction        | 0.182      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.117     |\n",
      "|    explained_variance   | 0.358      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0524    |\n",
      "|    n_updates            | 1824       |\n",
      "|    policy_gradient_loss | -0.0479    |\n",
      "|    value_loss           | 0.257      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=25000, episode_reward=23.12 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 23.1      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 25000     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.6152675 |\n",
      "|    clip_fraction        | 0.161     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.141    |\n",
      "|    explained_variance   | 0.57      |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.109    |\n",
      "|    n_updates            | 1843      |\n",
      "|    policy_gradient_loss | -0.0628   |\n",
      "|    value_loss           | 0.298     |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 56.8     |\n",
      "| time/              |          |\n",
      "|    fps             | 779      |\n",
      "|    iterations      | 98       |\n",
      "|    time_elapsed    | 32       |\n",
      "|    total_timesteps | 25088    |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 56.8       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 781        |\n",
      "|    iterations           | 99         |\n",
      "|    time_elapsed         | 32         |\n",
      "|    total_timesteps      | 25344      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.62610245 |\n",
      "|    clip_fraction        | 0.259      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.21      |\n",
      "|    explained_variance   | -0.365     |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0842    |\n",
      "|    n_updates            | 1862       |\n",
      "|    policy_gradient_loss | -0.0961    |\n",
      "|    value_loss           | 0.0906     |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=25500, episode_reward=34.00 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 230        |\n",
      "|    mean_reward          | 34         |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 25500      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.93240744 |\n",
      "|    clip_fraction        | 0.172      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.121     |\n",
      "|    explained_variance   | 0.414      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0885    |\n",
      "|    n_updates            | 1881       |\n",
      "|    policy_gradient_loss | -0.0725    |\n",
      "|    value_loss           | 0.344      |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 56.8     |\n",
      "| time/              |          |\n",
      "|    fps             | 777      |\n",
      "|    iterations      | 100      |\n",
      "|    time_elapsed    | 32       |\n",
      "|    total_timesteps | 25600    |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 57.2      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 779       |\n",
      "|    iterations           | 101       |\n",
      "|    time_elapsed         | 33        |\n",
      "|    total_timesteps      | 25856     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.5928326 |\n",
      "|    clip_fraction        | 0.212     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.132    |\n",
      "|    explained_variance   | 0.515     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.169    |\n",
      "|    n_updates            | 1900      |\n",
      "|    policy_gradient_loss | -0.0874   |\n",
      "|    value_loss           | 0.115     |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=26000, episode_reward=6.90 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 6.9       |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 26000     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.7423669 |\n",
      "|    clip_fraction        | 0.275     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.175    |\n",
      "|    explained_variance   | 0.703     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.112    |\n",
      "|    n_updates            | 1919      |\n",
      "|    policy_gradient_loss | -0.0789   |\n",
      "|    value_loss           | 0.208     |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 57.2     |\n",
      "| time/              |          |\n",
      "|    fps             | 778      |\n",
      "|    iterations      | 102      |\n",
      "|    time_elapsed    | 33       |\n",
      "|    total_timesteps | 26112    |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 57.2       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 777        |\n",
      "|    iterations           | 103        |\n",
      "|    time_elapsed         | 33         |\n",
      "|    total_timesteps      | 26368      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.45659065 |\n",
      "|    clip_fraction        | 0.241      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.168     |\n",
      "|    explained_variance   | -0.352     |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0519    |\n",
      "|    n_updates            | 1938       |\n",
      "|    policy_gradient_loss | -0.0905    |\n",
      "|    value_loss           | 0.0693     |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=26500, episode_reward=18.45 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 18.5      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 26500     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.6105207 |\n",
      "|    clip_fraction        | 0.196     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.122    |\n",
      "|    explained_variance   | 0.692     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0987   |\n",
      "|    n_updates            | 1957      |\n",
      "|    policy_gradient_loss | -0.0878   |\n",
      "|    value_loss           | 0.136     |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 57.2     |\n",
      "| time/              |          |\n",
      "|    fps             | 775      |\n",
      "|    iterations      | 104      |\n",
      "|    time_elapsed    | 34       |\n",
      "|    total_timesteps | 26624    |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 57.6       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 776        |\n",
      "|    iterations           | 105        |\n",
      "|    time_elapsed         | 34         |\n",
      "|    total_timesteps      | 26880      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.60159063 |\n",
      "|    clip_fraction        | 0.207      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.132     |\n",
      "|    explained_variance   | 0.811      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.122     |\n",
      "|    n_updates            | 1976       |\n",
      "|    policy_gradient_loss | -0.0853    |\n",
      "|    value_loss           | 0.122      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=27000, episode_reward=11.55 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "--------------------------------------\n",
      "| eval/                   |          |\n",
      "|    mean_ep_length       | 230      |\n",
      "|    mean_reward          | 11.5     |\n",
      "| time/                   |          |\n",
      "|    total_timesteps      | 27000    |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 0.455292 |\n",
      "|    clip_fraction        | 0.239    |\n",
      "|    clip_range           | 0.4      |\n",
      "|    entropy_loss         | -0.177   |\n",
      "|    explained_variance   | -0.02    |\n",
      "|    learning_rate        | 0.000389 |\n",
      "|    loss                 | -0.0744  |\n",
      "|    n_updates            | 1995     |\n",
      "|    policy_gradient_loss | -0.0785  |\n",
      "|    value_loss           | 0.0808   |\n",
      "--------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 57.6     |\n",
      "| time/              |          |\n",
      "|    fps             | 774      |\n",
      "|    iterations      | 106      |\n",
      "|    time_elapsed    | 35       |\n",
      "|    total_timesteps | 27136    |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 57.6      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 775       |\n",
      "|    iterations           | 107       |\n",
      "|    time_elapsed         | 35        |\n",
      "|    total_timesteps      | 27392     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.7436659 |\n",
      "|    clip_fraction        | 0.256     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.146    |\n",
      "|    explained_variance   | 0.287     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.104    |\n",
      "|    n_updates            | 2014      |\n",
      "|    policy_gradient_loss | -0.0881   |\n",
      "|    value_loss           | 0.146     |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=27500, episode_reward=24.41 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 24.4      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 27500     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.3185465 |\n",
      "|    clip_fraction        | 0.278     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.14     |\n",
      "|    explained_variance   | 0.658     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.141    |\n",
      "|    n_updates            | 2033      |\n",
      "|    policy_gradient_loss | -0.0696   |\n",
      "|    value_loss           | 0.137     |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 58.1     |\n",
      "| time/              |          |\n",
      "|    fps             | 773      |\n",
      "|    iterations      | 108      |\n",
      "|    time_elapsed    | 35       |\n",
      "|    total_timesteps | 27648    |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 58.1      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 775       |\n",
      "|    iterations           | 109       |\n",
      "|    time_elapsed         | 35        |\n",
      "|    total_timesteps      | 27904     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.7076095 |\n",
      "|    clip_fraction        | 0.194     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.107    |\n",
      "|    explained_variance   | 0.229     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0152   |\n",
      "|    n_updates            | 2052      |\n",
      "|    policy_gradient_loss | -0.0699   |\n",
      "|    value_loss           | 0.22      |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=28000, episode_reward=34.08 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 34.1      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 28000     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.0891602 |\n",
      "|    clip_fraction        | 0.303     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.144    |\n",
      "|    explained_variance   | -0.721    |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.1      |\n",
      "|    n_updates            | 2071      |\n",
      "|    policy_gradient_loss | -0.106    |\n",
      "|    value_loss           | 0.0503    |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 58.1     |\n",
      "| time/              |          |\n",
      "|    fps             | 773      |\n",
      "|    iterations      | 110      |\n",
      "|    time_elapsed    | 36       |\n",
      "|    total_timesteps | 28160    |\n",
      "---------------------------------\n",
      "--------------------------------------\n",
      "| rollout/                |          |\n",
      "|    ep_len_mean          | 889      |\n",
      "|    ep_rew_mean          | 58.1     |\n",
      "| time/                   |          |\n",
      "|    fps                  | 776      |\n",
      "|    iterations           | 111      |\n",
      "|    time_elapsed         | 36       |\n",
      "|    total_timesteps      | 28416    |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 0.715273 |\n",
      "|    clip_fraction        | 0.251    |\n",
      "|    clip_range           | 0.4      |\n",
      "|    entropy_loss         | -0.15    |\n",
      "|    explained_variance   | 0.609    |\n",
      "|    learning_rate        | 0.000389 |\n",
      "|    loss                 | -0.18    |\n",
      "|    n_updates            | 2090     |\n",
      "|    policy_gradient_loss | -0.109   |\n",
      "|    value_loss           | 0.111    |\n",
      "--------------------------------------\n",
      "Eval num_timesteps=28500, episode_reward=52.89 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 230        |\n",
      "|    mean_reward          | 52.9       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 28500      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.55005825 |\n",
      "|    clip_fraction        | 0.215      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.134     |\n",
      "|    explained_variance   | 0.621      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.159     |\n",
      "|    n_updates            | 2109       |\n",
      "|    policy_gradient_loss | -0.0871    |\n",
      "|    value_loss           | 0.189      |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 57.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 775      |\n",
      "|    iterations      | 112      |\n",
      "|    time_elapsed    | 36       |\n",
      "|    total_timesteps | 28672    |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 57.9      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 777       |\n",
      "|    iterations           | 113       |\n",
      "|    time_elapsed         | 37        |\n",
      "|    total_timesteps      | 28928     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.0501537 |\n",
      "|    clip_fraction        | 0.309     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.178    |\n",
      "|    explained_variance   | -0.92     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.154    |\n",
      "|    n_updates            | 2128      |\n",
      "|    policy_gradient_loss | -0.0964   |\n",
      "|    value_loss           | 0.0806    |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=29000, episode_reward=33.34 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 33.3      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 29000     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.5340203 |\n",
      "|    clip_fraction        | 0.187     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.126    |\n",
      "|    explained_variance   | 0.642     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.156    |\n",
      "|    n_updates            | 2147      |\n",
      "|    policy_gradient_loss | -0.0886   |\n",
      "|    value_loss           | 0.0795    |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 57.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 776      |\n",
      "|    iterations      | 114      |\n",
      "|    time_elapsed    | 37       |\n",
      "|    total_timesteps | 29184    |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 57.8      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 779       |\n",
      "|    iterations           | 115       |\n",
      "|    time_elapsed         | 37        |\n",
      "|    total_timesteps      | 29440     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.5791121 |\n",
      "|    clip_fraction        | 0.229     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.135    |\n",
      "|    explained_variance   | 0.72      |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.125    |\n",
      "|    n_updates            | 2166      |\n",
      "|    policy_gradient_loss | -0.0934   |\n",
      "|    value_loss           | 0.0978    |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=29500, episode_reward=48.63 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "--------------------------------------\n",
      "| eval/                   |          |\n",
      "|    mean_ep_length       | 230      |\n",
      "|    mean_reward          | 48.6     |\n",
      "| time/                   |          |\n",
      "|    total_timesteps      | 29500    |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 0.535123 |\n",
      "|    clip_fraction        | 0.196    |\n",
      "|    clip_range           | 0.4      |\n",
      "|    entropy_loss         | -0.17    |\n",
      "|    explained_variance   | 0.0502   |\n",
      "|    learning_rate        | 0.000389 |\n",
      "|    loss                 | -0.1     |\n",
      "|    n_updates            | 2185     |\n",
      "|    policy_gradient_loss | -0.0749  |\n",
      "|    value_loss           | 0.19     |\n",
      "--------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 57.8     |\n",
      "| time/              |          |\n",
      "|    fps             | 778      |\n",
      "|    iterations      | 116      |\n",
      "|    time_elapsed    | 38       |\n",
      "|    total_timesteps | 29696    |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 57.8      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 779       |\n",
      "|    iterations           | 117       |\n",
      "|    time_elapsed         | 38        |\n",
      "|    total_timesteps      | 29952     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.6054611 |\n",
      "|    clip_fraction        | 0.282     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.108    |\n",
      "|    explained_variance   | 0.0339    |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0942   |\n",
      "|    n_updates            | 2204      |\n",
      "|    policy_gradient_loss | -0.0944   |\n",
      "|    value_loss           | 0.0899    |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=30000, episode_reward=83.17 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 230        |\n",
      "|    mean_reward          | 83.2       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 30000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.66643804 |\n",
      "|    clip_fraction        | 0.261      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.134     |\n",
      "|    explained_variance   | 0.679      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.081     |\n",
      "|    n_updates            | 2223       |\n",
      "|    policy_gradient_loss | -0.0919    |\n",
      "|    value_loss           | 0.0912     |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 57.8     |\n",
      "| time/              |          |\n",
      "|    fps             | 779      |\n",
      "|    iterations      | 118      |\n",
      "|    time_elapsed    | 38       |\n",
      "|    total_timesteps | 30208    |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 57.9      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 781       |\n",
      "|    iterations           | 119       |\n",
      "|    time_elapsed         | 38        |\n",
      "|    total_timesteps      | 30464     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.5919699 |\n",
      "|    clip_fraction        | 0.239     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.158    |\n",
      "|    explained_variance   | 0.378     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.104    |\n",
      "|    n_updates            | 2242      |\n",
      "|    policy_gradient_loss | -0.0752   |\n",
      "|    value_loss           | 0.302     |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=30500, episode_reward=22.41 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "--------------------------------------\n",
      "| eval/                   |          |\n",
      "|    mean_ep_length       | 230      |\n",
      "|    mean_reward          | 22.4     |\n",
      "| time/                   |          |\n",
      "|    total_timesteps      | 30500    |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 0.725259 |\n",
      "|    clip_fraction        | 0.292    |\n",
      "|    clip_range           | 0.4      |\n",
      "|    entropy_loss         | -0.154   |\n",
      "|    explained_variance   | 0.0188   |\n",
      "|    learning_rate        | 0.000389 |\n",
      "|    loss                 | -0.1     |\n",
      "|    n_updates            | 2261     |\n",
      "|    policy_gradient_loss | -0.116   |\n",
      "|    value_loss           | 0.0902   |\n",
      "--------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 57.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 780      |\n",
      "|    iterations      | 120      |\n",
      "|    time_elapsed    | 39       |\n",
      "|    total_timesteps | 30720    |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 57.9       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 783        |\n",
      "|    iterations           | 121        |\n",
      "|    time_elapsed         | 39         |\n",
      "|    total_timesteps      | 30976      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.96615803 |\n",
      "|    clip_fraction        | 0.218      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.117     |\n",
      "|    explained_variance   | 0.73       |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.141     |\n",
      "|    n_updates            | 2280       |\n",
      "|    policy_gradient_loss | -0.0817    |\n",
      "|    value_loss           | 0.0694     |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=31000, episode_reward=58.21 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 230        |\n",
      "|    mean_reward          | 58.2       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 31000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.78423476 |\n",
      "|    clip_fraction        | 0.253      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.129     |\n",
      "|    explained_variance   | 0.635      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.139     |\n",
      "|    n_updates            | 2299       |\n",
      "|    policy_gradient_loss | -0.0861    |\n",
      "|    value_loss           | 0.0959     |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 58.2     |\n",
      "| time/              |          |\n",
      "|    fps             | 782      |\n",
      "|    iterations      | 122      |\n",
      "|    time_elapsed    | 39       |\n",
      "|    total_timesteps | 31232    |\n",
      "---------------------------------\n",
      "--------------------------------------\n",
      "| rollout/                |          |\n",
      "|    ep_len_mean          | 889      |\n",
      "|    ep_rew_mean          | 58.2     |\n",
      "| time/                   |          |\n",
      "|    fps                  | 784      |\n",
      "|    iterations           | 123      |\n",
      "|    time_elapsed         | 40       |\n",
      "|    total_timesteps      | 31488    |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 0.899653 |\n",
      "|    clip_fraction        | 0.291    |\n",
      "|    clip_range           | 0.4      |\n",
      "|    entropy_loss         | -0.194   |\n",
      "|    explained_variance   | 0.576    |\n",
      "|    learning_rate        | 0.000389 |\n",
      "|    loss                 | -0.0857  |\n",
      "|    n_updates            | 2318     |\n",
      "|    policy_gradient_loss | -0.0834  |\n",
      "|    value_loss           | 0.257    |\n",
      "--------------------------------------\n",
      "Eval num_timesteps=31500, episode_reward=79.31 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 230        |\n",
      "|    mean_reward          | 79.3       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 31500      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.86321807 |\n",
      "|    clip_fraction        | 0.242      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.119     |\n",
      "|    explained_variance   | -0.47      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.175     |\n",
      "|    n_updates            | 2337       |\n",
      "|    policy_gradient_loss | -0.0844    |\n",
      "|    value_loss           | 0.0779     |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 58.2     |\n",
      "| time/              |          |\n",
      "|    fps             | 782      |\n",
      "|    iterations      | 124      |\n",
      "|    time_elapsed    | 40       |\n",
      "|    total_timesteps | 31744    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=32000, episode_reward=31.76 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 230        |\n",
      "|    mean_reward          | 31.8       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 32000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.74065757 |\n",
      "|    clip_fraction        | 0.191      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.127     |\n",
      "|    explained_variance   | -0.319     |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.147     |\n",
      "|    n_updates            | 2356       |\n",
      "|    policy_gradient_loss | -0.0653    |\n",
      "|    value_loss           | 0.181      |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 58.2     |\n",
      "| time/              |          |\n",
      "|    fps             | 781      |\n",
      "|    iterations      | 125      |\n",
      "|    time_elapsed    | 40       |\n",
      "|    total_timesteps | 32000    |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 58.1       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 782        |\n",
      "|    iterations           | 126        |\n",
      "|    time_elapsed         | 41         |\n",
      "|    total_timesteps      | 32256      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.32931015 |\n",
      "|    clip_fraction        | 0.168      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.139     |\n",
      "|    explained_variance   | 0.626      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.134     |\n",
      "|    n_updates            | 2375       |\n",
      "|    policy_gradient_loss | -0.0575    |\n",
      "|    value_loss           | 0.197      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=32500, episode_reward=29.77 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 230        |\n",
      "|    mean_reward          | 29.8       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 32500      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.80588025 |\n",
      "|    clip_fraction        | 0.265      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.152     |\n",
      "|    explained_variance   | -0.28      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.114     |\n",
      "|    n_updates            | 2394       |\n",
      "|    policy_gradient_loss | -0.087     |\n",
      "|    value_loss           | 0.0352     |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 58.1     |\n",
      "| time/              |          |\n",
      "|    fps             | 781      |\n",
      "|    iterations      | 127      |\n",
      "|    time_elapsed    | 41       |\n",
      "|    total_timesteps | 32512    |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 58.1      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 782       |\n",
      "|    iterations           | 128       |\n",
      "|    time_elapsed         | 41        |\n",
      "|    total_timesteps      | 32768     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.8886883 |\n",
      "|    clip_fraction        | 0.243     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.117    |\n",
      "|    explained_variance   | 0.426     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.111    |\n",
      "|    n_updates            | 2413      |\n",
      "|    policy_gradient_loss | -0.0988   |\n",
      "|    value_loss           | 0.182     |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=33000, episode_reward=24.12 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 230        |\n",
      "|    mean_reward          | 24.1       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 33000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.75340253 |\n",
      "|    clip_fraction        | 0.229      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.126     |\n",
      "|    explained_variance   | 0.685      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0833    |\n",
      "|    n_updates            | 2432       |\n",
      "|    policy_gradient_loss | -0.0872    |\n",
      "|    value_loss           | 0.11       |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 58.1     |\n",
      "| time/              |          |\n",
      "|    fps             | 781      |\n",
      "|    iterations      | 129      |\n",
      "|    time_elapsed    | 42       |\n",
      "|    total_timesteps | 33024    |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 58.1      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 781       |\n",
      "|    iterations           | 130       |\n",
      "|    time_elapsed         | 42        |\n",
      "|    total_timesteps      | 33280     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.9040053 |\n",
      "|    clip_fraction        | 0.228     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.17     |\n",
      "|    explained_variance   | 0.725     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.146    |\n",
      "|    n_updates            | 2451      |\n",
      "|    policy_gradient_loss | -0.0852   |\n",
      "|    value_loss           | 0.183     |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=33500, episode_reward=6.24 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 230        |\n",
      "|    mean_reward          | 6.24       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 33500      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.91436875 |\n",
      "|    clip_fraction        | 0.257      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.116     |\n",
      "|    explained_variance   | 0.352      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.138     |\n",
      "|    n_updates            | 2470       |\n",
      "|    policy_gradient_loss | -0.091     |\n",
      "|    value_loss           | 0.0487     |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 58.1     |\n",
      "| time/              |          |\n",
      "|    fps             | 780      |\n",
      "|    iterations      | 131      |\n",
      "|    time_elapsed    | 42       |\n",
      "|    total_timesteps | 33536    |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 58.2       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 779        |\n",
      "|    iterations           | 132        |\n",
      "|    time_elapsed         | 43         |\n",
      "|    total_timesteps      | 33792      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.84824216 |\n",
      "|    clip_fraction        | 0.259      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.121     |\n",
      "|    explained_variance   | 0.674      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.134     |\n",
      "|    n_updates            | 2489       |\n",
      "|    policy_gradient_loss | -0.102     |\n",
      "|    value_loss           | 0.0966     |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=34000, episode_reward=43.07 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 43.1      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 34000     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.5111095 |\n",
      "|    clip_fraction        | 0.225     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.149    |\n",
      "|    explained_variance   | 0.712     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0905   |\n",
      "|    n_updates            | 2508      |\n",
      "|    policy_gradient_loss | -0.0922   |\n",
      "|    value_loss           | 0.311     |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 58.2     |\n",
      "| time/              |          |\n",
      "|    fps             | 776      |\n",
      "|    iterations      | 133      |\n",
      "|    time_elapsed    | 43       |\n",
      "|    total_timesteps | 34048    |\n",
      "---------------------------------\n",
      "--------------------------------------\n",
      "| rollout/                |          |\n",
      "|    ep_len_mean          | 889      |\n",
      "|    ep_rew_mean          | 58.2     |\n",
      "| time/                   |          |\n",
      "|    fps                  | 776      |\n",
      "|    iterations           | 134      |\n",
      "|    time_elapsed         | 44       |\n",
      "|    total_timesteps      | 34304    |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 0.724917 |\n",
      "|    clip_fraction        | 0.232    |\n",
      "|    clip_range           | 0.4      |\n",
      "|    entropy_loss         | -0.137   |\n",
      "|    explained_variance   | -0.0264  |\n",
      "|    learning_rate        | 0.000389 |\n",
      "|    loss                 | -0.0925  |\n",
      "|    n_updates            | 2527     |\n",
      "|    policy_gradient_loss | -0.0698  |\n",
      "|    value_loss           | 0.109    |\n",
      "--------------------------------------\n",
      "Eval num_timesteps=34500, episode_reward=51.41 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 230        |\n",
      "|    mean_reward          | 51.4       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 34500      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.84551764 |\n",
      "|    clip_fraction        | 0.199      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0945    |\n",
      "|    explained_variance   | 0.7        |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.11      |\n",
      "|    n_updates            | 2546       |\n",
      "|    policy_gradient_loss | -0.0842    |\n",
      "|    value_loss           | 0.086      |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 58.2     |\n",
      "| time/              |          |\n",
      "|    fps             | 775      |\n",
      "|    iterations      | 135      |\n",
      "|    time_elapsed    | 44       |\n",
      "|    total_timesteps | 34560    |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 58.3       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 775        |\n",
      "|    iterations           | 136        |\n",
      "|    time_elapsed         | 44         |\n",
      "|    total_timesteps      | 34816      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.89539474 |\n",
      "|    clip_fraction        | 0.25       |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.122     |\n",
      "|    explained_variance   | 0.799      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0465    |\n",
      "|    n_updates            | 2565       |\n",
      "|    policy_gradient_loss | -0.0847    |\n",
      "|    value_loss           | 0.09       |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=35000, episode_reward=27.76 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 230        |\n",
      "|    mean_reward          | 27.8       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 35000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.37793797 |\n",
      "|    clip_fraction        | 0.183      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.15      |\n",
      "|    explained_variance   | 0.688      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0682    |\n",
      "|    n_updates            | 2584       |\n",
      "|    policy_gradient_loss | -0.0696    |\n",
      "|    value_loss           | 0.234      |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 58.3     |\n",
      "| time/              |          |\n",
      "|    fps             | 773      |\n",
      "|    iterations      | 137      |\n",
      "|    time_elapsed    | 45       |\n",
      "|    total_timesteps | 35072    |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 58.3      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 774       |\n",
      "|    iterations           | 138       |\n",
      "|    time_elapsed         | 45        |\n",
      "|    total_timesteps      | 35328     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.1232785 |\n",
      "|    clip_fraction        | 0.253     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0976   |\n",
      "|    explained_variance   | 0.361     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.104    |\n",
      "|    n_updates            | 2603      |\n",
      "|    policy_gradient_loss | -0.105    |\n",
      "|    value_loss           | 0.0607    |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=35500, episode_reward=46.79 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 46.8      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 35500     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.6781734 |\n",
      "|    clip_fraction        | 0.242     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.134    |\n",
      "|    explained_variance   | 0.346     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.119    |\n",
      "|    n_updates            | 2622      |\n",
      "|    policy_gradient_loss | -0.0955   |\n",
      "|    value_loss           | 0.11      |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 58.5     |\n",
      "| time/              |          |\n",
      "|    fps             | 774      |\n",
      "|    iterations      | 139      |\n",
      "|    time_elapsed    | 45       |\n",
      "|    total_timesteps | 35584    |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 58.5      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 774       |\n",
      "|    iterations           | 140       |\n",
      "|    time_elapsed         | 46        |\n",
      "|    total_timesteps      | 35840     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.4642681 |\n",
      "|    clip_fraction        | 0.206     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.147    |\n",
      "|    explained_variance   | 0.679     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0361   |\n",
      "|    n_updates            | 2641      |\n",
      "|    policy_gradient_loss | -0.0879   |\n",
      "|    value_loss           | 0.374     |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=36000, episode_reward=40.29 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 40.3      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 36000     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.0032048 |\n",
      "|    clip_fraction        | 0.272     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.125    |\n",
      "|    explained_variance   | -0.671    |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.117    |\n",
      "|    n_updates            | 2660      |\n",
      "|    policy_gradient_loss | -0.0896   |\n",
      "|    value_loss           | 0.0463    |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 58.5     |\n",
      "| time/              |          |\n",
      "|    fps             | 774      |\n",
      "|    iterations      | 141      |\n",
      "|    time_elapsed    | 46       |\n",
      "|    total_timesteps | 36096    |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 58.5      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 776       |\n",
      "|    iterations           | 142       |\n",
      "|    time_elapsed         | 46        |\n",
      "|    total_timesteps      | 36352     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.8612464 |\n",
      "|    clip_fraction        | 0.212     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.111    |\n",
      "|    explained_variance   | 0.446     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0409   |\n",
      "|    n_updates            | 2679      |\n",
      "|    policy_gradient_loss | -0.0708   |\n",
      "|    value_loss           | 0.113     |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=36500, episode_reward=33.01 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 33        |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 36500     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.6515149 |\n",
      "|    clip_fraction        | 0.196     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.111    |\n",
      "|    explained_variance   | 0.757     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0462   |\n",
      "|    n_updates            | 2698      |\n",
      "|    policy_gradient_loss | -0.0754   |\n",
      "|    value_loss           | 0.0975    |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 58.6     |\n",
      "| time/              |          |\n",
      "|    fps             | 773      |\n",
      "|    iterations      | 143      |\n",
      "|    time_elapsed    | 47       |\n",
      "|    total_timesteps | 36608    |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 58.6      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 775       |\n",
      "|    iterations           | 144       |\n",
      "|    time_elapsed         | 47        |\n",
      "|    total_timesteps      | 36864     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.8683574 |\n",
      "|    clip_fraction        | 0.344     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.173    |\n",
      "|    explained_variance   | 0.534     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.00303  |\n",
      "|    n_updates            | 2717      |\n",
      "|    policy_gradient_loss | -0.0716   |\n",
      "|    value_loss           | 0.215     |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=37000, episode_reward=38.62 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 38.6      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 37000     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.0897322 |\n",
      "|    clip_fraction        | 0.219     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0958   |\n",
      "|    explained_variance   | 0.492     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0979   |\n",
      "|    n_updates            | 2736      |\n",
      "|    policy_gradient_loss | -0.0792   |\n",
      "|    value_loss           | 0.0732    |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 58.6     |\n",
      "| time/              |          |\n",
      "|    fps             | 774      |\n",
      "|    iterations      | 145      |\n",
      "|    time_elapsed    | 47       |\n",
      "|    total_timesteps | 37120    |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 58.7      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 776       |\n",
      "|    iterations           | 146       |\n",
      "|    time_elapsed         | 48        |\n",
      "|    total_timesteps      | 37376     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.8185898 |\n",
      "|    clip_fraction        | 0.225     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.125    |\n",
      "|    explained_variance   | 0.631     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.17     |\n",
      "|    n_updates            | 2755      |\n",
      "|    policy_gradient_loss | -0.088    |\n",
      "|    value_loss           | 0.122     |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=37500, episode_reward=32.02 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 230        |\n",
      "|    mean_reward          | 32         |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 37500      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.29804993 |\n",
      "|    clip_fraction        | 0.166      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.103     |\n",
      "|    explained_variance   | 0.479      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.101     |\n",
      "|    n_updates            | 2774       |\n",
      "|    policy_gradient_loss | -0.0627    |\n",
      "|    value_loss           | 0.287      |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 58.7     |\n",
      "| time/              |          |\n",
      "|    fps             | 776      |\n",
      "|    iterations      | 147      |\n",
      "|    time_elapsed    | 48       |\n",
      "|    total_timesteps | 37632    |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 58.7      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 778       |\n",
      "|    iterations           | 148       |\n",
      "|    time_elapsed         | 48        |\n",
      "|    total_timesteps      | 37888     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.8371268 |\n",
      "|    clip_fraction        | 0.218     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.117    |\n",
      "|    explained_variance   | -0.226    |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.113    |\n",
      "|    n_updates            | 2793      |\n",
      "|    policy_gradient_loss | -0.0845   |\n",
      "|    value_loss           | 0.0449    |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=38000, episode_reward=34.69 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 34.7      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 38000     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.8038571 |\n",
      "|    clip_fraction        | 0.165     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0861   |\n",
      "|    explained_variance   | 0.494     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0914   |\n",
      "|    n_updates            | 2812      |\n",
      "|    policy_gradient_loss | -0.0682   |\n",
      "|    value_loss           | 0.102     |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 58.7     |\n",
      "| time/              |          |\n",
      "|    fps             | 777      |\n",
      "|    iterations      | 149      |\n",
      "|    time_elapsed    | 49       |\n",
      "|    total_timesteps | 38144    |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 58.8      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 777       |\n",
      "|    iterations           | 150       |\n",
      "|    time_elapsed         | 49        |\n",
      "|    total_timesteps      | 38400     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.3070261 |\n",
      "|    clip_fraction        | 0.218     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0945   |\n",
      "|    explained_variance   | 0.727     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.101    |\n",
      "|    n_updates            | 2831      |\n",
      "|    policy_gradient_loss | -0.101    |\n",
      "|    value_loss           | 0.121     |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=38500, episode_reward=20.28 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "--------------------------------------\n",
      "| eval/                   |          |\n",
      "|    mean_ep_length       | 230      |\n",
      "|    mean_reward          | 20.3     |\n",
      "| time/                   |          |\n",
      "|    total_timesteps      | 38500    |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 1.511895 |\n",
      "|    clip_fraction        | 0.282    |\n",
      "|    clip_range           | 0.4      |\n",
      "|    entropy_loss         | -0.152   |\n",
      "|    explained_variance   | 0.171    |\n",
      "|    learning_rate        | 0.000389 |\n",
      "|    loss                 | 0.00641  |\n",
      "|    n_updates            | 2850     |\n",
      "|    policy_gradient_loss | -0.0746  |\n",
      "|    value_loss           | 0.617    |\n",
      "--------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 58.8     |\n",
      "| time/              |          |\n",
      "|    fps             | 777      |\n",
      "|    iterations      | 151      |\n",
      "|    time_elapsed    | 49       |\n",
      "|    total_timesteps | 38656    |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 58.8      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 779       |\n",
      "|    iterations           | 152       |\n",
      "|    time_elapsed         | 49        |\n",
      "|    total_timesteps      | 38912     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.8889013 |\n",
      "|    clip_fraction        | 0.156     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0906   |\n",
      "|    explained_variance   | 0.618     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.17     |\n",
      "|    n_updates            | 2869      |\n",
      "|    policy_gradient_loss | -0.0783   |\n",
      "|    value_loss           | 0.101     |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=39000, episode_reward=16.16 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 230        |\n",
      "|    mean_reward          | 16.2       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 39000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.81298006 |\n",
      "|    clip_fraction        | 0.229      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.132     |\n",
      "|    explained_variance   | 0.498      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.165     |\n",
      "|    n_updates            | 2888       |\n",
      "|    policy_gradient_loss | -0.0934    |\n",
      "|    value_loss           | 0.125      |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 58.5     |\n",
      "| time/              |          |\n",
      "|    fps             | 778      |\n",
      "|    iterations      | 153      |\n",
      "|    time_elapsed    | 50       |\n",
      "|    total_timesteps | 39168    |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 58.5       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 778        |\n",
      "|    iterations           | 154        |\n",
      "|    time_elapsed         | 50         |\n",
      "|    total_timesteps      | 39424      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.47108608 |\n",
      "|    clip_fraction        | 0.161      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.118     |\n",
      "|    explained_variance   | 0.246      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.103     |\n",
      "|    n_updates            | 2907       |\n",
      "|    policy_gradient_loss | -0.0673    |\n",
      "|    value_loss           | 0.278      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=39500, episode_reward=18.77 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 18.8      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 39500     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.5309101 |\n",
      "|    clip_fraction        | 0.157     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0898   |\n",
      "|    explained_variance   | 0.206     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0763   |\n",
      "|    n_updates            | 2926      |\n",
      "|    policy_gradient_loss | -0.0641   |\n",
      "|    value_loss           | 0.0402    |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 58.5     |\n",
      "| time/              |          |\n",
      "|    fps             | 777      |\n",
      "|    iterations      | 155      |\n",
      "|    time_elapsed    | 51       |\n",
      "|    total_timesteps | 39680    |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 58.5       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 778        |\n",
      "|    iterations           | 156        |\n",
      "|    time_elapsed         | 51         |\n",
      "|    total_timesteps      | 39936      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.82837856 |\n",
      "|    clip_fraction        | 0.245      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.118     |\n",
      "|    explained_variance   | 0.567      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0808    |\n",
      "|    n_updates            | 2945       |\n",
      "|    policy_gradient_loss | -0.0936    |\n",
      "|    value_loss           | 0.179      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=40000, episode_reward=4.54 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 230        |\n",
      "|    mean_reward          | 4.54       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 40000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.77480984 |\n",
      "|    clip_fraction        | 0.186      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.101     |\n",
      "|    explained_variance   | 0.633      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0409    |\n",
      "|    n_updates            | 2964       |\n",
      "|    policy_gradient_loss | -0.0534    |\n",
      "|    value_loss           | 0.183      |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 58.4     |\n",
      "| time/              |          |\n",
      "|    fps             | 776      |\n",
      "|    iterations      | 157      |\n",
      "|    time_elapsed    | 51       |\n",
      "|    total_timesteps | 40192    |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 58.4       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 777        |\n",
      "|    iterations           | 158        |\n",
      "|    time_elapsed         | 52         |\n",
      "|    total_timesteps      | 40448      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.84317374 |\n",
      "|    clip_fraction        | 0.24       |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.143     |\n",
      "|    explained_variance   | -0.857     |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.1       |\n",
      "|    n_updates            | 2983       |\n",
      "|    policy_gradient_loss | -0.0567    |\n",
      "|    value_loss           | 0.23       |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=40500, episode_reward=49.95 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 49.9      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 40500     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.4879843 |\n",
      "|    clip_fraction        | 0.251     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0998   |\n",
      "|    explained_variance   | 0.722     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.184    |\n",
      "|    n_updates            | 3002      |\n",
      "|    policy_gradient_loss | -0.102    |\n",
      "|    value_loss           | 0.0625    |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 58.4     |\n",
      "| time/              |          |\n",
      "|    fps             | 776      |\n",
      "|    iterations      | 159      |\n",
      "|    time_elapsed    | 52       |\n",
      "|    total_timesteps | 40704    |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 58.5      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 776       |\n",
      "|    iterations           | 160       |\n",
      "|    time_elapsed         | 52        |\n",
      "|    total_timesteps      | 40960     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.0055256 |\n",
      "|    clip_fraction        | 0.23      |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.102    |\n",
      "|    explained_variance   | 0.562     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0471   |\n",
      "|    n_updates            | 3021      |\n",
      "|    policy_gradient_loss | -0.0998   |\n",
      "|    value_loss           | 0.133     |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=41000, episode_reward=11.10 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 11.1      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 41000     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.3535726 |\n",
      "|    clip_fraction        | 0.238     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.129    |\n",
      "|    explained_variance   | 0.734     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.101    |\n",
      "|    n_updates            | 3040      |\n",
      "|    policy_gradient_loss | -0.0687   |\n",
      "|    value_loss           | 0.201     |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 58.5     |\n",
      "| time/              |          |\n",
      "|    fps             | 775      |\n",
      "|    iterations      | 161      |\n",
      "|    time_elapsed    | 53       |\n",
      "|    total_timesteps | 41216    |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 58.5       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 773        |\n",
      "|    iterations           | 162        |\n",
      "|    time_elapsed         | 53         |\n",
      "|    total_timesteps      | 41472      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.90994287 |\n",
      "|    clip_fraction        | 0.234      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.119     |\n",
      "|    explained_variance   | -0.0619    |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.154     |\n",
      "|    n_updates            | 3059       |\n",
      "|    policy_gradient_loss | -0.0994    |\n",
      "|    value_loss           | 0.049      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=41500, episode_reward=10.71 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 10.7      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 41500     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.7779103 |\n",
      "|    clip_fraction        | 0.177     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0859   |\n",
      "|    explained_variance   | 0.487     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0701   |\n",
      "|    n_updates            | 3078      |\n",
      "|    policy_gradient_loss | -0.0606   |\n",
      "|    value_loss           | 0.152     |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 58.5     |\n",
      "| time/              |          |\n",
      "|    fps             | 770      |\n",
      "|    iterations      | 163      |\n",
      "|    time_elapsed    | 54       |\n",
      "|    total_timesteps | 41728    |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 58.8       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 771        |\n",
      "|    iterations           | 164        |\n",
      "|    time_elapsed         | 54         |\n",
      "|    total_timesteps      | 41984      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.66959864 |\n",
      "|    clip_fraction        | 0.15       |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0924    |\n",
      "|    explained_variance   | 0.725      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0701    |\n",
      "|    n_updates            | 3097       |\n",
      "|    policy_gradient_loss | -0.0615    |\n",
      "|    value_loss           | 0.152      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=42000, episode_reward=-22.08 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | -22.1     |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 42000     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.1497934 |\n",
      "|    clip_fraction        | 0.248     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.142    |\n",
      "|    explained_variance   | -0.219    |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.152    |\n",
      "|    n_updates            | 3116      |\n",
      "|    policy_gradient_loss | -0.0779   |\n",
      "|    value_loss           | 0.133     |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 58.8     |\n",
      "| time/              |          |\n",
      "|    fps             | 771      |\n",
      "|    iterations      | 165      |\n",
      "|    time_elapsed    | 54       |\n",
      "|    total_timesteps | 42240    |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 58.8       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 772        |\n",
      "|    iterations           | 166        |\n",
      "|    time_elapsed         | 54         |\n",
      "|    total_timesteps      | 42496      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.70834094 |\n",
      "|    clip_fraction        | 0.177      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.103     |\n",
      "|    explained_variance   | 0.534      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0572    |\n",
      "|    n_updates            | 3135       |\n",
      "|    policy_gradient_loss | -0.0627    |\n",
      "|    value_loss           | 0.133      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=42500, episode_reward=-29.09 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | -29.1     |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 42500     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.0608858 |\n",
      "|    clip_fraction        | 0.259     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.101    |\n",
      "|    explained_variance   | 0.628     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.128    |\n",
      "|    n_updates            | 3154      |\n",
      "|    policy_gradient_loss | -0.102    |\n",
      "|    value_loss           | 0.122     |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 59.4     |\n",
      "| time/              |          |\n",
      "|    fps             | 772      |\n",
      "|    iterations      | 167      |\n",
      "|    time_elapsed    | 55       |\n",
      "|    total_timesteps | 42752    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=43000, episode_reward=-42.30 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | -42.3     |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 43000     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.9748819 |\n",
      "|    clip_fraction        | 0.274     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.131    |\n",
      "|    explained_variance   | 0.731     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.123    |\n",
      "|    n_updates            | 3173      |\n",
      "|    policy_gradient_loss | -0.0824   |\n",
      "|    value_loss           | 0.154     |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 59.4     |\n",
      "| time/              |          |\n",
      "|    fps             | 770      |\n",
      "|    iterations      | 168      |\n",
      "|    time_elapsed    | 55       |\n",
      "|    total_timesteps | 43008    |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 59.4      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 770       |\n",
      "|    iterations           | 169       |\n",
      "|    time_elapsed         | 56        |\n",
      "|    total_timesteps      | 43264     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.0276122 |\n",
      "|    clip_fraction        | 0.254     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.121    |\n",
      "|    explained_variance   | -0.0588   |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.096    |\n",
      "|    n_updates            | 3192      |\n",
      "|    policy_gradient_loss | -0.1      |\n",
      "|    value_loss           | 0.0563    |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=43500, episode_reward=21.82 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 21.8      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 43500     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.1081424 |\n",
      "|    clip_fraction        | 0.179     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0927   |\n",
      "|    explained_variance   | 0.529     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0815   |\n",
      "|    n_updates            | 3211      |\n",
      "|    policy_gradient_loss | -0.0797   |\n",
      "|    value_loss           | 0.23      |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 59.4     |\n",
      "| time/              |          |\n",
      "|    fps             | 768      |\n",
      "|    iterations      | 170      |\n",
      "|    time_elapsed    | 56       |\n",
      "|    total_timesteps | 43520    |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 59.8      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 769       |\n",
      "|    iterations           | 171       |\n",
      "|    time_elapsed         | 56        |\n",
      "|    total_timesteps      | 43776     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.3993613 |\n",
      "|    clip_fraction        | 0.21      |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0967   |\n",
      "|    explained_variance   | 0.482     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0802   |\n",
      "|    n_updates            | 3230      |\n",
      "|    policy_gradient_loss | -0.08     |\n",
      "|    value_loss           | 0.241     |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=44000, episode_reward=4.04 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 4.04      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 44000     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.5412449 |\n",
      "|    clip_fraction        | 0.328     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.149    |\n",
      "|    explained_variance   | 0.346     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0828   |\n",
      "|    n_updates            | 3249      |\n",
      "|    policy_gradient_loss | -0.0927   |\n",
      "|    value_loss           | 0.0988    |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 59.8     |\n",
      "| time/              |          |\n",
      "|    fps             | 769      |\n",
      "|    iterations      | 172      |\n",
      "|    time_elapsed    | 57       |\n",
      "|    total_timesteps | 44032    |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 59.8      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 770       |\n",
      "|    iterations           | 173       |\n",
      "|    time_elapsed         | 57        |\n",
      "|    total_timesteps      | 44288     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.8838817 |\n",
      "|    clip_fraction        | 0.208     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0812   |\n",
      "|    explained_variance   | 0.591     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.12     |\n",
      "|    n_updates            | 3268      |\n",
      "|    policy_gradient_loss | -0.0707   |\n",
      "|    value_loss           | 0.0791    |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=44500, episode_reward=-7.58 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | -7.58     |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 44500     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.5368981 |\n",
      "|    clip_fraction        | 0.164     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0744   |\n",
      "|    explained_variance   | 0.513     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0179   |\n",
      "|    n_updates            | 3287      |\n",
      "|    policy_gradient_loss | -0.0531   |\n",
      "|    value_loss           | 0.119     |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 60       |\n",
      "| time/              |          |\n",
      "|    fps             | 770      |\n",
      "|    iterations      | 174      |\n",
      "|    time_elapsed    | 57       |\n",
      "|    total_timesteps | 44544    |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 60         |\n",
      "| time/                   |            |\n",
      "|    fps                  | 771        |\n",
      "|    iterations           | 175        |\n",
      "|    time_elapsed         | 58         |\n",
      "|    total_timesteps      | 44800      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.80580163 |\n",
      "|    clip_fraction        | 0.215      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.119     |\n",
      "|    explained_variance   | 0.747      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0812    |\n",
      "|    n_updates            | 3306       |\n",
      "|    policy_gradient_loss | -0.0833    |\n",
      "|    value_loss           | 0.195      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=45000, episode_reward=34.05 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 34        |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 45000     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.0379314 |\n",
      "|    clip_fraction        | 0.228     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0961   |\n",
      "|    explained_variance   | -0.117    |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.104    |\n",
      "|    n_updates            | 3325      |\n",
      "|    policy_gradient_loss | -0.0818   |\n",
      "|    value_loss           | 0.125     |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 60       |\n",
      "| time/              |          |\n",
      "|    fps             | 770      |\n",
      "|    iterations      | 176      |\n",
      "|    time_elapsed    | 58       |\n",
      "|    total_timesteps | 45056    |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 60        |\n",
      "| time/                   |           |\n",
      "|    fps                  | 772       |\n",
      "|    iterations           | 177       |\n",
      "|    time_elapsed         | 58        |\n",
      "|    total_timesteps      | 45312     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.5829718 |\n",
      "|    clip_fraction        | 0.146     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0624   |\n",
      "|    explained_variance   | 0.287     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0985   |\n",
      "|    n_updates            | 3344      |\n",
      "|    policy_gradient_loss | -0.051    |\n",
      "|    value_loss           | 0.365     |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=45500, episode_reward=44.52 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "--------------------------------------\n",
      "| eval/                   |          |\n",
      "|    mean_ep_length       | 230      |\n",
      "|    mean_reward          | 44.5     |\n",
      "| time/                   |          |\n",
      "|    total_timesteps      | 45500    |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 1.1492   |\n",
      "|    clip_fraction        | 0.197    |\n",
      "|    clip_range           | 0.4      |\n",
      "|    entropy_loss         | -0.109   |\n",
      "|    explained_variance   | -0.136   |\n",
      "|    learning_rate        | 0.000389 |\n",
      "|    loss                 | -0.107   |\n",
      "|    n_updates            | 3363     |\n",
      "|    policy_gradient_loss | -0.0787  |\n",
      "|    value_loss           | 0.317    |\n",
      "--------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 60.1     |\n",
      "| time/              |          |\n",
      "|    fps             | 772      |\n",
      "|    iterations      | 178      |\n",
      "|    time_elapsed    | 59       |\n",
      "|    total_timesteps | 45568    |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 60.1       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 773        |\n",
      "|    iterations           | 179        |\n",
      "|    time_elapsed         | 59         |\n",
      "|    total_timesteps      | 45824      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.74627167 |\n",
      "|    clip_fraction        | 0.226      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.11      |\n",
      "|    explained_variance   | -0.0339    |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0923    |\n",
      "|    n_updates            | 3382       |\n",
      "|    policy_gradient_loss | -0.0777    |\n",
      "|    value_loss           | 0.137      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=46000, episode_reward=57.05 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 57        |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 46000     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.9920589 |\n",
      "|    clip_fraction        | 0.2       |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0689   |\n",
      "|    explained_variance   | 0.54      |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0269   |\n",
      "|    n_updates            | 3401      |\n",
      "|    policy_gradient_loss | -0.0699   |\n",
      "|    value_loss           | 0.164     |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 60.1     |\n",
      "| time/              |          |\n",
      "|    fps             | 773      |\n",
      "|    iterations      | 180      |\n",
      "|    time_elapsed    | 59       |\n",
      "|    total_timesteps | 46080    |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 60        |\n",
      "| time/                   |           |\n",
      "|    fps                  | 774       |\n",
      "|    iterations           | 181       |\n",
      "|    time_elapsed         | 59        |\n",
      "|    total_timesteps      | 46336     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.5140512 |\n",
      "|    clip_fraction        | 0.159     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0815   |\n",
      "|    explained_variance   | 0.589     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0882   |\n",
      "|    n_updates            | 3420      |\n",
      "|    policy_gradient_loss | -0.0674   |\n",
      "|    value_loss           | 0.203     |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=46500, episode_reward=30.24 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 30.2      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 46500     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.6281737 |\n",
      "|    clip_fraction        | 0.152     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.106    |\n",
      "|    explained_variance   | 0.307     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | 0.0218    |\n",
      "|    n_updates            | 3439      |\n",
      "|    policy_gradient_loss | -0.0502   |\n",
      "|    value_loss           | 0.211     |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 60       |\n",
      "| time/              |          |\n",
      "|    fps             | 774      |\n",
      "|    iterations      | 182      |\n",
      "|    time_elapsed    | 60       |\n",
      "|    total_timesteps | 46592    |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 60        |\n",
      "| time/                   |           |\n",
      "|    fps                  | 773       |\n",
      "|    iterations           | 183       |\n",
      "|    time_elapsed         | 60        |\n",
      "|    total_timesteps      | 46848     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.0870962 |\n",
      "|    clip_fraction        | 0.206     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0809   |\n",
      "|    explained_variance   | 0.215     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0893   |\n",
      "|    n_updates            | 3458      |\n",
      "|    policy_gradient_loss | -0.0683   |\n",
      "|    value_loss           | 0.0847    |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=47000, episode_reward=4.64 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 230        |\n",
      "|    mean_reward          | 4.64       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 47000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.98489594 |\n",
      "|    clip_fraction        | 0.189      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0715    |\n",
      "|    explained_variance   | 0.589      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0753    |\n",
      "|    n_updates            | 3477       |\n",
      "|    policy_gradient_loss | -0.0733    |\n",
      "|    value_loss           | 0.182      |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 60       |\n",
      "| time/              |          |\n",
      "|    fps             | 772      |\n",
      "|    iterations      | 184      |\n",
      "|    time_elapsed    | 61       |\n",
      "|    total_timesteps | 47104    |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 60.1      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 773       |\n",
      "|    iterations           | 185       |\n",
      "|    time_elapsed         | 61        |\n",
      "|    total_timesteps      | 47360     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.8372305 |\n",
      "|    clip_fraction        | 0.239     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.11     |\n",
      "|    explained_variance   | 0.402     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0975   |\n",
      "|    n_updates            | 3496      |\n",
      "|    policy_gradient_loss | -0.0797   |\n",
      "|    value_loss           | 0.169     |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=47500, episode_reward=28.39 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 28.4      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 47500     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.3683392 |\n",
      "|    clip_fraction        | 0.281     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.117    |\n",
      "|    explained_variance   | -0.199    |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.122    |\n",
      "|    n_updates            | 3515      |\n",
      "|    policy_gradient_loss | -0.107    |\n",
      "|    value_loss           | 0.0979    |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 60.1     |\n",
      "| time/              |          |\n",
      "|    fps             | 772      |\n",
      "|    iterations      | 186      |\n",
      "|    time_elapsed    | 61       |\n",
      "|    total_timesteps | 47616    |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 60.1       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 772        |\n",
      "|    iterations           | 187        |\n",
      "|    time_elapsed         | 61         |\n",
      "|    total_timesteps      | 47872      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.71067846 |\n",
      "|    clip_fraction        | 0.134      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0615    |\n",
      "|    explained_variance   | 0.333      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0871    |\n",
      "|    n_updates            | 3534       |\n",
      "|    policy_gradient_loss | -0.0592    |\n",
      "|    value_loss           | 0.154      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=48000, episode_reward=33.39 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 33.4      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 48000     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.7340443 |\n",
      "|    clip_fraction        | 0.165     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0831   |\n",
      "|    explained_variance   | 0.749     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.103    |\n",
      "|    n_updates            | 3553      |\n",
      "|    policy_gradient_loss | -0.0668   |\n",
      "|    value_loss           | 0.0819    |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 60.2     |\n",
      "| time/              |          |\n",
      "|    fps             | 771      |\n",
      "|    iterations      | 188      |\n",
      "|    time_elapsed    | 62       |\n",
      "|    total_timesteps | 48128    |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 60.2       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 772        |\n",
      "|    iterations           | 189        |\n",
      "|    time_elapsed         | 62         |\n",
      "|    total_timesteps      | 48384      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.73913264 |\n",
      "|    clip_fraction        | 0.211      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.154     |\n",
      "|    explained_variance   | 0.0843     |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.088     |\n",
      "|    n_updates            | 3572       |\n",
      "|    policy_gradient_loss | -0.0663    |\n",
      "|    value_loss           | 0.523      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=48500, episode_reward=25.75 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 25.8      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 48500     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.3050684 |\n",
      "|    clip_fraction        | 0.201     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0646   |\n",
      "|    explained_variance   | -0.0809   |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0647   |\n",
      "|    n_updates            | 3591      |\n",
      "|    policy_gradient_loss | -0.0768   |\n",
      "|    value_loss           | 0.0721    |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 60.2     |\n",
      "| time/              |          |\n",
      "|    fps             | 769      |\n",
      "|    iterations      | 190      |\n",
      "|    time_elapsed    | 63       |\n",
      "|    total_timesteps | 48640    |\n",
      "---------------------------------\n",
      "--------------------------------------\n",
      "| rollout/                |          |\n",
      "|    ep_len_mean          | 889      |\n",
      "|    ep_rew_mean          | 60.2     |\n",
      "| time/                   |          |\n",
      "|    fps                  | 770      |\n",
      "|    iterations           | 191      |\n",
      "|    time_elapsed         | 63       |\n",
      "|    total_timesteps      | 48896    |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 30.46008 |\n",
      "|    clip_fraction        | 0.165    |\n",
      "|    clip_range           | 0.4      |\n",
      "|    entropy_loss         | -0.0701  |\n",
      "|    explained_variance   | 0.49     |\n",
      "|    learning_rate        | 0.000389 |\n",
      "|    loss                 | -0.117   |\n",
      "|    n_updates            | 3610     |\n",
      "|    policy_gradient_loss | -0.0835  |\n",
      "|    value_loss           | 0.259    |\n",
      "--------------------------------------\n",
      "Eval num_timesteps=49000, episode_reward=23.50 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 23.5      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 49000     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.8202138 |\n",
      "|    clip_fraction        | 0.208     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.108    |\n",
      "|    explained_variance   | 0.426     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0721   |\n",
      "|    n_updates            | 3629      |\n",
      "|    policy_gradient_loss | -0.0838   |\n",
      "|    value_loss           | 0.261     |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 60.2     |\n",
      "| time/              |          |\n",
      "|    fps             | 769      |\n",
      "|    iterations      | 192      |\n",
      "|    time_elapsed    | 63       |\n",
      "|    total_timesteps | 49152    |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 60.2       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 770        |\n",
      "|    iterations           | 193        |\n",
      "|    time_elapsed         | 64         |\n",
      "|    total_timesteps      | 49408      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.67371154 |\n",
      "|    clip_fraction        | 0.233      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.153     |\n",
      "|    explained_variance   | 0.222      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0978    |\n",
      "|    n_updates            | 3648       |\n",
      "|    policy_gradient_loss | -0.0667    |\n",
      "|    value_loss           | 0.0473     |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=49500, episode_reward=5.00 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 5         |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 49500     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.1036923 |\n",
      "|    clip_fraction        | 0.168     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0624   |\n",
      "|    explained_variance   | 0.473     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0672   |\n",
      "|    n_updates            | 3667      |\n",
      "|    policy_gradient_loss | -0.0572   |\n",
      "|    value_loss           | 0.134     |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 60.2     |\n",
      "| time/              |          |\n",
      "|    fps             | 769      |\n",
      "|    iterations      | 194      |\n",
      "|    time_elapsed    | 64       |\n",
      "|    total_timesteps | 49664    |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 60.4       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 770        |\n",
      "|    iterations           | 195        |\n",
      "|    time_elapsed         | 64         |\n",
      "|    total_timesteps      | 49920      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.88462436 |\n",
      "|    clip_fraction        | 0.189      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.078     |\n",
      "|    explained_variance   | 0.629      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0762    |\n",
      "|    n_updates            | 3686       |\n",
      "|    policy_gradient_loss | -0.0756    |\n",
      "|    value_loss           | 0.0814     |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=50000, episode_reward=0.24 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 0.244     |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 50000     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.6864125 |\n",
      "|    clip_fraction        | 0.228     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.17     |\n",
      "|    explained_variance   | 0.725     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.121    |\n",
      "|    n_updates            | 3705      |\n",
      "|    policy_gradient_loss | -0.0619   |\n",
      "|    value_loss           | 0.1       |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 60.4     |\n",
      "| time/              |          |\n",
      "|    fps             | 768      |\n",
      "|    iterations      | 196      |\n",
      "|    time_elapsed    | 65       |\n",
      "|    total_timesteps | 50176    |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 60.4       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 768        |\n",
      "|    iterations           | 197        |\n",
      "|    time_elapsed         | 65         |\n",
      "|    total_timesteps      | 50432      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.83768666 |\n",
      "|    clip_fraction        | 0.204      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0747    |\n",
      "|    explained_variance   | 0.724      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0984    |\n",
      "|    n_updates            | 3724       |\n",
      "|    policy_gradient_loss | -0.0725    |\n",
      "|    value_loss           | 0.0299     |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=50500, episode_reward=0.23 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 0.228     |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 50500     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.6670557 |\n",
      "|    clip_fraction        | 0.165     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.086    |\n",
      "|    explained_variance   | 0.532     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0937   |\n",
      "|    n_updates            | 3743      |\n",
      "|    policy_gradient_loss | -0.0788   |\n",
      "|    value_loss           | 0.156     |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 60.3     |\n",
      "| time/              |          |\n",
      "|    fps             | 768      |\n",
      "|    iterations      | 198      |\n",
      "|    time_elapsed    | 65       |\n",
      "|    total_timesteps | 50688    |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 60.3      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 769       |\n",
      "|    iterations           | 199       |\n",
      "|    time_elapsed         | 66        |\n",
      "|    total_timesteps      | 50944     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.0595658 |\n",
      "|    clip_fraction        | 0.21      |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0974   |\n",
      "|    explained_variance   | 0.441     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.047    |\n",
      "|    n_updates            | 3762      |\n",
      "|    policy_gradient_loss | -0.0742   |\n",
      "|    value_loss           | 0.132     |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=51000, episode_reward=14.61 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 14.6      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 51000     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.6459952 |\n",
      "|    clip_fraction        | 0.227     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.131    |\n",
      "|    explained_variance   | -0.486    |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0812   |\n",
      "|    n_updates            | 3781      |\n",
      "|    policy_gradient_loss | -0.0798   |\n",
      "|    value_loss           | 0.0595    |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 60.3     |\n",
      "| time/              |          |\n",
      "|    fps             | 769      |\n",
      "|    iterations      | 200      |\n",
      "|    time_elapsed    | 66       |\n",
      "|    total_timesteps | 51200    |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 60.3      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 770       |\n",
      "|    iterations           | 201       |\n",
      "|    time_elapsed         | 66        |\n",
      "|    total_timesteps      | 51456     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.4313873 |\n",
      "|    clip_fraction        | 0.258     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0834   |\n",
      "|    explained_variance   | 0.395     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.14     |\n",
      "|    n_updates            | 3800      |\n",
      "|    policy_gradient_loss | -0.106    |\n",
      "|    value_loss           | 0.134     |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=51500, episode_reward=-3.08 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | -3.08     |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 51500     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.3777928 |\n",
      "|    clip_fraction        | 0.185     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0592   |\n",
      "|    explained_variance   | 0.591     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0508   |\n",
      "|    n_updates            | 3819      |\n",
      "|    policy_gradient_loss | -0.0697   |\n",
      "|    value_loss           | 0.0564    |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 60.3     |\n",
      "| time/              |          |\n",
      "|    fps             | 770      |\n",
      "|    iterations      | 202      |\n",
      "|    time_elapsed    | 67       |\n",
      "|    total_timesteps | 51712    |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 60.3      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 770       |\n",
      "|    iterations           | 203       |\n",
      "|    time_elapsed         | 67        |\n",
      "|    total_timesteps      | 51968     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.7254224 |\n",
      "|    clip_fraction        | 0.262     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.119    |\n",
      "|    explained_variance   | -0.0262   |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0569   |\n",
      "|    n_updates            | 3838      |\n",
      "|    policy_gradient_loss | -0.0683   |\n",
      "|    value_loss           | 0.164     |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=52000, episode_reward=29.38 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 29.4      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 52000     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.8627721 |\n",
      "|    clip_fraction        | 0.159     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0618   |\n",
      "|    explained_variance   | 0.622     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.12     |\n",
      "|    n_updates            | 3857      |\n",
      "|    policy_gradient_loss | -0.0664   |\n",
      "|    value_loss           | 0.0398    |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 60.3     |\n",
      "| time/              |          |\n",
      "|    fps             | 769      |\n",
      "|    iterations      | 204      |\n",
      "|    time_elapsed    | 67       |\n",
      "|    total_timesteps | 52224    |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 60.6      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 771       |\n",
      "|    iterations           | 205       |\n",
      "|    time_elapsed         | 68        |\n",
      "|    total_timesteps      | 52480     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.2766839 |\n",
      "|    clip_fraction        | 0.215     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0864   |\n",
      "|    explained_variance   | 0.284     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.156    |\n",
      "|    n_updates            | 3876      |\n",
      "|    policy_gradient_loss | -0.0855   |\n",
      "|    value_loss           | 0.173     |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=52500, episode_reward=62.86 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 62.9      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 52500     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.0250511 |\n",
      "|    clip_fraction        | 0.186     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0794   |\n",
      "|    explained_variance   | 0.313     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0255   |\n",
      "|    n_updates            | 3895      |\n",
      "|    policy_gradient_loss | -0.0514   |\n",
      "|    value_loss           | 0.454     |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 60.6     |\n",
      "| time/              |          |\n",
      "|    fps             | 770      |\n",
      "|    iterations      | 206      |\n",
      "|    time_elapsed    | 68       |\n",
      "|    total_timesteps | 52736    |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 60.6      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 772       |\n",
      "|    iterations           | 207       |\n",
      "|    time_elapsed         | 68        |\n",
      "|    total_timesteps      | 52992     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 3.0671568 |\n",
      "|    clip_fraction        | 0.352     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0634   |\n",
      "|    explained_variance   | -0.59     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.118    |\n",
      "|    n_updates            | 3914      |\n",
      "|    policy_gradient_loss | -0.119    |\n",
      "|    value_loss           | 0.0957    |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=53000, episode_reward=39.87 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 39.9      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 53000     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.1834066 |\n",
      "|    clip_fraction        | 0.194     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0736   |\n",
      "|    explained_variance   | 0.42      |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0585   |\n",
      "|    n_updates            | 3933      |\n",
      "|    policy_gradient_loss | -0.0674   |\n",
      "|    value_loss           | 0.139     |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 60.6     |\n",
      "| time/              |          |\n",
      "|    fps             | 771      |\n",
      "|    iterations      | 208      |\n",
      "|    time_elapsed    | 68       |\n",
      "|    total_timesteps | 53248    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=53500, episode_reward=46.36 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 46.4      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 53500     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.1533889 |\n",
      "|    clip_fraction        | 0.164     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0727   |\n",
      "|    explained_variance   | 0.474     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0544   |\n",
      "|    n_updates            | 3952      |\n",
      "|    policy_gradient_loss | -0.0513   |\n",
      "|    value_loss           | 0.0805    |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 60.7     |\n",
      "| time/              |          |\n",
      "|    fps             | 771      |\n",
      "|    iterations      | 209      |\n",
      "|    time_elapsed    | 69       |\n",
      "|    total_timesteps | 53504    |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 60.7      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 772       |\n",
      "|    iterations           | 210       |\n",
      "|    time_elapsed         | 69        |\n",
      "|    total_timesteps      | 53760     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.7862947 |\n",
      "|    clip_fraction        | 0.216     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.129    |\n",
      "|    explained_variance   | -0.14     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | 0.0468    |\n",
      "|    n_updates            | 3971      |\n",
      "|    policy_gradient_loss | -0.0386   |\n",
      "|    value_loss           | 1.26      |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=54000, episode_reward=79.12 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 79.1      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 54000     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.3549054 |\n",
      "|    clip_fraction        | 0.178     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0644   |\n",
      "|    explained_variance   | 0.565     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0884   |\n",
      "|    n_updates            | 3990      |\n",
      "|    policy_gradient_loss | -0.0679   |\n",
      "|    value_loss           | 0.0979    |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 60.7     |\n",
      "| time/              |          |\n",
      "|    fps             | 771      |\n",
      "|    iterations      | 211      |\n",
      "|    time_elapsed    | 69       |\n",
      "|    total_timesteps | 54016    |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 60.8      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 772       |\n",
      "|    iterations           | 212       |\n",
      "|    time_elapsed         | 70        |\n",
      "|    total_timesteps      | 54272     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.2415206 |\n",
      "|    clip_fraction        | 0.221     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0819   |\n",
      "|    explained_variance   | 0.311     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.156    |\n",
      "|    n_updates            | 4009      |\n",
      "|    policy_gradient_loss | -0.101    |\n",
      "|    value_loss           | 0.16      |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=54500, episode_reward=68.91 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 230        |\n",
      "|    mean_reward          | 68.9       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 54500      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.96270734 |\n",
      "|    clip_fraction        | 0.174      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0782    |\n",
      "|    explained_variance   | 0.437      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | 0.0292     |\n",
      "|    n_updates            | 4028       |\n",
      "|    policy_gradient_loss | -0.0454    |\n",
      "|    value_loss           | 0.864      |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 60.8     |\n",
      "| time/              |          |\n",
      "|    fps             | 772      |\n",
      "|    iterations      | 213      |\n",
      "|    time_elapsed    | 70       |\n",
      "|    total_timesteps | 54528    |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 60.8      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 773       |\n",
      "|    iterations           | 214       |\n",
      "|    time_elapsed         | 70        |\n",
      "|    total_timesteps      | 54784     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.6479879 |\n",
      "|    clip_fraction        | 0.238     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.112    |\n",
      "|    explained_variance   | -0.345    |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.126    |\n",
      "|    n_updates            | 4047      |\n",
      "|    policy_gradient_loss | -0.0746   |\n",
      "|    value_loss           | 0.0687    |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=55000, episode_reward=88.77 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 88.8      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 55000     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.6601081 |\n",
      "|    clip_fraction        | 0.149     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0649   |\n",
      "|    explained_variance   | 0.585     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0268   |\n",
      "|    n_updates            | 4066      |\n",
      "|    policy_gradient_loss | -0.0568   |\n",
      "|    value_loss           | 0.157     |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 60.8     |\n",
      "| time/              |          |\n",
      "|    fps             | 772      |\n",
      "|    iterations      | 215      |\n",
      "|    time_elapsed    | 71       |\n",
      "|    total_timesteps | 55040    |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 60.7      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 773       |\n",
      "|    iterations           | 216       |\n",
      "|    time_elapsed         | 71        |\n",
      "|    total_timesteps      | 55296     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.6217628 |\n",
      "|    clip_fraction        | 0.141     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0495   |\n",
      "|    explained_variance   | 0.733     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0265   |\n",
      "|    n_updates            | 4085      |\n",
      "|    policy_gradient_loss | -0.051    |\n",
      "|    value_loss           | 0.0853    |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=55500, episode_reward=86.45 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 230        |\n",
      "|    mean_reward          | 86.5       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 55500      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.46679094 |\n",
      "|    clip_fraction        | 0.231      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.179     |\n",
      "|    explained_variance   | 0.456      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | 0.163      |\n",
      "|    n_updates            | 4104       |\n",
      "|    policy_gradient_loss | -0.045     |\n",
      "|    value_loss           | 0.484      |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 60.7     |\n",
      "| time/              |          |\n",
      "|    fps             | 772      |\n",
      "|    iterations      | 217      |\n",
      "|    time_elapsed    | 71       |\n",
      "|    total_timesteps | 55552    |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 60.7       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 773        |\n",
      "|    iterations           | 218        |\n",
      "|    time_elapsed         | 72         |\n",
      "|    total_timesteps      | 55808      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.94148755 |\n",
      "|    clip_fraction        | 0.175      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0703    |\n",
      "|    explained_variance   | 0.599      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.107     |\n",
      "|    n_updates            | 4123       |\n",
      "|    policy_gradient_loss | -0.0606    |\n",
      "|    value_loss           | 0.136      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=56000, episode_reward=74.75 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 230        |\n",
      "|    mean_reward          | 74.8       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 56000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.40381733 |\n",
      "|    clip_fraction        | 0.128      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.056     |\n",
      "|    explained_variance   | 0.669      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0964    |\n",
      "|    n_updates            | 4142       |\n",
      "|    policy_gradient_loss | -0.053     |\n",
      "|    value_loss           | 0.095      |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 60.8     |\n",
      "| time/              |          |\n",
      "|    fps             | 773      |\n",
      "|    iterations      | 219      |\n",
      "|    time_elapsed    | 72       |\n",
      "|    total_timesteps | 56064    |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 60.8      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 774       |\n",
      "|    iterations           | 220       |\n",
      "|    time_elapsed         | 72        |\n",
      "|    total_timesteps      | 56320     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.7762008 |\n",
      "|    clip_fraction        | 0.235     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.118    |\n",
      "|    explained_variance   | 0.72      |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.138    |\n",
      "|    n_updates            | 4161      |\n",
      "|    policy_gradient_loss | -0.0712   |\n",
      "|    value_loss           | 0.214     |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=56500, episode_reward=35.25 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 230        |\n",
      "|    mean_reward          | 35.2       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 56500      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.55524796 |\n",
      "|    clip_fraction        | 0.188      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.112     |\n",
      "|    explained_variance   | 0.505      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0735    |\n",
      "|    n_updates            | 4180       |\n",
      "|    policy_gradient_loss | -0.0651    |\n",
      "|    value_loss           | 0.0443     |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 60.8     |\n",
      "| time/              |          |\n",
      "|    fps             | 773      |\n",
      "|    iterations      | 221      |\n",
      "|    time_elapsed    | 73       |\n",
      "|    total_timesteps | 56576    |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 60.8      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 774       |\n",
      "|    iterations           | 222       |\n",
      "|    time_elapsed         | 73        |\n",
      "|    total_timesteps      | 56832     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.7003895 |\n",
      "|    clip_fraction        | 0.144     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0585   |\n",
      "|    explained_variance   | 0.598     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.116    |\n",
      "|    n_updates            | 4199      |\n",
      "|    policy_gradient_loss | -0.0595   |\n",
      "|    value_loss           | 0.178     |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=57000, episode_reward=17.66 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 17.7      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 57000     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.3814666 |\n",
      "|    clip_fraction        | 0.141     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0574   |\n",
      "|    explained_variance   | 0.446     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.114    |\n",
      "|    n_updates            | 4218      |\n",
      "|    policy_gradient_loss | -0.0563   |\n",
      "|    value_loss           | 0.247     |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 60.8     |\n",
      "| time/              |          |\n",
      "|    fps             | 773      |\n",
      "|    iterations      | 223      |\n",
      "|    time_elapsed    | 73       |\n",
      "|    total_timesteps | 57088    |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 60.8      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 773       |\n",
      "|    iterations           | 224       |\n",
      "|    time_elapsed         | 74        |\n",
      "|    total_timesteps      | 57344     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.7594228 |\n",
      "|    clip_fraction        | 0.295     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.167    |\n",
      "|    explained_variance   | 0.34      |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0994   |\n",
      "|    n_updates            | 4237      |\n",
      "|    policy_gradient_loss | -0.0748   |\n",
      "|    value_loss           | 0.119     |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=57500, episode_reward=69.67 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 69.7      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 57500     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.5386716 |\n",
      "|    clip_fraction        | 0.127     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0404   |\n",
      "|    explained_variance   | 0.437     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0516   |\n",
      "|    n_updates            | 4256      |\n",
      "|    policy_gradient_loss | -0.0427   |\n",
      "|    value_loss           | 0.119     |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 60.8     |\n",
      "| time/              |          |\n",
      "|    fps             | 773      |\n",
      "|    iterations      | 225      |\n",
      "|    time_elapsed    | 74       |\n",
      "|    total_timesteps | 57600    |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 60.8      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 773       |\n",
      "|    iterations           | 226       |\n",
      "|    time_elapsed         | 74        |\n",
      "|    total_timesteps      | 57856     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.2686487 |\n",
      "|    clip_fraction        | 0.201     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0655   |\n",
      "|    explained_variance   | 0.596     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0467   |\n",
      "|    n_updates            | 4275      |\n",
      "|    policy_gradient_loss | -0.0695   |\n",
      "|    value_loss           | 0.123     |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=58000, episode_reward=48.49 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 48.5      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 58000     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.0467852 |\n",
      "|    clip_fraction        | 0.236     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.138    |\n",
      "|    explained_variance   | 0.398     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0721   |\n",
      "|    n_updates            | 4294      |\n",
      "|    policy_gradient_loss | -0.0761   |\n",
      "|    value_loss           | 0.546     |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 60.8     |\n",
      "| time/              |          |\n",
      "|    fps             | 773      |\n",
      "|    iterations      | 227      |\n",
      "|    time_elapsed    | 75       |\n",
      "|    total_timesteps | 58112    |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 60.8      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 774       |\n",
      "|    iterations           | 228       |\n",
      "|    time_elapsed         | 75        |\n",
      "|    total_timesteps      | 58368     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.0349951 |\n",
      "|    clip_fraction        | 0.214     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.116    |\n",
      "|    explained_variance   | -0.0895   |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0745   |\n",
      "|    n_updates            | 4313      |\n",
      "|    policy_gradient_loss | -0.0902   |\n",
      "|    value_loss           | 0.0827    |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=58500, episode_reward=35.17 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 35.2      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 58500     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.9399118 |\n",
      "|    clip_fraction        | 0.178     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0817   |\n",
      "|    explained_variance   | 0.441     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0714   |\n",
      "|    n_updates            | 4332      |\n",
      "|    policy_gradient_loss | -0.0721   |\n",
      "|    value_loss           | 0.305     |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 60.8     |\n",
      "| time/              |          |\n",
      "|    fps             | 773      |\n",
      "|    iterations      | 229      |\n",
      "|    time_elapsed    | 75       |\n",
      "|    total_timesteps | 58624    |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 60.7      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 774       |\n",
      "|    iterations           | 230       |\n",
      "|    time_elapsed         | 75        |\n",
      "|    total_timesteps      | 58880     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.7588611 |\n",
      "|    clip_fraction        | 0.146     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0795   |\n",
      "|    explained_variance   | 0.212     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0974   |\n",
      "|    n_updates            | 4351      |\n",
      "|    policy_gradient_loss | -0.0594   |\n",
      "|    value_loss           | 0.335     |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=59000, episode_reward=62.63 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 62.6      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 59000     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.5053607 |\n",
      "|    clip_fraction        | 0.209     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.144    |\n",
      "|    explained_variance   | 0.37      |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0699   |\n",
      "|    n_updates            | 4370      |\n",
      "|    policy_gradient_loss | -0.0793   |\n",
      "|    value_loss           | 0.136     |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 60.7     |\n",
      "| time/              |          |\n",
      "|    fps             | 773      |\n",
      "|    iterations      | 231      |\n",
      "|    time_elapsed    | 76       |\n",
      "|    total_timesteps | 59136    |\n",
      "---------------------------------\n",
      "--------------------------------------\n",
      "| rollout/                |          |\n",
      "|    ep_len_mean          | 889      |\n",
      "|    ep_rew_mean          | 60.7     |\n",
      "| time/                   |          |\n",
      "|    fps                  | 774      |\n",
      "|    iterations           | 232      |\n",
      "|    time_elapsed         | 76       |\n",
      "|    total_timesteps      | 59392    |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 1.165285 |\n",
      "|    clip_fraction        | 0.192    |\n",
      "|    clip_range           | 0.4      |\n",
      "|    entropy_loss         | -0.0814  |\n",
      "|    explained_variance   | 0.648    |\n",
      "|    learning_rate        | 0.000389 |\n",
      "|    loss                 | -0.11    |\n",
      "|    n_updates            | 4389     |\n",
      "|    policy_gradient_loss | -0.0624  |\n",
      "|    value_loss           | 0.108    |\n",
      "--------------------------------------\n",
      "Eval num_timesteps=59500, episode_reward=42.77 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 230        |\n",
      "|    mean_reward          | 42.8       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 59500      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.81255364 |\n",
      "|    clip_fraction        | 0.177      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0705    |\n",
      "|    explained_variance   | 0.444      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.04      |\n",
      "|    n_updates            | 4408       |\n",
      "|    policy_gradient_loss | -0.0658    |\n",
      "|    value_loss           | 0.241      |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 60.7     |\n",
      "| time/              |          |\n",
      "|    fps             | 774      |\n",
      "|    iterations      | 233      |\n",
      "|    time_elapsed    | 77       |\n",
      "|    total_timesteps | 59648    |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 60.7      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 775       |\n",
      "|    iterations           | 234       |\n",
      "|    time_elapsed         | 77        |\n",
      "|    total_timesteps      | 59904     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.7932642 |\n",
      "|    clip_fraction        | 0.208     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.1      |\n",
      "|    explained_variance   | 0.0961    |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0246   |\n",
      "|    n_updates            | 4427      |\n",
      "|    policy_gradient_loss | -0.0496   |\n",
      "|    value_loss           | 0.504     |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=60000, episode_reward=27.90 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 230        |\n",
      "|    mean_reward          | 27.9       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 60000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.89944625 |\n",
      "|    clip_fraction        | 0.194      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.095     |\n",
      "|    explained_variance   | 0.39       |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.126     |\n",
      "|    n_updates            | 4446       |\n",
      "|    policy_gradient_loss | -0.0772    |\n",
      "|    value_loss           | 0.0944     |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 60.7     |\n",
      "| time/              |          |\n",
      "|    fps             | 775      |\n",
      "|    iterations      | 235      |\n",
      "|    time_elapsed    | 77       |\n",
      "|    total_timesteps | 60160    |\n",
      "---------------------------------\n",
      "--------------------------------------\n",
      "| rollout/                |          |\n",
      "|    ep_len_mean          | 889      |\n",
      "|    ep_rew_mean          | 60.7     |\n",
      "| time/                   |          |\n",
      "|    fps                  | 776      |\n",
      "|    iterations           | 236      |\n",
      "|    time_elapsed         | 77       |\n",
      "|    total_timesteps      | 60416    |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 1.009316 |\n",
      "|    clip_fraction        | 0.167    |\n",
      "|    clip_range           | 0.4      |\n",
      "|    entropy_loss         | -0.0675  |\n",
      "|    explained_variance   | 0.778    |\n",
      "|    learning_rate        | 0.000389 |\n",
      "|    loss                 | -0.129   |\n",
      "|    n_updates            | 4465     |\n",
      "|    policy_gradient_loss | -0.0755  |\n",
      "|    value_loss           | 0.176    |\n",
      "--------------------------------------\n",
      "Eval num_timesteps=60500, episode_reward=16.88 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 16.9      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 60500     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.7880783 |\n",
      "|    clip_fraction        | 0.181     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0759   |\n",
      "|    explained_variance   | 0.49      |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0799   |\n",
      "|    n_updates            | 4484      |\n",
      "|    policy_gradient_loss | -0.0638   |\n",
      "|    value_loss           | 0.196     |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 60.6     |\n",
      "| time/              |          |\n",
      "|    fps             | 776      |\n",
      "|    iterations      | 237      |\n",
      "|    time_elapsed    | 78       |\n",
      "|    total_timesteps | 60672    |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 60.6       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 775        |\n",
      "|    iterations           | 238        |\n",
      "|    time_elapsed         | 78         |\n",
      "|    total_timesteps      | 60928      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.24485934 |\n",
      "|    clip_fraction        | 0.129      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0971    |\n",
      "|    explained_variance   | 0.68       |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.05      |\n",
      "|    n_updates            | 4503       |\n",
      "|    policy_gradient_loss | -0.0535    |\n",
      "|    value_loss           | 0.13       |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=61000, episode_reward=25.04 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 25        |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 61000     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.1015992 |\n",
      "|    clip_fraction        | 0.204     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0787   |\n",
      "|    explained_variance   | 0.714     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.123    |\n",
      "|    n_updates            | 4522      |\n",
      "|    policy_gradient_loss | -0.0654   |\n",
      "|    value_loss           | 0.131     |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 60.6     |\n",
      "| time/              |          |\n",
      "|    fps             | 775      |\n",
      "|    iterations      | 239      |\n",
      "|    time_elapsed    | 78       |\n",
      "|    total_timesteps | 61184    |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 60.5      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 776       |\n",
      "|    iterations           | 240       |\n",
      "|    time_elapsed         | 79        |\n",
      "|    total_timesteps      | 61440     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.5133326 |\n",
      "|    clip_fraction        | 0.244     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0814   |\n",
      "|    explained_variance   | 0.659     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.128    |\n",
      "|    n_updates            | 4541      |\n",
      "|    policy_gradient_loss | -0.0919   |\n",
      "|    value_loss           | 0.116     |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=61500, episode_reward=-2.34 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | -2.34     |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 61500     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.1314498 |\n",
      "|    clip_fraction        | 0.25      |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.129    |\n",
      "|    explained_variance   | 0.677     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.122    |\n",
      "|    n_updates            | 4560      |\n",
      "|    policy_gradient_loss | -0.0842   |\n",
      "|    value_loss           | 0.179     |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 60.5     |\n",
      "| time/              |          |\n",
      "|    fps             | 776      |\n",
      "|    iterations      | 241      |\n",
      "|    time_elapsed    | 79       |\n",
      "|    total_timesteps | 61696    |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 60.5       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 777        |\n",
      "|    iterations           | 242        |\n",
      "|    time_elapsed         | 79         |\n",
      "|    total_timesteps      | 61952      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.90147334 |\n",
      "|    clip_fraction        | 0.216      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.112     |\n",
      "|    explained_variance   | 0.616      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.115     |\n",
      "|    n_updates            | 4579       |\n",
      "|    policy_gradient_loss | -0.0748    |\n",
      "|    value_loss           | 0.0516     |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=62000, episode_reward=5.71 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 230        |\n",
      "|    mean_reward          | 5.71       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 62000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.45412073 |\n",
      "|    clip_fraction        | 0.123      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0511    |\n",
      "|    explained_variance   | 0.691      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.045     |\n",
      "|    n_updates            | 4598       |\n",
      "|    policy_gradient_loss | -0.0608    |\n",
      "|    value_loss           | 0.175      |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 60.5     |\n",
      "| time/              |          |\n",
      "|    fps             | 776      |\n",
      "|    iterations      | 243      |\n",
      "|    time_elapsed    | 80       |\n",
      "|    total_timesteps | 62208    |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 60.4      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 777       |\n",
      "|    iterations           | 244       |\n",
      "|    time_elapsed         | 80        |\n",
      "|    total_timesteps      | 62464     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.5507896 |\n",
      "|    clip_fraction        | 0.128     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0658   |\n",
      "|    explained_variance   | 0.442     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0116   |\n",
      "|    n_updates            | 4617      |\n",
      "|    policy_gradient_loss | -0.0525   |\n",
      "|    value_loss           | 0.337     |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=62500, episode_reward=57.76 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 230        |\n",
      "|    mean_reward          | 57.8       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 62500      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.56556296 |\n",
      "|    clip_fraction        | 0.198      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.147     |\n",
      "|    explained_variance   | -0.221     |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0859    |\n",
      "|    n_updates            | 4636       |\n",
      "|    policy_gradient_loss | -0.0763    |\n",
      "|    value_loss           | 0.0736     |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 60.4     |\n",
      "| time/              |          |\n",
      "|    fps             | 776      |\n",
      "|    iterations      | 245      |\n",
      "|    time_elapsed    | 80       |\n",
      "|    total_timesteps | 62720    |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 60.4       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 777        |\n",
      "|    iterations           | 246        |\n",
      "|    time_elapsed         | 81         |\n",
      "|    total_timesteps      | 62976      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.63876003 |\n",
      "|    clip_fraction        | 0.153      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0688    |\n",
      "|    explained_variance   | 0.657      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0857    |\n",
      "|    n_updates            | 4655       |\n",
      "|    policy_gradient_loss | -0.0708    |\n",
      "|    value_loss           | 0.14       |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=63000, episode_reward=40.81 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 230        |\n",
      "|    mean_reward          | 40.8       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 63000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.48827976 |\n",
      "|    clip_fraction        | 0.109      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0697    |\n",
      "|    explained_variance   | 0.331      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.082     |\n",
      "|    n_updates            | 4674       |\n",
      "|    policy_gradient_loss | -0.05      |\n",
      "|    value_loss           | 0.32       |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 60.2     |\n",
      "| time/              |          |\n",
      "|    fps             | 776      |\n",
      "|    iterations      | 247      |\n",
      "|    time_elapsed    | 81       |\n",
      "|    total_timesteps | 63232    |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 60.2       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 777        |\n",
      "|    iterations           | 248        |\n",
      "|    time_elapsed         | 81         |\n",
      "|    total_timesteps      | 63488      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.53723335 |\n",
      "|    clip_fraction        | 0.205      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.162     |\n",
      "|    explained_variance   | 0.557      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0386    |\n",
      "|    n_updates            | 4693       |\n",
      "|    policy_gradient_loss | -0.0643    |\n",
      "|    value_loss           | 0.287      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=63500, episode_reward=20.94 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 20.9      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 63500     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.8082762 |\n",
      "|    clip_fraction        | 0.183     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0706   |\n",
      "|    explained_variance   | 0.53      |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0862   |\n",
      "|    n_updates            | 4712      |\n",
      "|    policy_gradient_loss | -0.0723   |\n",
      "|    value_loss           | 0.0558    |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 60.2     |\n",
      "| time/              |          |\n",
      "|    fps             | 777      |\n",
      "|    iterations      | 249      |\n",
      "|    time_elapsed    | 81       |\n",
      "|    total_timesteps | 63744    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=64000, episode_reward=31.31 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 230        |\n",
      "|    mean_reward          | 31.3       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 64000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.55971575 |\n",
      "|    clip_fraction        | 0.125      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0705    |\n",
      "|    explained_variance   | 0.731      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0784    |\n",
      "|    n_updates            | 4731       |\n",
      "|    policy_gradient_loss | -0.0481    |\n",
      "|    value_loss           | 0.157      |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 60.2     |\n",
      "| time/              |          |\n",
      "|    fps             | 776      |\n",
      "|    iterations      | 250      |\n",
      "|    time_elapsed    | 82       |\n",
      "|    total_timesteps | 64000    |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 60.2       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 776        |\n",
      "|    iterations           | 251        |\n",
      "|    time_elapsed         | 82         |\n",
      "|    total_timesteps      | 64256      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.71356875 |\n",
      "|    clip_fraction        | 0.206      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.106     |\n",
      "|    explained_variance   | 0.596      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.114     |\n",
      "|    n_updates            | 4750       |\n",
      "|    policy_gradient_loss | -0.0649    |\n",
      "|    value_loss           | 0.215      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=64500, episode_reward=31.89 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 31.9      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 64500     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.0081613 |\n",
      "|    clip_fraction        | 0.227     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.145    |\n",
      "|    explained_variance   | -0.606    |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.132    |\n",
      "|    n_updates            | 4769      |\n",
      "|    policy_gradient_loss | -0.0592   |\n",
      "|    value_loss           | 0.0576    |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 60.2     |\n",
      "| time/              |          |\n",
      "|    fps             | 774      |\n",
      "|    iterations      | 252      |\n",
      "|    time_elapsed    | 83       |\n",
      "|    total_timesteps | 64512    |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 60.2      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 775       |\n",
      "|    iterations           | 253       |\n",
      "|    time_elapsed         | 83        |\n",
      "|    total_timesteps      | 64768     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.9688788 |\n",
      "|    clip_fraction        | 0.205     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0903   |\n",
      "|    explained_variance   | 0.57      |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.116    |\n",
      "|    n_updates            | 4788      |\n",
      "|    policy_gradient_loss | -0.0795   |\n",
      "|    value_loss           | 0.136     |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=65000, episode_reward=28.72 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 28.7      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 65000     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.2684083 |\n",
      "|    clip_fraction        | 0.258     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.112    |\n",
      "|    explained_variance   | 0.372     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0782   |\n",
      "|    n_updates            | 4807      |\n",
      "|    policy_gradient_loss | -0.0813   |\n",
      "|    value_loss           | 0.216     |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 60.4     |\n",
      "| time/              |          |\n",
      "|    fps             | 774      |\n",
      "|    iterations      | 254      |\n",
      "|    time_elapsed    | 83       |\n",
      "|    total_timesteps | 65024    |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 60.4      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 774       |\n",
      "|    iterations           | 255       |\n",
      "|    time_elapsed         | 84        |\n",
      "|    total_timesteps      | 65280     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.6261545 |\n",
      "|    clip_fraction        | 0.203     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.17     |\n",
      "|    explained_variance   | 0.706     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0476   |\n",
      "|    n_updates            | 4826      |\n",
      "|    policy_gradient_loss | -0.0513   |\n",
      "|    value_loss           | 0.277     |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=65500, episode_reward=54.58 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 54.6      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 65500     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.9331926 |\n",
      "|    clip_fraction        | 0.222     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0751   |\n",
      "|    explained_variance   | 0.593     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0894   |\n",
      "|    n_updates            | 4845      |\n",
      "|    policy_gradient_loss | -0.088    |\n",
      "|    value_loss           | 0.0523    |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 60.4     |\n",
      "| time/              |          |\n",
      "|    fps             | 774      |\n",
      "|    iterations      | 256      |\n",
      "|    time_elapsed    | 84       |\n",
      "|    total_timesteps | 65536    |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 60.6      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 774       |\n",
      "|    iterations           | 257       |\n",
      "|    time_elapsed         | 84        |\n",
      "|    total_timesteps      | 65792     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.8976375 |\n",
      "|    clip_fraction        | 0.213     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0915   |\n",
      "|    explained_variance   | 0.313     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.207    |\n",
      "|    n_updates            | 4864      |\n",
      "|    policy_gradient_loss | -0.101    |\n",
      "|    value_loss           | 0.33      |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=66000, episode_reward=17.48 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 230        |\n",
      "|    mean_reward          | 17.5       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 66000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.71930575 |\n",
      "|    clip_fraction        | 0.116      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0688    |\n",
      "|    explained_variance   | 0.41       |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0389    |\n",
      "|    n_updates            | 4883       |\n",
      "|    policy_gradient_loss | -0.0329    |\n",
      "|    value_loss           | 0.309      |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 60.6     |\n",
      "| time/              |          |\n",
      "|    fps             | 774      |\n",
      "|    iterations      | 258      |\n",
      "|    time_elapsed    | 85       |\n",
      "|    total_timesteps | 66048    |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 60.6       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 775        |\n",
      "|    iterations           | 259        |\n",
      "|    time_elapsed         | 85         |\n",
      "|    total_timesteps      | 66304      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.87850326 |\n",
      "|    clip_fraction        | 0.253      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.162     |\n",
      "|    explained_variance   | -0.277     |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.134     |\n",
      "|    n_updates            | 4902       |\n",
      "|    policy_gradient_loss | -0.0932    |\n",
      "|    value_loss           | 0.062      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=66500, episode_reward=88.54 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 230        |\n",
      "|    mean_reward          | 88.5       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 66500      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.69689035 |\n",
      "|    clip_fraction        | 0.179      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0812    |\n",
      "|    explained_variance   | 0.642      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.122     |\n",
      "|    n_updates            | 4921       |\n",
      "|    policy_gradient_loss | -0.072     |\n",
      "|    value_loss           | 0.114      |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 60.6     |\n",
      "| time/              |          |\n",
      "|    fps             | 774      |\n",
      "|    iterations      | 260      |\n",
      "|    time_elapsed    | 85       |\n",
      "|    total_timesteps | 66560    |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 60.7      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 775       |\n",
      "|    iterations           | 261       |\n",
      "|    time_elapsed         | 86        |\n",
      "|    total_timesteps      | 66816     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.9952235 |\n",
      "|    clip_fraction        | 0.181     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0803   |\n",
      "|    explained_variance   | 0.516     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0751   |\n",
      "|    n_updates            | 4940      |\n",
      "|    policy_gradient_loss | -0.0693   |\n",
      "|    value_loss           | 0.117     |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=67000, episode_reward=39.41 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 230        |\n",
      "|    mean_reward          | 39.4       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 67000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.89151394 |\n",
      "|    clip_fraction        | 0.235      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.154     |\n",
      "|    explained_variance   | 0.209      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0438    |\n",
      "|    n_updates            | 4959       |\n",
      "|    policy_gradient_loss | -0.0547    |\n",
      "|    value_loss           | 0.374      |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 60.7     |\n",
      "| time/              |          |\n",
      "|    fps             | 775      |\n",
      "|    iterations      | 262      |\n",
      "|    time_elapsed    | 86       |\n",
      "|    total_timesteps | 67072    |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 60.7      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 776       |\n",
      "|    iterations           | 263       |\n",
      "|    time_elapsed         | 86        |\n",
      "|    total_timesteps      | 67328     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.8140464 |\n",
      "|    clip_fraction        | 0.175     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0694   |\n",
      "|    explained_variance   | 0.444     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.108    |\n",
      "|    n_updates            | 4978      |\n",
      "|    policy_gradient_loss | -0.0773   |\n",
      "|    value_loss           | 0.0871    |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=67500, episode_reward=70.10 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 70.1      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 67500     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.7132896 |\n",
      "|    clip_fraction        | 0.156     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0877   |\n",
      "|    explained_variance   | 0.49      |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0403   |\n",
      "|    n_updates            | 4997      |\n",
      "|    policy_gradient_loss | -0.0583   |\n",
      "|    value_loss           | 0.172     |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 60.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 775      |\n",
      "|    iterations      | 264      |\n",
      "|    time_elapsed    | 87       |\n",
      "|    total_timesteps | 67584    |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 60.9      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 775       |\n",
      "|    iterations           | 265       |\n",
      "|    time_elapsed         | 87        |\n",
      "|    total_timesteps      | 67840     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.7923197 |\n",
      "|    clip_fraction        | 0.216     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.108    |\n",
      "|    explained_variance   | 0.514     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0342   |\n",
      "|    n_updates            | 5016      |\n",
      "|    policy_gradient_loss | -0.0533   |\n",
      "|    value_loss           | 0.429     |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=68000, episode_reward=57.87 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 57.9      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 68000     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.4041004 |\n",
      "|    clip_fraction        | 0.269     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.101    |\n",
      "|    explained_variance   | -0.148    |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.156    |\n",
      "|    n_updates            | 5035      |\n",
      "|    policy_gradient_loss | -0.0889   |\n",
      "|    value_loss           | 0.0572    |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 60.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 775      |\n",
      "|    iterations      | 266      |\n",
      "|    time_elapsed    | 87       |\n",
      "|    total_timesteps | 68096    |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 60.9      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 776       |\n",
      "|    iterations           | 267       |\n",
      "|    time_elapsed         | 88        |\n",
      "|    total_timesteps      | 68352     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.0303605 |\n",
      "|    clip_fraction        | 0.165     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0674   |\n",
      "|    explained_variance   | 0.61      |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0977   |\n",
      "|    n_updates            | 5054      |\n",
      "|    policy_gradient_loss | -0.0669   |\n",
      "|    value_loss           | 0.0966    |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=68500, episode_reward=37.40 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "--------------------------------------\n",
      "| eval/                   |          |\n",
      "|    mean_ep_length       | 230      |\n",
      "|    mean_reward          | 37.4     |\n",
      "| time/                   |          |\n",
      "|    total_timesteps      | 68500    |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 1.001294 |\n",
      "|    clip_fraction        | 0.202    |\n",
      "|    clip_range           | 0.4      |\n",
      "|    entropy_loss         | -0.0837  |\n",
      "|    explained_variance   | 0.432    |\n",
      "|    learning_rate        | 0.000389 |\n",
      "|    loss                 | -0.125   |\n",
      "|    n_updates            | 5073     |\n",
      "|    policy_gradient_loss | -0.0783  |\n",
      "|    value_loss           | 0.104    |\n",
      "--------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 60.8     |\n",
      "| time/              |          |\n",
      "|    fps             | 776      |\n",
      "|    iterations      | 268      |\n",
      "|    time_elapsed    | 88       |\n",
      "|    total_timesteps | 68608    |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 60.8      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 777       |\n",
      "|    iterations           | 269       |\n",
      "|    time_elapsed         | 88        |\n",
      "|    total_timesteps      | 68864     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.7084207 |\n",
      "|    clip_fraction        | 0.165     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.101    |\n",
      "|    explained_variance   | 0.188     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0361   |\n",
      "|    n_updates            | 5092      |\n",
      "|    policy_gradient_loss | -0.0703   |\n",
      "|    value_loss           | 0.261     |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=69000, episode_reward=30.28 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 30.3      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 69000     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.7282028 |\n",
      "|    clip_fraction        | 0.129     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0517   |\n",
      "|    explained_variance   | 0.193     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0953   |\n",
      "|    n_updates            | 5111      |\n",
      "|    policy_gradient_loss | -0.0525   |\n",
      "|    value_loss           | 0.0728    |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 60.8     |\n",
      "| time/              |          |\n",
      "|    fps             | 777      |\n",
      "|    iterations      | 270      |\n",
      "|    time_elapsed    | 88       |\n",
      "|    total_timesteps | 69120    |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 60.7      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 777       |\n",
      "|    iterations           | 271       |\n",
      "|    time_elapsed         | 89        |\n",
      "|    total_timesteps      | 69376     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.9287069 |\n",
      "|    clip_fraction        | 0.16      |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0723   |\n",
      "|    explained_variance   | 0.616     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.11     |\n",
      "|    n_updates            | 5130      |\n",
      "|    policy_gradient_loss | -0.0617   |\n",
      "|    value_loss           | 0.0863    |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=69500, episode_reward=1.22 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 1.22      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 69500     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.6123274 |\n",
      "|    clip_fraction        | 0.14      |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0481   |\n",
      "|    explained_variance   | 0.599     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.115    |\n",
      "|    n_updates            | 5149      |\n",
      "|    policy_gradient_loss | -0.0786   |\n",
      "|    value_loss           | 0.1       |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 60.7     |\n",
      "| time/              |          |\n",
      "|    fps             | 776      |\n",
      "|    iterations      | 272      |\n",
      "|    time_elapsed    | 89       |\n",
      "|    total_timesteps | 69632    |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 60.7      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 777       |\n",
      "|    iterations           | 273       |\n",
      "|    time_elapsed         | 89        |\n",
      "|    total_timesteps      | 69888     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.7586398 |\n",
      "|    clip_fraction        | 0.25      |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0925   |\n",
      "|    explained_variance   | 0.282     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | 0.00891   |\n",
      "|    n_updates            | 5168      |\n",
      "|    policy_gradient_loss | -0.0706   |\n",
      "|    value_loss           | 0.183     |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=70000, episode_reward=81.63 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 230        |\n",
      "|    mean_reward          | 81.6       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 70000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.56333745 |\n",
      "|    clip_fraction        | 0.149      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0697    |\n",
      "|    explained_variance   | 0.493      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0727    |\n",
      "|    n_updates            | 5187       |\n",
      "|    policy_gradient_loss | -0.059     |\n",
      "|    value_loss           | 0.151      |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 60.7     |\n",
      "| time/              |          |\n",
      "|    fps             | 777      |\n",
      "|    iterations      | 274      |\n",
      "|    time_elapsed    | 90       |\n",
      "|    total_timesteps | 70144    |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 60.7      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 778       |\n",
      "|    iterations           | 275       |\n",
      "|    time_elapsed         | 90        |\n",
      "|    total_timesteps      | 70400     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.0913548 |\n",
      "|    clip_fraction        | 0.198     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0616   |\n",
      "|    explained_variance   | 0.624     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0529   |\n",
      "|    n_updates            | 5206      |\n",
      "|    policy_gradient_loss | -0.0698   |\n",
      "|    value_loss           | 0.0914    |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=70500, episode_reward=32.65 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 32.6      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 70500     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.0984507 |\n",
      "|    clip_fraction        | 0.215     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.12     |\n",
      "|    explained_variance   | 0.308     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.112    |\n",
      "|    n_updates            | 5225      |\n",
      "|    policy_gradient_loss | -0.0532   |\n",
      "|    value_loss           | 0.357     |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 60.7     |\n",
      "| time/              |          |\n",
      "|    fps             | 777      |\n",
      "|    iterations      | 276      |\n",
      "|    time_elapsed    | 90       |\n",
      "|    total_timesteps | 70656    |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 60.7      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 778       |\n",
      "|    iterations           | 277       |\n",
      "|    time_elapsed         | 91        |\n",
      "|    total_timesteps      | 70912     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.9280186 |\n",
      "|    clip_fraction        | 0.263     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0879   |\n",
      "|    explained_variance   | 0.39      |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.133    |\n",
      "|    n_updates            | 5244      |\n",
      "|    policy_gradient_loss | -0.0787   |\n",
      "|    value_loss           | 0.15      |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=71000, episode_reward=6.97 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 6.97      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 71000     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.7156715 |\n",
      "|    clip_fraction        | 0.156     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0697   |\n",
      "|    explained_variance   | 0.374     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0682   |\n",
      "|    n_updates            | 5263      |\n",
      "|    policy_gradient_loss | -0.0685   |\n",
      "|    value_loss           | 0.141     |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 60.7     |\n",
      "| time/              |          |\n",
      "|    fps             | 777      |\n",
      "|    iterations      | 278      |\n",
      "|    time_elapsed    | 91       |\n",
      "|    total_timesteps | 71168    |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 60.7      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 777       |\n",
      "|    iterations           | 279       |\n",
      "|    time_elapsed         | 91        |\n",
      "|    total_timesteps      | 71424     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.1595602 |\n",
      "|    clip_fraction        | 0.199     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.103    |\n",
      "|    explained_variance   | 0.604     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0688   |\n",
      "|    n_updates            | 5282      |\n",
      "|    policy_gradient_loss | -0.0589   |\n",
      "|    value_loss           | 0.189     |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=71500, episode_reward=8.63 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 8.63      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 71500     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.4727929 |\n",
      "|    clip_fraction        | 0.255     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0967   |\n",
      "|    explained_variance   | 0.0933    |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.132    |\n",
      "|    n_updates            | 5301      |\n",
      "|    policy_gradient_loss | -0.0767   |\n",
      "|    value_loss           | 0.0759    |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 60.7     |\n",
      "| time/              |          |\n",
      "|    fps             | 776      |\n",
      "|    iterations      | 280      |\n",
      "|    time_elapsed    | 92       |\n",
      "|    total_timesteps | 71680    |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 60.7       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 777        |\n",
      "|    iterations           | 281        |\n",
      "|    time_elapsed         | 92         |\n",
      "|    total_timesteps      | 71936      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.88433874 |\n",
      "|    clip_fraction        | 0.203      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0961    |\n",
      "|    explained_variance   | 0.439      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0871    |\n",
      "|    n_updates            | 5320       |\n",
      "|    policy_gradient_loss | -0.0672    |\n",
      "|    value_loss           | 0.289      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=72000, episode_reward=-15.51 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | -15.5     |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 72000     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.3577363 |\n",
      "|    clip_fraction        | 0.167     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0639   |\n",
      "|    explained_variance   | 0.566     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.122    |\n",
      "|    n_updates            | 5339      |\n",
      "|    policy_gradient_loss | -0.0711   |\n",
      "|    value_loss           | 0.0994    |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 60.6     |\n",
      "| time/              |          |\n",
      "|    fps             | 777      |\n",
      "|    iterations      | 282      |\n",
      "|    time_elapsed    | 92       |\n",
      "|    total_timesteps | 72192    |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 60.6      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 777       |\n",
      "|    iterations           | 283       |\n",
      "|    time_elapsed         | 93        |\n",
      "|    total_timesteps      | 72448     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.1683631 |\n",
      "|    clip_fraction        | 0.193     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.114    |\n",
      "|    explained_variance   | 0.35      |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.078    |\n",
      "|    n_updates            | 5358      |\n",
      "|    policy_gradient_loss | -0.0408   |\n",
      "|    value_loss           | 0.119     |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=72500, episode_reward=45.52 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 45.5      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 72500     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.8438229 |\n",
      "|    clip_fraction        | 0.194     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0927   |\n",
      "|    explained_variance   | 0.415     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0766   |\n",
      "|    n_updates            | 5377      |\n",
      "|    policy_gradient_loss | -0.0662   |\n",
      "|    value_loss           | 0.204     |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 60.6     |\n",
      "| time/              |          |\n",
      "|    fps             | 777      |\n",
      "|    iterations      | 284      |\n",
      "|    time_elapsed    | 93       |\n",
      "|    total_timesteps | 72704    |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 60.5      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 778       |\n",
      "|    iterations           | 285       |\n",
      "|    time_elapsed         | 93        |\n",
      "|    total_timesteps      | 72960     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.8968333 |\n",
      "|    clip_fraction        | 0.181     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0738   |\n",
      "|    explained_variance   | 0.677     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.144    |\n",
      "|    n_updates            | 5396      |\n",
      "|    policy_gradient_loss | -0.0842   |\n",
      "|    value_loss           | 0.0902    |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=73000, episode_reward=14.70 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 14.7      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 73000     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.5719372 |\n",
      "|    clip_fraction        | 0.166     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.113    |\n",
      "|    explained_variance   | 0.386     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | 0.0292    |\n",
      "|    n_updates            | 5415      |\n",
      "|    policy_gradient_loss | -0.0523   |\n",
      "|    value_loss           | 0.344     |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 60.5     |\n",
      "| time/              |          |\n",
      "|    fps             | 777      |\n",
      "|    iterations      | 286      |\n",
      "|    time_elapsed    | 94       |\n",
      "|    total_timesteps | 73216    |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 60.5       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 778        |\n",
      "|    iterations           | 287        |\n",
      "|    time_elapsed         | 94         |\n",
      "|    total_timesteps      | 73472      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.93361187 |\n",
      "|    clip_fraction        | 0.244      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.105     |\n",
      "|    explained_variance   | 0.153      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0857    |\n",
      "|    n_updates            | 5434       |\n",
      "|    policy_gradient_loss | -0.0965    |\n",
      "|    value_loss           | 0.0612     |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=73500, episode_reward=38.90 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 38.9      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 73500     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.7052338 |\n",
      "|    clip_fraction        | 0.178     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0787   |\n",
      "|    explained_variance   | 0.642     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.091    |\n",
      "|    n_updates            | 5453      |\n",
      "|    policy_gradient_loss | -0.0816   |\n",
      "|    value_loss           | 0.161     |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 60.5     |\n",
      "| time/              |          |\n",
      "|    fps             | 777      |\n",
      "|    iterations      | 288      |\n",
      "|    time_elapsed    | 94       |\n",
      "|    total_timesteps | 73728    |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 60.6      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 778       |\n",
      "|    iterations           | 289       |\n",
      "|    time_elapsed         | 95        |\n",
      "|    total_timesteps      | 73984     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.6786781 |\n",
      "|    clip_fraction        | 0.172     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0852   |\n",
      "|    explained_variance   | 0.335     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.141    |\n",
      "|    n_updates            | 5472      |\n",
      "|    policy_gradient_loss | -0.066    |\n",
      "|    value_loss           | 0.177     |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=74000, episode_reward=56.29 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 56.3      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 74000     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.7182348 |\n",
      "|    clip_fraction        | 0.234     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.125    |\n",
      "|    explained_variance   | -0.171    |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.111    |\n",
      "|    n_updates            | 5491      |\n",
      "|    policy_gradient_loss | -0.0602   |\n",
      "|    value_loss           | 0.104     |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 60.6     |\n",
      "| time/              |          |\n",
      "|    fps             | 777      |\n",
      "|    iterations      | 290      |\n",
      "|    time_elapsed    | 95       |\n",
      "|    total_timesteps | 74240    |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 60.6       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 778        |\n",
      "|    iterations           | 291        |\n",
      "|    time_elapsed         | 95         |\n",
      "|    total_timesteps      | 74496      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.55507106 |\n",
      "|    clip_fraction        | 0.172      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.1       |\n",
      "|    explained_variance   | 0.711      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0476    |\n",
      "|    n_updates            | 5510       |\n",
      "|    policy_gradient_loss | -0.0804    |\n",
      "|    value_loss           | 0.0872     |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=74500, episode_reward=-6.11 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | -6.11     |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 74500     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.6804217 |\n",
      "|    clip_fraction        | 0.174     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.088    |\n",
      "|    explained_variance   | 0.651     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.086    |\n",
      "|    n_updates            | 5529      |\n",
      "|    policy_gradient_loss | -0.0655   |\n",
      "|    value_loss           | 0.0859    |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 60.7     |\n",
      "| time/              |          |\n",
      "|    fps             | 777      |\n",
      "|    iterations      | 292      |\n",
      "|    time_elapsed    | 96       |\n",
      "|    total_timesteps | 74752    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=75000, episode_reward=-2.23 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | -2.23     |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 75000     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.5659422 |\n",
      "|    clip_fraction        | 0.151     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0975   |\n",
      "|    explained_variance   | 0.461     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0448   |\n",
      "|    n_updates            | 5548      |\n",
      "|    policy_gradient_loss | -0.058    |\n",
      "|    value_loss           | 0.362     |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 60.7     |\n",
      "| time/              |          |\n",
      "|    fps             | 776      |\n",
      "|    iterations      | 293      |\n",
      "|    time_elapsed    | 96       |\n",
      "|    total_timesteps | 75008    |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 60.7      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 777       |\n",
      "|    iterations           | 294       |\n",
      "|    time_elapsed         | 96        |\n",
      "|    total_timesteps      | 75264     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.2179716 |\n",
      "|    clip_fraction        | 0.21      |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0966   |\n",
      "|    explained_variance   | 0.0755    |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.125    |\n",
      "|    n_updates            | 5567      |\n",
      "|    policy_gradient_loss | -0.0824   |\n",
      "|    value_loss           | 0.102     |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=75500, episode_reward=10.42 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 10.4      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 75500     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.5551866 |\n",
      "|    clip_fraction        | 0.176     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0747   |\n",
      "|    explained_variance   | 0.648     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.119    |\n",
      "|    n_updates            | 5586      |\n",
      "|    policy_gradient_loss | -0.0738   |\n",
      "|    value_loss           | 0.18      |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 60.7     |\n",
      "| time/              |          |\n",
      "|    fps             | 776      |\n",
      "|    iterations      | 295      |\n",
      "|    time_elapsed    | 97       |\n",
      "|    total_timesteps | 75520    |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 60.6      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 777       |\n",
      "|    iterations           | 296       |\n",
      "|    time_elapsed         | 97        |\n",
      "|    total_timesteps      | 75776     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.2119242 |\n",
      "|    clip_fraction        | 0.181     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0719   |\n",
      "|    explained_variance   | 0.699     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0615   |\n",
      "|    n_updates            | 5605      |\n",
      "|    policy_gradient_loss | -0.0674   |\n",
      "|    value_loss           | 0.256     |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=76000, episode_reward=19.58 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 19.6      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 76000     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.0037668 |\n",
      "|    clip_fraction        | 0.247     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.136    |\n",
      "|    explained_variance   | 0.62      |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.162    |\n",
      "|    n_updates            | 5624      |\n",
      "|    policy_gradient_loss | -0.0727   |\n",
      "|    value_loss           | 0.0909    |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 60.6     |\n",
      "| time/              |          |\n",
      "|    fps             | 776      |\n",
      "|    iterations      | 297      |\n",
      "|    time_elapsed    | 97       |\n",
      "|    total_timesteps | 76032    |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 60.6       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 777        |\n",
      "|    iterations           | 298        |\n",
      "|    time_elapsed         | 98         |\n",
      "|    total_timesteps      | 76288      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.84741175 |\n",
      "|    clip_fraction        | 0.202      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0973    |\n",
      "|    explained_variance   | 0.756      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0539    |\n",
      "|    n_updates            | 5643       |\n",
      "|    policy_gradient_loss | -0.0665    |\n",
      "|    value_loss           | 0.0936     |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=76500, episode_reward=34.39 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 34.4      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 76500     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.6301178 |\n",
      "|    clip_fraction        | 0.171     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0777   |\n",
      "|    explained_variance   | 0.336     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0745   |\n",
      "|    n_updates            | 5662      |\n",
      "|    policy_gradient_loss | -0.0755   |\n",
      "|    value_loss           | 0.158     |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 60.8     |\n",
      "| time/              |          |\n",
      "|    fps             | 777      |\n",
      "|    iterations      | 299      |\n",
      "|    time_elapsed    | 98       |\n",
      "|    total_timesteps | 76544    |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 60.8      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 778       |\n",
      "|    iterations           | 300       |\n",
      "|    time_elapsed         | 98        |\n",
      "|    total_timesteps      | 76800     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.7732216 |\n",
      "|    clip_fraction        | 0.236     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.142    |\n",
      "|    explained_variance   | 0.832     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | 0.000105  |\n",
      "|    n_updates            | 5681      |\n",
      "|    policy_gradient_loss | -0.0852   |\n",
      "|    value_loss           | 0.171     |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=77000, episode_reward=26.00 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 230        |\n",
      "|    mean_reward          | 26         |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 77000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.82656854 |\n",
      "|    clip_fraction        | 0.187      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0916    |\n",
      "|    explained_variance   | 0.438      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.11      |\n",
      "|    n_updates            | 5700       |\n",
      "|    policy_gradient_loss | -0.0716    |\n",
      "|    value_loss           | 0.0821     |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 60.8     |\n",
      "| time/              |          |\n",
      "|    fps             | 777      |\n",
      "|    iterations      | 301      |\n",
      "|    time_elapsed    | 99       |\n",
      "|    total_timesteps | 77056    |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 60.8      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 778       |\n",
      "|    iterations           | 302       |\n",
      "|    time_elapsed         | 99        |\n",
      "|    total_timesteps      | 77312     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.2199948 |\n",
      "|    clip_fraction        | 0.16      |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0706   |\n",
      "|    explained_variance   | 0.707     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0823   |\n",
      "|    n_updates            | 5719      |\n",
      "|    policy_gradient_loss | -0.0621   |\n",
      "|    value_loss           | 0.154     |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=77500, episode_reward=45.38 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 230        |\n",
      "|    mean_reward          | 45.4       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 77500      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.75436485 |\n",
      "|    clip_fraction        | 0.188      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0919    |\n",
      "|    explained_variance   | 0.756      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0746    |\n",
      "|    n_updates            | 5738       |\n",
      "|    policy_gradient_loss | -0.066     |\n",
      "|    value_loss           | 0.196      |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 60.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 778      |\n",
      "|    iterations      | 303      |\n",
      "|    time_elapsed    | 99       |\n",
      "|    total_timesteps | 77568    |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 60.9      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 779       |\n",
      "|    iterations           | 304       |\n",
      "|    time_elapsed         | 99        |\n",
      "|    total_timesteps      | 77824     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.6038122 |\n",
      "|    clip_fraction        | 0.18      |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.118    |\n",
      "|    explained_variance   | 0.453     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0501   |\n",
      "|    n_updates            | 5757      |\n",
      "|    policy_gradient_loss | -0.0607   |\n",
      "|    value_loss           | 0.126     |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=78000, episode_reward=41.83 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 230        |\n",
      "|    mean_reward          | 41.8       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 78000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.59951437 |\n",
      "|    clip_fraction        | 0.197      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0848    |\n",
      "|    explained_variance   | 0.82       |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.166     |\n",
      "|    n_updates            | 5776       |\n",
      "|    policy_gradient_loss | -0.0781    |\n",
      "|    value_loss           | 0.106      |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 60.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 779      |\n",
      "|    iterations      | 305      |\n",
      "|    time_elapsed    | 100      |\n",
      "|    total_timesteps | 78080    |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 61        |\n",
      "| time/                   |           |\n",
      "|    fps                  | 779       |\n",
      "|    iterations           | 306       |\n",
      "|    time_elapsed         | 100       |\n",
      "|    total_timesteps      | 78336     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.0395039 |\n",
      "|    clip_fraction        | 0.186     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0712   |\n",
      "|    explained_variance   | 0.675     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0753   |\n",
      "|    n_updates            | 5795      |\n",
      "|    policy_gradient_loss | -0.0797   |\n",
      "|    value_loss           | 0.12      |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=78500, episode_reward=58.20 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 58.2      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 78500     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.8702769 |\n",
      "|    clip_fraction        | 0.267     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.106    |\n",
      "|    explained_variance   | 0.706     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0496   |\n",
      "|    n_updates            | 5814      |\n",
      "|    policy_gradient_loss | -0.0652   |\n",
      "|    value_loss           | 0.228     |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 61       |\n",
      "| time/              |          |\n",
      "|    fps             | 778      |\n",
      "|    iterations      | 307      |\n",
      "|    time_elapsed    | 100      |\n",
      "|    total_timesteps | 78592    |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 61         |\n",
      "| time/                   |            |\n",
      "|    fps                  | 779        |\n",
      "|    iterations           | 308        |\n",
      "|    time_elapsed         | 101        |\n",
      "|    total_timesteps      | 78848      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.97851074 |\n",
      "|    clip_fraction        | 0.183      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0674    |\n",
      "|    explained_variance   | 0.503      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0626    |\n",
      "|    n_updates            | 5833       |\n",
      "|    policy_gradient_loss | -0.0692    |\n",
      "|    value_loss           | 0.0505     |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=79000, episode_reward=25.06 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 230        |\n",
      "|    mean_reward          | 25.1       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 79000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.54517186 |\n",
      "|    clip_fraction        | 0.134      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0659    |\n",
      "|    explained_variance   | 0.796      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.044     |\n",
      "|    n_updates            | 5852       |\n",
      "|    policy_gradient_loss | -0.0552    |\n",
      "|    value_loss           | 0.152      |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 61       |\n",
      "| time/              |          |\n",
      "|    fps             | 778      |\n",
      "|    iterations      | 309      |\n",
      "|    time_elapsed    | 101      |\n",
      "|    total_timesteps | 79104    |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 61         |\n",
      "| time/                   |            |\n",
      "|    fps                  | 779        |\n",
      "|    iterations           | 310        |\n",
      "|    time_elapsed         | 101        |\n",
      "|    total_timesteps      | 79360      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.55238056 |\n",
      "|    clip_fraction        | 0.174      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0866    |\n",
      "|    explained_variance   | 0.277      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0683    |\n",
      "|    n_updates            | 5871       |\n",
      "|    policy_gradient_loss | -0.0559    |\n",
      "|    value_loss           | 0.249      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=79500, episode_reward=32.52 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 32.5      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 79500     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.2155747 |\n",
      "|    clip_fraction        | 0.307     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0993   |\n",
      "|    explained_variance   | 0.419     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0817   |\n",
      "|    n_updates            | 5890      |\n",
      "|    policy_gradient_loss | -0.102    |\n",
      "|    value_loss           | 0.118     |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 61       |\n",
      "| time/              |          |\n",
      "|    fps             | 778      |\n",
      "|    iterations      | 311      |\n",
      "|    time_elapsed    | 102      |\n",
      "|    total_timesteps | 79616    |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 61        |\n",
      "| time/                   |           |\n",
      "|    fps                  | 779       |\n",
      "|    iterations           | 312       |\n",
      "|    time_elapsed         | 102       |\n",
      "|    total_timesteps      | 79872     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.0142114 |\n",
      "|    clip_fraction        | 0.187     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0682   |\n",
      "|    explained_variance   | 0.857     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0875   |\n",
      "|    n_updates            | 5909      |\n",
      "|    policy_gradient_loss | -0.0758   |\n",
      "|    value_loss           | 0.0867    |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=80000, episode_reward=47.16 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 47.2      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 80000     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.8777287 |\n",
      "|    clip_fraction        | 0.211     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0618   |\n",
      "|    explained_variance   | 0.776     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0927   |\n",
      "|    n_updates            | 5928      |\n",
      "|    policy_gradient_loss | -0.0767   |\n",
      "|    value_loss           | 0.0586    |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 61.1     |\n",
      "| time/              |          |\n",
      "|    fps             | 778      |\n",
      "|    iterations      | 313      |\n",
      "|    time_elapsed    | 102      |\n",
      "|    total_timesteps | 80128    |\n",
      "---------------------------------\n",
      "--------------------------------------\n",
      "| rollout/                |          |\n",
      "|    ep_len_mean          | 889      |\n",
      "|    ep_rew_mean          | 61.1     |\n",
      "| time/                   |          |\n",
      "|    fps                  | 778      |\n",
      "|    iterations           | 314      |\n",
      "|    time_elapsed         | 103      |\n",
      "|    total_timesteps      | 80384    |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 1.511863 |\n",
      "|    clip_fraction        | 0.293    |\n",
      "|    clip_range           | 0.4      |\n",
      "|    entropy_loss         | -0.115   |\n",
      "|    explained_variance   | 0.437    |\n",
      "|    learning_rate        | 0.000389 |\n",
      "|    loss                 | -0.132   |\n",
      "|    n_updates            | 5947     |\n",
      "|    policy_gradient_loss | -0.0638  |\n",
      "|    value_loss           | 0.158    |\n",
      "--------------------------------------\n",
      "Eval num_timesteps=80500, episode_reward=50.65 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 230        |\n",
      "|    mean_reward          | 50.6       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 80500      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.73416305 |\n",
      "|    clip_fraction        | 0.155      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0802    |\n",
      "|    explained_variance   | 0.582      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0959    |\n",
      "|    n_updates            | 5966       |\n",
      "|    policy_gradient_loss | -0.0576    |\n",
      "|    value_loss           | 0.0402     |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 61.1     |\n",
      "| time/              |          |\n",
      "|    fps             | 777      |\n",
      "|    iterations      | 315      |\n",
      "|    time_elapsed    | 103      |\n",
      "|    total_timesteps | 80640    |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 61.1      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 777       |\n",
      "|    iterations           | 316       |\n",
      "|    time_elapsed         | 104       |\n",
      "|    total_timesteps      | 80896     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.7332585 |\n",
      "|    clip_fraction        | 0.157     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0598   |\n",
      "|    explained_variance   | 0.754     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.109    |\n",
      "|    n_updates            | 5985      |\n",
      "|    policy_gradient_loss | -0.0622   |\n",
      "|    value_loss           | 0.118     |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=81000, episode_reward=54.96 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 55        |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 81000     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.0073135 |\n",
      "|    clip_fraction        | 0.2       |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0761   |\n",
      "|    explained_variance   | 0.56      |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0817   |\n",
      "|    n_updates            | 6004      |\n",
      "|    policy_gradient_loss | -0.0617   |\n",
      "|    value_loss           | 0.145     |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 61.1     |\n",
      "| time/              |          |\n",
      "|    fps             | 777      |\n",
      "|    iterations      | 317      |\n",
      "|    time_elapsed    | 104      |\n",
      "|    total_timesteps | 81152    |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 61.1      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 777       |\n",
      "|    iterations           | 318       |\n",
      "|    time_elapsed         | 104       |\n",
      "|    total_timesteps      | 81408     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.5734696 |\n",
      "|    clip_fraction        | 0.249     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0981   |\n",
      "|    explained_variance   | 0.562     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0672   |\n",
      "|    n_updates            | 6023      |\n",
      "|    policy_gradient_loss | -0.0568   |\n",
      "|    value_loss           | 0.0492    |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=81500, episode_reward=44.66 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 230        |\n",
      "|    mean_reward          | 44.7       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 81500      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.72878444 |\n",
      "|    clip_fraction        | 0.157      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0886    |\n",
      "|    explained_variance   | 0.258      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0661    |\n",
      "|    n_updates            | 6042       |\n",
      "|    policy_gradient_loss | -0.0547    |\n",
      "|    value_loss           | 0.393      |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 61.1     |\n",
      "| time/              |          |\n",
      "|    fps             | 777      |\n",
      "|    iterations      | 319      |\n",
      "|    time_elapsed    | 105      |\n",
      "|    total_timesteps | 81664    |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 61.3      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 777       |\n",
      "|    iterations           | 320       |\n",
      "|    time_elapsed         | 105       |\n",
      "|    total_timesteps      | 81920     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.5878638 |\n",
      "|    clip_fraction        | 0.164     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0881   |\n",
      "|    explained_variance   | 0.667     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0801   |\n",
      "|    n_updates            | 6061      |\n",
      "|    policy_gradient_loss | -0.0553   |\n",
      "|    value_loss           | 0.119     |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=82000, episode_reward=21.38 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 21.4      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 82000     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.8092656 |\n",
      "|    clip_fraction        | 0.181     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.101    |\n",
      "|    explained_variance   | 0.385     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0587   |\n",
      "|    n_updates            | 6080      |\n",
      "|    policy_gradient_loss | -0.054    |\n",
      "|    value_loss           | 0.476     |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 61.3     |\n",
      "| time/              |          |\n",
      "|    fps             | 776      |\n",
      "|    iterations      | 321      |\n",
      "|    time_elapsed    | 105      |\n",
      "|    total_timesteps | 82176    |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 61.3      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 777       |\n",
      "|    iterations           | 322       |\n",
      "|    time_elapsed         | 106       |\n",
      "|    total_timesteps      | 82432     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.6844133 |\n",
      "|    clip_fraction        | 0.127     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0562   |\n",
      "|    explained_variance   | 0.607     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0954   |\n",
      "|    n_updates            | 6099      |\n",
      "|    policy_gradient_loss | -0.0591   |\n",
      "|    value_loss           | 0.0603    |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=82500, episode_reward=37.73 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 37.7      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 82500     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.8457933 |\n",
      "|    clip_fraction        | 0.187     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0881   |\n",
      "|    explained_variance   | 0.562     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.157    |\n",
      "|    n_updates            | 6118      |\n",
      "|    policy_gradient_loss | -0.08     |\n",
      "|    value_loss           | 0.136     |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 61.2     |\n",
      "| time/              |          |\n",
      "|    fps             | 777      |\n",
      "|    iterations      | 323      |\n",
      "|    time_elapsed    | 106      |\n",
      "|    total_timesteps | 82688    |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 61.2      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 777       |\n",
      "|    iterations           | 324       |\n",
      "|    time_elapsed         | 106       |\n",
      "|    total_timesteps      | 82944     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.4411286 |\n",
      "|    clip_fraction        | 0.154     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0649   |\n",
      "|    explained_variance   | 0.694     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.065    |\n",
      "|    n_updates            | 6137      |\n",
      "|    policy_gradient_loss | -0.062    |\n",
      "|    value_loss           | 0.156     |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=83000, episode_reward=61.16 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 61.2      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 83000     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.9545356 |\n",
      "|    clip_fraction        | 0.215     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0733   |\n",
      "|    explained_variance   | 0.302     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.12     |\n",
      "|    n_updates            | 6156      |\n",
      "|    policy_gradient_loss | -0.0873   |\n",
      "|    value_loss           | 0.049     |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 61.2     |\n",
      "| time/              |          |\n",
      "|    fps             | 777      |\n",
      "|    iterations      | 325      |\n",
      "|    time_elapsed    | 107      |\n",
      "|    total_timesteps | 83200    |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 61.2      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 777       |\n",
      "|    iterations           | 326       |\n",
      "|    time_elapsed         | 107       |\n",
      "|    total_timesteps      | 83456     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.9279932 |\n",
      "|    clip_fraction        | 0.178     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0736   |\n",
      "|    explained_variance   | 0.464     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0653   |\n",
      "|    n_updates            | 6175      |\n",
      "|    policy_gradient_loss | -0.0719   |\n",
      "|    value_loss           | 0.106     |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=83500, episode_reward=59.98 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "--------------------------------------\n",
      "| eval/                   |          |\n",
      "|    mean_ep_length       | 230      |\n",
      "|    mean_reward          | 60       |\n",
      "| time/                   |          |\n",
      "|    total_timesteps      | 83500    |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 1.202783 |\n",
      "|    clip_fraction        | 0.203    |\n",
      "|    clip_range           | 0.4      |\n",
      "|    entropy_loss         | -0.0654  |\n",
      "|    explained_variance   | 0.623    |\n",
      "|    learning_rate        | 0.000389 |\n",
      "|    loss                 | -0.0716  |\n",
      "|    n_updates            | 6194     |\n",
      "|    policy_gradient_loss | -0.0703  |\n",
      "|    value_loss           | 0.112    |\n",
      "--------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 61.1     |\n",
      "| time/              |          |\n",
      "|    fps             | 776      |\n",
      "|    iterations      | 327      |\n",
      "|    time_elapsed    | 107      |\n",
      "|    total_timesteps | 83712    |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 61.1      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 777       |\n",
      "|    iterations           | 328       |\n",
      "|    time_elapsed         | 108       |\n",
      "|    total_timesteps      | 83968     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.8105875 |\n",
      "|    clip_fraction        | 0.156     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0911   |\n",
      "|    explained_variance   | 0.707     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0202   |\n",
      "|    n_updates            | 6213      |\n",
      "|    policy_gradient_loss | -0.064    |\n",
      "|    value_loss           | 0.265     |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=84000, episode_reward=56.23 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 56.2      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 84000     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.7779555 |\n",
      "|    clip_fraction        | 0.153     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0812   |\n",
      "|    explained_variance   | 0.59      |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0518   |\n",
      "|    n_updates            | 6232      |\n",
      "|    policy_gradient_loss | -0.0575   |\n",
      "|    value_loss           | 0.0632    |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 61.1     |\n",
      "| time/              |          |\n",
      "|    fps             | 777      |\n",
      "|    iterations      | 329      |\n",
      "|    time_elapsed    | 108      |\n",
      "|    total_timesteps | 84224    |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 61.2      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 777       |\n",
      "|    iterations           | 330       |\n",
      "|    time_elapsed         | 108       |\n",
      "|    total_timesteps      | 84480     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.6705458 |\n",
      "|    clip_fraction        | 0.174     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0777   |\n",
      "|    explained_variance   | 0.501     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.077    |\n",
      "|    n_updates            | 6251      |\n",
      "|    policy_gradient_loss | -0.0692   |\n",
      "|    value_loss           | 0.13      |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=84500, episode_reward=67.57 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 67.6      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 84500     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.5795834 |\n",
      "|    clip_fraction        | 0.158     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.102    |\n",
      "|    explained_variance   | 0.592     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0917   |\n",
      "|    n_updates            | 6270      |\n",
      "|    policy_gradient_loss | -0.0599   |\n",
      "|    value_loss           | 0.306     |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 61.2     |\n",
      "| time/              |          |\n",
      "|    fps             | 777      |\n",
      "|    iterations      | 331      |\n",
      "|    time_elapsed    | 108      |\n",
      "|    total_timesteps | 84736    |\n",
      "---------------------------------\n",
      "--------------------------------------\n",
      "| rollout/                |          |\n",
      "|    ep_len_mean          | 889      |\n",
      "|    ep_rew_mean          | 61.2     |\n",
      "| time/                   |          |\n",
      "|    fps                  | 778      |\n",
      "|    iterations           | 332      |\n",
      "|    time_elapsed         | 109      |\n",
      "|    total_timesteps      | 84992    |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 1.334091 |\n",
      "|    clip_fraction        | 0.227    |\n",
      "|    clip_range           | 0.4      |\n",
      "|    entropy_loss         | -0.0943  |\n",
      "|    explained_variance   | -1.13    |\n",
      "|    learning_rate        | 0.000389 |\n",
      "|    loss                 | -0.0658  |\n",
      "|    n_updates            | 6289     |\n",
      "|    policy_gradient_loss | -0.0725  |\n",
      "|    value_loss           | 0.0582   |\n",
      "--------------------------------------\n",
      "Eval num_timesteps=85000, episode_reward=59.29 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 230        |\n",
      "|    mean_reward          | 59.3       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 85000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.63221496 |\n",
      "|    clip_fraction        | 0.174      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0868    |\n",
      "|    explained_variance   | 0.729      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.031     |\n",
      "|    n_updates            | 6308       |\n",
      "|    policy_gradient_loss | -0.0618    |\n",
      "|    value_loss           | 0.115      |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 61.2     |\n",
      "| time/              |          |\n",
      "|    fps             | 778      |\n",
      "|    iterations      | 333      |\n",
      "|    time_elapsed    | 109      |\n",
      "|    total_timesteps | 85248    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=85500, episode_reward=63.60 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 63.6      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 85500     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.1600327 |\n",
      "|    clip_fraction        | 0.186     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0769   |\n",
      "|    explained_variance   | 0.605     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.11     |\n",
      "|    n_updates            | 6327      |\n",
      "|    policy_gradient_loss | -0.0633   |\n",
      "|    value_loss           | 0.146     |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 61.3     |\n",
      "| time/              |          |\n",
      "|    fps             | 777      |\n",
      "|    iterations      | 334      |\n",
      "|    time_elapsed    | 109      |\n",
      "|    total_timesteps | 85504    |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 61.3      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 778       |\n",
      "|    iterations           | 335       |\n",
      "|    time_elapsed         | 110       |\n",
      "|    total_timesteps      | 85760     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.6565734 |\n",
      "|    clip_fraction        | 0.221     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.128    |\n",
      "|    explained_variance   | 0.569     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | 0.0727    |\n",
      "|    n_updates            | 6346      |\n",
      "|    policy_gradient_loss | -0.0582   |\n",
      "|    value_loss           | 0.287     |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=86000, episode_reward=36.01 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 36        |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 86000     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.0733125 |\n",
      "|    clip_fraction        | 0.155     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0688   |\n",
      "|    explained_variance   | 0.578     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.111    |\n",
      "|    n_updates            | 6365      |\n",
      "|    policy_gradient_loss | -0.0549   |\n",
      "|    value_loss           | 0.13      |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 61.3     |\n",
      "| time/              |          |\n",
      "|    fps             | 778      |\n",
      "|    iterations      | 336      |\n",
      "|    time_elapsed    | 110      |\n",
      "|    total_timesteps | 86016    |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 61.4       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 778        |\n",
      "|    iterations           | 337        |\n",
      "|    time_elapsed         | 110        |\n",
      "|    total_timesteps      | 86272      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.46852833 |\n",
      "|    clip_fraction        | 0.165      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0883    |\n",
      "|    explained_variance   | 0.172      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0935    |\n",
      "|    n_updates            | 6384       |\n",
      "|    policy_gradient_loss | -0.0657    |\n",
      "|    value_loss           | 0.177      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=86500, episode_reward=62.74 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 62.7      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 86500     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.7451116 |\n",
      "|    clip_fraction        | 0.219     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.102    |\n",
      "|    explained_variance   | 0.702     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0127   |\n",
      "|    n_updates            | 6403      |\n",
      "|    policy_gradient_loss | -0.055    |\n",
      "|    value_loss           | 0.333     |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 61.4     |\n",
      "| time/              |          |\n",
      "|    fps             | 778      |\n",
      "|    iterations      | 338      |\n",
      "|    time_elapsed    | 111      |\n",
      "|    total_timesteps | 86528    |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 61.4       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 778        |\n",
      "|    iterations           | 339        |\n",
      "|    time_elapsed         | 111        |\n",
      "|    total_timesteps      | 86784      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.78082395 |\n",
      "|    clip_fraction        | 0.223      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.108     |\n",
      "|    explained_variance   | 0.297      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.137     |\n",
      "|    n_updates            | 6422       |\n",
      "|    policy_gradient_loss | -0.0847    |\n",
      "|    value_loss           | 0.0431     |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=87000, episode_reward=77.23 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 77.2      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 87000     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.6876313 |\n",
      "|    clip_fraction        | 0.188     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0812   |\n",
      "|    explained_variance   | 0.702     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.104    |\n",
      "|    n_updates            | 6441      |\n",
      "|    policy_gradient_loss | -0.0709   |\n",
      "|    value_loss           | 0.137     |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 61.4     |\n",
      "| time/              |          |\n",
      "|    fps             | 777      |\n",
      "|    iterations      | 340      |\n",
      "|    time_elapsed    | 111      |\n",
      "|    total_timesteps | 87040    |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 61.3      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 777       |\n",
      "|    iterations           | 341       |\n",
      "|    time_elapsed         | 112       |\n",
      "|    total_timesteps      | 87296     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.5953187 |\n",
      "|    clip_fraction        | 0.131     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0629   |\n",
      "|    explained_variance   | 0.0141    |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | 0.00515   |\n",
      "|    n_updates            | 6460      |\n",
      "|    policy_gradient_loss | -0.0359   |\n",
      "|    value_loss           | 0.172     |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=87500, episode_reward=48.69 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 48.7      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 87500     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.6555779 |\n",
      "|    clip_fraction        | 0.192     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.137    |\n",
      "|    explained_variance   | -0.108    |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.107    |\n",
      "|    n_updates            | 6479      |\n",
      "|    policy_gradient_loss | -0.068    |\n",
      "|    value_loss           | 0.204     |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 61.3     |\n",
      "| time/              |          |\n",
      "|    fps             | 777      |\n",
      "|    iterations      | 342      |\n",
      "|    time_elapsed    | 112      |\n",
      "|    total_timesteps | 87552    |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 61.3      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 777       |\n",
      "|    iterations           | 343       |\n",
      "|    time_elapsed         | 112       |\n",
      "|    total_timesteps      | 87808     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.1304836 |\n",
      "|    clip_fraction        | 0.222     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0784   |\n",
      "|    explained_variance   | 0.829     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.168    |\n",
      "|    n_updates            | 6498      |\n",
      "|    policy_gradient_loss | -0.0706   |\n",
      "|    value_loss           | 0.0762    |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=88000, episode_reward=51.44 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 51.4      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 88000     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.5993657 |\n",
      "|    clip_fraction        | 0.148     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.075    |\n",
      "|    explained_variance   | 0.539     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0609   |\n",
      "|    n_updates            | 6517      |\n",
      "|    policy_gradient_loss | -0.0599   |\n",
      "|    value_loss           | 0.193     |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 61.3     |\n",
      "| time/              |          |\n",
      "|    fps             | 776      |\n",
      "|    iterations      | 344      |\n",
      "|    time_elapsed    | 113      |\n",
      "|    total_timesteps | 88064    |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 61.3      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 777       |\n",
      "|    iterations           | 345       |\n",
      "|    time_elapsed         | 113       |\n",
      "|    total_timesteps      | 88320     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.5084357 |\n",
      "|    clip_fraction        | 0.178     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.108    |\n",
      "|    explained_variance   | 0.363     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | 0.000999  |\n",
      "|    n_updates            | 6536      |\n",
      "|    policy_gradient_loss | -0.0515   |\n",
      "|    value_loss           | 0.297     |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=88500, episode_reward=49.13 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 230        |\n",
      "|    mean_reward          | 49.1       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 88500      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.84792006 |\n",
      "|    clip_fraction        | 0.225      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.11      |\n",
      "|    explained_variance   | -0.391     |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.129     |\n",
      "|    n_updates            | 6555       |\n",
      "|    policy_gradient_loss | -0.0961    |\n",
      "|    value_loss           | 0.0822     |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 61.3     |\n",
      "| time/              |          |\n",
      "|    fps             | 777      |\n",
      "|    iterations      | 346      |\n",
      "|    time_elapsed    | 113      |\n",
      "|    total_timesteps | 88576    |\n",
      "---------------------------------\n",
      "--------------------------------------\n",
      "| rollout/                |          |\n",
      "|    ep_len_mean          | 889      |\n",
      "|    ep_rew_mean          | 61.3     |\n",
      "| time/                   |          |\n",
      "|    fps                  | 777      |\n",
      "|    iterations           | 347      |\n",
      "|    time_elapsed         | 114      |\n",
      "|    total_timesteps      | 88832    |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 1.451904 |\n",
      "|    clip_fraction        | 0.192    |\n",
      "|    clip_range           | 0.4      |\n",
      "|    entropy_loss         | -0.0799  |\n",
      "|    explained_variance   | 0.735    |\n",
      "|    learning_rate        | 0.000389 |\n",
      "|    loss                 | -0.0851  |\n",
      "|    n_updates            | 6574     |\n",
      "|    policy_gradient_loss | -0.0606  |\n",
      "|    value_loss           | 0.155    |\n",
      "--------------------------------------\n",
      "Eval num_timesteps=89000, episode_reward=23.52 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 230        |\n",
      "|    mean_reward          | 23.5       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 89000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.77883554 |\n",
      "|    clip_fraction        | 0.176      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0853    |\n",
      "|    explained_variance   | 0.26       |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0793    |\n",
      "|    n_updates            | 6593       |\n",
      "|    policy_gradient_loss | -0.0574    |\n",
      "|    value_loss           | 0.295      |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 61.5     |\n",
      "| time/              |          |\n",
      "|    fps             | 776      |\n",
      "|    iterations      | 348      |\n",
      "|    time_elapsed    | 114      |\n",
      "|    total_timesteps | 89088    |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 61.5      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 777       |\n",
      "|    iterations           | 349       |\n",
      "|    time_elapsed         | 114       |\n",
      "|    total_timesteps      | 89344     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.4670595 |\n",
      "|    clip_fraction        | 0.269     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.122    |\n",
      "|    explained_variance   | -0.749    |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0437   |\n",
      "|    n_updates            | 6612      |\n",
      "|    policy_gradient_loss | -0.0751   |\n",
      "|    value_loss           | 0.152     |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=89500, episode_reward=36.11 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 36.1      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 89500     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.2037024 |\n",
      "|    clip_fraction        | 0.179     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0606   |\n",
      "|    explained_variance   | 0.872     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.117    |\n",
      "|    n_updates            | 6631      |\n",
      "|    policy_gradient_loss | -0.0729   |\n",
      "|    value_loss           | 0.108     |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 61.5     |\n",
      "| time/              |          |\n",
      "|    fps             | 777      |\n",
      "|    iterations      | 350      |\n",
      "|    time_elapsed    | 115      |\n",
      "|    total_timesteps | 89600    |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 61.8       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 778        |\n",
      "|    iterations           | 351        |\n",
      "|    time_elapsed         | 115        |\n",
      "|    total_timesteps      | 89856      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.72984385 |\n",
      "|    clip_fraction        | 0.19       |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0895    |\n",
      "|    explained_variance   | 0.609      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0624    |\n",
      "|    n_updates            | 6650       |\n",
      "|    policy_gradient_loss | -0.0828    |\n",
      "|    value_loss           | 0.204      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=90000, episode_reward=23.19 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 23.2      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 90000     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.9503259 |\n",
      "|    clip_fraction        | 0.19      |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.092    |\n",
      "|    explained_variance   | 0.632     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0832   |\n",
      "|    n_updates            | 6669      |\n",
      "|    policy_gradient_loss | -0.0603   |\n",
      "|    value_loss           | 0.223     |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 61.8     |\n",
      "| time/              |          |\n",
      "|    fps             | 777      |\n",
      "|    iterations      | 352      |\n",
      "|    time_elapsed    | 115      |\n",
      "|    total_timesteps | 90112    |\n",
      "---------------------------------\n",
      "--------------------------------------\n",
      "| rollout/                |          |\n",
      "|    ep_len_mean          | 889      |\n",
      "|    ep_rew_mean          | 61.8     |\n",
      "| time/                   |          |\n",
      "|    fps                  | 778      |\n",
      "|    iterations           | 353      |\n",
      "|    time_elapsed         | 116      |\n",
      "|    total_timesteps      | 90368    |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 1.268075 |\n",
      "|    clip_fraction        | 0.222    |\n",
      "|    clip_range           | 0.4      |\n",
      "|    entropy_loss         | -0.0848  |\n",
      "|    explained_variance   | 0.332    |\n",
      "|    learning_rate        | 0.000389 |\n",
      "|    loss                 | -0.0655  |\n",
      "|    n_updates            | 6688     |\n",
      "|    policy_gradient_loss | -0.0804  |\n",
      "|    value_loss           | 0.0553   |\n",
      "--------------------------------------\n",
      "Eval num_timesteps=90500, episode_reward=38.94 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 38.9      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 90500     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.6711934 |\n",
      "|    clip_fraction        | 0.119     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0661   |\n",
      "|    explained_variance   | 0.7       |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0736   |\n",
      "|    n_updates            | 6707      |\n",
      "|    policy_gradient_loss | -0.053    |\n",
      "|    value_loss           | 0.161     |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 61.8     |\n",
      "| time/              |          |\n",
      "|    fps             | 777      |\n",
      "|    iterations      | 354      |\n",
      "|    time_elapsed    | 116      |\n",
      "|    total_timesteps | 90624    |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 62        |\n",
      "| time/                   |           |\n",
      "|    fps                  | 777       |\n",
      "|    iterations           | 355       |\n",
      "|    time_elapsed         | 116       |\n",
      "|    total_timesteps      | 90880     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.4032464 |\n",
      "|    clip_fraction        | 0.138     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0676   |\n",
      "|    explained_variance   | 0.25      |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0389   |\n",
      "|    n_updates            | 6726      |\n",
      "|    policy_gradient_loss | -0.0429   |\n",
      "|    value_loss           | 0.178     |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=91000, episode_reward=66.85 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 66.8      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 91000     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.6397625 |\n",
      "|    clip_fraction        | 0.213     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.105    |\n",
      "|    explained_variance   | 0.221     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0952   |\n",
      "|    n_updates            | 6745      |\n",
      "|    policy_gradient_loss | -0.0692   |\n",
      "|    value_loss           | 0.0716    |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 62       |\n",
      "| time/              |          |\n",
      "|    fps             | 777      |\n",
      "|    iterations      | 356      |\n",
      "|    time_elapsed    | 117      |\n",
      "|    total_timesteps | 91136    |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 62         |\n",
      "| time/                   |            |\n",
      "|    fps                  | 778        |\n",
      "|    iterations           | 357        |\n",
      "|    time_elapsed         | 117        |\n",
      "|    total_timesteps      | 91392      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.43554744 |\n",
      "|    clip_fraction        | 0.0968     |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.064     |\n",
      "|    explained_variance   | 0.398      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0166    |\n",
      "|    n_updates            | 6764       |\n",
      "|    policy_gradient_loss | -0.0435    |\n",
      "|    value_loss           | 0.16       |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=91500, episode_reward=31.27 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 31.3      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 91500     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.2681022 |\n",
      "|    clip_fraction        | 0.153     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0598   |\n",
      "|    explained_variance   | 0.236     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0508   |\n",
      "|    n_updates            | 6783      |\n",
      "|    policy_gradient_loss | -0.0557   |\n",
      "|    value_loss           | 0.123     |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 62.3     |\n",
      "| time/              |          |\n",
      "|    fps             | 778      |\n",
      "|    iterations      | 358      |\n",
      "|    time_elapsed    | 117      |\n",
      "|    total_timesteps | 91648    |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 62.3      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 778       |\n",
      "|    iterations           | 359       |\n",
      "|    time_elapsed         | 117       |\n",
      "|    total_timesteps      | 91904     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.1727273 |\n",
      "|    clip_fraction        | 0.189     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0994   |\n",
      "|    explained_variance   | 0.631     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.138    |\n",
      "|    n_updates            | 6802      |\n",
      "|    policy_gradient_loss | -0.0734   |\n",
      "|    value_loss           | 0.117     |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=92000, episode_reward=39.53 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 39.5      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 92000     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.7437907 |\n",
      "|    clip_fraction        | 0.243     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0802   |\n",
      "|    explained_variance   | 0.0463    |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.18     |\n",
      "|    n_updates            | 6821      |\n",
      "|    policy_gradient_loss | -0.0974   |\n",
      "|    value_loss           | 0.0508    |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 62.3     |\n",
      "| time/              |          |\n",
      "|    fps             | 778      |\n",
      "|    iterations      | 360      |\n",
      "|    time_elapsed    | 118      |\n",
      "|    total_timesteps | 92160    |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 62.3      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 778       |\n",
      "|    iterations           | 361       |\n",
      "|    time_elapsed         | 118       |\n",
      "|    total_timesteps      | 92416     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.8425821 |\n",
      "|    clip_fraction        | 0.184     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0717   |\n",
      "|    explained_variance   | 0.631     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0716   |\n",
      "|    n_updates            | 6840      |\n",
      "|    policy_gradient_loss | -0.0722   |\n",
      "|    value_loss           | 0.212     |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=92500, episode_reward=25.07 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 230        |\n",
      "|    mean_reward          | 25.1       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 92500      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.86316836 |\n",
      "|    clip_fraction        | 0.182      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0889    |\n",
      "|    explained_variance   | 0.399      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0253    |\n",
      "|    n_updates            | 6859       |\n",
      "|    policy_gradient_loss | -0.051     |\n",
      "|    value_loss           | 0.224      |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 62.7     |\n",
      "| time/              |          |\n",
      "|    fps             | 778      |\n",
      "|    iterations      | 362      |\n",
      "|    time_elapsed    | 119      |\n",
      "|    total_timesteps | 92672    |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 62.7      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 779       |\n",
      "|    iterations           | 363       |\n",
      "|    time_elapsed         | 119       |\n",
      "|    total_timesteps      | 92928     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.6576891 |\n",
      "|    clip_fraction        | 0.194     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0919   |\n",
      "|    explained_variance   | 0.227     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0621   |\n",
      "|    n_updates            | 6878      |\n",
      "|    policy_gradient_loss | -0.0617   |\n",
      "|    value_loss           | 0.107     |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=93000, episode_reward=27.18 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 27.2      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 93000     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.5651023 |\n",
      "|    clip_fraction        | 0.175     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.071    |\n",
      "|    explained_variance   | 0.706     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.118    |\n",
      "|    n_updates            | 6897      |\n",
      "|    policy_gradient_loss | -0.0706   |\n",
      "|    value_loss           | 0.147     |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 62.7     |\n",
      "| time/              |          |\n",
      "|    fps             | 778      |\n",
      "|    iterations      | 364      |\n",
      "|    time_elapsed    | 119      |\n",
      "|    total_timesteps | 93184    |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 63        |\n",
      "| time/                   |           |\n",
      "|    fps                  | 779       |\n",
      "|    iterations           | 365       |\n",
      "|    time_elapsed         | 119       |\n",
      "|    total_timesteps      | 93440     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.0338736 |\n",
      "|    clip_fraction        | 0.197     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0779   |\n",
      "|    explained_variance   | 0.317     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0951   |\n",
      "|    n_updates            | 6916      |\n",
      "|    policy_gradient_loss | -0.0721   |\n",
      "|    value_loss           | 0.122     |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=93500, episode_reward=11.89 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 11.9      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 93500     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 2.3595753 |\n",
      "|    clip_fraction        | 0.283     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0735   |\n",
      "|    explained_variance   | 0.661     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.126    |\n",
      "|    n_updates            | 6935      |\n",
      "|    policy_gradient_loss | -0.0853   |\n",
      "|    value_loss           | 0.193     |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 63       |\n",
      "| time/              |          |\n",
      "|    fps             | 779      |\n",
      "|    iterations      | 366      |\n",
      "|    time_elapsed    | 120      |\n",
      "|    total_timesteps | 93696    |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 63         |\n",
      "| time/                   |            |\n",
      "|    fps                  | 780        |\n",
      "|    iterations           | 367        |\n",
      "|    time_elapsed         | 120        |\n",
      "|    total_timesteps      | 93952      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.95558566 |\n",
      "|    clip_fraction        | 0.175      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0638    |\n",
      "|    explained_variance   | 0.527      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.067     |\n",
      "|    n_updates            | 6954       |\n",
      "|    policy_gradient_loss | -0.0644    |\n",
      "|    value_loss           | 0.0642     |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=94000, episode_reward=61.99 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 230        |\n",
      "|    mean_reward          | 62         |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 94000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.64111644 |\n",
      "|    clip_fraction        | 0.156      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0704    |\n",
      "|    explained_variance   | 0.666      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0416    |\n",
      "|    n_updates            | 6973       |\n",
      "|    policy_gradient_loss | -0.0483    |\n",
      "|    value_loss           | 0.176      |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 63       |\n",
      "| time/              |          |\n",
      "|    fps             | 778      |\n",
      "|    iterations      | 368      |\n",
      "|    time_elapsed    | 121      |\n",
      "|    total_timesteps | 94208    |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 63.2       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 778        |\n",
      "|    iterations           | 369        |\n",
      "|    time_elapsed         | 121        |\n",
      "|    total_timesteps      | 94464      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.81373084 |\n",
      "|    clip_fraction        | 0.161      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0789    |\n",
      "|    explained_variance   | 0.865      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0751    |\n",
      "|    n_updates            | 6992       |\n",
      "|    policy_gradient_loss | -0.067     |\n",
      "|    value_loss           | 0.0892     |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=94500, episode_reward=36.87 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 36.9      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 94500     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.8599398 |\n",
      "|    clip_fraction        | 0.16      |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0578   |\n",
      "|    explained_variance   | 0.645     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.106    |\n",
      "|    n_updates            | 7011      |\n",
      "|    policy_gradient_loss | -0.0552   |\n",
      "|    value_loss           | 0.0449    |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 63.2     |\n",
      "| time/              |          |\n",
      "|    fps             | 778      |\n",
      "|    iterations      | 370      |\n",
      "|    time_elapsed    | 121      |\n",
      "|    total_timesteps | 94720    |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 63.2       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 778        |\n",
      "|    iterations           | 371        |\n",
      "|    time_elapsed         | 122        |\n",
      "|    total_timesteps      | 94976      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.44578266 |\n",
      "|    clip_fraction        | 0.112      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0548    |\n",
      "|    explained_variance   | 0.719      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0539    |\n",
      "|    n_updates            | 7030       |\n",
      "|    policy_gradient_loss | -0.0401    |\n",
      "|    value_loss           | 0.152      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=95000, episode_reward=29.45 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 29.4      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 95000     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.4921551 |\n",
      "|    clip_fraction        | 0.193     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0672   |\n",
      "|    explained_variance   | 0.396     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0902   |\n",
      "|    n_updates            | 7049      |\n",
      "|    policy_gradient_loss | -0.0681   |\n",
      "|    value_loss           | 0.0819    |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 63.4     |\n",
      "| time/              |          |\n",
      "|    fps             | 777      |\n",
      "|    iterations      | 372      |\n",
      "|    time_elapsed    | 122      |\n",
      "|    total_timesteps | 95232    |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 63.4      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 778       |\n",
      "|    iterations           | 373       |\n",
      "|    time_elapsed         | 122       |\n",
      "|    total_timesteps      | 95488     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.7092072 |\n",
      "|    clip_fraction        | 0.197     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.095    |\n",
      "|    explained_variance   | 0.842     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.029    |\n",
      "|    n_updates            | 7068      |\n",
      "|    policy_gradient_loss | -0.0611   |\n",
      "|    value_loss           | 0.139     |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=95500, episode_reward=-1.01 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | -1.01     |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 95500     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.7204692 |\n",
      "|    clip_fraction        | 0.241     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0696   |\n",
      "|    explained_variance   | 0.391     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.141    |\n",
      "|    n_updates            | 7087      |\n",
      "|    policy_gradient_loss | -0.0758   |\n",
      "|    value_loss           | 0.0792    |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 63.4     |\n",
      "| time/              |          |\n",
      "|    fps             | 777      |\n",
      "|    iterations      | 374      |\n",
      "|    time_elapsed    | 123      |\n",
      "|    total_timesteps | 95744    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=96000, episode_reward=20.61 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 230        |\n",
      "|    mean_reward          | 20.6       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 96000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.51104224 |\n",
      "|    clip_fraction        | 0.097      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0357    |\n",
      "|    explained_variance   | 0.76       |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0207    |\n",
      "|    n_updates            | 7106       |\n",
      "|    policy_gradient_loss | -0.0421    |\n",
      "|    value_loss           | 0.166      |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 63.4     |\n",
      "| time/              |          |\n",
      "|    fps             | 776      |\n",
      "|    iterations      | 375      |\n",
      "|    time_elapsed    | 123      |\n",
      "|    total_timesteps | 96000    |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 63.4       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 777        |\n",
      "|    iterations           | 376        |\n",
      "|    time_elapsed         | 123        |\n",
      "|    total_timesteps      | 96256      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.84137386 |\n",
      "|    clip_fraction        | 0.166      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.077     |\n",
      "|    explained_variance   | 0.423      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0568    |\n",
      "|    n_updates            | 7125       |\n",
      "|    policy_gradient_loss | -0.0499    |\n",
      "|    value_loss           | 0.229      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=96500, episode_reward=34.33 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 34.3      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 96500     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.7014409 |\n",
      "|    clip_fraction        | 0.175     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0762   |\n",
      "|    explained_variance   | 0.325     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0751   |\n",
      "|    n_updates            | 7144      |\n",
      "|    policy_gradient_loss | -0.0606   |\n",
      "|    value_loss           | 0.0527    |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 63.4     |\n",
      "| time/              |          |\n",
      "|    fps             | 777      |\n",
      "|    iterations      | 377      |\n",
      "|    time_elapsed    | 124      |\n",
      "|    total_timesteps | 96512    |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 63.4      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 777       |\n",
      "|    iterations           | 378       |\n",
      "|    time_elapsed         | 124       |\n",
      "|    total_timesteps      | 96768     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.6296206 |\n",
      "|    clip_fraction        | 0.129     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0632   |\n",
      "|    explained_variance   | 0.725     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0849   |\n",
      "|    n_updates            | 7163      |\n",
      "|    policy_gradient_loss | -0.0541   |\n",
      "|    value_loss           | 0.168     |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=97000, episode_reward=49.64 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "--------------------------------------\n",
      "| eval/                   |          |\n",
      "|    mean_ep_length       | 230      |\n",
      "|    mean_reward          | 49.6     |\n",
      "| time/                   |          |\n",
      "|    total_timesteps      | 97000    |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 0.674378 |\n",
      "|    clip_fraction        | 0.138    |\n",
      "|    clip_range           | 0.4      |\n",
      "|    entropy_loss         | -0.0687  |\n",
      "|    explained_variance   | 0.424    |\n",
      "|    learning_rate        | 0.000389 |\n",
      "|    loss                 | -0.0923  |\n",
      "|    n_updates            | 7182     |\n",
      "|    policy_gradient_loss | -0.0505  |\n",
      "|    value_loss           | 0.144    |\n",
      "--------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 63.3     |\n",
      "| time/              |          |\n",
      "|    fps             | 776      |\n",
      "|    iterations      | 379      |\n",
      "|    time_elapsed    | 124      |\n",
      "|    total_timesteps | 97024    |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 63.3       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 776        |\n",
      "|    iterations           | 380        |\n",
      "|    time_elapsed         | 125        |\n",
      "|    total_timesteps      | 97280      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.68336225 |\n",
      "|    clip_fraction        | 0.178      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.102     |\n",
      "|    explained_variance   | 0.487      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0879    |\n",
      "|    n_updates            | 7201       |\n",
      "|    policy_gradient_loss | -0.0502    |\n",
      "|    value_loss           | 0.288      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=97500, episode_reward=38.59 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 38.6      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 97500     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.3187088 |\n",
      "|    clip_fraction        | 0.233     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0778   |\n",
      "|    explained_variance   | 0.102     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0699   |\n",
      "|    n_updates            | 7220      |\n",
      "|    policy_gradient_loss | -0.0814   |\n",
      "|    value_loss           | 0.0959    |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 63.3     |\n",
      "| time/              |          |\n",
      "|    fps             | 776      |\n",
      "|    iterations      | 381      |\n",
      "|    time_elapsed    | 125      |\n",
      "|    total_timesteps | 97536    |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 63.1      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 776       |\n",
      "|    iterations           | 382       |\n",
      "|    time_elapsed         | 125       |\n",
      "|    total_timesteps      | 97792     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.8159181 |\n",
      "|    clip_fraction        | 0.152     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0617   |\n",
      "|    explained_variance   | 0.305     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0755   |\n",
      "|    n_updates            | 7239      |\n",
      "|    policy_gradient_loss | -0.0722   |\n",
      "|    value_loss           | 0.318     |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=98000, episode_reward=8.33 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 8.33      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 98000     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.0658638 |\n",
      "|    clip_fraction        | 0.178     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0924   |\n",
      "|    explained_variance   | 0.0992    |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.129    |\n",
      "|    n_updates            | 7258      |\n",
      "|    policy_gradient_loss | -0.0654   |\n",
      "|    value_loss           | 0.253     |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 63.1     |\n",
      "| time/              |          |\n",
      "|    fps             | 775      |\n",
      "|    iterations      | 383      |\n",
      "|    time_elapsed    | 126      |\n",
      "|    total_timesteps | 98048    |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 63.1      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 776       |\n",
      "|    iterations           | 384       |\n",
      "|    time_elapsed         | 126       |\n",
      "|    total_timesteps      | 98304     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.9511102 |\n",
      "|    clip_fraction        | 0.219     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0969   |\n",
      "|    explained_variance   | -0.343    |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0983   |\n",
      "|    n_updates            | 7277      |\n",
      "|    policy_gradient_loss | -0.07     |\n",
      "|    value_loss           | 0.0665    |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=98500, episode_reward=18.28 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 18.3      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 98500     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.8416553 |\n",
      "|    clip_fraction        | 0.122     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0755   |\n",
      "|    explained_variance   | 0.604     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0396   |\n",
      "|    n_updates            | 7296      |\n",
      "|    policy_gradient_loss | -0.0535   |\n",
      "|    value_loss           | 0.165     |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 63.1     |\n",
      "| time/              |          |\n",
      "|    fps             | 776      |\n",
      "|    iterations      | 385      |\n",
      "|    time_elapsed    | 126      |\n",
      "|    total_timesteps | 98560    |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 63        |\n",
      "| time/                   |           |\n",
      "|    fps                  | 777       |\n",
      "|    iterations           | 386       |\n",
      "|    time_elapsed         | 127       |\n",
      "|    total_timesteps      | 98816     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.1188173 |\n",
      "|    clip_fraction        | 0.18      |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0724   |\n",
      "|    explained_variance   | 0.576     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.126    |\n",
      "|    n_updates            | 7315      |\n",
      "|    policy_gradient_loss | -0.0722   |\n",
      "|    value_loss           | 0.13      |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=99000, episode_reward=7.70 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 7.7       |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 99000     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.8600386 |\n",
      "|    clip_fraction        | 0.241     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.103    |\n",
      "|    explained_variance   | 0.155     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0261   |\n",
      "|    n_updates            | 7334      |\n",
      "|    policy_gradient_loss | -0.0537   |\n",
      "|    value_loss           | 0.849     |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 63       |\n",
      "| time/              |          |\n",
      "|    fps             | 776      |\n",
      "|    iterations      | 387      |\n",
      "|    time_elapsed    | 127      |\n",
      "|    total_timesteps | 99072    |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 63        |\n",
      "| time/                   |           |\n",
      "|    fps                  | 776       |\n",
      "|    iterations           | 388       |\n",
      "|    time_elapsed         | 127       |\n",
      "|    total_timesteps      | 99328     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.0056981 |\n",
      "|    clip_fraction        | 0.139     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0645   |\n",
      "|    explained_variance   | 0.258     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.147    |\n",
      "|    n_updates            | 7353      |\n",
      "|    policy_gradient_loss | -0.054    |\n",
      "|    value_loss           | 0.0923    |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=99500, episode_reward=-1.79 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 230        |\n",
      "|    mean_reward          | -1.79      |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 99500      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.58113426 |\n",
      "|    clip_fraction        | 0.111      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0583    |\n",
      "|    explained_variance   | 0.539      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.00871   |\n",
      "|    n_updates            | 7372       |\n",
      "|    policy_gradient_loss | -0.0553    |\n",
      "|    value_loss           | 0.23       |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 62.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 776      |\n",
      "|    iterations      | 389      |\n",
      "|    time_elapsed    | 128      |\n",
      "|    total_timesteps | 99584    |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 62.9      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 777       |\n",
      "|    iterations           | 390       |\n",
      "|    time_elapsed         | 128       |\n",
      "|    total_timesteps      | 99840     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.5924182 |\n",
      "|    clip_fraction        | 0.121     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0778   |\n",
      "|    explained_variance   | 0.503     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | 0.189     |\n",
      "|    n_updates            | 7391      |\n",
      "|    policy_gradient_loss | -0.0469   |\n",
      "|    value_loss           | 0.376     |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=100000, episode_reward=6.47 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 6.47      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 100000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 2.0244594 |\n",
      "|    clip_fraction        | 0.183     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0612   |\n",
      "|    explained_variance   | 0.166     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | 0.0246    |\n",
      "|    n_updates            | 7410      |\n",
      "|    policy_gradient_loss | -0.0468   |\n",
      "|    value_loss           | 0.169     |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 62.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 777      |\n",
      "|    iterations      | 391      |\n",
      "|    time_elapsed    | 128      |\n",
      "|    total_timesteps | 100096   |\n",
      "---------------------------------\n",
      "--------------------------------------\n",
      "| rollout/                |          |\n",
      "|    ep_len_mean          | 889      |\n",
      "|    ep_rew_mean          | 62.9     |\n",
      "| time/                   |          |\n",
      "|    fps                  | 777      |\n",
      "|    iterations           | 392      |\n",
      "|    time_elapsed         | 129      |\n",
      "|    total_timesteps      | 100352   |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 0.749667 |\n",
      "|    clip_fraction        | 0.153    |\n",
      "|    clip_range           | 0.4      |\n",
      "|    entropy_loss         | -0.0565  |\n",
      "|    explained_variance   | 0.836    |\n",
      "|    learning_rate        | 0.000389 |\n",
      "|    loss                 | -0.058   |\n",
      "|    n_updates            | 7429     |\n",
      "|    policy_gradient_loss | -0.0684  |\n",
      "|    value_loss           | 0.142    |\n",
      "--------------------------------------\n",
      "Eval num_timesteps=100500, episode_reward=19.62 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 19.6      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 100500    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.3102753 |\n",
      "|    clip_fraction        | 0.211     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0742   |\n",
      "|    explained_variance   | 0.746     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0885   |\n",
      "|    n_updates            | 7448      |\n",
      "|    policy_gradient_loss | -0.0741   |\n",
      "|    value_loss           | 0.0944    |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 63.1     |\n",
      "| time/              |          |\n",
      "|    fps             | 777      |\n",
      "|    iterations      | 393      |\n",
      "|    time_elapsed    | 129      |\n",
      "|    total_timesteps | 100608   |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 63.1      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 777       |\n",
      "|    iterations           | 394       |\n",
      "|    time_elapsed         | 129       |\n",
      "|    total_timesteps      | 100864    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.4461019 |\n",
      "|    clip_fraction        | 0.161     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.091    |\n",
      "|    explained_variance   | 0.87      |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0863   |\n",
      "|    n_updates            | 7467      |\n",
      "|    policy_gradient_loss | -0.0416   |\n",
      "|    value_loss           | 0.124     |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=101000, episode_reward=51.71 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "--------------------------------------\n",
      "| eval/                   |          |\n",
      "|    mean_ep_length       | 230      |\n",
      "|    mean_reward          | 51.7     |\n",
      "| time/                   |          |\n",
      "|    total_timesteps      | 101000   |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 1.246335 |\n",
      "|    clip_fraction        | 0.161    |\n",
      "|    clip_range           | 0.4      |\n",
      "|    entropy_loss         | -0.0626  |\n",
      "|    explained_variance   | 0.893    |\n",
      "|    learning_rate        | 0.000389 |\n",
      "|    loss                 | -0.115   |\n",
      "|    n_updates            | 7486     |\n",
      "|    policy_gradient_loss | -0.0652  |\n",
      "|    value_loss           | 0.0589   |\n",
      "--------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 63.1     |\n",
      "| time/              |          |\n",
      "|    fps             | 777      |\n",
      "|    iterations      | 395      |\n",
      "|    time_elapsed    | 130      |\n",
      "|    total_timesteps | 101120   |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 63.3      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 778       |\n",
      "|    iterations           | 396       |\n",
      "|    time_elapsed         | 130       |\n",
      "|    total_timesteps      | 101376    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.3204353 |\n",
      "|    clip_fraction        | 0.152     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0588   |\n",
      "|    explained_variance   | -0.0425   |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.142    |\n",
      "|    n_updates            | 7505      |\n",
      "|    policy_gradient_loss | -0.0712   |\n",
      "|    value_loss           | 0.193     |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=101500, episode_reward=23.58 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 230        |\n",
      "|    mean_reward          | 23.6       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 101500     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.67167765 |\n",
      "|    clip_fraction        | 0.152      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0695    |\n",
      "|    explained_variance   | 0.84       |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0422    |\n",
      "|    n_updates            | 7524       |\n",
      "|    policy_gradient_loss | -0.052     |\n",
      "|    value_loss           | 0.164      |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 63.3     |\n",
      "| time/              |          |\n",
      "|    fps             | 777      |\n",
      "|    iterations      | 397      |\n",
      "|    time_elapsed    | 130      |\n",
      "|    total_timesteps | 101632   |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 63.3      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 778       |\n",
      "|    iterations           | 398       |\n",
      "|    time_elapsed         | 130       |\n",
      "|    total_timesteps      | 101888    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.0224828 |\n",
      "|    clip_fraction        | 0.197     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0682   |\n",
      "|    explained_variance   | 0.0589    |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0865   |\n",
      "|    n_updates            | 7543      |\n",
      "|    policy_gradient_loss | -0.0719   |\n",
      "|    value_loss           | 0.042     |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=102000, episode_reward=45.09 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 45.1      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 102000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.8791654 |\n",
      "|    clip_fraction        | 0.136     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0567   |\n",
      "|    explained_variance   | 0.724     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.11     |\n",
      "|    n_updates            | 7562      |\n",
      "|    policy_gradient_loss | -0.0637   |\n",
      "|    value_loss           | 0.211     |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 63.3     |\n",
      "| time/              |          |\n",
      "|    fps             | 778      |\n",
      "|    iterations      | 399      |\n",
      "|    time_elapsed    | 131      |\n",
      "|    total_timesteps | 102144   |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 63.2      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 778       |\n",
      "|    iterations           | 400       |\n",
      "|    time_elapsed         | 131       |\n",
      "|    total_timesteps      | 102400    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.2725749 |\n",
      "|    clip_fraction        | 0.203     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0748   |\n",
      "|    explained_variance   | 0.816     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.131    |\n",
      "|    n_updates            | 7581      |\n",
      "|    policy_gradient_loss | -0.0839   |\n",
      "|    value_loss           | 0.0997    |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=102500, episode_reward=22.53 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 22.5      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 102500    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.9964749 |\n",
      "|    clip_fraction        | 0.238     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0825   |\n",
      "|    explained_variance   | 0.845     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.126    |\n",
      "|    n_updates            | 7600      |\n",
      "|    policy_gradient_loss | -0.0753   |\n",
      "|    value_loss           | 0.101     |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 63.2     |\n",
      "| time/              |          |\n",
      "|    fps             | 776      |\n",
      "|    iterations      | 401      |\n",
      "|    time_elapsed    | 132      |\n",
      "|    total_timesteps | 102656   |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 63.2       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 776        |\n",
      "|    iterations           | 402        |\n",
      "|    time_elapsed         | 132        |\n",
      "|    total_timesteps      | 102912     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.88034135 |\n",
      "|    clip_fraction        | 0.178      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0735    |\n",
      "|    explained_variance   | 0.498      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.115     |\n",
      "|    n_updates            | 7619       |\n",
      "|    policy_gradient_loss | -0.061     |\n",
      "|    value_loss           | 0.139      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=103000, episode_reward=35.88 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 35.9      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 103000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.5679457 |\n",
      "|    clip_fraction        | 0.15      |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0684   |\n",
      "|    explained_variance   | 0.29      |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0492   |\n",
      "|    n_updates            | 7638      |\n",
      "|    policy_gradient_loss | -0.0543   |\n",
      "|    value_loss           | 0.125     |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 63.1     |\n",
      "| time/              |          |\n",
      "|    fps             | 776      |\n",
      "|    iterations      | 403      |\n",
      "|    time_elapsed    | 132      |\n",
      "|    total_timesteps | 103168   |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 63.1       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 776        |\n",
      "|    iterations           | 404        |\n",
      "|    time_elapsed         | 133        |\n",
      "|    total_timesteps      | 103424     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.46219295 |\n",
      "|    clip_fraction        | 0.134      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0785    |\n",
      "|    explained_variance   | 0.828      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0406    |\n",
      "|    n_updates            | 7657       |\n",
      "|    policy_gradient_loss | -0.0478    |\n",
      "|    value_loss           | 0.139      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=103500, episode_reward=34.65 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 34.7      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 103500    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.0573808 |\n",
      "|    clip_fraction        | 0.203     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0811   |\n",
      "|    explained_variance   | 0.253     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.11     |\n",
      "|    n_updates            | 7676      |\n",
      "|    policy_gradient_loss | -0.0771   |\n",
      "|    value_loss           | 0.0529    |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 63.1     |\n",
      "| time/              |          |\n",
      "|    fps             | 776      |\n",
      "|    iterations      | 405      |\n",
      "|    time_elapsed    | 133      |\n",
      "|    total_timesteps | 103680   |\n",
      "---------------------------------\n",
      "--------------------------------------\n",
      "| rollout/                |          |\n",
      "|    ep_len_mean          | 889      |\n",
      "|    ep_rew_mean          | 63.1     |\n",
      "| time/                   |          |\n",
      "|    fps                  | 776      |\n",
      "|    iterations           | 406      |\n",
      "|    time_elapsed         | 133      |\n",
      "|    total_timesteps      | 103936   |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 0.892579 |\n",
      "|    clip_fraction        | 0.147    |\n",
      "|    clip_range           | 0.4      |\n",
      "|    entropy_loss         | -0.0514  |\n",
      "|    explained_variance   | 0.742    |\n",
      "|    learning_rate        | 0.000389 |\n",
      "|    loss                 | -0.0532  |\n",
      "|    n_updates            | 7695     |\n",
      "|    policy_gradient_loss | -0.0566  |\n",
      "|    value_loss           | 0.105    |\n",
      "--------------------------------------\n",
      "Eval num_timesteps=104000, episode_reward=11.89 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 230        |\n",
      "|    mean_reward          | 11.9       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 104000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.63646156 |\n",
      "|    clip_fraction        | 0.124      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0663    |\n",
      "|    explained_variance   | 0.466      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0153    |\n",
      "|    n_updates            | 7714       |\n",
      "|    policy_gradient_loss | -0.0426    |\n",
      "|    value_loss           | 0.169      |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 62.8     |\n",
      "| time/              |          |\n",
      "|    fps             | 776      |\n",
      "|    iterations      | 407      |\n",
      "|    time_elapsed    | 134      |\n",
      "|    total_timesteps | 104192   |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 62.8      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 776       |\n",
      "|    iterations           | 408       |\n",
      "|    time_elapsed         | 134       |\n",
      "|    total_timesteps      | 104448    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.5702674 |\n",
      "|    clip_fraction        | 0.198     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0594   |\n",
      "|    explained_variance   | -0.0788   |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0571   |\n",
      "|    n_updates            | 7733      |\n",
      "|    policy_gradient_loss | -0.054    |\n",
      "|    value_loss           | 0.107     |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=104500, episode_reward=13.39 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 230        |\n",
      "|    mean_reward          | 13.4       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 104500     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.95292443 |\n",
      "|    clip_fraction        | 0.127      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0502    |\n",
      "|    explained_variance   | 0.436      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0752    |\n",
      "|    n_updates            | 7752       |\n",
      "|    policy_gradient_loss | -0.051     |\n",
      "|    value_loss           | 0.0792     |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 62.8     |\n",
      "| time/              |          |\n",
      "|    fps             | 776      |\n",
      "|    iterations      | 409      |\n",
      "|    time_elapsed    | 134      |\n",
      "|    total_timesteps | 104704   |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 62.9      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 776       |\n",
      "|    iterations           | 410       |\n",
      "|    time_elapsed         | 135       |\n",
      "|    total_timesteps      | 104960    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.8025893 |\n",
      "|    clip_fraction        | 0.145     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0661   |\n",
      "|    explained_variance   | 0.37      |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0707   |\n",
      "|    n_updates            | 7771      |\n",
      "|    policy_gradient_loss | -0.0512   |\n",
      "|    value_loss           | 0.0906    |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=105000, episode_reward=8.81 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 230        |\n",
      "|    mean_reward          | 8.81       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 105000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.84385896 |\n",
      "|    clip_fraction        | 0.18       |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0737    |\n",
      "|    explained_variance   | 0.548      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.046     |\n",
      "|    n_updates            | 7790       |\n",
      "|    policy_gradient_loss | -0.0593    |\n",
      "|    value_loss           | 0.312      |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 62.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 775      |\n",
      "|    iterations      | 411      |\n",
      "|    time_elapsed    | 135      |\n",
      "|    total_timesteps | 105216   |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 62.9       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 776        |\n",
      "|    iterations           | 412        |\n",
      "|    time_elapsed         | 135        |\n",
      "|    total_timesteps      | 105472     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.72258806 |\n",
      "|    clip_fraction        | 0.156      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.066     |\n",
      "|    explained_variance   | 0.676      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0419    |\n",
      "|    n_updates            | 7809       |\n",
      "|    policy_gradient_loss | -0.0607    |\n",
      "|    value_loss           | 0.0574     |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=105500, episode_reward=-2.50 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | -2.5      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 105500    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.8631977 |\n",
      "|    clip_fraction        | 0.134     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0517   |\n",
      "|    explained_variance   | 0.645     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0845   |\n",
      "|    n_updates            | 7828      |\n",
      "|    policy_gradient_loss | -0.0561   |\n",
      "|    value_loss           | 0.103     |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 62.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 776      |\n",
      "|    iterations      | 413      |\n",
      "|    time_elapsed    | 136      |\n",
      "|    total_timesteps | 105728   |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 62.7      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 776       |\n",
      "|    iterations           | 414       |\n",
      "|    time_elapsed         | 136       |\n",
      "|    total_timesteps      | 105984    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.9495702 |\n",
      "|    clip_fraction        | 0.186     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0678   |\n",
      "|    explained_variance   | 0.815     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0715   |\n",
      "|    n_updates            | 7847      |\n",
      "|    policy_gradient_loss | -0.0666   |\n",
      "|    value_loss           | 0.102     |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=106000, episode_reward=-7.00 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | -7        |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 106000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.6851954 |\n",
      "|    clip_fraction        | 0.21      |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0745   |\n",
      "|    explained_variance   | 0.643     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0743   |\n",
      "|    n_updates            | 7866      |\n",
      "|    policy_gradient_loss | -0.0624   |\n",
      "|    value_loss           | 0.06      |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 62.7     |\n",
      "| time/              |          |\n",
      "|    fps             | 776      |\n",
      "|    iterations      | 415      |\n",
      "|    time_elapsed    | 136      |\n",
      "|    total_timesteps | 106240   |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 62.7      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 777       |\n",
      "|    iterations           | 416       |\n",
      "|    time_elapsed         | 137       |\n",
      "|    total_timesteps      | 106496    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.1123934 |\n",
      "|    clip_fraction        | 0.128     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0452   |\n",
      "|    explained_variance   | 0.484     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0762   |\n",
      "|    n_updates            | 7885      |\n",
      "|    policy_gradient_loss | -0.0522   |\n",
      "|    value_loss           | 0.101     |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=106500, episode_reward=-6.15 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | -6.15     |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 106500    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.6800722 |\n",
      "|    clip_fraction        | 0.161     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0781   |\n",
      "|    explained_variance   | 0.623     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0348   |\n",
      "|    n_updates            | 7904      |\n",
      "|    policy_gradient_loss | -0.0443   |\n",
      "|    value_loss           | 0.0952    |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 63       |\n",
      "| time/              |          |\n",
      "|    fps             | 776      |\n",
      "|    iterations      | 417      |\n",
      "|    time_elapsed    | 137      |\n",
      "|    total_timesteps | 106752   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=107000, episode_reward=-4.77 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | -4.77     |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 107000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.7802248 |\n",
      "|    clip_fraction        | 0.158     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0708   |\n",
      "|    explained_variance   | 0.897     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0611   |\n",
      "|    n_updates            | 7923      |\n",
      "|    policy_gradient_loss | -0.0473   |\n",
      "|    value_loss           | 0.0832    |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 63       |\n",
      "| time/              |          |\n",
      "|    fps             | 776      |\n",
      "|    iterations      | 418      |\n",
      "|    time_elapsed    | 137      |\n",
      "|    total_timesteps | 107008   |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 63        |\n",
      "| time/                   |           |\n",
      "|    fps                  | 777       |\n",
      "|    iterations           | 419       |\n",
      "|    time_elapsed         | 137       |\n",
      "|    total_timesteps      | 107264    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.9663839 |\n",
      "|    clip_fraction        | 0.179     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0767   |\n",
      "|    explained_variance   | 0.505     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0994   |\n",
      "|    n_updates            | 7942      |\n",
      "|    policy_gradient_loss | -0.0552   |\n",
      "|    value_loss           | 0.0571    |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=107500, episode_reward=-10.46 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | -10.5     |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 107500    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.6637895 |\n",
      "|    clip_fraction        | 0.15      |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0642   |\n",
      "|    explained_variance   | 0.562     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0588   |\n",
      "|    n_updates            | 7961      |\n",
      "|    policy_gradient_loss | -0.0613   |\n",
      "|    value_loss           | 0.15      |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 63       |\n",
      "| time/              |          |\n",
      "|    fps             | 777      |\n",
      "|    iterations      | 420      |\n",
      "|    time_elapsed    | 138      |\n",
      "|    total_timesteps | 107520   |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 63        |\n",
      "| time/                   |           |\n",
      "|    fps                  | 777       |\n",
      "|    iterations           | 421       |\n",
      "|    time_elapsed         | 138       |\n",
      "|    total_timesteps      | 107776    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.7074874 |\n",
      "|    clip_fraction        | 0.185     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0838   |\n",
      "|    explained_variance   | 0.822     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0701   |\n",
      "|    n_updates            | 7980      |\n",
      "|    policy_gradient_loss | -0.0615   |\n",
      "|    value_loss           | 0.157     |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=108000, episode_reward=-41.91 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | -41.9     |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 108000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.0434256 |\n",
      "|    clip_fraction        | 0.226     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0942   |\n",
      "|    explained_variance   | -0.0174   |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0961   |\n",
      "|    n_updates            | 7999      |\n",
      "|    policy_gradient_loss | -0.0746   |\n",
      "|    value_loss           | 0.105     |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 63       |\n",
      "| time/              |          |\n",
      "|    fps             | 776      |\n",
      "|    iterations      | 422      |\n",
      "|    time_elapsed    | 139      |\n",
      "|    total_timesteps | 108032   |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 63         |\n",
      "| time/                   |            |\n",
      "|    fps                  | 777        |\n",
      "|    iterations           | 423        |\n",
      "|    time_elapsed         | 139        |\n",
      "|    total_timesteps      | 108288     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.46371835 |\n",
      "|    clip_fraction        | 0.124      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.067     |\n",
      "|    explained_variance   | 0.528      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0583    |\n",
      "|    n_updates            | 8018       |\n",
      "|    policy_gradient_loss | -0.0401    |\n",
      "|    value_loss           | 0.0873     |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=108500, episode_reward=-12.17 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | -12.2     |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 108500    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.4579334 |\n",
      "|    clip_fraction        | 0.182     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0663   |\n",
      "|    explained_variance   | 0.554     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0618   |\n",
      "|    n_updates            | 8037      |\n",
      "|    policy_gradient_loss | -0.062    |\n",
      "|    value_loss           | 0.128     |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 63.2     |\n",
      "| time/              |          |\n",
      "|    fps             | 777      |\n",
      "|    iterations      | 424      |\n",
      "|    time_elapsed    | 139      |\n",
      "|    total_timesteps | 108544   |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 63.2      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 777       |\n",
      "|    iterations           | 425       |\n",
      "|    time_elapsed         | 139       |\n",
      "|    total_timesteps      | 108800    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.6683674 |\n",
      "|    clip_fraction        | 0.189     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.1      |\n",
      "|    explained_variance   | 0.632     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0596   |\n",
      "|    n_updates            | 8056      |\n",
      "|    policy_gradient_loss | -0.0663   |\n",
      "|    value_loss           | 0.357     |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=109000, episode_reward=-13.19 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | -13.2     |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 109000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.7202561 |\n",
      "|    clip_fraction        | 0.146     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0507   |\n",
      "|    explained_variance   | 0.589     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.098    |\n",
      "|    n_updates            | 8075      |\n",
      "|    policy_gradient_loss | -0.0665   |\n",
      "|    value_loss           | 0.0536    |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 63.2     |\n",
      "| time/              |          |\n",
      "|    fps             | 777      |\n",
      "|    iterations      | 426      |\n",
      "|    time_elapsed    | 140      |\n",
      "|    total_timesteps | 109056   |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 63.2       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 778        |\n",
      "|    iterations           | 427        |\n",
      "|    time_elapsed         | 140        |\n",
      "|    total_timesteps      | 109312     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.93261683 |\n",
      "|    clip_fraction        | 0.166      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0627    |\n",
      "|    explained_variance   | 0.452      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0948    |\n",
      "|    n_updates            | 8094       |\n",
      "|    policy_gradient_loss | -0.0633    |\n",
      "|    value_loss           | 0.182      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=109500, episode_reward=-7.34 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 230        |\n",
      "|    mean_reward          | -7.34      |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 109500     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.69147265 |\n",
      "|    clip_fraction        | 0.158      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.108     |\n",
      "|    explained_variance   | 0.0685     |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0897    |\n",
      "|    n_updates            | 8113       |\n",
      "|    policy_gradient_loss | -0.0578    |\n",
      "|    value_loss           | 0.226      |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 63.2     |\n",
      "| time/              |          |\n",
      "|    fps             | 777      |\n",
      "|    iterations      | 428      |\n",
      "|    time_elapsed    | 140      |\n",
      "|    total_timesteps | 109568   |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 63.2      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 777       |\n",
      "|    iterations           | 429       |\n",
      "|    time_elapsed         | 141       |\n",
      "|    total_timesteps      | 109824    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.1335307 |\n",
      "|    clip_fraction        | 0.215     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.106    |\n",
      "|    explained_variance   | 0.537     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0105   |\n",
      "|    n_updates            | 8132      |\n",
      "|    policy_gradient_loss | -0.0518   |\n",
      "|    value_loss           | 0.0721    |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=110000, episode_reward=-16.27 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | -16.3     |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 110000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.1029011 |\n",
      "|    clip_fraction        | 0.163     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0662   |\n",
      "|    explained_variance   | 0.423     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0954   |\n",
      "|    n_updates            | 8151      |\n",
      "|    policy_gradient_loss | -0.065    |\n",
      "|    value_loss           | 0.139     |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 63.2     |\n",
      "| time/              |          |\n",
      "|    fps             | 776      |\n",
      "|    iterations      | 430      |\n",
      "|    time_elapsed    | 141      |\n",
      "|    total_timesteps | 110080   |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 63.8      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 777       |\n",
      "|    iterations           | 431       |\n",
      "|    time_elapsed         | 141       |\n",
      "|    total_timesteps      | 110336    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.0678173 |\n",
      "|    clip_fraction        | 0.162     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0642   |\n",
      "|    explained_variance   | 0.732     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0599   |\n",
      "|    n_updates            | 8170      |\n",
      "|    policy_gradient_loss | -0.0621   |\n",
      "|    value_loss           | 0.0778    |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=110500, episode_reward=-10.50 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | -10.5     |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 110500    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.1620761 |\n",
      "|    clip_fraction        | 0.152     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0936   |\n",
      "|    explained_variance   | 0.0199    |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | 0.0735    |\n",
      "|    n_updates            | 8189      |\n",
      "|    policy_gradient_loss | -0.0444   |\n",
      "|    value_loss           | 0.72      |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 63.8     |\n",
      "| time/              |          |\n",
      "|    fps             | 775      |\n",
      "|    iterations      | 432      |\n",
      "|    time_elapsed    | 142      |\n",
      "|    total_timesteps | 110592   |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 63.8      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 776       |\n",
      "|    iterations           | 433       |\n",
      "|    time_elapsed         | 142       |\n",
      "|    total_timesteps      | 110848    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.6761488 |\n",
      "|    clip_fraction        | 0.167     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0629   |\n",
      "|    explained_variance   | 0.3       |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0675   |\n",
      "|    n_updates            | 8208      |\n",
      "|    policy_gradient_loss | -0.068    |\n",
      "|    value_loss           | 0.0642    |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=111000, episode_reward=24.48 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 24.5      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 111000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.9394396 |\n",
      "|    clip_fraction        | 0.134     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0557   |\n",
      "|    explained_variance   | 0.347     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.118    |\n",
      "|    n_updates            | 8227      |\n",
      "|    policy_gradient_loss | -0.0621   |\n",
      "|    value_loss           | 0.17      |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 63.8     |\n",
      "| time/              |          |\n",
      "|    fps             | 775      |\n",
      "|    iterations      | 434      |\n",
      "|    time_elapsed    | 143      |\n",
      "|    total_timesteps | 111104   |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 63.9      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 776       |\n",
      "|    iterations           | 435       |\n",
      "|    time_elapsed         | 143       |\n",
      "|    total_timesteps      | 111360    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.9384761 |\n",
      "|    clip_fraction        | 0.163     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.076    |\n",
      "|    explained_variance   | 0.636     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0677   |\n",
      "|    n_updates            | 8246      |\n",
      "|    policy_gradient_loss | -0.0512   |\n",
      "|    value_loss           | 0.275     |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=111500, episode_reward=37.16 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 37.2      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 111500    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 2.6029463 |\n",
      "|    clip_fraction        | 0.268     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.088    |\n",
      "|    explained_variance   | 0.52      |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.109    |\n",
      "|    n_updates            | 8265      |\n",
      "|    policy_gradient_loss | -0.0852   |\n",
      "|    value_loss           | 0.0721    |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 63.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 776      |\n",
      "|    iterations      | 436      |\n",
      "|    time_elapsed    | 143      |\n",
      "|    total_timesteps | 111616   |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 63.9      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 776       |\n",
      "|    iterations           | 437       |\n",
      "|    time_elapsed         | 144       |\n",
      "|    total_timesteps      | 111872    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.3574951 |\n",
      "|    clip_fraction        | 0.199     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0701   |\n",
      "|    explained_variance   | 0.562     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0533   |\n",
      "|    n_updates            | 8284      |\n",
      "|    policy_gradient_loss | -0.0769   |\n",
      "|    value_loss           | 0.094     |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=112000, episode_reward=21.96 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 22        |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 112000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.9275446 |\n",
      "|    clip_fraction        | 0.166     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0656   |\n",
      "|    explained_variance   | 0.581     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0337   |\n",
      "|    n_updates            | 8303      |\n",
      "|    policy_gradient_loss | -0.0626   |\n",
      "|    value_loss           | 0.125     |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 64.2     |\n",
      "| time/              |          |\n",
      "|    fps             | 776      |\n",
      "|    iterations      | 438      |\n",
      "|    time_elapsed    | 144      |\n",
      "|    total_timesteps | 112128   |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 64.2      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 776       |\n",
      "|    iterations           | 439       |\n",
      "|    time_elapsed         | 144       |\n",
      "|    total_timesteps      | 112384    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.7271734 |\n",
      "|    clip_fraction        | 0.159     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.102    |\n",
      "|    explained_variance   | 0.712     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0324   |\n",
      "|    n_updates            | 8322      |\n",
      "|    policy_gradient_loss | -0.0495   |\n",
      "|    value_loss           | 0.33      |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=112500, episode_reward=-10.03 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 230        |\n",
      "|    mean_reward          | -10        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 112500     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.77522445 |\n",
      "|    clip_fraction        | 0.172      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0601    |\n",
      "|    explained_variance   | 0.578      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0688    |\n",
      "|    n_updates            | 8341       |\n",
      "|    policy_gradient_loss | -0.0725    |\n",
      "|    value_loss           | 0.0381     |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 64.2     |\n",
      "| time/              |          |\n",
      "|    fps             | 776      |\n",
      "|    iterations      | 440      |\n",
      "|    time_elapsed    | 145      |\n",
      "|    total_timesteps | 112640   |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 64.2      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 776       |\n",
      "|    iterations           | 441       |\n",
      "|    time_elapsed         | 145       |\n",
      "|    total_timesteps      | 112896    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.4503104 |\n",
      "|    clip_fraction        | 0.106     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0524   |\n",
      "|    explained_variance   | 0.602     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0906   |\n",
      "|    n_updates            | 8360      |\n",
      "|    policy_gradient_loss | -0.0512   |\n",
      "|    value_loss           | 0.141     |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=113000, episode_reward=56.49 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 230        |\n",
      "|    mean_reward          | 56.5       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 113000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.68884397 |\n",
      "|    clip_fraction        | 0.166      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0793    |\n",
      "|    explained_variance   | 0.51       |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | 0.0172     |\n",
      "|    n_updates            | 8379       |\n",
      "|    policy_gradient_loss | -0.0629    |\n",
      "|    value_loss           | 0.569      |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 64.3     |\n",
      "| time/              |          |\n",
      "|    fps             | 775      |\n",
      "|    iterations      | 442      |\n",
      "|    time_elapsed    | 145      |\n",
      "|    total_timesteps | 113152   |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 64.3      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 776       |\n",
      "|    iterations           | 443       |\n",
      "|    time_elapsed         | 146       |\n",
      "|    total_timesteps      | 113408    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.7870749 |\n",
      "|    clip_fraction        | 0.289     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0755   |\n",
      "|    explained_variance   | 0.0861    |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.101    |\n",
      "|    n_updates            | 8398      |\n",
      "|    policy_gradient_loss | -0.0791   |\n",
      "|    value_loss           | 0.0534    |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=113500, episode_reward=20.06 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 230        |\n",
      "|    mean_reward          | 20.1       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 113500     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.30647385 |\n",
      "|    clip_fraction        | 0.108      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0527    |\n",
      "|    explained_variance   | 0.696      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0977    |\n",
      "|    n_updates            | 8417       |\n",
      "|    policy_gradient_loss | -0.0506    |\n",
      "|    value_loss           | 0.0841     |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 64.3     |\n",
      "| time/              |          |\n",
      "|    fps             | 776      |\n",
      "|    iterations      | 444      |\n",
      "|    time_elapsed    | 146      |\n",
      "|    total_timesteps | 113664   |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 64.4      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 776       |\n",
      "|    iterations           | 445       |\n",
      "|    time_elapsed         | 146       |\n",
      "|    total_timesteps      | 113920    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.6699053 |\n",
      "|    clip_fraction        | 0.144     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0488   |\n",
      "|    explained_variance   | 0.681     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.062    |\n",
      "|    n_updates            | 8436      |\n",
      "|    policy_gradient_loss | -0.059    |\n",
      "|    value_loss           | 0.0777    |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=114000, episode_reward=18.79 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 18.8      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 114000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.3143482 |\n",
      "|    clip_fraction        | 0.212     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.109    |\n",
      "|    explained_variance   | 0.778     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0734   |\n",
      "|    n_updates            | 8455      |\n",
      "|    policy_gradient_loss | -0.0698   |\n",
      "|    value_loss           | 0.267     |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 64.4     |\n",
      "| time/              |          |\n",
      "|    fps             | 776      |\n",
      "|    iterations      | 446      |\n",
      "|    time_elapsed    | 147      |\n",
      "|    total_timesteps | 114176   |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 64.4      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 777       |\n",
      "|    iterations           | 447       |\n",
      "|    time_elapsed         | 147       |\n",
      "|    total_timesteps      | 114432    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.4642265 |\n",
      "|    clip_fraction        | 0.179     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0518   |\n",
      "|    explained_variance   | 0.463     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0426   |\n",
      "|    n_updates            | 8474      |\n",
      "|    policy_gradient_loss | -0.0744   |\n",
      "|    value_loss           | 0.0474    |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=114500, episode_reward=15.93 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 15.9      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 114500    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.7921287 |\n",
      "|    clip_fraction        | 0.151     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0598   |\n",
      "|    explained_variance   | 0.69      |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.121    |\n",
      "|    n_updates            | 8493      |\n",
      "|    policy_gradient_loss | -0.0681   |\n",
      "|    value_loss           | 0.0991    |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 64.6     |\n",
      "| time/              |          |\n",
      "|    fps             | 776      |\n",
      "|    iterations      | 448      |\n",
      "|    time_elapsed    | 147      |\n",
      "|    total_timesteps | 114688   |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 64.6       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 776        |\n",
      "|    iterations           | 449        |\n",
      "|    time_elapsed         | 147        |\n",
      "|    total_timesteps      | 114944     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.58573455 |\n",
      "|    clip_fraction        | 0.133      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0754    |\n",
      "|    explained_variance   | 0.834      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0494    |\n",
      "|    n_updates            | 8512       |\n",
      "|    policy_gradient_loss | -0.0585    |\n",
      "|    value_loss           | 0.166      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=115000, episode_reward=-0.56 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | -0.56     |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 115000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.4397278 |\n",
      "|    clip_fraction        | 0.216     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.101    |\n",
      "|    explained_variance   | 0.415     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.112    |\n",
      "|    n_updates            | 8531      |\n",
      "|    policy_gradient_loss | -0.0678   |\n",
      "|    value_loss           | 0.0531    |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 64.6     |\n",
      "| time/              |          |\n",
      "|    fps             | 776      |\n",
      "|    iterations      | 450      |\n",
      "|    time_elapsed    | 148      |\n",
      "|    total_timesteps | 115200   |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 64.6       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 777        |\n",
      "|    iterations           | 451        |\n",
      "|    time_elapsed         | 148        |\n",
      "|    total_timesteps      | 115456     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.58166707 |\n",
      "|    clip_fraction        | 0.127      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0569    |\n",
      "|    explained_variance   | 0.674      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0578    |\n",
      "|    n_updates            | 8550       |\n",
      "|    policy_gradient_loss | -0.0478    |\n",
      "|    value_loss           | 0.0899     |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=115500, episode_reward=4.48 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 230        |\n",
      "|    mean_reward          | 4.48       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 115500     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.51777196 |\n",
      "|    clip_fraction        | 0.121      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0435    |\n",
      "|    explained_variance   | 0.658      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0332    |\n",
      "|    n_updates            | 8569       |\n",
      "|    policy_gradient_loss | -0.0441    |\n",
      "|    value_loss           | 0.0934     |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 64.7     |\n",
      "| time/              |          |\n",
      "|    fps             | 777      |\n",
      "|    iterations      | 452      |\n",
      "|    time_elapsed    | 148      |\n",
      "|    total_timesteps | 115712   |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 64.7       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 777        |\n",
      "|    iterations           | 453        |\n",
      "|    time_elapsed         | 149        |\n",
      "|    total_timesteps      | 115968     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.72087896 |\n",
      "|    clip_fraction        | 0.184      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.119     |\n",
      "|    explained_variance   | 0.645      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0705    |\n",
      "|    n_updates            | 8588       |\n",
      "|    policy_gradient_loss | -0.054     |\n",
      "|    value_loss           | 0.201      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=116000, episode_reward=8.09 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 8.09      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 116000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.8140827 |\n",
      "|    clip_fraction        | 0.145     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0481   |\n",
      "|    explained_variance   | 0.702     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.122    |\n",
      "|    n_updates            | 8607      |\n",
      "|    policy_gradient_loss | -0.0669   |\n",
      "|    value_loss           | 0.0441    |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 64.7     |\n",
      "| time/              |          |\n",
      "|    fps             | 777      |\n",
      "|    iterations      | 454      |\n",
      "|    time_elapsed    | 149      |\n",
      "|    total_timesteps | 116224   |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 64.6       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 777        |\n",
      "|    iterations           | 455        |\n",
      "|    time_elapsed         | 149        |\n",
      "|    total_timesteps      | 116480     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.66088456 |\n",
      "|    clip_fraction        | 0.124      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0544    |\n",
      "|    explained_variance   | 0.492      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0482    |\n",
      "|    n_updates            | 8626       |\n",
      "|    policy_gradient_loss | -0.0509    |\n",
      "|    value_loss           | 0.129      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=116500, episode_reward=28.01 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 28        |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 116500    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.1851511 |\n",
      "|    clip_fraction        | 0.184     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0776   |\n",
      "|    explained_variance   | 0.761     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0949   |\n",
      "|    n_updates            | 8645      |\n",
      "|    policy_gradient_loss | -0.0825   |\n",
      "|    value_loss           | 0.146     |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 64.6     |\n",
      "| time/              |          |\n",
      "|    fps             | 776      |\n",
      "|    iterations      | 456      |\n",
      "|    time_elapsed    | 150      |\n",
      "|    total_timesteps | 116736   |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 64.6      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 777       |\n",
      "|    iterations           | 457       |\n",
      "|    time_elapsed         | 150       |\n",
      "|    total_timesteps      | 116992    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.6621356 |\n",
      "|    clip_fraction        | 0.139     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0755   |\n",
      "|    explained_variance   | 0.113     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0279   |\n",
      "|    n_updates            | 8664      |\n",
      "|    policy_gradient_loss | -0.0349   |\n",
      "|    value_loss           | 0.0531    |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=117000, episode_reward=12.60 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 12.6      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 117000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.6819686 |\n",
      "|    clip_fraction        | 0.162     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0782   |\n",
      "|    explained_variance   | 0.253     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0875   |\n",
      "|    n_updates            | 8683      |\n",
      "|    policy_gradient_loss | -0.0682   |\n",
      "|    value_loss           | 0.171     |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 64.6     |\n",
      "| time/              |          |\n",
      "|    fps             | 776      |\n",
      "|    iterations      | 458      |\n",
      "|    time_elapsed    | 150      |\n",
      "|    total_timesteps | 117248   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=117500, episode_reward=6.24 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 6.24      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 117500    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.9009821 |\n",
      "|    clip_fraction        | 0.154     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0578   |\n",
      "|    explained_variance   | 0.681     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0676   |\n",
      "|    n_updates            | 8702      |\n",
      "|    policy_gradient_loss | -0.0759   |\n",
      "|    value_loss           | 0.0771    |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 64.8     |\n",
      "| time/              |          |\n",
      "|    fps             | 775      |\n",
      "|    iterations      | 459      |\n",
      "|    time_elapsed    | 151      |\n",
      "|    total_timesteps | 117504   |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 64.8      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 776       |\n",
      "|    iterations           | 460       |\n",
      "|    time_elapsed         | 151       |\n",
      "|    total_timesteps      | 117760    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.4484939 |\n",
      "|    clip_fraction        | 0.174     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.111    |\n",
      "|    explained_variance   | 0.77      |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0508   |\n",
      "|    n_updates            | 8721      |\n",
      "|    policy_gradient_loss | -0.037    |\n",
      "|    value_loss           | 0.214     |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=118000, episode_reward=45.14 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 45.1      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 118000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.3612423 |\n",
      "|    clip_fraction        | 0.147     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0532   |\n",
      "|    explained_variance   | 0.611     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0679   |\n",
      "|    n_updates            | 8740      |\n",
      "|    policy_gradient_loss | -0.0523   |\n",
      "|    value_loss           | 0.0684    |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 64.8     |\n",
      "| time/              |          |\n",
      "|    fps             | 775      |\n",
      "|    iterations      | 461      |\n",
      "|    time_elapsed    | 152      |\n",
      "|    total_timesteps | 118016   |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 64.9      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 776       |\n",
      "|    iterations           | 462       |\n",
      "|    time_elapsed         | 152       |\n",
      "|    total_timesteps      | 118272    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.6821941 |\n",
      "|    clip_fraction        | 0.124     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0654   |\n",
      "|    explained_variance   | -0.00682  |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0856   |\n",
      "|    n_updates            | 8759      |\n",
      "|    policy_gradient_loss | -0.0431   |\n",
      "|    value_loss           | 0.17      |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=118500, episode_reward=19.97 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 20        |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 118500    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.9913952 |\n",
      "|    clip_fraction        | 0.152     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0683   |\n",
      "|    explained_variance   | 0.75      |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.037    |\n",
      "|    n_updates            | 8778      |\n",
      "|    policy_gradient_loss | -0.0486   |\n",
      "|    value_loss           | 0.234     |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 64.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 775      |\n",
      "|    iterations      | 463      |\n",
      "|    time_elapsed    | 152      |\n",
      "|    total_timesteps | 118528   |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 64.9      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 775       |\n",
      "|    iterations           | 464       |\n",
      "|    time_elapsed         | 153       |\n",
      "|    total_timesteps      | 118784    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.8160023 |\n",
      "|    clip_fraction        | 0.176     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.08     |\n",
      "|    explained_variance   | 0.38      |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.00968  |\n",
      "|    n_updates            | 8797      |\n",
      "|    policy_gradient_loss | -0.0543   |\n",
      "|    value_loss           | 0.0563    |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=119000, episode_reward=23.84 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 23.8      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 119000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.8803859 |\n",
      "|    clip_fraction        | 0.148     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0653   |\n",
      "|    explained_variance   | 0.368     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0489   |\n",
      "|    n_updates            | 8816      |\n",
      "|    policy_gradient_loss | -0.0596   |\n",
      "|    value_loss           | 0.154     |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 64.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 775      |\n",
      "|    iterations      | 465      |\n",
      "|    time_elapsed    | 153      |\n",
      "|    total_timesteps | 119040   |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 64.9       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 775        |\n",
      "|    iterations           | 466        |\n",
      "|    time_elapsed         | 153        |\n",
      "|    total_timesteps      | 119296     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.98835826 |\n",
      "|    clip_fraction        | 0.178      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0657    |\n",
      "|    explained_variance   | 0.891      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.113     |\n",
      "|    n_updates            | 8835       |\n",
      "|    policy_gradient_loss | -0.062     |\n",
      "|    value_loss           | 0.0946     |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=119500, episode_reward=12.95 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "--------------------------------------\n",
      "| eval/                   |          |\n",
      "|    mean_ep_length       | 230      |\n",
      "|    mean_reward          | 13       |\n",
      "| time/                   |          |\n",
      "|    total_timesteps      | 119500   |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 0.69752  |\n",
      "|    clip_fraction        | 0.166    |\n",
      "|    clip_range           | 0.4      |\n",
      "|    entropy_loss         | -0.0856  |\n",
      "|    explained_variance   | 0.448    |\n",
      "|    learning_rate        | 0.000389 |\n",
      "|    loss                 | -0.0974  |\n",
      "|    n_updates            | 8854     |\n",
      "|    policy_gradient_loss | -0.0684  |\n",
      "|    value_loss           | 0.123    |\n",
      "--------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 64.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 775      |\n",
      "|    iterations      | 467      |\n",
      "|    time_elapsed    | 154      |\n",
      "|    total_timesteps | 119552   |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 64.9      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 775       |\n",
      "|    iterations           | 468       |\n",
      "|    time_elapsed         | 154       |\n",
      "|    total_timesteps      | 119808    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.1540339 |\n",
      "|    clip_fraction        | 0.16      |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0519   |\n",
      "|    explained_variance   | 0.696     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.1      |\n",
      "|    n_updates            | 8873      |\n",
      "|    policy_gradient_loss | -0.0639   |\n",
      "|    value_loss           | 0.0653    |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=120000, episode_reward=9.39 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 230        |\n",
      "|    mean_reward          | 9.39       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 120000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.54593635 |\n",
      "|    clip_fraction        | 0.13       |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0658    |\n",
      "|    explained_variance   | 0.475      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0577    |\n",
      "|    n_updates            | 8892       |\n",
      "|    policy_gradient_loss | -0.0555    |\n",
      "|    value_loss           | 0.145      |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 65       |\n",
      "| time/              |          |\n",
      "|    fps             | 775      |\n",
      "|    iterations      | 469      |\n",
      "|    time_elapsed    | 154      |\n",
      "|    total_timesteps | 120064   |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 65         |\n",
      "| time/                   |            |\n",
      "|    fps                  | 776        |\n",
      "|    iterations           | 470        |\n",
      "|    time_elapsed         | 155        |\n",
      "|    total_timesteps      | 120320     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.90017915 |\n",
      "|    clip_fraction        | 0.185      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0837    |\n",
      "|    explained_variance   | 0.708      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0197    |\n",
      "|    n_updates            | 8911       |\n",
      "|    policy_gradient_loss | -0.0479    |\n",
      "|    value_loss           | 0.4        |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=120500, episode_reward=21.73 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "--------------------------------------\n",
      "| eval/                   |          |\n",
      "|    mean_ep_length       | 230      |\n",
      "|    mean_reward          | 21.7     |\n",
      "| time/                   |          |\n",
      "|    total_timesteps      | 120500   |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 1.045609 |\n",
      "|    clip_fraction        | 0.159    |\n",
      "|    clip_range           | 0.4      |\n",
      "|    entropy_loss         | -0.0703  |\n",
      "|    explained_variance   | -1.18    |\n",
      "|    learning_rate        | 0.000389 |\n",
      "|    loss                 | -0.0514  |\n",
      "|    n_updates            | 8930     |\n",
      "|    policy_gradient_loss | -0.0504  |\n",
      "|    value_loss           | 0.0688   |\n",
      "--------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 65       |\n",
      "| time/              |          |\n",
      "|    fps             | 775      |\n",
      "|    iterations      | 471      |\n",
      "|    time_elapsed    | 155      |\n",
      "|    total_timesteps | 120576   |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 65        |\n",
      "| time/                   |           |\n",
      "|    fps                  | 775       |\n",
      "|    iterations           | 472       |\n",
      "|    time_elapsed         | 155       |\n",
      "|    total_timesteps      | 120832    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.9750331 |\n",
      "|    clip_fraction        | 0.231     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0922   |\n",
      "|    explained_variance   | 0.543     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.127    |\n",
      "|    n_updates            | 8949      |\n",
      "|    policy_gradient_loss | -0.0808   |\n",
      "|    value_loss           | 0.123     |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=121000, episode_reward=40.70 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 230        |\n",
      "|    mean_reward          | 40.7       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 121000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.81405365 |\n",
      "|    clip_fraction        | 0.18       |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0719    |\n",
      "|    explained_variance   | 0.668      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0557    |\n",
      "|    n_updates            | 8968       |\n",
      "|    policy_gradient_loss | -0.0641    |\n",
      "|    value_loss           | 0.132      |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 65.2     |\n",
      "| time/              |          |\n",
      "|    fps             | 775      |\n",
      "|    iterations      | 473      |\n",
      "|    time_elapsed    | 156      |\n",
      "|    total_timesteps | 121088   |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 65.2      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 775       |\n",
      "|    iterations           | 474       |\n",
      "|    time_elapsed         | 156       |\n",
      "|    total_timesteps      | 121344    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.7070141 |\n",
      "|    clip_fraction        | 0.202     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.123    |\n",
      "|    explained_variance   | -0.145    |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0394   |\n",
      "|    n_updates            | 8987      |\n",
      "|    policy_gradient_loss | -0.0606   |\n",
      "|    value_loss           | 0.119     |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=121500, episode_reward=53.16 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 230        |\n",
      "|    mean_reward          | 53.2       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 121500     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.59144646 |\n",
      "|    clip_fraction        | 0.138      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0424    |\n",
      "|    explained_variance   | 0.548      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0371    |\n",
      "|    n_updates            | 9006       |\n",
      "|    policy_gradient_loss | -0.0434    |\n",
      "|    value_loss           | 0.0575     |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 65.2     |\n",
      "| time/              |          |\n",
      "|    fps             | 775      |\n",
      "|    iterations      | 475      |\n",
      "|    time_elapsed    | 156      |\n",
      "|    total_timesteps | 121600   |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 65.5       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 775        |\n",
      "|    iterations           | 476        |\n",
      "|    time_elapsed         | 157        |\n",
      "|    total_timesteps      | 121856     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.68520117 |\n",
      "|    clip_fraction        | 0.161      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0687    |\n",
      "|    explained_variance   | 0.597      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0192    |\n",
      "|    n_updates            | 9025       |\n",
      "|    policy_gradient_loss | -0.0722    |\n",
      "|    value_loss           | 0.0987     |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=122000, episode_reward=42.82 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 42.8      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 122000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.8118539 |\n",
      "|    clip_fraction        | 0.171     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.101    |\n",
      "|    explained_variance   | 0.438     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | 0.0146    |\n",
      "|    n_updates            | 9044      |\n",
      "|    policy_gradient_loss | -0.0323   |\n",
      "|    value_loss           | 0.467     |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 65.5     |\n",
      "| time/              |          |\n",
      "|    fps             | 775      |\n",
      "|    iterations      | 477      |\n",
      "|    time_elapsed    | 157      |\n",
      "|    total_timesteps | 122112   |\n",
      "---------------------------------\n",
      "--------------------------------------\n",
      "| rollout/                |          |\n",
      "|    ep_len_mean          | 889      |\n",
      "|    ep_rew_mean          | 65.5     |\n",
      "| time/                   |          |\n",
      "|    fps                  | 776      |\n",
      "|    iterations           | 478      |\n",
      "|    time_elapsed         | 157      |\n",
      "|    total_timesteps      | 122368   |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 1.223618 |\n",
      "|    clip_fraction        | 0.201    |\n",
      "|    clip_range           | 0.4      |\n",
      "|    entropy_loss         | -0.0671  |\n",
      "|    explained_variance   | 0.33     |\n",
      "|    learning_rate        | 0.000389 |\n",
      "|    loss                 | -0.121   |\n",
      "|    n_updates            | 9063     |\n",
      "|    policy_gradient_loss | -0.0739  |\n",
      "|    value_loss           | 0.0454   |\n",
      "--------------------------------------\n",
      "Eval num_timesteps=122500, episode_reward=4.62 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 4.62      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 122500    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.0246391 |\n",
      "|    clip_fraction        | 0.162     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0687   |\n",
      "|    explained_variance   | 0.398     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0612   |\n",
      "|    n_updates            | 9082      |\n",
      "|    policy_gradient_loss | -0.0696   |\n",
      "|    value_loss           | 0.146     |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 65.5     |\n",
      "| time/              |          |\n",
      "|    fps             | 776      |\n",
      "|    iterations      | 479      |\n",
      "|    time_elapsed    | 157      |\n",
      "|    total_timesteps | 122624   |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 65.4      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 776       |\n",
      "|    iterations           | 480       |\n",
      "|    time_elapsed         | 158       |\n",
      "|    total_timesteps      | 122880    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.7263372 |\n",
      "|    clip_fraction        | 0.146     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0688   |\n",
      "|    explained_variance   | 0.69      |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0972   |\n",
      "|    n_updates            | 9101      |\n",
      "|    policy_gradient_loss | -0.0515   |\n",
      "|    value_loss           | 0.172     |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=123000, episode_reward=7.02 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 7.02      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 123000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.6129705 |\n",
      "|    clip_fraction        | 0.181     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0968   |\n",
      "|    explained_variance   | 0.126     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0939   |\n",
      "|    n_updates            | 9120      |\n",
      "|    policy_gradient_loss | -0.0454   |\n",
      "|    value_loss           | 0.109     |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 65.4     |\n",
      "| time/              |          |\n",
      "|    fps             | 776      |\n",
      "|    iterations      | 481      |\n",
      "|    time_elapsed    | 158      |\n",
      "|    total_timesteps | 123136   |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 65.4      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 777       |\n",
      "|    iterations           | 482       |\n",
      "|    time_elapsed         | 158       |\n",
      "|    total_timesteps      | 123392    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.4792447 |\n",
      "|    clip_fraction        | 0.136     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0558   |\n",
      "|    explained_variance   | 0.647     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0553   |\n",
      "|    n_updates            | 9139      |\n",
      "|    policy_gradient_loss | -0.062    |\n",
      "|    value_loss           | 0.0657    |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=123500, episode_reward=-1.00 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | -0.996    |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 123500    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.1034076 |\n",
      "|    clip_fraction        | 0.167     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0767   |\n",
      "|    explained_variance   | 0.342     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0646   |\n",
      "|    n_updates            | 9158      |\n",
      "|    policy_gradient_loss | -0.0722   |\n",
      "|    value_loss           | 0.111     |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 65.5     |\n",
      "| time/              |          |\n",
      "|    fps             | 776      |\n",
      "|    iterations      | 483      |\n",
      "|    time_elapsed    | 159      |\n",
      "|    total_timesteps | 123648   |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 65.5      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 777       |\n",
      "|    iterations           | 484       |\n",
      "|    time_elapsed         | 159       |\n",
      "|    total_timesteps      | 123904    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.7394921 |\n",
      "|    clip_fraction        | 0.182     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0929   |\n",
      "|    explained_variance   | 0.442     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | 0.0954    |\n",
      "|    n_updates            | 9177      |\n",
      "|    policy_gradient_loss | -0.0491   |\n",
      "|    value_loss           | 0.248     |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=124000, episode_reward=21.95 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 21.9      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 124000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.5360852 |\n",
      "|    clip_fraction        | 0.168     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0814   |\n",
      "|    explained_variance   | 0.15      |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0379   |\n",
      "|    n_updates            | 9196      |\n",
      "|    policy_gradient_loss | -0.0527   |\n",
      "|    value_loss           | 0.0802    |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 65.5     |\n",
      "| time/              |          |\n",
      "|    fps             | 777      |\n",
      "|    iterations      | 485      |\n",
      "|    time_elapsed    | 159      |\n",
      "|    total_timesteps | 124160   |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 65.5      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 777       |\n",
      "|    iterations           | 486       |\n",
      "|    time_elapsed         | 159       |\n",
      "|    total_timesteps      | 124416    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.6361954 |\n",
      "|    clip_fraction        | 0.149     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0601   |\n",
      "|    explained_variance   | 0.349     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0811   |\n",
      "|    n_updates            | 9215      |\n",
      "|    policy_gradient_loss | -0.065    |\n",
      "|    value_loss           | 0.185     |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=124500, episode_reward=46.44 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 230        |\n",
      "|    mean_reward          | 46.4       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 124500     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.78500736 |\n",
      "|    clip_fraction        | 0.188      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.065     |\n",
      "|    explained_variance   | 0.602      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0467    |\n",
      "|    n_updates            | 9234       |\n",
      "|    policy_gradient_loss | -0.0629    |\n",
      "|    value_loss           | 0.252      |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 65.5     |\n",
      "| time/              |          |\n",
      "|    fps             | 777      |\n",
      "|    iterations      | 487      |\n",
      "|    time_elapsed    | 160      |\n",
      "|    total_timesteps | 124672   |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 65.5       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 778        |\n",
      "|    iterations           | 488        |\n",
      "|    time_elapsed         | 160        |\n",
      "|    total_timesteps      | 124928     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.63009894 |\n",
      "|    clip_fraction        | 0.196      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.105     |\n",
      "|    explained_variance   | 0.415      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | 0.294      |\n",
      "|    n_updates            | 9253       |\n",
      "|    policy_gradient_loss | -0.0543    |\n",
      "|    value_loss           | 0.269      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=125000, episode_reward=41.06 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 41.1      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 125000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.1456935 |\n",
      "|    clip_fraction        | 0.15      |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.057    |\n",
      "|    explained_variance   | 0.772     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0554   |\n",
      "|    n_updates            | 9272      |\n",
      "|    policy_gradient_loss | -0.0581   |\n",
      "|    value_loss           | 0.0712    |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 65.5     |\n",
      "| time/              |          |\n",
      "|    fps             | 777      |\n",
      "|    iterations      | 489      |\n",
      "|    time_elapsed    | 161      |\n",
      "|    total_timesteps | 125184   |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 65.8       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 777        |\n",
      "|    iterations           | 490        |\n",
      "|    time_elapsed         | 161        |\n",
      "|    total_timesteps      | 125440     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.71241164 |\n",
      "|    clip_fraction        | 0.135      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0639    |\n",
      "|    explained_variance   | 0.434      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0682    |\n",
      "|    n_updates            | 9291       |\n",
      "|    policy_gradient_loss | -0.0515    |\n",
      "|    value_loss           | 0.207      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=125500, episode_reward=11.19 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 11.2      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 125500    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.0838777 |\n",
      "|    clip_fraction        | 0.205     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.122    |\n",
      "|    explained_variance   | 0.644     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | 0.0487    |\n",
      "|    n_updates            | 9310      |\n",
      "|    policy_gradient_loss | -0.0532   |\n",
      "|    value_loss           | 0.403     |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 65.8     |\n",
      "| time/              |          |\n",
      "|    fps             | 777      |\n",
      "|    iterations      | 491      |\n",
      "|    time_elapsed    | 161      |\n",
      "|    total_timesteps | 125696   |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 65.8       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 777        |\n",
      "|    iterations           | 492        |\n",
      "|    time_elapsed         | 161        |\n",
      "|    total_timesteps      | 125952     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.57378817 |\n",
      "|    clip_fraction        | 0.167      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.067     |\n",
      "|    explained_variance   | 0.234      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.118     |\n",
      "|    n_updates            | 9329       |\n",
      "|    policy_gradient_loss | -0.0676    |\n",
      "|    value_loss           | 0.0834     |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=126000, episode_reward=8.38 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 8.38      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 126000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.6270608 |\n",
      "|    clip_fraction        | 0.145     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0663   |\n",
      "|    explained_variance   | 0.654     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.00111  |\n",
      "|    n_updates            | 9348      |\n",
      "|    policy_gradient_loss | -0.055    |\n",
      "|    value_loss           | 0.202     |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 65.8     |\n",
      "| time/              |          |\n",
      "|    fps             | 777      |\n",
      "|    iterations      | 493      |\n",
      "|    time_elapsed    | 162      |\n",
      "|    total_timesteps | 126208   |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 65.8       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 777        |\n",
      "|    iterations           | 494        |\n",
      "|    time_elapsed         | 162        |\n",
      "|    total_timesteps      | 126464     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.40092394 |\n",
      "|    clip_fraction        | 0.103      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0541    |\n",
      "|    explained_variance   | 0.512      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0307    |\n",
      "|    n_updates            | 9367       |\n",
      "|    policy_gradient_loss | -0.0373    |\n",
      "|    value_loss           | 0.163      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=126500, episode_reward=0.53 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 0.527     |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 126500    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.7714306 |\n",
      "|    clip_fraction        | 0.227     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.131    |\n",
      "|    explained_variance   | 0.319     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0604   |\n",
      "|    n_updates            | 9386      |\n",
      "|    policy_gradient_loss | -0.0774   |\n",
      "|    value_loss           | 0.223     |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 65.8     |\n",
      "| time/              |          |\n",
      "|    fps             | 776      |\n",
      "|    iterations      | 495      |\n",
      "|    time_elapsed    | 163      |\n",
      "|    total_timesteps | 126720   |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 65.8      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 777       |\n",
      "|    iterations           | 496       |\n",
      "|    time_elapsed         | 163       |\n",
      "|    total_timesteps      | 126976    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.4165277 |\n",
      "|    clip_fraction        | 0.154     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0738   |\n",
      "|    explained_variance   | 0.643     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0224   |\n",
      "|    n_updates            | 9405      |\n",
      "|    policy_gradient_loss | -0.0482   |\n",
      "|    value_loss           | 0.175     |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=127000, episode_reward=13.06 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 13.1      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 127000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.5972821 |\n",
      "|    clip_fraction        | 0.134     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0594   |\n",
      "|    explained_variance   | 0.783     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0905   |\n",
      "|    n_updates            | 9424      |\n",
      "|    policy_gradient_loss | -0.0592   |\n",
      "|    value_loss           | 0.0591    |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 65.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 776      |\n",
      "|    iterations      | 497      |\n",
      "|    time_elapsed    | 163      |\n",
      "|    total_timesteps | 127232   |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 65.9      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 777       |\n",
      "|    iterations           | 498       |\n",
      "|    time_elapsed         | 164       |\n",
      "|    total_timesteps      | 127488    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.0671718 |\n",
      "|    clip_fraction        | 0.269     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.104    |\n",
      "|    explained_variance   | 0.787     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.104    |\n",
      "|    n_updates            | 9443      |\n",
      "|    policy_gradient_loss | -0.0635   |\n",
      "|    value_loss           | 0.134     |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=127500, episode_reward=2.78 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 230        |\n",
      "|    mean_reward          | 2.78       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 127500     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.75102544 |\n",
      "|    clip_fraction        | 0.157      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0557    |\n",
      "|    explained_variance   | 0.466      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.106     |\n",
      "|    n_updates            | 9462       |\n",
      "|    policy_gradient_loss | -0.0561    |\n",
      "|    value_loss           | 0.0564     |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 65.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 776      |\n",
      "|    iterations      | 499      |\n",
      "|    time_elapsed    | 164      |\n",
      "|    total_timesteps | 127744   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=128000, episode_reward=5.65 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 5.65      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 128000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.7455113 |\n",
      "|    clip_fraction        | 0.156     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0733   |\n",
      "|    explained_variance   | 0.756     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0978   |\n",
      "|    n_updates            | 9481      |\n",
      "|    policy_gradient_loss | -0.0298   |\n",
      "|    value_loss           | 0.114     |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 65.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 776      |\n",
      "|    iterations      | 500      |\n",
      "|    time_elapsed    | 164      |\n",
      "|    total_timesteps | 128000   |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 66.1       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 776        |\n",
      "|    iterations           | 501        |\n",
      "|    time_elapsed         | 165        |\n",
      "|    total_timesteps      | 128256     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.91316795 |\n",
      "|    clip_fraction        | 0.184      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0893    |\n",
      "|    explained_variance   | 0.495      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.104     |\n",
      "|    n_updates            | 9500       |\n",
      "|    policy_gradient_loss | -0.0559    |\n",
      "|    value_loss           | 0.376      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=128500, episode_reward=15.89 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 15.9      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 128500    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.7281964 |\n",
      "|    clip_fraction        | 0.218     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.113    |\n",
      "|    explained_variance   | 0.076     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.126    |\n",
      "|    n_updates            | 9519      |\n",
      "|    policy_gradient_loss | -0.0688   |\n",
      "|    value_loss           | 0.113     |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 66.1     |\n",
      "| time/              |          |\n",
      "|    fps             | 775      |\n",
      "|    iterations      | 502      |\n",
      "|    time_elapsed    | 165      |\n",
      "|    total_timesteps | 128512   |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 66.1      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 776       |\n",
      "|    iterations           | 503       |\n",
      "|    time_elapsed         | 165       |\n",
      "|    total_timesteps      | 128768    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.6729176 |\n",
      "|    clip_fraction        | 0.155     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0788   |\n",
      "|    explained_variance   | 0.496     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0959   |\n",
      "|    n_updates            | 9538      |\n",
      "|    policy_gradient_loss | -0.0426   |\n",
      "|    value_loss           | 0.141     |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=129000, episode_reward=6.93 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 6.93      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 129000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.9776337 |\n",
      "|    clip_fraction        | 0.178     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0679   |\n",
      "|    explained_variance   | 0.558     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0676   |\n",
      "|    n_updates            | 9557      |\n",
      "|    policy_gradient_loss | -0.0845   |\n",
      "|    value_loss           | 0.101     |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 66.2     |\n",
      "| time/              |          |\n",
      "|    fps             | 776      |\n",
      "|    iterations      | 504      |\n",
      "|    time_elapsed    | 166      |\n",
      "|    total_timesteps | 129024   |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 66.2      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 776       |\n",
      "|    iterations           | 505       |\n",
      "|    time_elapsed         | 166       |\n",
      "|    total_timesteps      | 129280    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.1250901 |\n",
      "|    clip_fraction        | 0.264     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.118    |\n",
      "|    explained_variance   | 0.821     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0905   |\n",
      "|    n_updates            | 9576      |\n",
      "|    policy_gradient_loss | -0.083    |\n",
      "|    value_loss           | 0.153     |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=129500, episode_reward=58.94 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 58.9      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 129500    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.0985485 |\n",
      "|    clip_fraction        | 0.204     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.074    |\n",
      "|    explained_variance   | 0.0493    |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0813   |\n",
      "|    n_updates            | 9595      |\n",
      "|    policy_gradient_loss | -0.0836   |\n",
      "|    value_loss           | 0.0646    |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 66.2     |\n",
      "| time/              |          |\n",
      "|    fps             | 775      |\n",
      "|    iterations      | 506      |\n",
      "|    time_elapsed    | 166      |\n",
      "|    total_timesteps | 129536   |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 66.2      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 776       |\n",
      "|    iterations           | 507       |\n",
      "|    time_elapsed         | 167       |\n",
      "|    total_timesteps      | 129792    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.9471811 |\n",
      "|    clip_fraction        | 0.161     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0704   |\n",
      "|    explained_variance   | 0.662     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0836   |\n",
      "|    n_updates            | 9614      |\n",
      "|    policy_gradient_loss | -0.071    |\n",
      "|    value_loss           | 0.167     |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=130000, episode_reward=49.88 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 230        |\n",
      "|    mean_reward          | 49.9       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 130000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.43400645 |\n",
      "|    clip_fraction        | 0.131      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0756    |\n",
      "|    explained_variance   | 0.382      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.059     |\n",
      "|    n_updates            | 9633       |\n",
      "|    policy_gradient_loss | -0.0452    |\n",
      "|    value_loss           | 0.455      |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 66.3     |\n",
      "| time/              |          |\n",
      "|    fps             | 776      |\n",
      "|    iterations      | 508      |\n",
      "|    time_elapsed    | 167      |\n",
      "|    total_timesteps | 130048   |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 66.3      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 776       |\n",
      "|    iterations           | 509       |\n",
      "|    time_elapsed         | 167       |\n",
      "|    total_timesteps      | 130304    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.9394913 |\n",
      "|    clip_fraction        | 0.212     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.107    |\n",
      "|    explained_variance   | 0.218     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.151    |\n",
      "|    n_updates            | 9652      |\n",
      "|    policy_gradient_loss | -0.086    |\n",
      "|    value_loss           | 0.0873    |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=130500, episode_reward=28.79 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 28.8      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 130500    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.9695011 |\n",
      "|    clip_fraction        | 0.177     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0726   |\n",
      "|    explained_variance   | 0.316     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0826   |\n",
      "|    n_updates            | 9671      |\n",
      "|    policy_gradient_loss | -0.0746   |\n",
      "|    value_loss           | 0.195     |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 66.3     |\n",
      "| time/              |          |\n",
      "|    fps             | 776      |\n",
      "|    iterations      | 510      |\n",
      "|    time_elapsed    | 168      |\n",
      "|    total_timesteps | 130560   |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 66.3      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 777       |\n",
      "|    iterations           | 511       |\n",
      "|    time_elapsed         | 168       |\n",
      "|    total_timesteps      | 130816    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 2.0731874 |\n",
      "|    clip_fraction        | 0.22      |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0639   |\n",
      "|    explained_variance   | 0.641     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.144    |\n",
      "|    n_updates            | 9690      |\n",
      "|    policy_gradient_loss | -0.0766   |\n",
      "|    value_loss           | 0.115     |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=131000, episode_reward=6.99 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 6.99      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 131000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.2732702 |\n",
      "|    clip_fraction        | 0.174     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0698   |\n",
      "|    explained_variance   | 0.794     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0636   |\n",
      "|    n_updates            | 9709      |\n",
      "|    policy_gradient_loss | -0.0655   |\n",
      "|    value_loss           | 0.182     |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 66.3     |\n",
      "| time/              |          |\n",
      "|    fps             | 776      |\n",
      "|    iterations      | 512      |\n",
      "|    time_elapsed    | 168      |\n",
      "|    total_timesteps | 131072   |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 66.3      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 777       |\n",
      "|    iterations           | 513       |\n",
      "|    time_elapsed         | 168       |\n",
      "|    total_timesteps      | 131328    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.0192306 |\n",
      "|    clip_fraction        | 0.169     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0769   |\n",
      "|    explained_variance   | 0.596     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0346   |\n",
      "|    n_updates            | 9728      |\n",
      "|    policy_gradient_loss | -0.0509   |\n",
      "|    value_loss           | 0.0634    |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=131500, episode_reward=68.88 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 68.9      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 131500    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.1888621 |\n",
      "|    clip_fraction        | 0.199     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0902   |\n",
      "|    explained_variance   | 0.349     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.112    |\n",
      "|    n_updates            | 9747      |\n",
      "|    policy_gradient_loss | -0.0696   |\n",
      "|    value_loss           | 0.14      |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 66.3     |\n",
      "| time/              |          |\n",
      "|    fps             | 777      |\n",
      "|    iterations      | 514      |\n",
      "|    time_elapsed    | 169      |\n",
      "|    total_timesteps | 131584   |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 66.3      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 777       |\n",
      "|    iterations           | 515       |\n",
      "|    time_elapsed         | 169       |\n",
      "|    total_timesteps      | 131840    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.8807647 |\n",
      "|    clip_fraction        | 0.161     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0837   |\n",
      "|    explained_variance   | 0.516     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | 0.115     |\n",
      "|    n_updates            | 9766      |\n",
      "|    policy_gradient_loss | -0.0395   |\n",
      "|    value_loss           | 0.35      |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=132000, episode_reward=-8.52 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | -8.52     |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 132000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.0107875 |\n",
      "|    clip_fraction        | 0.244     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.11     |\n",
      "|    explained_variance   | 0.0626    |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0901   |\n",
      "|    n_updates            | 9785      |\n",
      "|    policy_gradient_loss | -0.08     |\n",
      "|    value_loss           | 0.0715    |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 66.3     |\n",
      "| time/              |          |\n",
      "|    fps             | 777      |\n",
      "|    iterations      | 516      |\n",
      "|    time_elapsed    | 169      |\n",
      "|    total_timesteps | 132096   |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 66.3       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 777        |\n",
      "|    iterations           | 517        |\n",
      "|    time_elapsed         | 170        |\n",
      "|    total_timesteps      | 132352     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.83526766 |\n",
      "|    clip_fraction        | 0.159      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0773    |\n",
      "|    explained_variance   | 0.441      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0872    |\n",
      "|    n_updates            | 9804       |\n",
      "|    policy_gradient_loss | -0.0687    |\n",
      "|    value_loss           | 0.112      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=132500, episode_reward=-14.28 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | -14.3     |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 132500    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.0337783 |\n",
      "|    clip_fraction        | 0.165     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0691   |\n",
      "|    explained_variance   | 0.718     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0741   |\n",
      "|    n_updates            | 9823      |\n",
      "|    policy_gradient_loss | -0.0635   |\n",
      "|    value_loss           | 0.0502    |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 66.4     |\n",
      "| time/              |          |\n",
      "|    fps             | 777      |\n",
      "|    iterations      | 518      |\n",
      "|    time_elapsed    | 170      |\n",
      "|    total_timesteps | 132608   |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 66.4       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 777        |\n",
      "|    iterations           | 519        |\n",
      "|    time_elapsed         | 170        |\n",
      "|    total_timesteps      | 132864     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.56381226 |\n",
      "|    clip_fraction        | 0.147      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0972    |\n",
      "|    explained_variance   | 0.827      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.000378  |\n",
      "|    n_updates            | 9842       |\n",
      "|    policy_gradient_loss | -0.0421    |\n",
      "|    value_loss           | 0.167      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=133000, episode_reward=60.02 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 60        |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 133000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.9637268 |\n",
      "|    clip_fraction        | 0.198     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0606   |\n",
      "|    explained_variance   | 0.59      |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.102    |\n",
      "|    n_updates            | 9861      |\n",
      "|    policy_gradient_loss | -0.0723   |\n",
      "|    value_loss           | 0.05      |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 66.4     |\n",
      "| time/              |          |\n",
      "|    fps             | 777      |\n",
      "|    iterations      | 520      |\n",
      "|    time_elapsed    | 171      |\n",
      "|    total_timesteps | 133120   |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 66.4      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 778       |\n",
      "|    iterations           | 521       |\n",
      "|    time_elapsed         | 171       |\n",
      "|    total_timesteps      | 133376    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.8200307 |\n",
      "|    clip_fraction        | 0.181     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0875   |\n",
      "|    explained_variance   | 0.656     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.104    |\n",
      "|    n_updates            | 9880      |\n",
      "|    policy_gradient_loss | -0.073    |\n",
      "|    value_loss           | 0.124     |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=133500, episode_reward=18.21 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 18.2      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 133500    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.5521633 |\n",
      "|    clip_fraction        | 0.146     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0768   |\n",
      "|    explained_variance   | 0.585     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.00529  |\n",
      "|    n_updates            | 9899      |\n",
      "|    policy_gradient_loss | -0.046    |\n",
      "|    value_loss           | 0.268     |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 66.4     |\n",
      "| time/              |          |\n",
      "|    fps             | 778      |\n",
      "|    iterations      | 522      |\n",
      "|    time_elapsed    | 171      |\n",
      "|    total_timesteps | 133632   |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 66.4      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 778       |\n",
      "|    iterations           | 523       |\n",
      "|    time_elapsed         | 172       |\n",
      "|    total_timesteps      | 133888    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.2501693 |\n",
      "|    clip_fraction        | 0.195     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0733   |\n",
      "|    explained_variance   | -0.366    |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0634   |\n",
      "|    n_updates            | 9918      |\n",
      "|    policy_gradient_loss | -0.065    |\n",
      "|    value_loss           | 0.0574    |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=134000, episode_reward=59.30 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 59.3      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 134000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.6499952 |\n",
      "|    clip_fraction        | 0.163     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0732   |\n",
      "|    explained_variance   | 0.64      |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0758   |\n",
      "|    n_updates            | 9937      |\n",
      "|    policy_gradient_loss | -0.0633   |\n",
      "|    value_loss           | 0.125     |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 66.4     |\n",
      "| time/              |          |\n",
      "|    fps             | 777      |\n",
      "|    iterations      | 524      |\n",
      "|    time_elapsed    | 172      |\n",
      "|    total_timesteps | 134144   |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 66.6      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 777       |\n",
      "|    iterations           | 525       |\n",
      "|    time_elapsed         | 172       |\n",
      "|    total_timesteps      | 134400    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.0930632 |\n",
      "|    clip_fraction        | 0.178     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0731   |\n",
      "|    explained_variance   | 0.638     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0724   |\n",
      "|    n_updates            | 9956      |\n",
      "|    policy_gradient_loss | -0.0591   |\n",
      "|    value_loss           | 0.0878    |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=134500, episode_reward=106.00 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 106       |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 134500    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.5392554 |\n",
      "|    clip_fraction        | 0.128     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0932   |\n",
      "|    explained_variance   | 0.271     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0112   |\n",
      "|    n_updates            | 9975      |\n",
      "|    policy_gradient_loss | -0.0273   |\n",
      "|    value_loss           | 1.99      |\n",
      "---------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 66.6     |\n",
      "| time/              |          |\n",
      "|    fps             | 777      |\n",
      "|    iterations      | 526      |\n",
      "|    time_elapsed    | 173      |\n",
      "|    total_timesteps | 134656   |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 66.6      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 777       |\n",
      "|    iterations           | 527       |\n",
      "|    time_elapsed         | 173       |\n",
      "|    total_timesteps      | 134912    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.9713219 |\n",
      "|    clip_fraction        | 0.148     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0656   |\n",
      "|    explained_variance   | 0.56      |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0583   |\n",
      "|    n_updates            | 9994      |\n",
      "|    policy_gradient_loss | -0.0518   |\n",
      "|    value_loss           | 0.0835    |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=135000, episode_reward=64.89 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 64.9      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 135000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.7839659 |\n",
      "|    clip_fraction        | 0.171     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0898   |\n",
      "|    explained_variance   | 0.443     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0712   |\n",
      "|    n_updates            | 10013     |\n",
      "|    policy_gradient_loss | -0.0617   |\n",
      "|    value_loss           | 0.146     |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 66.8     |\n",
      "| time/              |          |\n",
      "|    fps             | 777      |\n",
      "|    iterations      | 528      |\n",
      "|    time_elapsed    | 173      |\n",
      "|    total_timesteps | 135168   |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 66.8       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 777        |\n",
      "|    iterations           | 529        |\n",
      "|    time_elapsed         | 174        |\n",
      "|    total_timesteps      | 135424     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.27198753 |\n",
      "|    clip_fraction        | 0.137      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0748    |\n",
      "|    explained_variance   | 0.536      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0353    |\n",
      "|    n_updates            | 10032      |\n",
      "|    policy_gradient_loss | -0.0376    |\n",
      "|    value_loss           | 0.301      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=135500, episode_reward=88.01 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 230        |\n",
      "|    mean_reward          | 88         |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 135500     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.87984455 |\n",
      "|    clip_fraction        | 0.148      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0674    |\n",
      "|    explained_variance   | 0.384      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.112     |\n",
      "|    n_updates            | 10051      |\n",
      "|    policy_gradient_loss | -0.0555    |\n",
      "|    value_loss           | 0.041      |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 66.8     |\n",
      "| time/              |          |\n",
      "|    fps             | 776      |\n",
      "|    iterations      | 530      |\n",
      "|    time_elapsed    | 174      |\n",
      "|    total_timesteps | 135680   |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 66.8      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 776       |\n",
      "|    iterations           | 531       |\n",
      "|    time_elapsed         | 175       |\n",
      "|    total_timesteps      | 135936    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.4404011 |\n",
      "|    clip_fraction        | 0.155     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0677   |\n",
      "|    explained_variance   | 0.677     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.098    |\n",
      "|    n_updates            | 10070     |\n",
      "|    policy_gradient_loss | -0.0663   |\n",
      "|    value_loss           | 0.158     |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=136000, episode_reward=77.04 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "--------------------------------------\n",
      "| eval/                   |          |\n",
      "|    mean_ep_length       | 230      |\n",
      "|    mean_reward          | 77       |\n",
      "| time/                   |          |\n",
      "|    total_timesteps      | 136000   |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 1.40442  |\n",
      "|    clip_fraction        | 0.183    |\n",
      "|    clip_range           | 0.4      |\n",
      "|    entropy_loss         | -0.0521  |\n",
      "|    explained_variance   | 0.801    |\n",
      "|    learning_rate        | 0.000389 |\n",
      "|    loss                 | -0.064   |\n",
      "|    n_updates            | 10089    |\n",
      "|    policy_gradient_loss | -0.0659  |\n",
      "|    value_loss           | 0.103    |\n",
      "--------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 67       |\n",
      "| time/              |          |\n",
      "|    fps             | 775      |\n",
      "|    iterations      | 532      |\n",
      "|    time_elapsed    | 175      |\n",
      "|    total_timesteps | 136192   |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 67         |\n",
      "| time/                   |            |\n",
      "|    fps                  | 776        |\n",
      "|    iterations           | 533        |\n",
      "|    time_elapsed         | 175        |\n",
      "|    total_timesteps      | 136448     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.99983346 |\n",
      "|    clip_fraction        | 0.213      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.105     |\n",
      "|    explained_variance   | 0.521      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0668    |\n",
      "|    n_updates            | 10108      |\n",
      "|    policy_gradient_loss | -0.08      |\n",
      "|    value_loss           | 0.173      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=136500, episode_reward=19.77 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "--------------------------------------\n",
      "| eval/                   |          |\n",
      "|    mean_ep_length       | 230      |\n",
      "|    mean_reward          | 19.8     |\n",
      "| time/                   |          |\n",
      "|    total_timesteps      | 136500   |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 1.157953 |\n",
      "|    clip_fraction        | 0.153    |\n",
      "|    clip_range           | 0.4      |\n",
      "|    entropy_loss         | -0.0544  |\n",
      "|    explained_variance   | 0.82     |\n",
      "|    learning_rate        | 0.000389 |\n",
      "|    loss                 | -0.084   |\n",
      "|    n_updates            | 10127    |\n",
      "|    policy_gradient_loss | -0.0719  |\n",
      "|    value_loss           | 0.066    |\n",
      "--------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 67       |\n",
      "| time/              |          |\n",
      "|    fps             | 776      |\n",
      "|    iterations      | 534      |\n",
      "|    time_elapsed    | 176      |\n",
      "|    total_timesteps | 136704   |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 66.9      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 776       |\n",
      "|    iterations           | 535       |\n",
      "|    time_elapsed         | 176       |\n",
      "|    total_timesteps      | 136960    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.1434852 |\n",
      "|    clip_fraction        | 0.168     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0698   |\n",
      "|    explained_variance   | 0.542     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0981   |\n",
      "|    n_updates            | 10146     |\n",
      "|    policy_gradient_loss | -0.0697   |\n",
      "|    value_loss           | 0.135     |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=137000, episode_reward=32.82 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 32.8      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 137000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.5599604 |\n",
      "|    clip_fraction        | 0.139     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0776   |\n",
      "|    explained_variance   | 0.784     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0727   |\n",
      "|    n_updates            | 10165     |\n",
      "|    policy_gradient_loss | -0.0512   |\n",
      "|    value_loss           | 0.179     |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 66.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 775      |\n",
      "|    iterations      | 536      |\n",
      "|    time_elapsed    | 176      |\n",
      "|    total_timesteps | 137216   |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 66.9      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 776       |\n",
      "|    iterations           | 537       |\n",
      "|    time_elapsed         | 177       |\n",
      "|    total_timesteps      | 137472    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.8816775 |\n",
      "|    clip_fraction        | 0.198     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.071    |\n",
      "|    explained_variance   | 0.265     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0126   |\n",
      "|    n_updates            | 10184     |\n",
      "|    policy_gradient_loss | -0.0628   |\n",
      "|    value_loss           | 0.0329    |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=137500, episode_reward=42.23 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 230        |\n",
      "|    mean_reward          | 42.2       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 137500     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.43316334 |\n",
      "|    clip_fraction        | 0.109      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0558    |\n",
      "|    explained_variance   | 0.684      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0163    |\n",
      "|    n_updates            | 10203      |\n",
      "|    policy_gradient_loss | -0.0485    |\n",
      "|    value_loss           | 0.151      |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 66.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 776      |\n",
      "|    iterations      | 538      |\n",
      "|    time_elapsed    | 177      |\n",
      "|    total_timesteps | 137728   |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 67.1       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 776        |\n",
      "|    iterations           | 539        |\n",
      "|    time_elapsed         | 177        |\n",
      "|    total_timesteps      | 137984     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.46975574 |\n",
      "|    clip_fraction        | 0.142      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0682    |\n",
      "|    explained_variance   | 0.801      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0772    |\n",
      "|    n_updates            | 10222      |\n",
      "|    policy_gradient_loss | -0.0632    |\n",
      "|    value_loss           | 0.0771     |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=138000, episode_reward=43.31 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 43.3      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 138000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 2.5404468 |\n",
      "|    clip_fraction        | 0.277     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0973   |\n",
      "|    explained_variance   | 0.693     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0623   |\n",
      "|    n_updates            | 10241     |\n",
      "|    policy_gradient_loss | -0.0495   |\n",
      "|    value_loss           | 0.0674    |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 67.1     |\n",
      "| time/              |          |\n",
      "|    fps             | 776      |\n",
      "|    iterations      | 540      |\n",
      "|    time_elapsed    | 178      |\n",
      "|    total_timesteps | 138240   |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 67.1       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 776        |\n",
      "|    iterations           | 541        |\n",
      "|    time_elapsed         | 178        |\n",
      "|    total_timesteps      | 138496     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.74068546 |\n",
      "|    clip_fraction        | 0.126      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0438    |\n",
      "|    explained_variance   | 0.788      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0439    |\n",
      "|    n_updates            | 10260      |\n",
      "|    policy_gradient_loss | -0.0469    |\n",
      "|    value_loss           | 0.121      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=138500, episode_reward=39.88 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 39.9      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 138500    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.2766167 |\n",
      "|    clip_fraction        | 0.196     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.06     |\n",
      "|    explained_variance   | 0.322     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.106    |\n",
      "|    n_updates            | 10279     |\n",
      "|    policy_gradient_loss | -0.0802   |\n",
      "|    value_loss           | 0.106     |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 66.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 776      |\n",
      "|    iterations      | 542      |\n",
      "|    time_elapsed    | 178      |\n",
      "|    total_timesteps | 138752   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=139000, episode_reward=33.12 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 230        |\n",
      "|    mean_reward          | 33.1       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 139000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.45972744 |\n",
      "|    clip_fraction        | 0.132      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0975    |\n",
      "|    explained_variance   | 0.381      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | 0.000722   |\n",
      "|    n_updates            | 10298      |\n",
      "|    policy_gradient_loss | -0.0413    |\n",
      "|    value_loss           | 0.164      |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 66.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 776      |\n",
      "|    iterations      | 543      |\n",
      "|    time_elapsed    | 179      |\n",
      "|    total_timesteps | 139008   |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 66.9      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 776       |\n",
      "|    iterations           | 544       |\n",
      "|    time_elapsed         | 179       |\n",
      "|    total_timesteps      | 139264    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.2299037 |\n",
      "|    clip_fraction        | 0.23      |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0891   |\n",
      "|    explained_variance   | 0.209     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.123    |\n",
      "|    n_updates            | 10317     |\n",
      "|    policy_gradient_loss | -0.0696   |\n",
      "|    value_loss           | 0.0616    |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=139500, episode_reward=35.19 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "--------------------------------------\n",
      "| eval/                   |          |\n",
      "|    mean_ep_length       | 230      |\n",
      "|    mean_reward          | 35.2     |\n",
      "| time/                   |          |\n",
      "|    total_timesteps      | 139500   |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 1.834872 |\n",
      "|    clip_fraction        | 0.168    |\n",
      "|    clip_range           | 0.4      |\n",
      "|    entropy_loss         | -0.0564  |\n",
      "|    explained_variance   | 0.722    |\n",
      "|    learning_rate        | 0.000389 |\n",
      "|    loss                 | -0.149   |\n",
      "|    n_updates            | 10336    |\n",
      "|    policy_gradient_loss | -0.0737  |\n",
      "|    value_loss           | 0.105    |\n",
      "--------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 66.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 776      |\n",
      "|    iterations      | 545      |\n",
      "|    time_elapsed    | 179      |\n",
      "|    total_timesteps | 139520   |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 67.2      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 777       |\n",
      "|    iterations           | 546       |\n",
      "|    time_elapsed         | 179       |\n",
      "|    total_timesteps      | 139776    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.1225001 |\n",
      "|    clip_fraction        | 0.164     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0777   |\n",
      "|    explained_variance   | 0.617     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0965   |\n",
      "|    n_updates            | 10355     |\n",
      "|    policy_gradient_loss | -0.0499   |\n",
      "|    value_loss           | 0.182     |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=140000, episode_reward=-3.09 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | -3.09     |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 140000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.5896638 |\n",
      "|    clip_fraction        | 0.142     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0822   |\n",
      "|    explained_variance   | 0.229     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | 0.0353    |\n",
      "|    n_updates            | 10374     |\n",
      "|    policy_gradient_loss | -0.0302   |\n",
      "|    value_loss           | 0.113     |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 67.2     |\n",
      "| time/              |          |\n",
      "|    fps             | 777      |\n",
      "|    iterations      | 547      |\n",
      "|    time_elapsed    | 180      |\n",
      "|    total_timesteps | 140032   |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 67.2       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 777        |\n",
      "|    iterations           | 548        |\n",
      "|    time_elapsed         | 180        |\n",
      "|    total_timesteps      | 140288     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.80630136 |\n",
      "|    clip_fraction        | 0.168      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0632    |\n",
      "|    explained_variance   | 0.894      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.119     |\n",
      "|    n_updates            | 10393      |\n",
      "|    policy_gradient_loss | -0.0627    |\n",
      "|    value_loss           | 0.0506     |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=140500, episode_reward=16.95 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 17        |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 140500    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.4831955 |\n",
      "|    clip_fraction        | 0.194     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0756   |\n",
      "|    explained_variance   | 0.559     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0496   |\n",
      "|    n_updates            | 10412     |\n",
      "|    policy_gradient_loss | -0.0565   |\n",
      "|    value_loss           | 0.0723    |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 67.3     |\n",
      "| time/              |          |\n",
      "|    fps             | 777      |\n",
      "|    iterations      | 549      |\n",
      "|    time_elapsed    | 180      |\n",
      "|    total_timesteps | 140544   |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 67.3       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 777        |\n",
      "|    iterations           | 550        |\n",
      "|    time_elapsed         | 181        |\n",
      "|    total_timesteps      | 140800     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.59976274 |\n",
      "|    clip_fraction        | 0.135      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0837    |\n",
      "|    explained_variance   | 0.595      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.095     |\n",
      "|    n_updates            | 10431      |\n",
      "|    policy_gradient_loss | -0.0656    |\n",
      "|    value_loss           | 0.0934     |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=141000, episode_reward=17.20 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 17.2      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 141000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.7989255 |\n",
      "|    clip_fraction        | 0.208     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0745   |\n",
      "|    explained_variance   | 0.302     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.149    |\n",
      "|    n_updates            | 10450     |\n",
      "|    policy_gradient_loss | -0.0772   |\n",
      "|    value_loss           | 0.0389    |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 67.3     |\n",
      "| time/              |          |\n",
      "|    fps             | 776      |\n",
      "|    iterations      | 551      |\n",
      "|    time_elapsed    | 181      |\n",
      "|    total_timesteps | 141056   |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 67.3      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 777       |\n",
      "|    iterations           | 552       |\n",
      "|    time_elapsed         | 181       |\n",
      "|    total_timesteps      | 141312    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.3099227 |\n",
      "|    clip_fraction        | 0.164     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0586   |\n",
      "|    explained_variance   | 0.798     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.116    |\n",
      "|    n_updates            | 10469     |\n",
      "|    policy_gradient_loss | -0.0581   |\n",
      "|    value_loss           | 0.104     |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=141500, episode_reward=15.51 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 15.5      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 141500    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.4728848 |\n",
      "|    clip_fraction        | 0.141     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0784   |\n",
      "|    explained_variance   | 0.527     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | 0.0471    |\n",
      "|    n_updates            | 10488     |\n",
      "|    policy_gradient_loss | -0.0577   |\n",
      "|    value_loss           | 0.324     |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 67.2     |\n",
      "| time/              |          |\n",
      "|    fps             | 776      |\n",
      "|    iterations      | 553      |\n",
      "|    time_elapsed    | 182      |\n",
      "|    total_timesteps | 141568   |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 67.2      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 777       |\n",
      "|    iterations           | 554       |\n",
      "|    time_elapsed         | 182       |\n",
      "|    total_timesteps      | 141824    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.8280108 |\n",
      "|    clip_fraction        | 0.259     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.115    |\n",
      "|    explained_variance   | -0.379    |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.1      |\n",
      "|    n_updates            | 10507     |\n",
      "|    policy_gradient_loss | -0.0879   |\n",
      "|    value_loss           | 0.0752    |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=142000, episode_reward=11.82 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 11.8      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 142000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.5363108 |\n",
      "|    clip_fraction        | 0.0979    |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0429   |\n",
      "|    explained_variance   | 0.784     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0227   |\n",
      "|    n_updates            | 10526     |\n",
      "|    policy_gradient_loss | -0.0408   |\n",
      "|    value_loss           | 0.162     |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 67.2     |\n",
      "| time/              |          |\n",
      "|    fps             | 776      |\n",
      "|    iterations      | 555      |\n",
      "|    time_elapsed    | 182      |\n",
      "|    total_timesteps | 142080   |\n",
      "---------------------------------\n",
      "--------------------------------------\n",
      "| rollout/                |          |\n",
      "|    ep_len_mean          | 889      |\n",
      "|    ep_rew_mean          | 67.5     |\n",
      "| time/                   |          |\n",
      "|    fps                  | 776      |\n",
      "|    iterations           | 556      |\n",
      "|    time_elapsed         | 183      |\n",
      "|    total_timesteps      | 142336   |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 0.797526 |\n",
      "|    clip_fraction        | 0.127    |\n",
      "|    clip_range           | 0.4      |\n",
      "|    entropy_loss         | -0.0664  |\n",
      "|    explained_variance   | 0.652    |\n",
      "|    learning_rate        | 0.000389 |\n",
      "|    loss                 | -0.0679  |\n",
      "|    n_updates            | 10545    |\n",
      "|    policy_gradient_loss | -0.0562  |\n",
      "|    value_loss           | 0.109    |\n",
      "--------------------------------------\n",
      "Eval num_timesteps=142500, episode_reward=17.99 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "--------------------------------------\n",
      "| eval/                   |          |\n",
      "|    mean_ep_length       | 230      |\n",
      "|    mean_reward          | 18       |\n",
      "| time/                   |          |\n",
      "|    total_timesteps      | 142500   |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 0.902431 |\n",
      "|    clip_fraction        | 0.231    |\n",
      "|    clip_range           | 0.4      |\n",
      "|    entropy_loss         | -0.0932  |\n",
      "|    explained_variance   | 0.827    |\n",
      "|    learning_rate        | 0.000389 |\n",
      "|    loss                 | -0.095   |\n",
      "|    n_updates            | 10564    |\n",
      "|    policy_gradient_loss | -0.0695  |\n",
      "|    value_loss           | 0.201    |\n",
      "--------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 67.5     |\n",
      "| time/              |          |\n",
      "|    fps             | 776      |\n",
      "|    iterations      | 557      |\n",
      "|    time_elapsed    | 183      |\n",
      "|    total_timesteps | 142592   |\n",
      "---------------------------------\n",
      "--------------------------------------\n",
      "| rollout/                |          |\n",
      "|    ep_len_mean          | 889      |\n",
      "|    ep_rew_mean          | 67.5     |\n",
      "| time/                   |          |\n",
      "|    fps                  | 776      |\n",
      "|    iterations           | 558      |\n",
      "|    time_elapsed         | 183      |\n",
      "|    total_timesteps      | 142848   |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 1.125792 |\n",
      "|    clip_fraction        | 0.197    |\n",
      "|    clip_range           | 0.4      |\n",
      "|    entropy_loss         | -0.0605  |\n",
      "|    explained_variance   | 0.323    |\n",
      "|    learning_rate        | 0.000389 |\n",
      "|    loss                 | -0.0777  |\n",
      "|    n_updates            | 10583    |\n",
      "|    policy_gradient_loss | -0.0655  |\n",
      "|    value_loss           | 0.0536   |\n",
      "--------------------------------------\n",
      "Eval num_timesteps=143000, episode_reward=25.34 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 230        |\n",
      "|    mean_reward          | 25.3       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 143000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.80671966 |\n",
      "|    clip_fraction        | 0.119      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0504    |\n",
      "|    explained_variance   | 0.721      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.1       |\n",
      "|    n_updates            | 10602      |\n",
      "|    policy_gradient_loss | -0.0698    |\n",
      "|    value_loss           | 0.159      |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 67.5     |\n",
      "| time/              |          |\n",
      "|    fps             | 776      |\n",
      "|    iterations      | 559      |\n",
      "|    time_elapsed    | 184      |\n",
      "|    total_timesteps | 143104   |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 67.6      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 776       |\n",
      "|    iterations           | 560       |\n",
      "|    time_elapsed         | 184       |\n",
      "|    total_timesteps      | 143360    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.5956963 |\n",
      "|    clip_fraction        | 0.195     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0928   |\n",
      "|    explained_variance   | 0.78      |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0526   |\n",
      "|    n_updates            | 10621     |\n",
      "|    policy_gradient_loss | -0.0515   |\n",
      "|    value_loss           | 0.169     |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=143500, episode_reward=39.62 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 39.6      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 143500    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.8527603 |\n",
      "|    clip_fraction        | 0.178     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0877   |\n",
      "|    explained_variance   | 0.406     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0593   |\n",
      "|    n_updates            | 10640     |\n",
      "|    policy_gradient_loss | -0.0515   |\n",
      "|    value_loss           | 0.0402    |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 67.6     |\n",
      "| time/              |          |\n",
      "|    fps             | 776      |\n",
      "|    iterations      | 561      |\n",
      "|    time_elapsed    | 185      |\n",
      "|    total_timesteps | 143616   |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 67.6       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 776        |\n",
      "|    iterations           | 562        |\n",
      "|    time_elapsed         | 185        |\n",
      "|    total_timesteps      | 143872     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.38961756 |\n",
      "|    clip_fraction        | 0.133      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0601    |\n",
      "|    explained_variance   | 0.893      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0924    |\n",
      "|    n_updates            | 10659      |\n",
      "|    policy_gradient_loss | -0.0588    |\n",
      "|    value_loss           | 0.058      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=144000, episode_reward=34.74 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "--------------------------------------\n",
      "| eval/                   |          |\n",
      "|    mean_ep_length       | 230      |\n",
      "|    mean_reward          | 34.7     |\n",
      "| time/                   |          |\n",
      "|    total_timesteps      | 144000   |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 0.567317 |\n",
      "|    clip_fraction        | 0.136    |\n",
      "|    clip_range           | 0.4      |\n",
      "|    entropy_loss         | -0.0604  |\n",
      "|    explained_variance   | 0.554    |\n",
      "|    learning_rate        | 0.000389 |\n",
      "|    loss                 | -0.0528  |\n",
      "|    n_updates            | 10678    |\n",
      "|    policy_gradient_loss | -0.0579  |\n",
      "|    value_loss           | 0.104    |\n",
      "--------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 67.8     |\n",
      "| time/              |          |\n",
      "|    fps             | 775      |\n",
      "|    iterations      | 563      |\n",
      "|    time_elapsed    | 185      |\n",
      "|    total_timesteps | 144128   |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 67.8       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 776        |\n",
      "|    iterations           | 564        |\n",
      "|    time_elapsed         | 185        |\n",
      "|    total_timesteps      | 144384     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.43993855 |\n",
      "|    clip_fraction        | 0.164      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0731    |\n",
      "|    explained_variance   | 0.873      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | 0.000585   |\n",
      "|    n_updates            | 10697      |\n",
      "|    policy_gradient_loss | -0.0423    |\n",
      "|    value_loss           | 0.148      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=144500, episode_reward=29.94 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "--------------------------------------\n",
      "| eval/                   |          |\n",
      "|    mean_ep_length       | 230      |\n",
      "|    mean_reward          | 29.9     |\n",
      "| time/                   |          |\n",
      "|    total_timesteps      | 144500   |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 1.176415 |\n",
      "|    clip_fraction        | 0.203    |\n",
      "|    clip_range           | 0.4      |\n",
      "|    entropy_loss         | -0.0694  |\n",
      "|    explained_variance   | 0.358    |\n",
      "|    learning_rate        | 0.000389 |\n",
      "|    loss                 | -0.0748  |\n",
      "|    n_updates            | 10716    |\n",
      "|    policy_gradient_loss | -0.0807  |\n",
      "|    value_loss           | 0.0355   |\n",
      "--------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 67.8     |\n",
      "| time/              |          |\n",
      "|    fps             | 775      |\n",
      "|    iterations      | 565      |\n",
      "|    time_elapsed    | 186      |\n",
      "|    total_timesteps | 144640   |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 67.8      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 776       |\n",
      "|    iterations           | 566       |\n",
      "|    time_elapsed         | 186       |\n",
      "|    total_timesteps      | 144896    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.8202852 |\n",
      "|    clip_fraction        | 0.149     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0734   |\n",
      "|    explained_variance   | 0.65      |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0909   |\n",
      "|    n_updates            | 10735     |\n",
      "|    policy_gradient_loss | -0.0571   |\n",
      "|    value_loss           | 0.211     |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=145000, episode_reward=34.39 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 230        |\n",
      "|    mean_reward          | 34.4       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 145000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.64054966 |\n",
      "|    clip_fraction        | 0.169      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0882    |\n",
      "|    explained_variance   | 0.817      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.108     |\n",
      "|    n_updates            | 10754      |\n",
      "|    policy_gradient_loss | -0.0618    |\n",
      "|    value_loss           | 0.182      |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 67.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 776      |\n",
      "|    iterations      | 567      |\n",
      "|    time_elapsed    | 186      |\n",
      "|    total_timesteps | 145152   |\n",
      "---------------------------------\n",
      "--------------------------------------\n",
      "| rollout/                |          |\n",
      "|    ep_len_mean          | 889      |\n",
      "|    ep_rew_mean          | 67.9     |\n",
      "| time/                   |          |\n",
      "|    fps                  | 776      |\n",
      "|    iterations           | 568      |\n",
      "|    time_elapsed         | 187      |\n",
      "|    total_timesteps      | 145408   |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 1.106996 |\n",
      "|    clip_fraction        | 0.221    |\n",
      "|    clip_range           | 0.4      |\n",
      "|    entropy_loss         | -0.0769  |\n",
      "|    explained_variance   | 0.138    |\n",
      "|    learning_rate        | 0.000389 |\n",
      "|    loss                 | -0.075   |\n",
      "|    n_updates            | 10773    |\n",
      "|    policy_gradient_loss | -0.0823  |\n",
      "|    value_loss           | 0.0672   |\n",
      "--------------------------------------\n",
      "Eval num_timesteps=145500, episode_reward=43.43 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 43.4      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 145500    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.9758411 |\n",
      "|    clip_fraction        | 0.145     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0586   |\n",
      "|    explained_variance   | 0.795     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.105    |\n",
      "|    n_updates            | 10792     |\n",
      "|    policy_gradient_loss | -0.056    |\n",
      "|    value_loss           | 0.132     |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 67.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 776      |\n",
      "|    iterations      | 569      |\n",
      "|    time_elapsed    | 187      |\n",
      "|    total_timesteps | 145664   |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 67.9       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 776        |\n",
      "|    iterations           | 570        |\n",
      "|    time_elapsed         | 187        |\n",
      "|    total_timesteps      | 145920     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.66693956 |\n",
      "|    clip_fraction        | 0.169      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0771    |\n",
      "|    explained_variance   | 0.649      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0677    |\n",
      "|    n_updates            | 10811      |\n",
      "|    policy_gradient_loss | -0.0593    |\n",
      "|    value_loss           | 0.0718     |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=146000, episode_reward=36.15 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 230        |\n",
      "|    mean_reward          | 36.2       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 146000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.51547563 |\n",
      "|    clip_fraction        | 0.167      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.12      |\n",
      "|    explained_variance   | 0.216      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0685    |\n",
      "|    n_updates            | 10830      |\n",
      "|    policy_gradient_loss | -0.076     |\n",
      "|    value_loss           | 0.375      |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 67.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 776      |\n",
      "|    iterations      | 571      |\n",
      "|    time_elapsed    | 188      |\n",
      "|    total_timesteps | 146176   |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 67.9      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 776       |\n",
      "|    iterations           | 572       |\n",
      "|    time_elapsed         | 188       |\n",
      "|    total_timesteps      | 146432    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.0128522 |\n",
      "|    clip_fraction        | 0.152     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.047    |\n",
      "|    explained_variance   | 0.209     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0746   |\n",
      "|    n_updates            | 10849     |\n",
      "|    policy_gradient_loss | -0.0673   |\n",
      "|    value_loss           | 0.045     |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=146500, episode_reward=37.79 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 37.8      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 146500    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.3627642 |\n",
      "|    clip_fraction        | 0.186     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0572   |\n",
      "|    explained_variance   | 0.722     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.085    |\n",
      "|    n_updates            | 10868     |\n",
      "|    policy_gradient_loss | -0.0759   |\n",
      "|    value_loss           | 0.12      |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 67.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 776      |\n",
      "|    iterations      | 573      |\n",
      "|    time_elapsed    | 188      |\n",
      "|    total_timesteps | 146688   |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 67.9       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 777        |\n",
      "|    iterations           | 574        |\n",
      "|    time_elapsed         | 189        |\n",
      "|    total_timesteps      | 146944     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.46362895 |\n",
      "|    clip_fraction        | 0.162      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0995    |\n",
      "|    explained_variance   | 0.0243     |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0399    |\n",
      "|    n_updates            | 10887      |\n",
      "|    policy_gradient_loss | -0.0572    |\n",
      "|    value_loss           | 0.219      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=147000, episode_reward=57.79 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 57.8      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 147000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.7770685 |\n",
      "|    clip_fraction        | 0.216     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0694   |\n",
      "|    explained_variance   | 0.537     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0992   |\n",
      "|    n_updates            | 10906     |\n",
      "|    policy_gradient_loss | -0.0768   |\n",
      "|    value_loss           | 0.0429    |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 67.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 776      |\n",
      "|    iterations      | 575      |\n",
      "|    time_elapsed    | 189      |\n",
      "|    total_timesteps | 147200   |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 67.9      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 777       |\n",
      "|    iterations           | 576       |\n",
      "|    time_elapsed         | 189       |\n",
      "|    total_timesteps      | 147456    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.8007458 |\n",
      "|    clip_fraction        | 0.133     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0528   |\n",
      "|    explained_variance   | 0.473     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0147   |\n",
      "|    n_updates            | 10925     |\n",
      "|    policy_gradient_loss | -0.0608   |\n",
      "|    value_loss           | 0.0968    |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=147500, episode_reward=43.59 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 43.6      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 147500    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.6493947 |\n",
      "|    clip_fraction        | 0.148     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.058    |\n",
      "|    explained_variance   | 0.506     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0587   |\n",
      "|    n_updates            | 10944     |\n",
      "|    policy_gradient_loss | -0.0616   |\n",
      "|    value_loss           | 0.071     |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 67.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 776      |\n",
      "|    iterations      | 577      |\n",
      "|    time_elapsed    | 190      |\n",
      "|    total_timesteps | 147712   |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 67.9       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 777        |\n",
      "|    iterations           | 578        |\n",
      "|    time_elapsed         | 190        |\n",
      "|    total_timesteps      | 147968     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.95269984 |\n",
      "|    clip_fraction        | 0.21       |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.105     |\n",
      "|    explained_variance   | -0.0538    |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.085     |\n",
      "|    n_updates            | 10963      |\n",
      "|    policy_gradient_loss | -0.0863    |\n",
      "|    value_loss           | 0.267      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=148000, episode_reward=53.83 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 53.8      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 148000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.6834547 |\n",
      "|    clip_fraction        | 0.173     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0636   |\n",
      "|    explained_variance   | 0.43      |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.104    |\n",
      "|    n_updates            | 10982     |\n",
      "|    policy_gradient_loss | -0.072    |\n",
      "|    value_loss           | 0.0388    |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 67.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 776      |\n",
      "|    iterations      | 579      |\n",
      "|    time_elapsed    | 190      |\n",
      "|    total_timesteps | 148224   |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 67.9      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 777       |\n",
      "|    iterations           | 580       |\n",
      "|    time_elapsed         | 191       |\n",
      "|    total_timesteps      | 148480    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.9332555 |\n",
      "|    clip_fraction        | 0.146     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0544   |\n",
      "|    explained_variance   | 0.107     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0763   |\n",
      "|    n_updates            | 11001     |\n",
      "|    policy_gradient_loss | -0.0582   |\n",
      "|    value_loss           | 0.14      |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=148500, episode_reward=35.29 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 230        |\n",
      "|    mean_reward          | 35.3       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 148500     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.65569365 |\n",
      "|    clip_fraction        | 0.164      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.11      |\n",
      "|    explained_variance   | 0.245      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.02      |\n",
      "|    n_updates            | 11020      |\n",
      "|    policy_gradient_loss | -0.0431    |\n",
      "|    value_loss           | 0.364      |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 67.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 776      |\n",
      "|    iterations      | 581      |\n",
      "|    time_elapsed    | 191      |\n",
      "|    total_timesteps | 148736   |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 67.9       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 777        |\n",
      "|    iterations           | 582        |\n",
      "|    time_elapsed         | 191        |\n",
      "|    total_timesteps      | 148992     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.69516164 |\n",
      "|    clip_fraction        | 0.142      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.06      |\n",
      "|    explained_variance   | 0.169      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.112     |\n",
      "|    n_updates            | 11039      |\n",
      "|    policy_gradient_loss | -0.0574    |\n",
      "|    value_loss           | 0.0515     |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=149000, episode_reward=7.32 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 230        |\n",
      "|    mean_reward          | 7.32       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 149000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.79849696 |\n",
      "|    clip_fraction        | 0.136      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0608    |\n",
      "|    explained_variance   | 0.574      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0635    |\n",
      "|    n_updates            | 11058      |\n",
      "|    policy_gradient_loss | -0.0422    |\n",
      "|    value_loss           | 0.0659     |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 67.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 776      |\n",
      "|    iterations      | 583      |\n",
      "|    time_elapsed    | 192      |\n",
      "|    total_timesteps | 149248   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=149500, episode_reward=7.08 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 7.08      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 149500    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.0611327 |\n",
      "|    clip_fraction        | 0.169     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0659   |\n",
      "|    explained_variance   | 0.329     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0988   |\n",
      "|    n_updates            | 11077     |\n",
      "|    policy_gradient_loss | -0.059    |\n",
      "|    value_loss           | 0.0805    |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 68       |\n",
      "| time/              |          |\n",
      "|    fps             | 776      |\n",
      "|    iterations      | 584      |\n",
      "|    time_elapsed    | 192      |\n",
      "|    total_timesteps | 149504   |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 68         |\n",
      "| time/                   |            |\n",
      "|    fps                  | 776        |\n",
      "|    iterations           | 585        |\n",
      "|    time_elapsed         | 192        |\n",
      "|    total_timesteps      | 149760     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.87414366 |\n",
      "|    clip_fraction        | 0.157      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0804    |\n",
      "|    explained_variance   | -0.298     |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0191    |\n",
      "|    n_updates            | 11096      |\n",
      "|    policy_gradient_loss | -0.0558    |\n",
      "|    value_loss           | 0.12       |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=150000, episode_reward=23.45 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 23.4      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 150000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.8798473 |\n",
      "|    clip_fraction        | 0.16      |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0529   |\n",
      "|    explained_variance   | 0.57      |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.162    |\n",
      "|    n_updates            | 11115     |\n",
      "|    policy_gradient_loss | -0.0578   |\n",
      "|    value_loss           | 0.0365    |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 68       |\n",
      "| time/              |          |\n",
      "|    fps             | 775      |\n",
      "|    iterations      | 586      |\n",
      "|    time_elapsed    | 193      |\n",
      "|    total_timesteps | 150016   |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 68.2      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 775       |\n",
      "|    iterations           | 587       |\n",
      "|    time_elapsed         | 193       |\n",
      "|    total_timesteps      | 150272    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.3915522 |\n",
      "|    clip_fraction        | 0.185     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0585   |\n",
      "|    explained_variance   | 0.459     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.14     |\n",
      "|    n_updates            | 11134     |\n",
      "|    policy_gradient_loss | -0.0834   |\n",
      "|    value_loss           | 0.102     |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=150500, episode_reward=6.86 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 6.86      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 150500    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.5011058 |\n",
      "|    clip_fraction        | 0.164     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0836   |\n",
      "|    explained_variance   | 0.0526    |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | 0.0471    |\n",
      "|    n_updates            | 11153     |\n",
      "|    policy_gradient_loss | -0.0441   |\n",
      "|    value_loss           | 0.489     |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 68.2     |\n",
      "| time/              |          |\n",
      "|    fps             | 775      |\n",
      "|    iterations      | 588      |\n",
      "|    time_elapsed    | 194      |\n",
      "|    total_timesteps | 150528   |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 68.2      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 775       |\n",
      "|    iterations           | 589       |\n",
      "|    time_elapsed         | 194       |\n",
      "|    total_timesteps      | 150784    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 2.0265021 |\n",
      "|    clip_fraction        | 0.22      |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0622   |\n",
      "|    explained_variance   | 0.301     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.109    |\n",
      "|    n_updates            | 11172     |\n",
      "|    policy_gradient_loss | -0.0787   |\n",
      "|    value_loss           | 0.0637    |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=151000, episode_reward=11.03 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 230        |\n",
      "|    mean_reward          | 11         |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 151000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.61186934 |\n",
      "|    clip_fraction        | 0.125      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.062     |\n",
      "|    explained_variance   | 0.0338     |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.00739   |\n",
      "|    n_updates            | 11191      |\n",
      "|    policy_gradient_loss | -0.0583    |\n",
      "|    value_loss           | 0.117      |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 68.2     |\n",
      "| time/              |          |\n",
      "|    fps             | 775      |\n",
      "|    iterations      | 590      |\n",
      "|    time_elapsed    | 194      |\n",
      "|    total_timesteps | 151040   |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 68.4      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 775       |\n",
      "|    iterations           | 591       |\n",
      "|    time_elapsed         | 195       |\n",
      "|    total_timesteps      | 151296    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.1763686 |\n",
      "|    clip_fraction        | 0.163     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0727   |\n",
      "|    explained_variance   | 0.518     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.108    |\n",
      "|    n_updates            | 11210     |\n",
      "|    policy_gradient_loss | -0.0658   |\n",
      "|    value_loss           | 0.0858    |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=151500, episode_reward=23.29 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 23.3      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 151500    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.0755792 |\n",
      "|    clip_fraction        | 0.207     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0734   |\n",
      "|    explained_variance   | 0.52      |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0801   |\n",
      "|    n_updates            | 11229     |\n",
      "|    policy_gradient_loss | -0.0547   |\n",
      "|    value_loss           | 0.396     |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 68.4     |\n",
      "| time/              |          |\n",
      "|    fps             | 775      |\n",
      "|    iterations      | 592      |\n",
      "|    time_elapsed    | 195      |\n",
      "|    total_timesteps | 151552   |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 68.4      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 775       |\n",
      "|    iterations           | 593       |\n",
      "|    time_elapsed         | 195       |\n",
      "|    total_timesteps      | 151808    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.5525537 |\n",
      "|    clip_fraction        | 0.128     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.06     |\n",
      "|    explained_variance   | 0.685     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0415   |\n",
      "|    n_updates            | 11248     |\n",
      "|    policy_gradient_loss | -0.0488   |\n",
      "|    value_loss           | 0.0472    |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=152000, episode_reward=7.31 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 7.31      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 152000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.1641991 |\n",
      "|    clip_fraction        | 0.146     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0573   |\n",
      "|    explained_variance   | 0.588     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0747   |\n",
      "|    n_updates            | 11267     |\n",
      "|    policy_gradient_loss | -0.0631   |\n",
      "|    value_loss           | 0.0718    |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 68.7     |\n",
      "| time/              |          |\n",
      "|    fps             | 775      |\n",
      "|    iterations      | 594      |\n",
      "|    time_elapsed    | 196      |\n",
      "|    total_timesteps | 152064   |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 68.7       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 775        |\n",
      "|    iterations           | 595        |\n",
      "|    time_elapsed         | 196        |\n",
      "|    total_timesteps      | 152320     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.59579766 |\n",
      "|    clip_fraction        | 0.13       |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0594    |\n",
      "|    explained_variance   | 0.768      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | 0.0837     |\n",
      "|    n_updates            | 11286      |\n",
      "|    policy_gradient_loss | -0.0577    |\n",
      "|    value_loss           | 0.287      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=152500, episode_reward=27.47 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 27.5      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 152500    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.9086414 |\n",
      "|    clip_fraction        | 0.203     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0916   |\n",
      "|    explained_variance   | 0.523     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0482   |\n",
      "|    n_updates            | 11305     |\n",
      "|    policy_gradient_loss | -0.0698   |\n",
      "|    value_loss           | 0.0592    |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 68.7     |\n",
      "| time/              |          |\n",
      "|    fps             | 775      |\n",
      "|    iterations      | 596      |\n",
      "|    time_elapsed    | 196      |\n",
      "|    total_timesteps | 152576   |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 68.7      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 775       |\n",
      "|    iterations           | 597       |\n",
      "|    time_elapsed         | 197       |\n",
      "|    total_timesteps      | 152832    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.8889829 |\n",
      "|    clip_fraction        | 0.162     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0668   |\n",
      "|    explained_variance   | 0.357     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0845   |\n",
      "|    n_updates            | 11324     |\n",
      "|    policy_gradient_loss | -0.0762   |\n",
      "|    value_loss           | 0.146     |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=153000, episode_reward=14.74 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 14.7      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 153000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.3441312 |\n",
      "|    clip_fraction        | 0.178     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0668   |\n",
      "|    explained_variance   | 0.745     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0689   |\n",
      "|    n_updates            | 11343     |\n",
      "|    policy_gradient_loss | -0.0618   |\n",
      "|    value_loss           | 0.231     |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 68.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 775      |\n",
      "|    iterations      | 598      |\n",
      "|    time_elapsed    | 197      |\n",
      "|    total_timesteps | 153088   |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 68.9      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 776       |\n",
      "|    iterations           | 599       |\n",
      "|    time_elapsed         | 197       |\n",
      "|    total_timesteps      | 153344    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 2.1353016 |\n",
      "|    clip_fraction        | 0.301     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0978   |\n",
      "|    explained_variance   | 0.495     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | 0.0054    |\n",
      "|    n_updates            | 11362     |\n",
      "|    policy_gradient_loss | -0.0699   |\n",
      "|    value_loss           | 0.13      |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=153500, episode_reward=22.23 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 230        |\n",
      "|    mean_reward          | 22.2       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 153500     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.95919216 |\n",
      "|    clip_fraction        | 0.151      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.06      |\n",
      "|    explained_variance   | 0.62       |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0798    |\n",
      "|    n_updates            | 11381      |\n",
      "|    policy_gradient_loss | -0.0686    |\n",
      "|    value_loss           | 0.0726     |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 68.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 775      |\n",
      "|    iterations      | 600      |\n",
      "|    time_elapsed    | 197      |\n",
      "|    total_timesteps | 153600   |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 69        |\n",
      "| time/                   |           |\n",
      "|    fps                  | 776       |\n",
      "|    iterations           | 601       |\n",
      "|    time_elapsed         | 198       |\n",
      "|    total_timesteps      | 153856    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.6944073 |\n",
      "|    clip_fraction        | 0.15      |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0664   |\n",
      "|    explained_variance   | 0.716     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0685   |\n",
      "|    n_updates            | 11400     |\n",
      "|    policy_gradient_loss | -0.0549   |\n",
      "|    value_loss           | 0.0989    |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=154000, episode_reward=14.10 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 14.1      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 154000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.3432512 |\n",
      "|    clip_fraction        | 0.221     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0814   |\n",
      "|    explained_variance   | 0.751     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0559   |\n",
      "|    n_updates            | 11419     |\n",
      "|    policy_gradient_loss | -0.0597   |\n",
      "|    value_loss           | 0.141     |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 69       |\n",
      "| time/              |          |\n",
      "|    fps             | 776      |\n",
      "|    iterations      | 602      |\n",
      "|    time_elapsed    | 198      |\n",
      "|    total_timesteps | 154112   |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 69        |\n",
      "| time/                   |           |\n",
      "|    fps                  | 776       |\n",
      "|    iterations           | 603       |\n",
      "|    time_elapsed         | 198       |\n",
      "|    total_timesteps      | 154368    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 2.5307555 |\n",
      "|    clip_fraction        | 0.278     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0696   |\n",
      "|    explained_variance   | 0.371     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.125    |\n",
      "|    n_updates            | 11438     |\n",
      "|    policy_gradient_loss | -0.0922   |\n",
      "|    value_loss           | 0.0466    |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=154500, episode_reward=4.36 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 230        |\n",
      "|    mean_reward          | 4.36       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 154500     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.76973784 |\n",
      "|    clip_fraction        | 0.143      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0515    |\n",
      "|    explained_variance   | 0.765      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0848    |\n",
      "|    n_updates            | 11457      |\n",
      "|    policy_gradient_loss | -0.0546    |\n",
      "|    value_loss           | 0.108      |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 69       |\n",
      "| time/              |          |\n",
      "|    fps             | 776      |\n",
      "|    iterations      | 604      |\n",
      "|    time_elapsed    | 199      |\n",
      "|    total_timesteps | 154624   |\n",
      "---------------------------------\n",
      "--------------------------------------\n",
      "| rollout/                |          |\n",
      "|    ep_len_mean          | 889      |\n",
      "|    ep_rew_mean          | 69       |\n",
      "| time/                   |          |\n",
      "|    fps                  | 776      |\n",
      "|    iterations           | 605      |\n",
      "|    time_elapsed         | 199      |\n",
      "|    total_timesteps      | 154880   |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 1.681614 |\n",
      "|    clip_fraction        | 0.198    |\n",
      "|    clip_range           | 0.4      |\n",
      "|    entropy_loss         | -0.0755  |\n",
      "|    explained_variance   | 0.865    |\n",
      "|    learning_rate        | 0.000389 |\n",
      "|    loss                 | -0.0955  |\n",
      "|    n_updates            | 11476    |\n",
      "|    policy_gradient_loss | -0.0764  |\n",
      "|    value_loss           | 0.0713   |\n",
      "--------------------------------------\n",
      "Eval num_timesteps=155000, episode_reward=22.96 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 23        |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 155000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.0253968 |\n",
      "|    clip_fraction        | 0.205     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.105    |\n",
      "|    explained_variance   | -0.756    |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0458   |\n",
      "|    n_updates            | 11495     |\n",
      "|    policy_gradient_loss | -0.0608   |\n",
      "|    value_loss           | 0.17      |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 69       |\n",
      "| time/              |          |\n",
      "|    fps             | 776      |\n",
      "|    iterations      | 606      |\n",
      "|    time_elapsed    | 199      |\n",
      "|    total_timesteps | 155136   |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 69        |\n",
      "| time/                   |           |\n",
      "|    fps                  | 776       |\n",
      "|    iterations           | 607       |\n",
      "|    time_elapsed         | 200       |\n",
      "|    total_timesteps      | 155392    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.0300614 |\n",
      "|    clip_fraction        | 0.127     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0549   |\n",
      "|    explained_variance   | 0.182     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0409   |\n",
      "|    n_updates            | 11514     |\n",
      "|    policy_gradient_loss | -0.0588   |\n",
      "|    value_loss           | 0.0807    |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=155500, episode_reward=36.79 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 36.8      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 155500    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.7039973 |\n",
      "|    clip_fraction        | 0.137     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0627   |\n",
      "|    explained_variance   | 0.781     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0562   |\n",
      "|    n_updates            | 11533     |\n",
      "|    policy_gradient_loss | -0.054    |\n",
      "|    value_loss           | 0.106     |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 69.1     |\n",
      "| time/              |          |\n",
      "|    fps             | 776      |\n",
      "|    iterations      | 608      |\n",
      "|    time_elapsed    | 200      |\n",
      "|    total_timesteps | 155648   |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 69.1       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 776        |\n",
      "|    iterations           | 609        |\n",
      "|    time_elapsed         | 200        |\n",
      "|    total_timesteps      | 155904     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.77772623 |\n",
      "|    clip_fraction        | 0.168      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0856    |\n",
      "|    explained_variance   | -0.00839   |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.00635   |\n",
      "|    n_updates            | 11552      |\n",
      "|    policy_gradient_loss | -0.0325    |\n",
      "|    value_loss           | 1.91       |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=156000, episode_reward=61.14 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 61.1      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 156000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.4631133 |\n",
      "|    clip_fraction        | 0.218     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0744   |\n",
      "|    explained_variance   | 0.148     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.118    |\n",
      "|    n_updates            | 11571     |\n",
      "|    policy_gradient_loss | -0.0818   |\n",
      "|    value_loss           | 0.0594    |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 69.1     |\n",
      "| time/              |          |\n",
      "|    fps             | 776      |\n",
      "|    iterations      | 610      |\n",
      "|    time_elapsed    | 201      |\n",
      "|    total_timesteps | 156160   |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 69.1      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 777       |\n",
      "|    iterations           | 611       |\n",
      "|    time_elapsed         | 201       |\n",
      "|    total_timesteps      | 156416    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.5372704 |\n",
      "|    clip_fraction        | 0.126     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0636   |\n",
      "|    explained_variance   | 0.334     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.105    |\n",
      "|    n_updates            | 11590     |\n",
      "|    policy_gradient_loss | -0.0659   |\n",
      "|    value_loss           | 0.178     |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=156500, episode_reward=54.74 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 54.7      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 156500    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.1654112 |\n",
      "|    clip_fraction        | 0.209     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0905   |\n",
      "|    explained_variance   | 0.365     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0742   |\n",
      "|    n_updates            | 11609     |\n",
      "|    policy_gradient_loss | -0.0755   |\n",
      "|    value_loss           | 0.256     |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 69.1     |\n",
      "| time/              |          |\n",
      "|    fps             | 776      |\n",
      "|    iterations      | 612      |\n",
      "|    time_elapsed    | 201      |\n",
      "|    total_timesteps | 156672   |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 69.1      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 776       |\n",
      "|    iterations           | 613       |\n",
      "|    time_elapsed         | 201       |\n",
      "|    total_timesteps      | 156928    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.0429889 |\n",
      "|    clip_fraction        | 0.239     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0857   |\n",
      "|    explained_variance   | 0.711     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0483   |\n",
      "|    n_updates            | 11628     |\n",
      "|    policy_gradient_loss | -0.0508   |\n",
      "|    value_loss           | 0.0898    |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=157000, episode_reward=27.16 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 230        |\n",
      "|    mean_reward          | 27.2       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 157000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.50053394 |\n",
      "|    clip_fraction        | 0.146      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0568    |\n",
      "|    explained_variance   | 0.448      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0542    |\n",
      "|    n_updates            | 11647      |\n",
      "|    policy_gradient_loss | -0.0589    |\n",
      "|    value_loss           | 0.0771     |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 69.1     |\n",
      "| time/              |          |\n",
      "|    fps             | 776      |\n",
      "|    iterations      | 614      |\n",
      "|    time_elapsed    | 202      |\n",
      "|    total_timesteps | 157184   |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 69.2       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 777        |\n",
      "|    iterations           | 615        |\n",
      "|    time_elapsed         | 202        |\n",
      "|    total_timesteps      | 157440     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.73680997 |\n",
      "|    clip_fraction        | 0.183      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0879    |\n",
      "|    explained_variance   | 0.0176     |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.104     |\n",
      "|    n_updates            | 11666      |\n",
      "|    policy_gradient_loss | -0.0754    |\n",
      "|    value_loss           | 0.194      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=157500, episode_reward=13.05 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 13        |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 157500    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.6577152 |\n",
      "|    clip_fraction        | 0.161     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.085    |\n",
      "|    explained_variance   | 0.626     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0349   |\n",
      "|    n_updates            | 11685     |\n",
      "|    policy_gradient_loss | -0.0358   |\n",
      "|    value_loss           | 0.205     |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 69.2     |\n",
      "| time/              |          |\n",
      "|    fps             | 776      |\n",
      "|    iterations      | 616      |\n",
      "|    time_elapsed    | 202      |\n",
      "|    total_timesteps | 157696   |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 69.2       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 776        |\n",
      "|    iterations           | 617        |\n",
      "|    time_elapsed         | 203        |\n",
      "|    total_timesteps      | 157952     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.73536193 |\n",
      "|    clip_fraction        | 0.133      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0588    |\n",
      "|    explained_variance   | 0.441      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0917    |\n",
      "|    n_updates            | 11704      |\n",
      "|    policy_gradient_loss | -0.0538    |\n",
      "|    value_loss           | 0.0491     |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=158000, episode_reward=16.32 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 16.3      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 158000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.5534308 |\n",
      "|    clip_fraction        | 0.13      |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0682   |\n",
      "|    explained_variance   | 0.291     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0816   |\n",
      "|    n_updates            | 11723     |\n",
      "|    policy_gradient_loss | -0.0607   |\n",
      "|    value_loss           | 0.178     |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 69.2     |\n",
      "| time/              |          |\n",
      "|    fps             | 776      |\n",
      "|    iterations      | 618      |\n",
      "|    time_elapsed    | 203      |\n",
      "|    total_timesteps | 158208   |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 69.4      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 776       |\n",
      "|    iterations           | 619       |\n",
      "|    time_elapsed         | 203       |\n",
      "|    total_timesteps      | 158464    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.1252997 |\n",
      "|    clip_fraction        | 0.205     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.101    |\n",
      "|    explained_variance   | 0.786     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0822   |\n",
      "|    n_updates            | 11742     |\n",
      "|    policy_gradient_loss | -0.0735   |\n",
      "|    value_loss           | 0.116     |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=158500, episode_reward=37.17 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 37.2      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 158500    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.8406011 |\n",
      "|    clip_fraction        | 0.163     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0741   |\n",
      "|    explained_variance   | 0.444     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.00739  |\n",
      "|    n_updates            | 11761     |\n",
      "|    policy_gradient_loss | -0.045    |\n",
      "|    value_loss           | 0.0554    |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 69.4     |\n",
      "| time/              |          |\n",
      "|    fps             | 776      |\n",
      "|    iterations      | 620      |\n",
      "|    time_elapsed    | 204      |\n",
      "|    total_timesteps | 158720   |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 69.4      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 776       |\n",
      "|    iterations           | 621       |\n",
      "|    time_elapsed         | 204       |\n",
      "|    total_timesteps      | 158976    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.1095449 |\n",
      "|    clip_fraction        | 0.166     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0626   |\n",
      "|    explained_variance   | 0.524     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0829   |\n",
      "|    n_updates            | 11780     |\n",
      "|    policy_gradient_loss | -0.0673   |\n",
      "|    value_loss           | 0.054     |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=159000, episode_reward=119.74 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 120       |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 159000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.9650476 |\n",
      "|    clip_fraction        | 0.195     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0881   |\n",
      "|    explained_variance   | 0.576     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0856   |\n",
      "|    n_updates            | 11799     |\n",
      "|    policy_gradient_loss | -0.065    |\n",
      "|    value_loss           | 0.114     |\n",
      "---------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 69.5     |\n",
      "| time/              |          |\n",
      "|    fps             | 776      |\n",
      "|    iterations      | 622      |\n",
      "|    time_elapsed    | 205      |\n",
      "|    total_timesteps | 159232   |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 69.5      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 776       |\n",
      "|    iterations           | 623       |\n",
      "|    time_elapsed         | 205       |\n",
      "|    total_timesteps      | 159488    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.8503628 |\n",
      "|    clip_fraction        | 0.185     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0865   |\n",
      "|    explained_variance   | 0.793     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0664   |\n",
      "|    n_updates            | 11818     |\n",
      "|    policy_gradient_loss | -0.0632   |\n",
      "|    value_loss           | 0.15      |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=159500, episode_reward=65.19 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 230        |\n",
      "|    mean_reward          | 65.2       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 159500     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.84166706 |\n",
      "|    clip_fraction        | 0.164      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0658    |\n",
      "|    explained_variance   | 0.331      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0611    |\n",
      "|    n_updates            | 11837      |\n",
      "|    policy_gradient_loss | -0.0535    |\n",
      "|    value_loss           | 0.066      |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 69.5     |\n",
      "| time/              |          |\n",
      "|    fps             | 776      |\n",
      "|    iterations      | 624      |\n",
      "|    time_elapsed    | 205      |\n",
      "|    total_timesteps | 159744   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=160000, episode_reward=45.23 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 230        |\n",
      "|    mean_reward          | 45.2       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 160000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.80586815 |\n",
      "|    clip_fraction        | 0.157      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0674    |\n",
      "|    explained_variance   | 0.422      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.109     |\n",
      "|    n_updates            | 11856      |\n",
      "|    policy_gradient_loss | -0.0706    |\n",
      "|    value_loss           | 0.133      |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 69.5     |\n",
      "| time/              |          |\n",
      "|    fps             | 775      |\n",
      "|    iterations      | 625      |\n",
      "|    time_elapsed    | 206      |\n",
      "|    total_timesteps | 160000   |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 69.6       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 776        |\n",
      "|    iterations           | 626        |\n",
      "|    time_elapsed         | 206        |\n",
      "|    total_timesteps      | 160256     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.85514176 |\n",
      "|    clip_fraction        | 0.131      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0777    |\n",
      "|    explained_variance   | 0.429      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.08      |\n",
      "|    n_updates            | 11875      |\n",
      "|    policy_gradient_loss | -0.058     |\n",
      "|    value_loss           | 0.128      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=160500, episode_reward=38.12 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "--------------------------------------\n",
      "| eval/                   |          |\n",
      "|    mean_ep_length       | 230      |\n",
      "|    mean_reward          | 38.1     |\n",
      "| time/                   |          |\n",
      "|    total_timesteps      | 160500   |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 1.34437  |\n",
      "|    clip_fraction        | 0.222    |\n",
      "|    clip_range           | 0.4      |\n",
      "|    entropy_loss         | -0.0992  |\n",
      "|    explained_variance   | 0.685    |\n",
      "|    learning_rate        | 0.000389 |\n",
      "|    loss                 | -0.111   |\n",
      "|    n_updates            | 11894    |\n",
      "|    policy_gradient_loss | -0.0696  |\n",
      "|    value_loss           | 0.0669   |\n",
      "--------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 69.6     |\n",
      "| time/              |          |\n",
      "|    fps             | 776      |\n",
      "|    iterations      | 627      |\n",
      "|    time_elapsed    | 206      |\n",
      "|    total_timesteps | 160512   |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 69.6      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 776       |\n",
      "|    iterations           | 628       |\n",
      "|    time_elapsed         | 207       |\n",
      "|    total_timesteps      | 160768    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.8266113 |\n",
      "|    clip_fraction        | 0.138     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0621   |\n",
      "|    explained_variance   | 0.438     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0594   |\n",
      "|    n_updates            | 11913     |\n",
      "|    policy_gradient_loss | -0.0575   |\n",
      "|    value_loss           | 0.101     |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=161000, episode_reward=20.04 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 20        |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 161000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.4976865 |\n",
      "|    clip_fraction        | 0.119     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0606   |\n",
      "|    explained_variance   | 0.592     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0185   |\n",
      "|    n_updates            | 11932     |\n",
      "|    policy_gradient_loss | -0.0423   |\n",
      "|    value_loss           | 0.0884    |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 69.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 776      |\n",
      "|    iterations      | 629      |\n",
      "|    time_elapsed    | 207      |\n",
      "|    total_timesteps | 161024   |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 69.9       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 776        |\n",
      "|    iterations           | 630        |\n",
      "|    time_elapsed         | 207        |\n",
      "|    total_timesteps      | 161280     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.75913763 |\n",
      "|    clip_fraction        | 0.175      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.123     |\n",
      "|    explained_variance   | 0.598      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.105     |\n",
      "|    n_updates            | 11951      |\n",
      "|    policy_gradient_loss | -0.0549    |\n",
      "|    value_loss           | 0.244      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=161500, episode_reward=57.52 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 57.5      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 161500    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.5067086 |\n",
      "|    clip_fraction        | 0.201     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0614   |\n",
      "|    explained_variance   | 0.371     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.103    |\n",
      "|    n_updates            | 11970     |\n",
      "|    policy_gradient_loss | -0.0745   |\n",
      "|    value_loss           | 0.0369    |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 69.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 776      |\n",
      "|    iterations      | 631      |\n",
      "|    time_elapsed    | 208      |\n",
      "|    total_timesteps | 161536   |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 69.9      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 776       |\n",
      "|    iterations           | 632       |\n",
      "|    time_elapsed         | 208       |\n",
      "|    total_timesteps      | 161792    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.6960486 |\n",
      "|    clip_fraction        | 0.134     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0652   |\n",
      "|    explained_variance   | 0.487     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0371   |\n",
      "|    n_updates            | 11989     |\n",
      "|    policy_gradient_loss | -0.0522   |\n",
      "|    value_loss           | 0.105     |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=162000, episode_reward=26.37 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 26.4      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 162000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.2954433 |\n",
      "|    clip_fraction        | 0.15      |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0915   |\n",
      "|    explained_variance   | 0.579     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | 0.0147    |\n",
      "|    n_updates            | 12008     |\n",
      "|    policy_gradient_loss | -0.0582   |\n",
      "|    value_loss           | 0.235     |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 70.1     |\n",
      "| time/              |          |\n",
      "|    fps             | 776      |\n",
      "|    iterations      | 633      |\n",
      "|    time_elapsed    | 208      |\n",
      "|    total_timesteps | 162048   |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 70.1      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 776       |\n",
      "|    iterations           | 634       |\n",
      "|    time_elapsed         | 208       |\n",
      "|    total_timesteps      | 162304    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.0615795 |\n",
      "|    clip_fraction        | 0.207     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.118    |\n",
      "|    explained_variance   | 0.552     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.112    |\n",
      "|    n_updates            | 12027     |\n",
      "|    policy_gradient_loss | -0.0822   |\n",
      "|    value_loss           | 0.0615    |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=162500, episode_reward=8.12 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 230        |\n",
      "|    mean_reward          | 8.12       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 162500     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.69710577 |\n",
      "|    clip_fraction        | 0.128      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0578    |\n",
      "|    explained_variance   | 0.181      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0475    |\n",
      "|    n_updates            | 12046      |\n",
      "|    policy_gradient_loss | -0.0535    |\n",
      "|    value_loss           | 0.126      |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 70.1     |\n",
      "| time/              |          |\n",
      "|    fps             | 775      |\n",
      "|    iterations      | 635      |\n",
      "|    time_elapsed    | 209      |\n",
      "|    total_timesteps | 162560   |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 70.3      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 776       |\n",
      "|    iterations           | 636       |\n",
      "|    time_elapsed         | 209       |\n",
      "|    total_timesteps      | 162816    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.7934387 |\n",
      "|    clip_fraction        | 0.166     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0505   |\n",
      "|    explained_variance   | 0.474     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0461   |\n",
      "|    n_updates            | 12065     |\n",
      "|    policy_gradient_loss | -0.0847   |\n",
      "|    value_loss           | 0.113     |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=163000, episode_reward=-1.14 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | -1.14     |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 163000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.7612498 |\n",
      "|    clip_fraction        | 0.151     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.104    |\n",
      "|    explained_variance   | 0.739     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0438   |\n",
      "|    n_updates            | 12084     |\n",
      "|    policy_gradient_loss | -0.0311   |\n",
      "|    value_loss           | 0.265     |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 70.3     |\n",
      "| time/              |          |\n",
      "|    fps             | 776      |\n",
      "|    iterations      | 637      |\n",
      "|    time_elapsed    | 210      |\n",
      "|    total_timesteps | 163072   |\n",
      "---------------------------------\n",
      "--------------------------------------\n",
      "| rollout/                |          |\n",
      "|    ep_len_mean          | 889      |\n",
      "|    ep_rew_mean          | 70.3     |\n",
      "| time/                   |          |\n",
      "|    fps                  | 776      |\n",
      "|    iterations           | 638      |\n",
      "|    time_elapsed         | 210      |\n",
      "|    total_timesteps      | 163328   |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 0.927794 |\n",
      "|    clip_fraction        | 0.173    |\n",
      "|    clip_range           | 0.4      |\n",
      "|    entropy_loss         | -0.0682  |\n",
      "|    explained_variance   | 0.375    |\n",
      "|    learning_rate        | 0.000389 |\n",
      "|    loss                 | -0.0437  |\n",
      "|    n_updates            | 12103    |\n",
      "|    policy_gradient_loss | -0.0571  |\n",
      "|    value_loss           | 0.037    |\n",
      "--------------------------------------\n",
      "Eval num_timesteps=163500, episode_reward=8.76 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 230        |\n",
      "|    mean_reward          | 8.76       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 163500     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.40937525 |\n",
      "|    clip_fraction        | 0.116      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0609    |\n",
      "|    explained_variance   | 0.457      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0149    |\n",
      "|    n_updates            | 12122      |\n",
      "|    policy_gradient_loss | -0.05      |\n",
      "|    value_loss           | 0.117      |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 70.4     |\n",
      "| time/              |          |\n",
      "|    fps             | 776      |\n",
      "|    iterations      | 639      |\n",
      "|    time_elapsed    | 210      |\n",
      "|    total_timesteps | 163584   |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 70.4       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 776        |\n",
      "|    iterations           | 640        |\n",
      "|    time_elapsed         | 211        |\n",
      "|    total_timesteps      | 163840     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.45310825 |\n",
      "|    clip_fraction        | 0.124      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0675    |\n",
      "|    explained_variance   | 0.669      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0418    |\n",
      "|    n_updates            | 12141      |\n",
      "|    policy_gradient_loss | -0.0482    |\n",
      "|    value_loss           | 0.271      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=164000, episode_reward=10.36 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 230        |\n",
      "|    mean_reward          | 10.4       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 164000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.70525134 |\n",
      "|    clip_fraction        | 0.253      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.12      |\n",
      "|    explained_variance   | 0.215      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.168     |\n",
      "|    n_updates            | 12160      |\n",
      "|    policy_gradient_loss | -0.0999    |\n",
      "|    value_loss           | 0.0637     |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 70.4     |\n",
      "| time/              |          |\n",
      "|    fps             | 776      |\n",
      "|    iterations      | 641      |\n",
      "|    time_elapsed    | 211      |\n",
      "|    total_timesteps | 164096   |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 70.4      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 776       |\n",
      "|    iterations           | 642       |\n",
      "|    time_elapsed         | 211       |\n",
      "|    total_timesteps      | 164352    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.8067614 |\n",
      "|    clip_fraction        | 0.142     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0627   |\n",
      "|    explained_variance   | 0.537     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.109    |\n",
      "|    n_updates            | 12179     |\n",
      "|    policy_gradient_loss | -0.0601   |\n",
      "|    value_loss           | 0.124     |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=164500, episode_reward=16.48 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 230        |\n",
      "|    mean_reward          | 16.5       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 164500     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.74273264 |\n",
      "|    clip_fraction        | 0.167      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0765    |\n",
      "|    explained_variance   | 0.619      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0855    |\n",
      "|    n_updates            | 12198      |\n",
      "|    policy_gradient_loss | -0.0549    |\n",
      "|    value_loss           | 0.0637     |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 70.6     |\n",
      "| time/              |          |\n",
      "|    fps             | 776      |\n",
      "|    iterations      | 643      |\n",
      "|    time_elapsed    | 212      |\n",
      "|    total_timesteps | 164608   |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 70.6      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 776       |\n",
      "|    iterations           | 644       |\n",
      "|    time_elapsed         | 212       |\n",
      "|    total_timesteps      | 164864    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.6707107 |\n",
      "|    clip_fraction        | 0.157     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.121    |\n",
      "|    explained_variance   | 0.66      |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | 0.0629    |\n",
      "|    n_updates            | 12217     |\n",
      "|    policy_gradient_loss | -0.0477   |\n",
      "|    value_loss           | 0.595     |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=165000, episode_reward=11.14 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 11.1      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 165000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.9498422 |\n",
      "|    clip_fraction        | 0.181     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0708   |\n",
      "|    explained_variance   | 0.585     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0982   |\n",
      "|    n_updates            | 12236     |\n",
      "|    policy_gradient_loss | -0.0694   |\n",
      "|    value_loss           | 0.0557    |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 70.6     |\n",
      "| time/              |          |\n",
      "|    fps             | 775      |\n",
      "|    iterations      | 645      |\n",
      "|    time_elapsed    | 212      |\n",
      "|    total_timesteps | 165120   |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 70.8       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 776        |\n",
      "|    iterations           | 646        |\n",
      "|    time_elapsed         | 213        |\n",
      "|    total_timesteps      | 165376     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.80300516 |\n",
      "|    clip_fraction        | 0.143      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0765    |\n",
      "|    explained_variance   | 0.427      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0265    |\n",
      "|    n_updates            | 12255      |\n",
      "|    policy_gradient_loss | -0.0571    |\n",
      "|    value_loss           | 0.162      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=165500, episode_reward=5.63 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 230        |\n",
      "|    mean_reward          | 5.63       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 165500     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.36124283 |\n",
      "|    clip_fraction        | 0.168      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0886    |\n",
      "|    explained_variance   | 0.56       |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | 0.00376    |\n",
      "|    n_updates            | 12274      |\n",
      "|    policy_gradient_loss | -0.048     |\n",
      "|    value_loss           | 0.433      |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 70.8     |\n",
      "| time/              |          |\n",
      "|    fps             | 776      |\n",
      "|    iterations      | 647      |\n",
      "|    time_elapsed    | 213      |\n",
      "|    total_timesteps | 165632   |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 70.8      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 776       |\n",
      "|    iterations           | 648       |\n",
      "|    time_elapsed         | 213       |\n",
      "|    total_timesteps      | 165888    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.2804164 |\n",
      "|    clip_fraction        | 0.235     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0979   |\n",
      "|    explained_variance   | 0.0336    |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0905   |\n",
      "|    n_updates            | 12293     |\n",
      "|    policy_gradient_loss | -0.07     |\n",
      "|    value_loss           | 0.0532    |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=166000, episode_reward=35.84 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 35.8      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 166000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.8409122 |\n",
      "|    clip_fraction        | 0.15      |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0631   |\n",
      "|    explained_variance   | 0.462     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.107    |\n",
      "|    n_updates            | 12312     |\n",
      "|    policy_gradient_loss | -0.056    |\n",
      "|    value_loss           | 0.223     |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 70.8     |\n",
      "| time/              |          |\n",
      "|    fps             | 776      |\n",
      "|    iterations      | 649      |\n",
      "|    time_elapsed    | 214      |\n",
      "|    total_timesteps | 166144   |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 70.9      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 776       |\n",
      "|    iterations           | 650       |\n",
      "|    time_elapsed         | 214       |\n",
      "|    total_timesteps      | 166400    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.8655039 |\n",
      "|    clip_fraction        | 0.14      |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0456   |\n",
      "|    explained_variance   | 0.828     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0548   |\n",
      "|    n_updates            | 12331     |\n",
      "|    policy_gradient_loss | -0.0518   |\n",
      "|    value_loss           | 0.069     |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=166500, episode_reward=22.79 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 230        |\n",
      "|    mean_reward          | 22.8       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 166500     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.88671786 |\n",
      "|    clip_fraction        | 0.212      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.125     |\n",
      "|    explained_variance   | 0.753      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | 0.025      |\n",
      "|    n_updates            | 12350      |\n",
      "|    policy_gradient_loss | -0.0586    |\n",
      "|    value_loss           | 0.273      |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 70.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 776      |\n",
      "|    iterations      | 651      |\n",
      "|    time_elapsed    | 214      |\n",
      "|    total_timesteps | 166656   |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 70.9       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 776        |\n",
      "|    iterations           | 652        |\n",
      "|    time_elapsed         | 215        |\n",
      "|    total_timesteps      | 166912     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.53962237 |\n",
      "|    clip_fraction        | 0.136      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0571    |\n",
      "|    explained_variance   | 0.78       |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0777    |\n",
      "|    n_updates            | 12369      |\n",
      "|    policy_gradient_loss | -0.0501    |\n",
      "|    value_loss           | 0.0663     |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=167000, episode_reward=25.68 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "--------------------------------------\n",
      "| eval/                   |          |\n",
      "|    mean_ep_length       | 230      |\n",
      "|    mean_reward          | 25.7     |\n",
      "| time/                   |          |\n",
      "|    total_timesteps      | 167000   |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 0.505877 |\n",
      "|    clip_fraction        | 0.12     |\n",
      "|    clip_range           | 0.4      |\n",
      "|    entropy_loss         | -0.0609  |\n",
      "|    explained_variance   | 0.198    |\n",
      "|    learning_rate        | 0.000389 |\n",
      "|    loss                 | -0.0496  |\n",
      "|    n_updates            | 12388    |\n",
      "|    policy_gradient_loss | -0.0517  |\n",
      "|    value_loss           | 0.152    |\n",
      "--------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 71.1     |\n",
      "| time/              |          |\n",
      "|    fps             | 775      |\n",
      "|    iterations      | 653      |\n",
      "|    time_elapsed    | 215      |\n",
      "|    total_timesteps | 167168   |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 71.1      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 776       |\n",
      "|    iterations           | 654       |\n",
      "|    time_elapsed         | 215       |\n",
      "|    total_timesteps      | 167424    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.7534944 |\n",
      "|    clip_fraction        | 0.15      |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0691   |\n",
      "|    explained_variance   | 0.552     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0886   |\n",
      "|    n_updates            | 12407     |\n",
      "|    policy_gradient_loss | -0.0466   |\n",
      "|    value_loss           | 0.361     |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=167500, episode_reward=26.90 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "--------------------------------------\n",
      "| eval/                   |          |\n",
      "|    mean_ep_length       | 230      |\n",
      "|    mean_reward          | 26.9     |\n",
      "| time/                   |          |\n",
      "|    total_timesteps      | 167500   |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 1.40664  |\n",
      "|    clip_fraction        | 0.205    |\n",
      "|    clip_range           | 0.4      |\n",
      "|    entropy_loss         | -0.0828  |\n",
      "|    explained_variance   | 0.346    |\n",
      "|    learning_rate        | 0.000389 |\n",
      "|    loss                 | -0.124   |\n",
      "|    n_updates            | 12426    |\n",
      "|    policy_gradient_loss | -0.0683  |\n",
      "|    value_loss           | 0.0589   |\n",
      "--------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 71.1     |\n",
      "| time/              |          |\n",
      "|    fps             | 775      |\n",
      "|    iterations      | 655      |\n",
      "|    time_elapsed    | 216      |\n",
      "|    total_timesteps | 167680   |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 71.1       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 775        |\n",
      "|    iterations           | 656        |\n",
      "|    time_elapsed         | 216        |\n",
      "|    total_timesteps      | 167936     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.50861686 |\n",
      "|    clip_fraction        | 0.129      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0586    |\n",
      "|    explained_variance   | 0.389      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0417    |\n",
      "|    n_updates            | 12445      |\n",
      "|    policy_gradient_loss | -0.0625    |\n",
      "|    value_loss           | 0.114      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=168000, episode_reward=44.23 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 230        |\n",
      "|    mean_reward          | 44.2       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 168000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.55258036 |\n",
      "|    clip_fraction        | 0.117      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0502    |\n",
      "|    explained_variance   | 0.141      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0293    |\n",
      "|    n_updates            | 12464      |\n",
      "|    policy_gradient_loss | -0.0431    |\n",
      "|    value_loss           | 0.185      |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 71.2     |\n",
      "| time/              |          |\n",
      "|    fps             | 775      |\n",
      "|    iterations      | 657      |\n",
      "|    time_elapsed    | 216      |\n",
      "|    total_timesteps | 168192   |\n",
      "---------------------------------\n",
      "--------------------------------------\n",
      "| rollout/                |          |\n",
      "|    ep_len_mean          | 889      |\n",
      "|    ep_rew_mean          | 71.2     |\n",
      "| time/                   |          |\n",
      "|    fps                  | 775      |\n",
      "|    iterations           | 658      |\n",
      "|    time_elapsed         | 217      |\n",
      "|    total_timesteps      | 168448   |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 0.581321 |\n",
      "|    clip_fraction        | 0.202    |\n",
      "|    clip_range           | 0.4      |\n",
      "|    entropy_loss         | -0.0957  |\n",
      "|    explained_variance   | 0.344    |\n",
      "|    learning_rate        | 0.000389 |\n",
      "|    loss                 | -0.0351  |\n",
      "|    n_updates            | 12483    |\n",
      "|    policy_gradient_loss | -0.0492  |\n",
      "|    value_loss           | 0.457    |\n",
      "--------------------------------------\n",
      "Eval num_timesteps=168500, episode_reward=52.77 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 52.8      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 168500    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.7821282 |\n",
      "|    clip_fraction        | 0.148     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0496   |\n",
      "|    explained_variance   | 0.819     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0679   |\n",
      "|    n_updates            | 12502     |\n",
      "|    policy_gradient_loss | -0.0695   |\n",
      "|    value_loss           | 0.0407    |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 71.2     |\n",
      "| time/              |          |\n",
      "|    fps             | 775      |\n",
      "|    iterations      | 659      |\n",
      "|    time_elapsed    | 217      |\n",
      "|    total_timesteps | 168704   |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 71.3      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 776       |\n",
      "|    iterations           | 660       |\n",
      "|    time_elapsed         | 217       |\n",
      "|    total_timesteps      | 168960    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.3509636 |\n",
      "|    clip_fraction        | 0.1       |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0412   |\n",
      "|    explained_variance   | 0.429     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0777   |\n",
      "|    n_updates            | 12521     |\n",
      "|    policy_gradient_loss | -0.0447   |\n",
      "|    value_loss           | 0.0981    |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=169000, episode_reward=59.59 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 230        |\n",
      "|    mean_reward          | 59.6       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 169000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.78270197 |\n",
      "|    clip_fraction        | 0.17       |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0677    |\n",
      "|    explained_variance   | 0.789      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0529    |\n",
      "|    n_updates            | 12540      |\n",
      "|    policy_gradient_loss | -0.0526    |\n",
      "|    value_loss           | 0.251      |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 71.3     |\n",
      "| time/              |          |\n",
      "|    fps             | 776      |\n",
      "|    iterations      | 661      |\n",
      "|    time_elapsed    | 218      |\n",
      "|    total_timesteps | 169216   |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 71.3      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 776       |\n",
      "|    iterations           | 662       |\n",
      "|    time_elapsed         | 218       |\n",
      "|    total_timesteps      | 169472    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.9604579 |\n",
      "|    clip_fraction        | 0.162     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0713   |\n",
      "|    explained_variance   | 0.512     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0747   |\n",
      "|    n_updates            | 12559     |\n",
      "|    policy_gradient_loss | -0.0545   |\n",
      "|    value_loss           | 0.0326    |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=169500, episode_reward=24.45 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 24.5      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 169500    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.4740413 |\n",
      "|    clip_fraction        | 0.171     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0602   |\n",
      "|    explained_variance   | 0.746     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0845   |\n",
      "|    n_updates            | 12578     |\n",
      "|    policy_gradient_loss | -0.0593   |\n",
      "|    value_loss           | 0.0747    |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 71.3     |\n",
      "| time/              |          |\n",
      "|    fps             | 776      |\n",
      "|    iterations      | 663      |\n",
      "|    time_elapsed    | 218      |\n",
      "|    total_timesteps | 169728   |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 71.5       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 776        |\n",
      "|    iterations           | 664        |\n",
      "|    time_elapsed         | 218        |\n",
      "|    total_timesteps      | 169984     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.60082406 |\n",
      "|    clip_fraction        | 0.143      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0702    |\n",
      "|    explained_variance   | 0.84       |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0904    |\n",
      "|    n_updates            | 12597      |\n",
      "|    policy_gradient_loss | -0.0692    |\n",
      "|    value_loss           | 0.146      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=170000, episode_reward=49.82 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 49.8      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 170000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.0032096 |\n",
      "|    clip_fraction        | 0.194     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0806   |\n",
      "|    explained_variance   | 0.29      |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0571   |\n",
      "|    n_updates            | 12616     |\n",
      "|    policy_gradient_loss | -0.0537   |\n",
      "|    value_loss           | 0.0883    |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 71.5     |\n",
      "| time/              |          |\n",
      "|    fps             | 776      |\n",
      "|    iterations      | 665      |\n",
      "|    time_elapsed    | 219      |\n",
      "|    total_timesteps | 170240   |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 71.5      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 776       |\n",
      "|    iterations           | 666       |\n",
      "|    time_elapsed         | 219       |\n",
      "|    total_timesteps      | 170496    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.1757156 |\n",
      "|    clip_fraction        | 0.185     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0645   |\n",
      "|    explained_variance   | 0.868     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0767   |\n",
      "|    n_updates            | 12635     |\n",
      "|    policy_gradient_loss | -0.0735   |\n",
      "|    value_loss           | 0.0365    |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=170500, episode_reward=39.32 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 39.3      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 170500    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.7096299 |\n",
      "|    clip_fraction        | 0.152     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0609   |\n",
      "|    explained_variance   | 0.431     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0991   |\n",
      "|    n_updates            | 12654     |\n",
      "|    policy_gradient_loss | -0.0661   |\n",
      "|    value_loss           | 0.0992    |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 71.5     |\n",
      "| time/              |          |\n",
      "|    fps             | 776      |\n",
      "|    iterations      | 667      |\n",
      "|    time_elapsed    | 219      |\n",
      "|    total_timesteps | 170752   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=171000, episode_reward=47.35 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "--------------------------------------\n",
      "| eval/                   |          |\n",
      "|    mean_ep_length       | 230      |\n",
      "|    mean_reward          | 47.3     |\n",
      "| time/                   |          |\n",
      "|    total_timesteps      | 171000   |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 1.47624  |\n",
      "|    clip_fraction        | 0.173    |\n",
      "|    clip_range           | 0.4      |\n",
      "|    entropy_loss         | -0.0798  |\n",
      "|    explained_variance   | 0.801    |\n",
      "|    learning_rate        | 0.000389 |\n",
      "|    loss                 | -0.0884  |\n",
      "|    n_updates            | 12673    |\n",
      "|    policy_gradient_loss | -0.0513  |\n",
      "|    value_loss           | 0.19     |\n",
      "--------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 71.5     |\n",
      "| time/              |          |\n",
      "|    fps             | 776      |\n",
      "|    iterations      | 668      |\n",
      "|    time_elapsed    | 220      |\n",
      "|    total_timesteps | 171008   |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 71.5       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 776        |\n",
      "|    iterations           | 669        |\n",
      "|    time_elapsed         | 220        |\n",
      "|    total_timesteps      | 171264     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.54996526 |\n",
      "|    clip_fraction        | 0.155      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0718    |\n",
      "|    explained_variance   | 0.518      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0874    |\n",
      "|    n_updates            | 12692      |\n",
      "|    policy_gradient_loss | -0.0544    |\n",
      "|    value_loss           | 0.0493     |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=171500, episode_reward=37.48 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 230        |\n",
      "|    mean_reward          | 37.5       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 171500     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.94902706 |\n",
      "|    clip_fraction        | 0.134      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0524    |\n",
      "|    explained_variance   | 0.441      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0647    |\n",
      "|    n_updates            | 12711      |\n",
      "|    policy_gradient_loss | -0.0639    |\n",
      "|    value_loss           | 0.163      |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 71.5     |\n",
      "| time/              |          |\n",
      "|    fps             | 775      |\n",
      "|    iterations      | 670      |\n",
      "|    time_elapsed    | 221      |\n",
      "|    total_timesteps | 171520   |\n",
      "---------------------------------\n",
      "--------------------------------------\n",
      "| rollout/                |          |\n",
      "|    ep_len_mean          | 889      |\n",
      "|    ep_rew_mean          | 71.8     |\n",
      "| time/                   |          |\n",
      "|    fps                  | 776      |\n",
      "|    iterations           | 671      |\n",
      "|    time_elapsed         | 221      |\n",
      "|    total_timesteps      | 171776   |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 0.441436 |\n",
      "|    clip_fraction        | 0.127    |\n",
      "|    clip_range           | 0.4      |\n",
      "|    entropy_loss         | -0.0676  |\n",
      "|    explained_variance   | 0.802    |\n",
      "|    learning_rate        | 0.000389 |\n",
      "|    loss                 | -0.0256  |\n",
      "|    n_updates            | 12730    |\n",
      "|    policy_gradient_loss | -0.0376  |\n",
      "|    value_loss           | 0.173    |\n",
      "--------------------------------------\n",
      "Eval num_timesteps=172000, episode_reward=66.04 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 66        |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 172000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.9625914 |\n",
      "|    clip_fraction        | 0.174     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.086    |\n",
      "|    explained_variance   | 0.606     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0577   |\n",
      "|    n_updates            | 12749     |\n",
      "|    policy_gradient_loss | -0.0481   |\n",
      "|    value_loss           | 0.207     |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 71.8     |\n",
      "| time/              |          |\n",
      "|    fps             | 775      |\n",
      "|    iterations      | 672      |\n",
      "|    time_elapsed    | 221      |\n",
      "|    total_timesteps | 172032   |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 71.8      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 776       |\n",
      "|    iterations           | 673       |\n",
      "|    time_elapsed         | 221       |\n",
      "|    total_timesteps      | 172288    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.5894806 |\n",
      "|    clip_fraction        | 0.128     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0562   |\n",
      "|    explained_variance   | 0.639     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0943   |\n",
      "|    n_updates            | 12768     |\n",
      "|    policy_gradient_loss | -0.0506   |\n",
      "|    value_loss           | 0.0428    |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=172500, episode_reward=71.20 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 230        |\n",
      "|    mean_reward          | 71.2       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 172500     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.57736903 |\n",
      "|    clip_fraction        | 0.124      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.051     |\n",
      "|    explained_variance   | 0.476      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.114     |\n",
      "|    n_updates            | 12787      |\n",
      "|    policy_gradient_loss | -0.0563    |\n",
      "|    value_loss           | 0.107      |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 72.1     |\n",
      "| time/              |          |\n",
      "|    fps             | 775      |\n",
      "|    iterations      | 674      |\n",
      "|    time_elapsed    | 222      |\n",
      "|    total_timesteps | 172544   |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 72.1      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 775       |\n",
      "|    iterations           | 675       |\n",
      "|    time_elapsed         | 222       |\n",
      "|    total_timesteps      | 172800    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.6259987 |\n",
      "|    clip_fraction        | 0.175     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.105    |\n",
      "|    explained_variance   | 0.847     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0468   |\n",
      "|    n_updates            | 12806     |\n",
      "|    policy_gradient_loss | -0.0565   |\n",
      "|    value_loss           | 0.287     |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=173000, episode_reward=61.69 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 61.7      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 173000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.7738987 |\n",
      "|    clip_fraction        | 0.174     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0722   |\n",
      "|    explained_variance   | 0.226     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0527   |\n",
      "|    n_updates            | 12825     |\n",
      "|    policy_gradient_loss | -0.073    |\n",
      "|    value_loss           | 0.0569    |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 72.1     |\n",
      "| time/              |          |\n",
      "|    fps             | 775      |\n",
      "|    iterations      | 676      |\n",
      "|    time_elapsed    | 223      |\n",
      "|    total_timesteps | 173056   |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 72.1      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 775       |\n",
      "|    iterations           | 677       |\n",
      "|    time_elapsed         | 223       |\n",
      "|    total_timesteps      | 173312    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.6966063 |\n",
      "|    clip_fraction        | 0.138     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0643   |\n",
      "|    explained_variance   | 0.611     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.044    |\n",
      "|    n_updates            | 12844     |\n",
      "|    policy_gradient_loss | -0.0497   |\n",
      "|    value_loss           | 0.201     |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=173500, episode_reward=57.59 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 57.6      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 173500    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.6614633 |\n",
      "|    clip_fraction        | 0.141     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0633   |\n",
      "|    explained_variance   | 0.72      |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0646   |\n",
      "|    n_updates            | 12863     |\n",
      "|    policy_gradient_loss | -0.043    |\n",
      "|    value_loss           | 0.231     |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 72.1     |\n",
      "| time/              |          |\n",
      "|    fps             | 775      |\n",
      "|    iterations      | 678      |\n",
      "|    time_elapsed    | 223      |\n",
      "|    total_timesteps | 173568   |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 72.1       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 775        |\n",
      "|    iterations           | 679        |\n",
      "|    time_elapsed         | 224        |\n",
      "|    total_timesteps      | 173824     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.83866924 |\n",
      "|    clip_fraction        | 0.164      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0805    |\n",
      "|    explained_variance   | 0.657      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0749    |\n",
      "|    n_updates            | 12882      |\n",
      "|    policy_gradient_loss | -0.0624    |\n",
      "|    value_loss           | 0.132      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=174000, episode_reward=37.56 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 230        |\n",
      "|    mean_reward          | 37.6       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 174000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.63587403 |\n",
      "|    clip_fraction        | 0.123      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0524    |\n",
      "|    explained_variance   | 0.14       |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0796    |\n",
      "|    n_updates            | 12901      |\n",
      "|    policy_gradient_loss | -0.0508    |\n",
      "|    value_loss           | 0.111      |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 72.1     |\n",
      "| time/              |          |\n",
      "|    fps             | 775      |\n",
      "|    iterations      | 680      |\n",
      "|    time_elapsed    | 224      |\n",
      "|    total_timesteps | 174080   |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 72.1       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 775        |\n",
      "|    iterations           | 681        |\n",
      "|    time_elapsed         | 224        |\n",
      "|    total_timesteps      | 174336     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.66702497 |\n",
      "|    clip_fraction        | 0.144      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.077     |\n",
      "|    explained_variance   | 0.572      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0921    |\n",
      "|    n_updates            | 12920      |\n",
      "|    policy_gradient_loss | -0.038     |\n",
      "|    value_loss           | 0.148      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=174500, episode_reward=53.11 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 230        |\n",
      "|    mean_reward          | 53.1       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 174500     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.92754805 |\n",
      "|    clip_fraction        | 0.203      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0984    |\n",
      "|    explained_variance   | 0.876      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0661    |\n",
      "|    n_updates            | 12939      |\n",
      "|    policy_gradient_loss | -0.0449    |\n",
      "|    value_loss           | 0.178      |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 72.1     |\n",
      "| time/              |          |\n",
      "|    fps             | 775      |\n",
      "|    iterations      | 682      |\n",
      "|    time_elapsed    | 225      |\n",
      "|    total_timesteps | 174592   |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 72.1      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 775       |\n",
      "|    iterations           | 683       |\n",
      "|    time_elapsed         | 225       |\n",
      "|    total_timesteps      | 174848    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.5744171 |\n",
      "|    clip_fraction        | 0.149     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0544   |\n",
      "|    explained_variance   | 0.334     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0128   |\n",
      "|    n_updates            | 12958     |\n",
      "|    policy_gradient_loss | -0.0554   |\n",
      "|    value_loss           | 0.0579    |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=175000, episode_reward=71.16 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 71.2      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 175000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.8278237 |\n",
      "|    clip_fraction        | 0.145     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0487   |\n",
      "|    explained_variance   | 0.514     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0801   |\n",
      "|    n_updates            | 12977     |\n",
      "|    policy_gradient_loss | -0.0516   |\n",
      "|    value_loss           | 0.186     |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 72.1     |\n",
      "| time/              |          |\n",
      "|    fps             | 774      |\n",
      "|    iterations      | 684      |\n",
      "|    time_elapsed    | 225      |\n",
      "|    total_timesteps | 175104   |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 72.2      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 775       |\n",
      "|    iterations           | 685       |\n",
      "|    time_elapsed         | 226       |\n",
      "|    total_timesteps      | 175360    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.5256983 |\n",
      "|    clip_fraction        | 0.0845    |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0434   |\n",
      "|    explained_variance   | 0.822     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0235   |\n",
      "|    n_updates            | 12996     |\n",
      "|    policy_gradient_loss | -0.0391   |\n",
      "|    value_loss           | 0.18      |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=175500, episode_reward=44.28 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 44.3      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 175500    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.2729921 |\n",
      "|    clip_fraction        | 0.25      |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0834   |\n",
      "|    explained_variance   | 0.751     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0732   |\n",
      "|    n_updates            | 13015     |\n",
      "|    policy_gradient_loss | -0.0898   |\n",
      "|    value_loss           | 0.14      |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 72.2     |\n",
      "| time/              |          |\n",
      "|    fps             | 775      |\n",
      "|    iterations      | 686      |\n",
      "|    time_elapsed    | 226      |\n",
      "|    total_timesteps | 175616   |\n",
      "---------------------------------\n",
      "--------------------------------------\n",
      "| rollout/                |          |\n",
      "|    ep_len_mean          | 889      |\n",
      "|    ep_rew_mean          | 72.2     |\n",
      "| time/                   |          |\n",
      "|    fps                  | 775      |\n",
      "|    iterations           | 687      |\n",
      "|    time_elapsed         | 226      |\n",
      "|    total_timesteps      | 175872   |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 0.719839 |\n",
      "|    clip_fraction        | 0.148    |\n",
      "|    clip_range           | 0.4      |\n",
      "|    entropy_loss         | -0.0556  |\n",
      "|    explained_variance   | 0.455    |\n",
      "|    learning_rate        | 0.000389 |\n",
      "|    loss                 | -0.104   |\n",
      "|    n_updates            | 13034    |\n",
      "|    policy_gradient_loss | -0.0558  |\n",
      "|    value_loss           | 0.101    |\n",
      "--------------------------------------\n",
      "Eval num_timesteps=176000, episode_reward=33.62 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "--------------------------------------\n",
      "| eval/                   |          |\n",
      "|    mean_ep_length       | 230      |\n",
      "|    mean_reward          | 33.6     |\n",
      "| time/                   |          |\n",
      "|    total_timesteps      | 176000   |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 1.115778 |\n",
      "|    clip_fraction        | 0.105    |\n",
      "|    clip_range           | 0.4      |\n",
      "|    entropy_loss         | -0.0361  |\n",
      "|    explained_variance   | 0.614    |\n",
      "|    learning_rate        | 0.000389 |\n",
      "|    loss                 | -0.0216  |\n",
      "|    n_updates            | 13053    |\n",
      "|    policy_gradient_loss | -0.0412  |\n",
      "|    value_loss           | 0.0989   |\n",
      "--------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 72.5     |\n",
      "| time/              |          |\n",
      "|    fps             | 775      |\n",
      "|    iterations      | 688      |\n",
      "|    time_elapsed    | 227      |\n",
      "|    total_timesteps | 176128   |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 72.5      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 775       |\n",
      "|    iterations           | 689       |\n",
      "|    time_elapsed         | 227       |\n",
      "|    total_timesteps      | 176384    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.2934871 |\n",
      "|    clip_fraction        | 0.22      |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0726   |\n",
      "|    explained_variance   | 0.847     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0229   |\n",
      "|    n_updates            | 13072     |\n",
      "|    policy_gradient_loss | -0.0556   |\n",
      "|    value_loss           | 0.166     |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=176500, episode_reward=9.05 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 9.05      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 176500    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.0693896 |\n",
      "|    clip_fraction        | 0.154     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0412   |\n",
      "|    explained_variance   | 0.455     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0827   |\n",
      "|    n_updates            | 13091     |\n",
      "|    policy_gradient_loss | -0.0621   |\n",
      "|    value_loss           | 0.0436    |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 72.5     |\n",
      "| time/              |          |\n",
      "|    fps             | 775      |\n",
      "|    iterations      | 690      |\n",
      "|    time_elapsed    | 227      |\n",
      "|    total_timesteps | 176640   |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 72.5      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 775       |\n",
      "|    iterations           | 691       |\n",
      "|    time_elapsed         | 228       |\n",
      "|    total_timesteps      | 176896    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.2860168 |\n",
      "|    clip_fraction        | 0.134     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0459   |\n",
      "|    explained_variance   | 0.579     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0875   |\n",
      "|    n_updates            | 13110     |\n",
      "|    policy_gradient_loss | -0.0688   |\n",
      "|    value_loss           | 0.149     |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=177000, episode_reward=17.52 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 17.5      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 177000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.8545128 |\n",
      "|    clip_fraction        | 0.183     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0738   |\n",
      "|    explained_variance   | 0.812     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0887   |\n",
      "|    n_updates            | 13129     |\n",
      "|    policy_gradient_loss | -0.0575   |\n",
      "|    value_loss           | 0.148     |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 72.7     |\n",
      "| time/              |          |\n",
      "|    fps             | 775      |\n",
      "|    iterations      | 692      |\n",
      "|    time_elapsed    | 228      |\n",
      "|    total_timesteps | 177152   |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 72.7      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 776       |\n",
      "|    iterations           | 693       |\n",
      "|    time_elapsed         | 228       |\n",
      "|    total_timesteps      | 177408    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.9839634 |\n",
      "|    clip_fraction        | 0.16      |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0651   |\n",
      "|    explained_variance   | 0.707     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0659   |\n",
      "|    n_updates            | 13148     |\n",
      "|    policy_gradient_loss | -0.0605   |\n",
      "|    value_loss           | 0.0859    |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=177500, episode_reward=60.03 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 60        |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 177500    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.7283014 |\n",
      "|    clip_fraction        | 0.153     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0527   |\n",
      "|    explained_variance   | 0.672     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0619   |\n",
      "|    n_updates            | 13167     |\n",
      "|    policy_gradient_loss | -0.0615   |\n",
      "|    value_loss           | 0.0763    |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 72.7     |\n",
      "| time/              |          |\n",
      "|    fps             | 775      |\n",
      "|    iterations      | 694      |\n",
      "|    time_elapsed    | 228      |\n",
      "|    total_timesteps | 177664   |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 72.7      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 776       |\n",
      "|    iterations           | 695       |\n",
      "|    time_elapsed         | 229       |\n",
      "|    total_timesteps      | 177920    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.3828846 |\n",
      "|    clip_fraction        | 0.122     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0548   |\n",
      "|    explained_variance   | 0.762     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0488   |\n",
      "|    n_updates            | 13186     |\n",
      "|    policy_gradient_loss | -0.0384   |\n",
      "|    value_loss           | 0.0618    |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=178000, episode_reward=70.78 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 70.8      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 178000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.9065542 |\n",
      "|    clip_fraction        | 0.204     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.102    |\n",
      "|    explained_variance   | 0.868     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0947   |\n",
      "|    n_updates            | 13205     |\n",
      "|    policy_gradient_loss | -0.0633   |\n",
      "|    value_loss           | 0.118     |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 72.7     |\n",
      "| time/              |          |\n",
      "|    fps             | 775      |\n",
      "|    iterations      | 696      |\n",
      "|    time_elapsed    | 229      |\n",
      "|    total_timesteps | 178176   |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 72.7      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 776       |\n",
      "|    iterations           | 697       |\n",
      "|    time_elapsed         | 229       |\n",
      "|    total_timesteps      | 178432    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.9955412 |\n",
      "|    clip_fraction        | 0.152     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0451   |\n",
      "|    explained_variance   | 0.682     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.106    |\n",
      "|    n_updates            | 13224     |\n",
      "|    policy_gradient_loss | -0.0556   |\n",
      "|    value_loss           | 0.0297    |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=178500, episode_reward=27.19 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 27.2      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 178500    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.6099813 |\n",
      "|    clip_fraction        | 0.123     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0454   |\n",
      "|    explained_variance   | 0.69      |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.103    |\n",
      "|    n_updates            | 13243     |\n",
      "|    policy_gradient_loss | -0.0582   |\n",
      "|    value_loss           | 0.189     |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 72.7     |\n",
      "| time/              |          |\n",
      "|    fps             | 775      |\n",
      "|    iterations      | 698      |\n",
      "|    time_elapsed    | 230      |\n",
      "|    total_timesteps | 178688   |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 73.1       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 776        |\n",
      "|    iterations           | 699        |\n",
      "|    time_elapsed         | 230        |\n",
      "|    total_timesteps      | 178944     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.87089485 |\n",
      "|    clip_fraction        | 0.191      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.096     |\n",
      "|    explained_variance   | 0.734      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0329    |\n",
      "|    n_updates            | 13262      |\n",
      "|    policy_gradient_loss | -0.0665    |\n",
      "|    value_loss           | 0.275      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=179000, episode_reward=54.09 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 230        |\n",
      "|    mean_reward          | 54.1       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 179000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.68663174 |\n",
      "|    clip_fraction        | 0.195      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.074     |\n",
      "|    explained_variance   | 0.595      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0158    |\n",
      "|    n_updates            | 13281      |\n",
      "|    policy_gradient_loss | -0.0571    |\n",
      "|    value_loss           | 0.0513     |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 73.1     |\n",
      "| time/              |          |\n",
      "|    fps             | 775      |\n",
      "|    iterations      | 700      |\n",
      "|    time_elapsed    | 230      |\n",
      "|    total_timesteps | 179200   |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 73.1      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 776       |\n",
      "|    iterations           | 701       |\n",
      "|    time_elapsed         | 231       |\n",
      "|    total_timesteps      | 179456    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.3015561 |\n",
      "|    clip_fraction        | 0.157     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0421   |\n",
      "|    explained_variance   | 0.697     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.069    |\n",
      "|    n_updates            | 13300     |\n",
      "|    policy_gradient_loss | -0.0571   |\n",
      "|    value_loss           | 0.148     |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=179500, episode_reward=33.35 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 33.3      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 179500    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.6373394 |\n",
      "|    clip_fraction        | 0.13      |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.055    |\n",
      "|    explained_variance   | 0.774     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0365   |\n",
      "|    n_updates            | 13319     |\n",
      "|    policy_gradient_loss | -0.0489   |\n",
      "|    value_loss           | 0.0516    |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 73.2     |\n",
      "| time/              |          |\n",
      "|    fps             | 775      |\n",
      "|    iterations      | 702      |\n",
      "|    time_elapsed    | 231      |\n",
      "|    total_timesteps | 179712   |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 73.2       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 776        |\n",
      "|    iterations           | 703        |\n",
      "|    time_elapsed         | 231        |\n",
      "|    total_timesteps      | 179968     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.59432566 |\n",
      "|    clip_fraction        | 0.151      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0777    |\n",
      "|    explained_variance   | 0.82       |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0263    |\n",
      "|    n_updates            | 13338      |\n",
      "|    policy_gradient_loss | -0.0435    |\n",
      "|    value_loss           | 0.271      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=180000, episode_reward=2.74 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 2.74      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 180000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.8202811 |\n",
      "|    clip_fraction        | 0.118     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0403   |\n",
      "|    explained_variance   | 0.688     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0557   |\n",
      "|    n_updates            | 13357     |\n",
      "|    policy_gradient_loss | -0.0473   |\n",
      "|    value_loss           | 0.0478    |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 73.2     |\n",
      "| time/              |          |\n",
      "|    fps             | 775      |\n",
      "|    iterations      | 704      |\n",
      "|    time_elapsed    | 232      |\n",
      "|    total_timesteps | 180224   |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 73.4       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 775        |\n",
      "|    iterations           | 705        |\n",
      "|    time_elapsed         | 232        |\n",
      "|    total_timesteps      | 180480     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.64454603 |\n",
      "|    clip_fraction        | 0.12       |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.049     |\n",
      "|    explained_variance   | 0.698      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0654    |\n",
      "|    n_updates            | 13376      |\n",
      "|    policy_gradient_loss | -0.0525    |\n",
      "|    value_loss           | 0.225      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=180500, episode_reward=29.98 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 230        |\n",
      "|    mean_reward          | 30         |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 180500     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.82538974 |\n",
      "|    clip_fraction        | 0.175      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0943    |\n",
      "|    explained_variance   | 0.746      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0795    |\n",
      "|    n_updates            | 13395      |\n",
      "|    policy_gradient_loss | -0.0612    |\n",
      "|    value_loss           | 0.2        |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 73.4     |\n",
      "| time/              |          |\n",
      "|    fps             | 775      |\n",
      "|    iterations      | 706      |\n",
      "|    time_elapsed    | 233      |\n",
      "|    total_timesteps | 180736   |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 73.4       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 775        |\n",
      "|    iterations           | 707        |\n",
      "|    time_elapsed         | 233        |\n",
      "|    total_timesteps      | 180992     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.73281544 |\n",
      "|    clip_fraction        | 0.106      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0409    |\n",
      "|    explained_variance   | -0.429     |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0474    |\n",
      "|    n_updates            | 13414      |\n",
      "|    policy_gradient_loss | -0.0438    |\n",
      "|    value_loss           | 0.0529     |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=181000, episode_reward=1.38 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 230        |\n",
      "|    mean_reward          | 1.38       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 181000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.72181785 |\n",
      "|    clip_fraction        | 0.12       |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0455    |\n",
      "|    explained_variance   | 0.724      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | 0.0186     |\n",
      "|    n_updates            | 13433      |\n",
      "|    policy_gradient_loss | -0.0485    |\n",
      "|    value_loss           | 0.218      |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 73.4     |\n",
      "| time/              |          |\n",
      "|    fps             | 774      |\n",
      "|    iterations      | 708      |\n",
      "|    time_elapsed    | 233      |\n",
      "|    total_timesteps | 181248   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=181500, episode_reward=-22.77 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | -22.8     |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 181500    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.2910981 |\n",
      "|    clip_fraction        | 0.2       |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.063    |\n",
      "|    explained_variance   | 0.82      |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.11     |\n",
      "|    n_updates            | 13452     |\n",
      "|    policy_gradient_loss | -0.0629   |\n",
      "|    value_loss           | 0.0605    |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 73.5     |\n",
      "| time/              |          |\n",
      "|    fps             | 774      |\n",
      "|    iterations      | 709      |\n",
      "|    time_elapsed    | 234      |\n",
      "|    total_timesteps | 181504   |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 73.5      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 774       |\n",
      "|    iterations           | 710       |\n",
      "|    time_elapsed         | 234       |\n",
      "|    total_timesteps      | 181760    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.4857051 |\n",
      "|    clip_fraction        | 0.141     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0758   |\n",
      "|    explained_variance   | 0.75      |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0111   |\n",
      "|    n_updates            | 13471     |\n",
      "|    policy_gradient_loss | -0.0529   |\n",
      "|    value_loss           | 0.239     |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=182000, episode_reward=34.17 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 34.2      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 182000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.0662287 |\n",
      "|    clip_fraction        | 0.132     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0312   |\n",
      "|    explained_variance   | 0.698     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.064    |\n",
      "|    n_updates            | 13490     |\n",
      "|    policy_gradient_loss | -0.0576   |\n",
      "|    value_loss           | 0.0769    |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 73.5     |\n",
      "| time/              |          |\n",
      "|    fps             | 774      |\n",
      "|    iterations      | 711      |\n",
      "|    time_elapsed    | 235      |\n",
      "|    total_timesteps | 182016   |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 73.5      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 774       |\n",
      "|    iterations           | 712       |\n",
      "|    time_elapsed         | 235       |\n",
      "|    total_timesteps      | 182272    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.4775574 |\n",
      "|    clip_fraction        | 0.168     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0513   |\n",
      "|    explained_variance   | 0.463     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0886   |\n",
      "|    n_updates            | 13509     |\n",
      "|    policy_gradient_loss | -0.0455   |\n",
      "|    value_loss           | 0.195     |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=182500, episode_reward=34.84 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 34.8      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 182500    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.9008038 |\n",
      "|    clip_fraction        | 0.187     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0797   |\n",
      "|    explained_variance   | 0.696     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.118    |\n",
      "|    n_updates            | 13528     |\n",
      "|    policy_gradient_loss | -0.0692   |\n",
      "|    value_loss           | 0.28      |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 73.5     |\n",
      "| time/              |          |\n",
      "|    fps             | 774      |\n",
      "|    iterations      | 713      |\n",
      "|    time_elapsed    | 235      |\n",
      "|    total_timesteps | 182528   |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 73.5      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 774       |\n",
      "|    iterations           | 714       |\n",
      "|    time_elapsed         | 235       |\n",
      "|    total_timesteps      | 182784    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.4627641 |\n",
      "|    clip_fraction        | 0.133     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0783   |\n",
      "|    explained_variance   | 0.325     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0338   |\n",
      "|    n_updates            | 13547     |\n",
      "|    policy_gradient_loss | -0.044    |\n",
      "|    value_loss           | 0.0646    |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=183000, episode_reward=40.73 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 40.7      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 183000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.7619664 |\n",
      "|    clip_fraction        | 0.117     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0387   |\n",
      "|    explained_variance   | 0.647     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0718   |\n",
      "|    n_updates            | 13566     |\n",
      "|    policy_gradient_loss | -0.0478   |\n",
      "|    value_loss           | 0.124     |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 73.5     |\n",
      "| time/              |          |\n",
      "|    fps             | 774      |\n",
      "|    iterations      | 715      |\n",
      "|    time_elapsed    | 236      |\n",
      "|    total_timesteps | 183040   |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 73.6      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 774       |\n",
      "|    iterations           | 716       |\n",
      "|    time_elapsed         | 236       |\n",
      "|    total_timesteps      | 183296    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.5630706 |\n",
      "|    clip_fraction        | 0.134     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0646   |\n",
      "|    explained_variance   | 0.728     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0969   |\n",
      "|    n_updates            | 13585     |\n",
      "|    policy_gradient_loss | -0.0378   |\n",
      "|    value_loss           | 0.136     |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=183500, episode_reward=50.32 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 230        |\n",
      "|    mean_reward          | 50.3       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 183500     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.80430484 |\n",
      "|    clip_fraction        | 0.206      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.102     |\n",
      "|    explained_variance   | 0.794      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0338    |\n",
      "|    n_updates            | 13604      |\n",
      "|    policy_gradient_loss | -0.0595    |\n",
      "|    value_loss           | 0.154      |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 73.6     |\n",
      "| time/              |          |\n",
      "|    fps             | 774      |\n",
      "|    iterations      | 717      |\n",
      "|    time_elapsed    | 237      |\n",
      "|    total_timesteps | 183552   |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 73.6      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 774       |\n",
      "|    iterations           | 718       |\n",
      "|    time_elapsed         | 237       |\n",
      "|    total_timesteps      | 183808    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.6415485 |\n",
      "|    clip_fraction        | 0.0859    |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0349   |\n",
      "|    explained_variance   | 0.814     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0677   |\n",
      "|    n_updates            | 13623     |\n",
      "|    policy_gradient_loss | -0.0318   |\n",
      "|    value_loss           | 0.0506    |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=184000, episode_reward=20.62 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 20.6      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 184000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.6075264 |\n",
      "|    clip_fraction        | 0.112     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.041    |\n",
      "|    explained_variance   | 0.377     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0743   |\n",
      "|    n_updates            | 13642     |\n",
      "|    policy_gradient_loss | -0.0512   |\n",
      "|    value_loss           | 0.139     |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 73.6     |\n",
      "| time/              |          |\n",
      "|    fps             | 774      |\n",
      "|    iterations      | 719      |\n",
      "|    time_elapsed    | 237      |\n",
      "|    total_timesteps | 184064   |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 73.6      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 775       |\n",
      "|    iterations           | 720       |\n",
      "|    time_elapsed         | 237       |\n",
      "|    total_timesteps      | 184320    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.2283388 |\n",
      "|    clip_fraction        | 0.167     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0776   |\n",
      "|    explained_variance   | 0.791     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0502   |\n",
      "|    n_updates            | 13661     |\n",
      "|    policy_gradient_loss | -0.0659   |\n",
      "|    value_loss           | 0.145     |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=184500, episode_reward=30.20 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 30.2      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 184500    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.3735149 |\n",
      "|    clip_fraction        | 0.218     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0748   |\n",
      "|    explained_variance   | 0.319     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0452   |\n",
      "|    n_updates            | 13680     |\n",
      "|    policy_gradient_loss | -0.0768   |\n",
      "|    value_loss           | 0.0722    |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 73.6     |\n",
      "| time/              |          |\n",
      "|    fps             | 774      |\n",
      "|    iterations      | 721      |\n",
      "|    time_elapsed    | 238      |\n",
      "|    total_timesteps | 184576   |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 73.6       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 775        |\n",
      "|    iterations           | 722        |\n",
      "|    time_elapsed         | 238        |\n",
      "|    total_timesteps      | 184832     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.69042456 |\n",
      "|    clip_fraction        | 0.12       |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.039     |\n",
      "|    explained_variance   | 0.291      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0545    |\n",
      "|    n_updates            | 13699      |\n",
      "|    policy_gradient_loss | -0.0568    |\n",
      "|    value_loss           | 0.0988     |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=185000, episode_reward=27.52 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 27.5      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 185000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.2924628 |\n",
      "|    clip_fraction        | 0.131     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0482   |\n",
      "|    explained_variance   | 0.859     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0665   |\n",
      "|    n_updates            | 13718     |\n",
      "|    policy_gradient_loss | -0.0534   |\n",
      "|    value_loss           | 0.0768    |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 73.8     |\n",
      "| time/              |          |\n",
      "|    fps             | 775      |\n",
      "|    iterations      | 723      |\n",
      "|    time_elapsed    | 238      |\n",
      "|    total_timesteps | 185088   |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 73.8      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 775       |\n",
      "|    iterations           | 724       |\n",
      "|    time_elapsed         | 239       |\n",
      "|    total_timesteps      | 185344    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.8773743 |\n",
      "|    clip_fraction        | 0.176     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0919   |\n",
      "|    explained_variance   | 0.594     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0857   |\n",
      "|    n_updates            | 13737     |\n",
      "|    policy_gradient_loss | -0.046    |\n",
      "|    value_loss           | 0.126     |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=185500, episode_reward=21.87 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 21.9      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 185500    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.6196728 |\n",
      "|    clip_fraction        | 0.107     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0347   |\n",
      "|    explained_variance   | 0.813     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0259   |\n",
      "|    n_updates            | 13756     |\n",
      "|    policy_gradient_loss | -0.0325   |\n",
      "|    value_loss           | 0.0301    |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 73.8     |\n",
      "| time/              |          |\n",
      "|    fps             | 775      |\n",
      "|    iterations      | 725      |\n",
      "|    time_elapsed    | 239      |\n",
      "|    total_timesteps | 185600   |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 73.9      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 775       |\n",
      "|    iterations           | 726       |\n",
      "|    time_elapsed         | 239       |\n",
      "|    total_timesteps      | 185856    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.9396204 |\n",
      "|    clip_fraction        | 0.135     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0496   |\n",
      "|    explained_variance   | 0.536     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0296   |\n",
      "|    n_updates            | 13775     |\n",
      "|    policy_gradient_loss | -0.0495   |\n",
      "|    value_loss           | 0.0535    |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=186000, episode_reward=49.81 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 230        |\n",
      "|    mean_reward          | 49.8       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 186000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.42679796 |\n",
      "|    clip_fraction        | 0.145      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0894    |\n",
      "|    explained_variance   | 0.368      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0269    |\n",
      "|    n_updates            | 13794      |\n",
      "|    policy_gradient_loss | -0.0298    |\n",
      "|    value_loss           | 0.584      |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 73.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 775      |\n",
      "|    iterations      | 727      |\n",
      "|    time_elapsed    | 240      |\n",
      "|    total_timesteps | 186112   |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 73.9      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 775       |\n",
      "|    iterations           | 728       |\n",
      "|    time_elapsed         | 240       |\n",
      "|    total_timesteps      | 186368    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 2.0795755 |\n",
      "|    clip_fraction        | 0.234     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.059    |\n",
      "|    explained_variance   | 0.239     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0912   |\n",
      "|    n_updates            | 13813     |\n",
      "|    policy_gradient_loss | -0.074    |\n",
      "|    value_loss           | 0.0531    |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=186500, episode_reward=15.07 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 15.1      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 186500    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 2.6355085 |\n",
      "|    clip_fraction        | 0.16      |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0385   |\n",
      "|    explained_variance   | 0.449     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.101    |\n",
      "|    n_updates            | 13832     |\n",
      "|    policy_gradient_loss | -0.0599   |\n",
      "|    value_loss           | 0.0645    |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 73.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 775      |\n",
      "|    iterations      | 729      |\n",
      "|    time_elapsed    | 240      |\n",
      "|    total_timesteps | 186624   |\n",
      "---------------------------------\n",
      "--------------------------------------\n",
      "| rollout/                |          |\n",
      "|    ep_len_mean          | 889      |\n",
      "|    ep_rew_mean          | 74.2     |\n",
      "| time/                   |          |\n",
      "|    fps                  | 775      |\n",
      "|    iterations           | 730      |\n",
      "|    time_elapsed         | 240      |\n",
      "|    total_timesteps      | 186880   |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 1.091581 |\n",
      "|    clip_fraction        | 0.161    |\n",
      "|    clip_range           | 0.4      |\n",
      "|    entropy_loss         | -0.0546  |\n",
      "|    explained_variance   | 0.761    |\n",
      "|    learning_rate        | 0.000389 |\n",
      "|    loss                 | -0.0726  |\n",
      "|    n_updates            | 13851    |\n",
      "|    policy_gradient_loss | -0.0646  |\n",
      "|    value_loss           | 0.153    |\n",
      "--------------------------------------\n",
      "Eval num_timesteps=187000, episode_reward=21.53 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 21.5      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 187000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.4692324 |\n",
      "|    clip_fraction        | 0.127     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0649   |\n",
      "|    explained_variance   | 0.349     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0256   |\n",
      "|    n_updates            | 13870     |\n",
      "|    policy_gradient_loss | -0.0406   |\n",
      "|    value_loss           | 0.185     |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 74.2     |\n",
      "| time/              |          |\n",
      "|    fps             | 775      |\n",
      "|    iterations      | 731      |\n",
      "|    time_elapsed    | 241      |\n",
      "|    total_timesteps | 187136   |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 74.2      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 775       |\n",
      "|    iterations           | 732       |\n",
      "|    time_elapsed         | 241       |\n",
      "|    total_timesteps      | 187392    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.8315364 |\n",
      "|    clip_fraction        | 0.111     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0397   |\n",
      "|    explained_variance   | 0.699     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0592   |\n",
      "|    n_updates            | 13889     |\n",
      "|    policy_gradient_loss | -0.0516   |\n",
      "|    value_loss           | 0.0408    |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=187500, episode_reward=20.70 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 230        |\n",
      "|    mean_reward          | 20.7       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 187500     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.85121787 |\n",
      "|    clip_fraction        | 0.124      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0553    |\n",
      "|    explained_variance   | 0.211      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.043     |\n",
      "|    n_updates            | 13908      |\n",
      "|    policy_gradient_loss | -0.0441    |\n",
      "|    value_loss           | 0.174      |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 74.5     |\n",
      "| time/              |          |\n",
      "|    fps             | 775      |\n",
      "|    iterations      | 733      |\n",
      "|    time_elapsed    | 242      |\n",
      "|    total_timesteps | 187648   |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 74.5      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 775       |\n",
      "|    iterations           | 734       |\n",
      "|    time_elapsed         | 242       |\n",
      "|    total_timesteps      | 187904    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.6171021 |\n",
      "|    clip_fraction        | 0.132     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0617   |\n",
      "|    explained_variance   | 0.718     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0743   |\n",
      "|    n_updates            | 13927     |\n",
      "|    policy_gradient_loss | -0.0585   |\n",
      "|    value_loss           | 0.276     |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=188000, episode_reward=19.76 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 19.8      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 188000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.7067578 |\n",
      "|    clip_fraction        | 0.107     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0422   |\n",
      "|    explained_variance   | 0.658     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0479   |\n",
      "|    n_updates            | 13946     |\n",
      "|    policy_gradient_loss | -0.0452   |\n",
      "|    value_loss           | 0.0987    |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 74.5     |\n",
      "| time/              |          |\n",
      "|    fps             | 775      |\n",
      "|    iterations      | 735      |\n",
      "|    time_elapsed    | 242      |\n",
      "|    total_timesteps | 188160   |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 74.5      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 775       |\n",
      "|    iterations           | 736       |\n",
      "|    time_elapsed         | 242       |\n",
      "|    total_timesteps      | 188416    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.8323639 |\n",
      "|    clip_fraction        | 0.146     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.05     |\n",
      "|    explained_variance   | 0.147     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0915   |\n",
      "|    n_updates            | 13965     |\n",
      "|    policy_gradient_loss | -0.0492   |\n",
      "|    value_loss           | 0.179     |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=188500, episode_reward=-5.12 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 230        |\n",
      "|    mean_reward          | -5.12      |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 188500     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.34412354 |\n",
      "|    clip_fraction        | 0.0874     |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0498    |\n",
      "|    explained_variance   | 0.097      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | 0.0381     |\n",
      "|    n_updates            | 13984      |\n",
      "|    policy_gradient_loss | -0.0192    |\n",
      "|    value_loss           | 0.987      |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 74.7     |\n",
      "| time/              |          |\n",
      "|    fps             | 775      |\n",
      "|    iterations      | 737      |\n",
      "|    time_elapsed    | 243      |\n",
      "|    total_timesteps | 188672   |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 74.7      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 775       |\n",
      "|    iterations           | 738       |\n",
      "|    time_elapsed         | 243       |\n",
      "|    total_timesteps      | 188928    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.5985974 |\n",
      "|    clip_fraction        | 0.17      |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0665   |\n",
      "|    explained_variance   | 0.637     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0554   |\n",
      "|    n_updates            | 14003     |\n",
      "|    policy_gradient_loss | -0.0514   |\n",
      "|    value_loss           | 0.131     |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=189000, episode_reward=9.86 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 230        |\n",
      "|    mean_reward          | 9.86       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 189000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.98362494 |\n",
      "|    clip_fraction        | 0.147      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0544    |\n",
      "|    explained_variance   | 0.404      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.129     |\n",
      "|    n_updates            | 14022      |\n",
      "|    policy_gradient_loss | -0.0684    |\n",
      "|    value_loss           | 0.1        |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 74.7     |\n",
      "| time/              |          |\n",
      "|    fps             | 775      |\n",
      "|    iterations      | 739      |\n",
      "|    time_elapsed    | 243      |\n",
      "|    total_timesteps | 189184   |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 74.7      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 775       |\n",
      "|    iterations           | 740       |\n",
      "|    time_elapsed         | 244       |\n",
      "|    total_timesteps      | 189440    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.2977263 |\n",
      "|    clip_fraction        | 0.142     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0556   |\n",
      "|    explained_variance   | 0.565     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0592   |\n",
      "|    n_updates            | 14041     |\n",
      "|    policy_gradient_loss | -0.0544   |\n",
      "|    value_loss           | 0.183     |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=189500, episode_reward=1.40 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 1.4       |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 189500    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.4937108 |\n",
      "|    clip_fraction        | 0.114     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0506   |\n",
      "|    explained_variance   | 0.19      |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.00923  |\n",
      "|    n_updates            | 14060     |\n",
      "|    policy_gradient_loss | -0.0309   |\n",
      "|    value_loss           | 0.655     |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 74.7     |\n",
      "| time/              |          |\n",
      "|    fps             | 775      |\n",
      "|    iterations      | 741      |\n",
      "|    time_elapsed    | 244      |\n",
      "|    total_timesteps | 189696   |\n",
      "---------------------------------\n",
      "--------------------------------------\n",
      "| rollout/                |          |\n",
      "|    ep_len_mean          | 889      |\n",
      "|    ep_rew_mean          | 74.7     |\n",
      "| time/                   |          |\n",
      "|    fps                  | 775      |\n",
      "|    iterations           | 742      |\n",
      "|    time_elapsed         | 244      |\n",
      "|    total_timesteps      | 189952   |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 0.736272 |\n",
      "|    clip_fraction        | 0.152    |\n",
      "|    clip_range           | 0.4      |\n",
      "|    entropy_loss         | -0.0629  |\n",
      "|    explained_variance   | 0.252    |\n",
      "|    learning_rate        | 0.000389 |\n",
      "|    loss                 | -0.107   |\n",
      "|    n_updates            | 14079    |\n",
      "|    policy_gradient_loss | -0.0559  |\n",
      "|    value_loss           | 0.113    |\n",
      "--------------------------------------\n",
      "Eval num_timesteps=190000, episode_reward=35.57 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 35.6      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 190000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.6881497 |\n",
      "|    clip_fraction        | 0.127     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0465   |\n",
      "|    explained_variance   | 0.557     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0766   |\n",
      "|    n_updates            | 14098     |\n",
      "|    policy_gradient_loss | -0.0533   |\n",
      "|    value_loss           | 0.107     |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 74.7     |\n",
      "| time/              |          |\n",
      "|    fps             | 775      |\n",
      "|    iterations      | 743      |\n",
      "|    time_elapsed    | 245      |\n",
      "|    total_timesteps | 190208   |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 75         |\n",
      "| time/                   |            |\n",
      "|    fps                  | 775        |\n",
      "|    iterations           | 744        |\n",
      "|    time_elapsed         | 245        |\n",
      "|    total_timesteps      | 190464     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.26746583 |\n",
      "|    clip_fraction        | 0.0929     |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0489    |\n",
      "|    explained_variance   | 0.604      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.00442   |\n",
      "|    n_updates            | 14117      |\n",
      "|    policy_gradient_loss | -0.0372    |\n",
      "|    value_loss           | 0.406      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=190500, episode_reward=18.86 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 18.9      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 190500    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.2801666 |\n",
      "|    clip_fraction        | 0.201     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0725   |\n",
      "|    explained_variance   | 0.484     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | 0.00355   |\n",
      "|    n_updates            | 14136     |\n",
      "|    policy_gradient_loss | -0.0625   |\n",
      "|    value_loss           | 0.162     |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 75       |\n",
      "| time/              |          |\n",
      "|    fps             | 775      |\n",
      "|    iterations      | 745      |\n",
      "|    time_elapsed    | 246      |\n",
      "|    total_timesteps | 190720   |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 75         |\n",
      "| time/                   |            |\n",
      "|    fps                  | 775        |\n",
      "|    iterations           | 746        |\n",
      "|    time_elapsed         | 246        |\n",
      "|    total_timesteps      | 190976     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.28734544 |\n",
      "|    clip_fraction        | 0.103      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0549    |\n",
      "|    explained_variance   | 0.0983     |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.023     |\n",
      "|    n_updates            | 14155      |\n",
      "|    policy_gradient_loss | -0.0455    |\n",
      "|    value_loss           | 0.272      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=191000, episode_reward=21.97 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 22        |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 191000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.0286531 |\n",
      "|    clip_fraction        | 0.159     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0523   |\n",
      "|    explained_variance   | 0.692     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0184   |\n",
      "|    n_updates            | 14174     |\n",
      "|    policy_gradient_loss | -0.0626   |\n",
      "|    value_loss           | 0.0806    |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 75.3     |\n",
      "| time/              |          |\n",
      "|    fps             | 775      |\n",
      "|    iterations      | 747      |\n",
      "|    time_elapsed    | 246      |\n",
      "|    total_timesteps | 191232   |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 75.3       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 775        |\n",
      "|    iterations           | 748        |\n",
      "|    time_elapsed         | 246        |\n",
      "|    total_timesteps      | 191488     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.26920998 |\n",
      "|    clip_fraction        | 0.0853     |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0573    |\n",
      "|    explained_variance   | 0.694      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | 0.00832    |\n",
      "|    n_updates            | 14193      |\n",
      "|    policy_gradient_loss | -0.0275    |\n",
      "|    value_loss           | 0.359      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=191500, episode_reward=16.54 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 16.5      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 191500    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.7393906 |\n",
      "|    clip_fraction        | 0.14      |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0608   |\n",
      "|    explained_variance   | 0.521     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0568   |\n",
      "|    n_updates            | 14212     |\n",
      "|    policy_gradient_loss | -0.0436   |\n",
      "|    value_loss           | 0.0992    |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 75.3     |\n",
      "| time/              |          |\n",
      "|    fps             | 775      |\n",
      "|    iterations      | 749      |\n",
      "|    time_elapsed    | 247      |\n",
      "|    total_timesteps | 191744   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=192000, episode_reward=-2.98 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 230        |\n",
      "|    mean_reward          | -2.98      |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 192000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.73042834 |\n",
      "|    clip_fraction        | 0.126      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0473    |\n",
      "|    explained_variance   | 0.524      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0168    |\n",
      "|    n_updates            | 14231      |\n",
      "|    policy_gradient_loss | -0.059     |\n",
      "|    value_loss           | 0.177      |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 75.3     |\n",
      "| time/              |          |\n",
      "|    fps             | 775      |\n",
      "|    iterations      | 750      |\n",
      "|    time_elapsed    | 247      |\n",
      "|    total_timesteps | 192000   |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 75.6      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 775       |\n",
      "|    iterations           | 751       |\n",
      "|    time_elapsed         | 247       |\n",
      "|    total_timesteps      | 192256    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.5880346 |\n",
      "|    clip_fraction        | 0.0993    |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0522   |\n",
      "|    explained_variance   | 0.491     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | 0.00838   |\n",
      "|    n_updates            | 14250     |\n",
      "|    policy_gradient_loss | -0.0326   |\n",
      "|    value_loss           | 0.363     |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=192500, episode_reward=40.76 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 40.8      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 192500    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.5609303 |\n",
      "|    clip_fraction        | 0.202     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0742   |\n",
      "|    explained_variance   | 0.642     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0727   |\n",
      "|    n_updates            | 14269     |\n",
      "|    policy_gradient_loss | -0.0571   |\n",
      "|    value_loss           | 0.0883    |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 75.6     |\n",
      "| time/              |          |\n",
      "|    fps             | 775      |\n",
      "|    iterations      | 752      |\n",
      "|    time_elapsed    | 248      |\n",
      "|    total_timesteps | 192512   |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 75.6      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 775       |\n",
      "|    iterations           | 753       |\n",
      "|    time_elapsed         | 248       |\n",
      "|    total_timesteps      | 192768    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.2560287 |\n",
      "|    clip_fraction        | 0.127     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0472   |\n",
      "|    explained_variance   | 0.581     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0548   |\n",
      "|    n_updates            | 14288     |\n",
      "|    policy_gradient_loss | -0.0471   |\n",
      "|    value_loss           | 0.161     |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=193000, episode_reward=17.64 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 230        |\n",
      "|    mean_reward          | 17.6       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 193000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.67257136 |\n",
      "|    clip_fraction        | 0.119      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0458    |\n",
      "|    explained_variance   | 0.589      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0453    |\n",
      "|    n_updates            | 14307      |\n",
      "|    policy_gradient_loss | -0.0432    |\n",
      "|    value_loss           | 0.134      |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 76.1     |\n",
      "| time/              |          |\n",
      "|    fps             | 775      |\n",
      "|    iterations      | 754      |\n",
      "|    time_elapsed    | 248      |\n",
      "|    total_timesteps | 193024   |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 76.1      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 775       |\n",
      "|    iterations           | 755       |\n",
      "|    time_elapsed         | 249       |\n",
      "|    total_timesteps      | 193280    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.5377792 |\n",
      "|    clip_fraction        | 0.133     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0674   |\n",
      "|    explained_variance   | 0.681     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | 0.00532   |\n",
      "|    n_updates            | 14326     |\n",
      "|    policy_gradient_loss | -0.0357   |\n",
      "|    value_loss           | 0.454     |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=193500, episode_reward=-4.30 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "--------------------------------------\n",
      "| eval/                   |          |\n",
      "|    mean_ep_length       | 230      |\n",
      "|    mean_reward          | -4.3     |\n",
      "| time/                   |          |\n",
      "|    total_timesteps      | 193500   |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 1.31091  |\n",
      "|    clip_fraction        | 0.184    |\n",
      "|    clip_range           | 0.4      |\n",
      "|    entropy_loss         | -0.0539  |\n",
      "|    explained_variance   | 0.462    |\n",
      "|    learning_rate        | 0.000389 |\n",
      "|    loss                 | -0.0916  |\n",
      "|    n_updates            | 14345    |\n",
      "|    policy_gradient_loss | -0.0556  |\n",
      "|    value_loss           | 0.0732   |\n",
      "--------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 76.1     |\n",
      "| time/              |          |\n",
      "|    fps             | 775      |\n",
      "|    iterations      | 756      |\n",
      "|    time_elapsed    | 249      |\n",
      "|    total_timesteps | 193536   |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 76.1      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 776       |\n",
      "|    iterations           | 757       |\n",
      "|    time_elapsed         | 249       |\n",
      "|    total_timesteps      | 193792    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.9865451 |\n",
      "|    clip_fraction        | 0.129     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0461   |\n",
      "|    explained_variance   | 0.246     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0561   |\n",
      "|    n_updates            | 14364     |\n",
      "|    policy_gradient_loss | -0.0621   |\n",
      "|    value_loss           | 0.254     |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=194000, episode_reward=43.39 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "--------------------------------------\n",
      "| eval/                   |          |\n",
      "|    mean_ep_length       | 230      |\n",
      "|    mean_reward          | 43.4     |\n",
      "| time/                   |          |\n",
      "|    total_timesteps      | 194000   |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 0.910685 |\n",
      "|    clip_fraction        | 0.173    |\n",
      "|    clip_range           | 0.4      |\n",
      "|    entropy_loss         | -0.0572  |\n",
      "|    explained_variance   | 0.709    |\n",
      "|    learning_rate        | 0.000389 |\n",
      "|    loss                 | -0.031   |\n",
      "|    n_updates            | 14383    |\n",
      "|    policy_gradient_loss | -0.0627  |\n",
      "|    value_loss           | 0.243    |\n",
      "--------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 76.2     |\n",
      "| time/              |          |\n",
      "|    fps             | 775      |\n",
      "|    iterations      | 758      |\n",
      "|    time_elapsed    | 250      |\n",
      "|    total_timesteps | 194048   |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 76.2       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 776        |\n",
      "|    iterations           | 759        |\n",
      "|    time_elapsed         | 250        |\n",
      "|    total_timesteps      | 194304     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.91939443 |\n",
      "|    clip_fraction        | 0.18       |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0722    |\n",
      "|    explained_variance   | 0.741      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.00633   |\n",
      "|    n_updates            | 14402      |\n",
      "|    policy_gradient_loss | -0.0447    |\n",
      "|    value_loss           | 0.0746     |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=194500, episode_reward=42.01 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 42        |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 194500    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.4820535 |\n",
      "|    clip_fraction        | 0.115     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0407   |\n",
      "|    explained_variance   | 0.235     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.105    |\n",
      "|    n_updates            | 14421     |\n",
      "|    policy_gradient_loss | -0.0513   |\n",
      "|    value_loss           | 0.147     |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 76.2     |\n",
      "| time/              |          |\n",
      "|    fps             | 775      |\n",
      "|    iterations      | 760      |\n",
      "|    time_elapsed    | 250      |\n",
      "|    total_timesteps | 194560   |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 76.5      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 776       |\n",
      "|    iterations           | 761       |\n",
      "|    time_elapsed         | 250       |\n",
      "|    total_timesteps      | 194816    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.3893113 |\n",
      "|    clip_fraction        | 0.125     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0449   |\n",
      "|    explained_variance   | 0.2       |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0463   |\n",
      "|    n_updates            | 14440     |\n",
      "|    policy_gradient_loss | -0.0463   |\n",
      "|    value_loss           | 0.236     |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=195000, episode_reward=44.34 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 44.3      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 195000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 3.3065548 |\n",
      "|    clip_fraction        | 0.221     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.064    |\n",
      "|    explained_variance   | 0.902     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.00645  |\n",
      "|    n_updates            | 14459     |\n",
      "|    policy_gradient_loss | -0.0504   |\n",
      "|    value_loss           | 0.145     |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 76.5     |\n",
      "| time/              |          |\n",
      "|    fps             | 775      |\n",
      "|    iterations      | 762      |\n",
      "|    time_elapsed    | 251      |\n",
      "|    total_timesteps | 195072   |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 76.5      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 776       |\n",
      "|    iterations           | 763       |\n",
      "|    time_elapsed         | 251       |\n",
      "|    total_timesteps      | 195328    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.1650577 |\n",
      "|    clip_fraction        | 0.12      |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0352   |\n",
      "|    explained_variance   | 0.629     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0597   |\n",
      "|    n_updates            | 14478     |\n",
      "|    policy_gradient_loss | -0.041    |\n",
      "|    value_loss           | 0.0697    |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=195500, episode_reward=37.00 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 230        |\n",
      "|    mean_reward          | 37         |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 195500     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.77659047 |\n",
      "|    clip_fraction        | 0.109      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0381    |\n",
      "|    explained_variance   | 0.425      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0359    |\n",
      "|    n_updates            | 14497      |\n",
      "|    policy_gradient_loss | -0.0423    |\n",
      "|    value_loss           | 0.186      |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 76.7     |\n",
      "| time/              |          |\n",
      "|    fps             | 775      |\n",
      "|    iterations      | 764      |\n",
      "|    time_elapsed    | 252      |\n",
      "|    total_timesteps | 195584   |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 76.7       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 775        |\n",
      "|    iterations           | 765        |\n",
      "|    time_elapsed         | 252        |\n",
      "|    total_timesteps      | 195840     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.36717787 |\n",
      "|    clip_fraction        | 0.0966     |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0493    |\n",
      "|    explained_variance   | 0.762      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | 0.00422    |\n",
      "|    n_updates            | 14516      |\n",
      "|    policy_gradient_loss | -0.0431    |\n",
      "|    value_loss           | 0.26       |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=196000, episode_reward=36.03 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 36        |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 196000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.7908933 |\n",
      "|    clip_fraction        | 0.156     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0626   |\n",
      "|    explained_variance   | 0.627     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0217   |\n",
      "|    n_updates            | 14535     |\n",
      "|    policy_gradient_loss | -0.0506   |\n",
      "|    value_loss           | 0.085     |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 76.7     |\n",
      "| time/              |          |\n",
      "|    fps             | 775      |\n",
      "|    iterations      | 766      |\n",
      "|    time_elapsed    | 252      |\n",
      "|    total_timesteps | 196096   |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 76.7       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 775        |\n",
      "|    iterations           | 767        |\n",
      "|    time_elapsed         | 253        |\n",
      "|    total_timesteps      | 196352     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.84392756 |\n",
      "|    clip_fraction        | 0.111      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0417    |\n",
      "|    explained_variance   | 0.329      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.104     |\n",
      "|    n_updates            | 14554      |\n",
      "|    policy_gradient_loss | -0.0474    |\n",
      "|    value_loss           | 0.135      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=196500, episode_reward=40.75 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 40.8      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 196500    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.4428691 |\n",
      "|    clip_fraction        | 0.168     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0432   |\n",
      "|    explained_variance   | 0.524     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0268   |\n",
      "|    n_updates            | 14573     |\n",
      "|    policy_gradient_loss | -0.0712   |\n",
      "|    value_loss           | 0.0938    |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 76.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 775      |\n",
      "|    iterations      | 768      |\n",
      "|    time_elapsed    | 253      |\n",
      "|    total_timesteps | 196608   |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 76.9       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 776        |\n",
      "|    iterations           | 769        |\n",
      "|    time_elapsed         | 253        |\n",
      "|    total_timesteps      | 196864     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.50698984 |\n",
      "|    clip_fraction        | 0.129      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0602    |\n",
      "|    explained_variance   | 0.861      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0526    |\n",
      "|    n_updates            | 14592      |\n",
      "|    policy_gradient_loss | -0.0424    |\n",
      "|    value_loss           | 0.273      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=197000, episode_reward=38.86 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 230        |\n",
      "|    mean_reward          | 38.9       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 197000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.82278085 |\n",
      "|    clip_fraction        | 0.17       |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0504    |\n",
      "|    explained_variance   | 0.823      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0843    |\n",
      "|    n_updates            | 14611      |\n",
      "|    policy_gradient_loss | -0.0716    |\n",
      "|    value_loss           | 0.0416     |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 76.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 775      |\n",
      "|    iterations      | 770      |\n",
      "|    time_elapsed    | 254      |\n",
      "|    total_timesteps | 197120   |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 77.1       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 775        |\n",
      "|    iterations           | 771        |\n",
      "|    time_elapsed         | 254        |\n",
      "|    total_timesteps      | 197376     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.53315324 |\n",
      "|    clip_fraction        | 0.0993     |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0329    |\n",
      "|    explained_variance   | 0.518      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0124    |\n",
      "|    n_updates            | 14630      |\n",
      "|    policy_gradient_loss | -0.0352    |\n",
      "|    value_loss           | 0.16       |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=197500, episode_reward=40.70 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 230        |\n",
      "|    mean_reward          | 40.7       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 197500     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.53397465 |\n",
      "|    clip_fraction        | 0.14       |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0575    |\n",
      "|    explained_variance   | 0.829      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.00218   |\n",
      "|    n_updates            | 14649      |\n",
      "|    policy_gradient_loss | -0.053     |\n",
      "|    value_loss           | 0.213      |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 77.1     |\n",
      "| time/              |          |\n",
      "|    fps             | 775      |\n",
      "|    iterations      | 772      |\n",
      "|    time_elapsed    | 254      |\n",
      "|    total_timesteps | 197632   |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 77.1       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 775        |\n",
      "|    iterations           | 773        |\n",
      "|    time_elapsed         | 255        |\n",
      "|    total_timesteps      | 197888     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.66567904 |\n",
      "|    clip_fraction        | 0.0946     |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0348    |\n",
      "|    explained_variance   | 0.48       |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | 0.00932    |\n",
      "|    n_updates            | 14668      |\n",
      "|    policy_gradient_loss | -0.0379    |\n",
      "|    value_loss           | 0.141      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=198000, episode_reward=33.98 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 230        |\n",
      "|    mean_reward          | 34         |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 198000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.66170317 |\n",
      "|    clip_fraction        | 0.171      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0572    |\n",
      "|    explained_variance   | 0.391      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0869    |\n",
      "|    n_updates            | 14687      |\n",
      "|    policy_gradient_loss | -0.0747    |\n",
      "|    value_loss           | 0.173      |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 77.1     |\n",
      "| time/              |          |\n",
      "|    fps             | 775      |\n",
      "|    iterations      | 774      |\n",
      "|    time_elapsed    | 255      |\n",
      "|    total_timesteps | 198144   |\n",
      "---------------------------------\n",
      "--------------------------------------\n",
      "| rollout/                |          |\n",
      "|    ep_len_mean          | 889      |\n",
      "|    ep_rew_mean          | 77.3     |\n",
      "| time/                   |          |\n",
      "|    fps                  | 775      |\n",
      "|    iterations           | 775      |\n",
      "|    time_elapsed         | 255      |\n",
      "|    total_timesteps      | 198400   |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 0.552542 |\n",
      "|    clip_fraction        | 0.126    |\n",
      "|    clip_range           | 0.4      |\n",
      "|    entropy_loss         | -0.0425  |\n",
      "|    explained_variance   | 0.866    |\n",
      "|    learning_rate        | 0.000389 |\n",
      "|    loss                 | -0.0578  |\n",
      "|    n_updates            | 14706    |\n",
      "|    policy_gradient_loss | -0.0392  |\n",
      "|    value_loss           | 0.0754   |\n",
      "--------------------------------------\n",
      "Eval num_timesteps=198500, episode_reward=0.18 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 230        |\n",
      "|    mean_reward          | 0.18       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 198500     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.43539685 |\n",
      "|    clip_fraction        | 0.072      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0351    |\n",
      "|    explained_variance   | 0.769      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.021     |\n",
      "|    n_updates            | 14725      |\n",
      "|    policy_gradient_loss | -0.0263    |\n",
      "|    value_loss           | 0.486      |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 77.3     |\n",
      "| time/              |          |\n",
      "|    fps             | 775      |\n",
      "|    iterations      | 776      |\n",
      "|    time_elapsed    | 256      |\n",
      "|    total_timesteps | 198656   |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 77.3       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 775        |\n",
      "|    iterations           | 777        |\n",
      "|    time_elapsed         | 256        |\n",
      "|    total_timesteps      | 198912     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.94460547 |\n",
      "|    clip_fraction        | 0.164      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0483    |\n",
      "|    explained_variance   | 0.782      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0706    |\n",
      "|    n_updates            | 14744      |\n",
      "|    policy_gradient_loss | -0.0661    |\n",
      "|    value_loss           | 0.0413     |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=199000, episode_reward=67.03 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 230        |\n",
      "|    mean_reward          | 67         |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 199000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.71328944 |\n",
      "|    clip_fraction        | 0.0938     |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0315    |\n",
      "|    explained_variance   | 0.628      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.038     |\n",
      "|    n_updates            | 14763      |\n",
      "|    policy_gradient_loss | -0.0403    |\n",
      "|    value_loss           | 0.162      |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 77.3     |\n",
      "| time/              |          |\n",
      "|    fps             | 774      |\n",
      "|    iterations      | 778      |\n",
      "|    time_elapsed    | 257      |\n",
      "|    total_timesteps | 199168   |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 77.3       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 775        |\n",
      "|    iterations           | 779        |\n",
      "|    time_elapsed         | 257        |\n",
      "|    total_timesteps      | 199424     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.41628963 |\n",
      "|    clip_fraction        | 0.0938     |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0486    |\n",
      "|    explained_variance   | -0.184     |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | 0.0378     |\n",
      "|    n_updates            | 14782      |\n",
      "|    policy_gradient_loss | -0.0234    |\n",
      "|    value_loss           | 2.16       |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=199500, episode_reward=49.11 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 49.1      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 199500    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.9860848 |\n",
      "|    clip_fraction        | 0.144     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0582   |\n",
      "|    explained_variance   | 0.145     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0848   |\n",
      "|    n_updates            | 14801     |\n",
      "|    policy_gradient_loss | -0.0536   |\n",
      "|    value_loss           | 0.0997    |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 77.3     |\n",
      "| time/              |          |\n",
      "|    fps             | 775      |\n",
      "|    iterations      | 780      |\n",
      "|    time_elapsed    | 257      |\n",
      "|    total_timesteps | 199680   |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 77.3      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 775       |\n",
      "|    iterations           | 781       |\n",
      "|    time_elapsed         | 257       |\n",
      "|    total_timesteps      | 199936    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.1894224 |\n",
      "|    clip_fraction        | 0.163     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0604   |\n",
      "|    explained_variance   | 0.64      |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0726   |\n",
      "|    n_updates            | 14820     |\n",
      "|    policy_gradient_loss | -0.068    |\n",
      "|    value_loss           | 0.175     |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=200000, episode_reward=31.70 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 230        |\n",
      "|    mean_reward          | 31.7       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 200000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.95002186 |\n",
      "|    clip_fraction        | 0.184      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0625    |\n",
      "|    explained_variance   | 0.664      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0456    |\n",
      "|    n_updates            | 14839      |\n",
      "|    policy_gradient_loss | -0.0689    |\n",
      "|    value_loss           | 0.077      |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 77.3     |\n",
      "| time/              |          |\n",
      "|    fps             | 775      |\n",
      "|    iterations      | 782      |\n",
      "|    time_elapsed    | 258      |\n",
      "|    total_timesteps | 200192   |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 77.3       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 775        |\n",
      "|    iterations           | 783        |\n",
      "|    time_elapsed         | 258        |\n",
      "|    total_timesteps      | 200448     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.19479196 |\n",
      "|    clip_fraction        | 0.0631     |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0419    |\n",
      "|    explained_variance   | 0.164      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | 0.0386     |\n",
      "|    n_updates            | 14858      |\n",
      "|    policy_gradient_loss | -0.0215    |\n",
      "|    value_loss           | 0.752      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=200500, episode_reward=44.37 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 44.4      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 200500    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.7433264 |\n",
      "|    clip_fraction        | 0.133     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0461   |\n",
      "|    explained_variance   | 0.741     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0693   |\n",
      "|    n_updates            | 14877     |\n",
      "|    policy_gradient_loss | -0.0508   |\n",
      "|    value_loss           | 0.1       |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 77.3     |\n",
      "| time/              |          |\n",
      "|    fps             | 775      |\n",
      "|    iterations      | 784      |\n",
      "|    time_elapsed    | 258      |\n",
      "|    total_timesteps | 200704   |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 77.4      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 775       |\n",
      "|    iterations           | 785       |\n",
      "|    time_elapsed         | 259       |\n",
      "|    total_timesteps      | 200960    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.4995083 |\n",
      "|    clip_fraction        | 0.0886    |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0546   |\n",
      "|    explained_variance   | 0.177     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0124   |\n",
      "|    n_updates            | 14896     |\n",
      "|    policy_gradient_loss | -0.0348   |\n",
      "|    value_loss           | 0.231     |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=201000, episode_reward=20.15 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 230        |\n",
      "|    mean_reward          | 20.1       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 201000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.61061084 |\n",
      "|    clip_fraction        | 0.0954     |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0458    |\n",
      "|    explained_variance   | 0.534      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.00736   |\n",
      "|    n_updates            | 14915      |\n",
      "|    policy_gradient_loss | -0.0324    |\n",
      "|    value_loss           | 0.337      |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 77.4     |\n",
      "| time/              |          |\n",
      "|    fps             | 775      |\n",
      "|    iterations      | 786      |\n",
      "|    time_elapsed    | 259      |\n",
      "|    total_timesteps | 201216   |\n",
      "---------------------------------\n",
      "--------------------------------------\n",
      "| rollout/                |          |\n",
      "|    ep_len_mean          | 889      |\n",
      "|    ep_rew_mean          | 77.4     |\n",
      "| time/                   |          |\n",
      "|    fps                  | 775      |\n",
      "|    iterations           | 787      |\n",
      "|    time_elapsed         | 259      |\n",
      "|    total_timesteps      | 201472   |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 0.523004 |\n",
      "|    clip_fraction        | 0.138    |\n",
      "|    clip_range           | 0.4      |\n",
      "|    entropy_loss         | -0.0647  |\n",
      "|    explained_variance   | 0.101    |\n",
      "|    learning_rate        | 0.000389 |\n",
      "|    loss                 | -0.0924  |\n",
      "|    n_updates            | 14934    |\n",
      "|    policy_gradient_loss | -0.051   |\n",
      "|    value_loss           | 0.217    |\n",
      "--------------------------------------\n",
      "Eval num_timesteps=201500, episode_reward=66.86 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 66.9      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 201500    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.5203967 |\n",
      "|    clip_fraction        | 0.117     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.05     |\n",
      "|    explained_variance   | 0.617     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.00238  |\n",
      "|    n_updates            | 14953     |\n",
      "|    policy_gradient_loss | -0.0457   |\n",
      "|    value_loss           | 0.207     |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 77.4     |\n",
      "| time/              |          |\n",
      "|    fps             | 775      |\n",
      "|    iterations      | 788      |\n",
      "|    time_elapsed    | 260      |\n",
      "|    total_timesteps | 201728   |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 77.4       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 775        |\n",
      "|    iterations           | 789        |\n",
      "|    time_elapsed         | 260        |\n",
      "|    total_timesteps      | 201984     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.98290455 |\n",
      "|    clip_fraction        | 0.145      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0481    |\n",
      "|    explained_variance   | 0.831      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.085     |\n",
      "|    n_updates            | 14972      |\n",
      "|    policy_gradient_loss | -0.0528    |\n",
      "|    value_loss           | 0.104      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=202000, episode_reward=71.09 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 71.1      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 202000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.2003745 |\n",
      "|    clip_fraction        | 0.206     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.126    |\n",
      "|    explained_variance   | 0.612     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | 0.0949    |\n",
      "|    n_updates            | 14991     |\n",
      "|    policy_gradient_loss | -0.0489   |\n",
      "|    value_loss           | 0.292     |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 77.4     |\n",
      "| time/              |          |\n",
      "|    fps             | 775      |\n",
      "|    iterations      | 790      |\n",
      "|    time_elapsed    | 260      |\n",
      "|    total_timesteps | 202240   |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 77.4      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 775       |\n",
      "|    iterations           | 791       |\n",
      "|    time_elapsed         | 260       |\n",
      "|    total_timesteps      | 202496    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.7841592 |\n",
      "|    clip_fraction        | 0.173     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0717   |\n",
      "|    explained_variance   | 0.839     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.105    |\n",
      "|    n_updates            | 15010     |\n",
      "|    policy_gradient_loss | -0.0517   |\n",
      "|    value_loss           | 0.0623    |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=202500, episode_reward=57.48 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 57.5      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 202500    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.5997895 |\n",
      "|    clip_fraction        | 0.122     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0508   |\n",
      "|    explained_variance   | 0.252     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | 0.0138    |\n",
      "|    n_updates            | 15029     |\n",
      "|    policy_gradient_loss | -0.0491   |\n",
      "|    value_loss           | 0.324     |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 77.3     |\n",
      "| time/              |          |\n",
      "|    fps             | 775      |\n",
      "|    iterations      | 792      |\n",
      "|    time_elapsed    | 261      |\n",
      "|    total_timesteps | 202752   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=203000, episode_reward=42.75 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 42.7      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 203000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.5074402 |\n",
      "|    clip_fraction        | 0.102     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0766   |\n",
      "|    explained_variance   | -0.111    |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | 0.108     |\n",
      "|    n_updates            | 15048     |\n",
      "|    policy_gradient_loss | -0.0252   |\n",
      "|    value_loss           | 1.36      |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 77.3     |\n",
      "| time/              |          |\n",
      "|    fps             | 774      |\n",
      "|    iterations      | 793      |\n",
      "|    time_elapsed    | 261      |\n",
      "|    total_timesteps | 203008   |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 77.3      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 775       |\n",
      "|    iterations           | 794       |\n",
      "|    time_elapsed         | 262       |\n",
      "|    total_timesteps      | 203264    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.6772208 |\n",
      "|    clip_fraction        | 0.154     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0696   |\n",
      "|    explained_variance   | 0.401     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.073    |\n",
      "|    n_updates            | 15067     |\n",
      "|    policy_gradient_loss | -0.0565   |\n",
      "|    value_loss           | 0.0812    |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=203500, episode_reward=50.29 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "--------------------------------------\n",
      "| eval/                   |          |\n",
      "|    mean_ep_length       | 230      |\n",
      "|    mean_reward          | 50.3     |\n",
      "| time/                   |          |\n",
      "|    total_timesteps      | 203500   |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 4.412833 |\n",
      "|    clip_fraction        | 0.172    |\n",
      "|    clip_range           | 0.4      |\n",
      "|    entropy_loss         | -0.0743  |\n",
      "|    explained_variance   | 0.645    |\n",
      "|    learning_rate        | 0.000389 |\n",
      "|    loss                 | -0.0481  |\n",
      "|    n_updates            | 15086    |\n",
      "|    policy_gradient_loss | -0.0645  |\n",
      "|    value_loss           | 0.379    |\n",
      "--------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 77.3     |\n",
      "| time/              |          |\n",
      "|    fps             | 774      |\n",
      "|    iterations      | 795      |\n",
      "|    time_elapsed    | 262      |\n",
      "|    total_timesteps | 203520   |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 77.2       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 775        |\n",
      "|    iterations           | 796        |\n",
      "|    time_elapsed         | 262        |\n",
      "|    total_timesteps      | 203776     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.49673247 |\n",
      "|    clip_fraction        | 0.109      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0529    |\n",
      "|    explained_variance   | 0.233      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.082     |\n",
      "|    n_updates            | 15105      |\n",
      "|    policy_gradient_loss | -0.0441    |\n",
      "|    value_loss           | 0.169      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=204000, episode_reward=40.87 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 230        |\n",
      "|    mean_reward          | 40.9       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 204000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.53129613 |\n",
      "|    clip_fraction        | 0.14       |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0774    |\n",
      "|    explained_variance   | 0.166      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.125     |\n",
      "|    n_updates            | 15124      |\n",
      "|    policy_gradient_loss | -0.0474    |\n",
      "|    value_loss           | 0.496      |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 77.2     |\n",
      "| time/              |          |\n",
      "|    fps             | 774      |\n",
      "|    iterations      | 797      |\n",
      "|    time_elapsed    | 263      |\n",
      "|    total_timesteps | 204032   |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 77.2      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 774       |\n",
      "|    iterations           | 798       |\n",
      "|    time_elapsed         | 263       |\n",
      "|    total_timesteps      | 204288    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.2930518 |\n",
      "|    clip_fraction        | 0.167     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0577   |\n",
      "|    explained_variance   | 0.809     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.147    |\n",
      "|    n_updates            | 15143     |\n",
      "|    policy_gradient_loss | -0.0671   |\n",
      "|    value_loss           | 0.0766    |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=204500, episode_reward=40.67 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 40.7      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 204500    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.2143197 |\n",
      "|    clip_fraction        | 0.194     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0806   |\n",
      "|    explained_variance   | 0.599     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0603   |\n",
      "|    n_updates            | 15162     |\n",
      "|    policy_gradient_loss | -0.0705   |\n",
      "|    value_loss           | 0.198     |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 77.3     |\n",
      "| time/              |          |\n",
      "|    fps             | 774      |\n",
      "|    iterations      | 799      |\n",
      "|    time_elapsed    | 263      |\n",
      "|    total_timesteps | 204544   |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 77.3      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 774       |\n",
      "|    iterations           | 800       |\n",
      "|    time_elapsed         | 264       |\n",
      "|    total_timesteps      | 204800    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.9550978 |\n",
      "|    clip_fraction        | 0.162     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0864   |\n",
      "|    explained_variance   | 0.28      |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.00645  |\n",
      "|    n_updates            | 15181     |\n",
      "|    policy_gradient_loss | -0.0357   |\n",
      "|    value_loss           | 0.286     |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=205000, episode_reward=22.54 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 230        |\n",
      "|    mean_reward          | 22.5       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 205000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.86274105 |\n",
      "|    clip_fraction        | 0.126      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0563    |\n",
      "|    explained_variance   | 0.216      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0757    |\n",
      "|    n_updates            | 15200      |\n",
      "|    policy_gradient_loss | -0.056     |\n",
      "|    value_loss           | 0.0567     |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 77.3     |\n",
      "| time/              |          |\n",
      "|    fps             | 774      |\n",
      "|    iterations      | 801      |\n",
      "|    time_elapsed    | 264      |\n",
      "|    total_timesteps | 205056   |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 77.3      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 774       |\n",
      "|    iterations           | 802       |\n",
      "|    time_elapsed         | 265       |\n",
      "|    total_timesteps      | 205312    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.8392476 |\n",
      "|    clip_fraction        | 0.171     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0626   |\n",
      "|    explained_variance   | 0.673     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0866   |\n",
      "|    n_updates            | 15219     |\n",
      "|    policy_gradient_loss | -0.0596   |\n",
      "|    value_loss           | 0.208     |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=205500, episode_reward=46.21 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 46.2      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 205500    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.4073184 |\n",
      "|    clip_fraction        | 0.191     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0752   |\n",
      "|    explained_variance   | 0.691     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.169    |\n",
      "|    n_updates            | 15238     |\n",
      "|    policy_gradient_loss | -0.0719   |\n",
      "|    value_loss           | 0.166     |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 77.4     |\n",
      "| time/              |          |\n",
      "|    fps             | 774      |\n",
      "|    iterations      | 803      |\n",
      "|    time_elapsed    | 265      |\n",
      "|    total_timesteps | 205568   |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 77.4      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 774       |\n",
      "|    iterations           | 804       |\n",
      "|    time_elapsed         | 265       |\n",
      "|    total_timesteps      | 205824    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.3154407 |\n",
      "|    clip_fraction        | 0.0857    |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0709   |\n",
      "|    explained_variance   | 0.456     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | 0.0187    |\n",
      "|    n_updates            | 15257     |\n",
      "|    policy_gradient_loss | -0.0257   |\n",
      "|    value_loss           | 0.135     |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=206000, episode_reward=61.32 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 230        |\n",
      "|    mean_reward          | 61.3       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 206000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.44706085 |\n",
      "|    clip_fraction        | 0.116      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0443    |\n",
      "|    explained_variance   | 0.794      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0904    |\n",
      "|    n_updates            | 15276      |\n",
      "|    policy_gradient_loss | -0.0244    |\n",
      "|    value_loss           | 0.105      |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 77.4     |\n",
      "| time/              |          |\n",
      "|    fps             | 773      |\n",
      "|    iterations      | 805      |\n",
      "|    time_elapsed    | 266      |\n",
      "|    total_timesteps | 206080   |\n",
      "---------------------------------\n",
      "--------------------------------------\n",
      "| rollout/                |          |\n",
      "|    ep_len_mean          | 889      |\n",
      "|    ep_rew_mean          | 77.4     |\n",
      "| time/                   |          |\n",
      "|    fps                  | 774      |\n",
      "|    iterations           | 806      |\n",
      "|    time_elapsed         | 266      |\n",
      "|    total_timesteps      | 206336   |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 1.303654 |\n",
      "|    clip_fraction        | 0.182    |\n",
      "|    clip_range           | 0.4      |\n",
      "|    entropy_loss         | -0.0683  |\n",
      "|    explained_variance   | 0.293    |\n",
      "|    learning_rate        | 0.000389 |\n",
      "|    loss                 | -0.112   |\n",
      "|    n_updates            | 15295    |\n",
      "|    policy_gradient_loss | -0.0571  |\n",
      "|    value_loss           | 0.118    |\n",
      "--------------------------------------\n",
      "Eval num_timesteps=206500, episode_reward=51.30 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 51.3      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 206500    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.1754944 |\n",
      "|    clip_fraction        | 0.206     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0755   |\n",
      "|    explained_variance   | 0.841     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0549   |\n",
      "|    n_updates            | 15314     |\n",
      "|    policy_gradient_loss | -0.0697   |\n",
      "|    value_loss           | 0.128     |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 77.4     |\n",
      "| time/              |          |\n",
      "|    fps             | 774      |\n",
      "|    iterations      | 807      |\n",
      "|    time_elapsed    | 266      |\n",
      "|    total_timesteps | 206592   |\n",
      "---------------------------------\n",
      "--------------------------------------\n",
      "| rollout/                |          |\n",
      "|    ep_len_mean          | 889      |\n",
      "|    ep_rew_mean          | 77.4     |\n",
      "| time/                   |          |\n",
      "|    fps                  | 774      |\n",
      "|    iterations           | 808      |\n",
      "|    time_elapsed         | 267      |\n",
      "|    total_timesteps      | 206848   |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 0.851995 |\n",
      "|    clip_fraction        | 0.145    |\n",
      "|    clip_range           | 0.4      |\n",
      "|    entropy_loss         | -0.058   |\n",
      "|    explained_variance   | 0.498    |\n",
      "|    learning_rate        | 0.000389 |\n",
      "|    loss                 | -0.0891  |\n",
      "|    n_updates            | 15333    |\n",
      "|    policy_gradient_loss | -0.0552  |\n",
      "|    value_loss           | 0.0845   |\n",
      "--------------------------------------\n",
      "Eval num_timesteps=207000, episode_reward=41.85 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "--------------------------------------\n",
      "| eval/                   |          |\n",
      "|    mean_ep_length       | 230      |\n",
      "|    mean_reward          | 41.9     |\n",
      "| time/                   |          |\n",
      "|    total_timesteps      | 207000   |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 1.012485 |\n",
      "|    clip_fraction        | 0.161    |\n",
      "|    clip_range           | 0.4      |\n",
      "|    entropy_loss         | -0.0549  |\n",
      "|    explained_variance   | 0.756    |\n",
      "|    learning_rate        | 0.000389 |\n",
      "|    loss                 | -0.0486  |\n",
      "|    n_updates            | 15352    |\n",
      "|    policy_gradient_loss | -0.0715  |\n",
      "|    value_loss           | 0.123    |\n",
      "--------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 77.4     |\n",
      "| time/              |          |\n",
      "|    fps             | 774      |\n",
      "|    iterations      | 809      |\n",
      "|    time_elapsed    | 267      |\n",
      "|    total_timesteps | 207104   |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 77.3      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 774       |\n",
      "|    iterations           | 810       |\n",
      "|    time_elapsed         | 267       |\n",
      "|    total_timesteps      | 207360    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.9714328 |\n",
      "|    clip_fraction        | 0.166     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0747   |\n",
      "|    explained_variance   | 0.85      |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0533   |\n",
      "|    n_updates            | 15371     |\n",
      "|    policy_gradient_loss | -0.0643   |\n",
      "|    value_loss           | 0.0993    |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=207500, episode_reward=77.00 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 77        |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 207500    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.7954608 |\n",
      "|    clip_fraction        | 0.194     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.12     |\n",
      "|    explained_variance   | 0.5       |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0678   |\n",
      "|    n_updates            | 15390     |\n",
      "|    policy_gradient_loss | -0.053    |\n",
      "|    value_loss           | 0.208     |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 77.3     |\n",
      "| time/              |          |\n",
      "|    fps             | 774      |\n",
      "|    iterations      | 811      |\n",
      "|    time_elapsed    | 268      |\n",
      "|    total_timesteps | 207616   |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 77.3       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 774        |\n",
      "|    iterations           | 812        |\n",
      "|    time_elapsed         | 268        |\n",
      "|    total_timesteps      | 207872     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.42195565 |\n",
      "|    clip_fraction        | 0.0999     |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0455    |\n",
      "|    explained_variance   | 0.609      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0163    |\n",
      "|    n_updates            | 15409      |\n",
      "|    policy_gradient_loss | -0.0325    |\n",
      "|    value_loss           | 0.123      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=208000, episode_reward=50.00 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 50        |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 208000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.5484228 |\n",
      "|    clip_fraction        | 0.202     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.068    |\n",
      "|    explained_variance   | 0.739     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.124    |\n",
      "|    n_updates            | 15428     |\n",
      "|    policy_gradient_loss | -0.0787   |\n",
      "|    value_loss           | 0.0729    |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 77.2     |\n",
      "| time/              |          |\n",
      "|    fps             | 774      |\n",
      "|    iterations      | 813      |\n",
      "|    time_elapsed    | 268      |\n",
      "|    total_timesteps | 208128   |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 77.2      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 774       |\n",
      "|    iterations           | 814       |\n",
      "|    time_elapsed         | 268       |\n",
      "|    total_timesteps      | 208384    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.5422307 |\n",
      "|    clip_fraction        | 0.144     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.114    |\n",
      "|    explained_variance   | 0.649     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0384   |\n",
      "|    n_updates            | 15447     |\n",
      "|    policy_gradient_loss | -0.0385   |\n",
      "|    value_loss           | 0.257     |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=208500, episode_reward=37.85 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 230        |\n",
      "|    mean_reward          | 37.8       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 208500     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.62456214 |\n",
      "|    clip_fraction        | 0.176      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0765    |\n",
      "|    explained_variance   | -0.0405    |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.103     |\n",
      "|    n_updates            | 15466      |\n",
      "|    policy_gradient_loss | -0.0571    |\n",
      "|    value_loss           | 0.0537     |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 77.2     |\n",
      "| time/              |          |\n",
      "|    fps             | 774      |\n",
      "|    iterations      | 815      |\n",
      "|    time_elapsed    | 269      |\n",
      "|    total_timesteps | 208640   |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 77.2       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 774        |\n",
      "|    iterations           | 816        |\n",
      "|    time_elapsed         | 269        |\n",
      "|    total_timesteps      | 208896     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.56870157 |\n",
      "|    clip_fraction        | 0.132      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0465    |\n",
      "|    explained_variance   | 0.761      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.111     |\n",
      "|    n_updates            | 15485      |\n",
      "|    policy_gradient_loss | -0.0553    |\n",
      "|    value_loss           | 0.108      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=209000, episode_reward=76.90 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 76.9      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 209000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.7896712 |\n",
      "|    clip_fraction        | 0.142     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0666   |\n",
      "|    explained_variance   | 0.668     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0394   |\n",
      "|    n_updates            | 15504     |\n",
      "|    policy_gradient_loss | -0.0484   |\n",
      "|    value_loss           | 0.181     |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 77.2     |\n",
      "| time/              |          |\n",
      "|    fps             | 774      |\n",
      "|    iterations      | 817      |\n",
      "|    time_elapsed    | 269      |\n",
      "|    total_timesteps | 209152   |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 77.2       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 774        |\n",
      "|    iterations           | 818        |\n",
      "|    time_elapsed         | 270        |\n",
      "|    total_timesteps      | 209408     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.33031097 |\n",
      "|    clip_fraction        | 0.113      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0718    |\n",
      "|    explained_variance   | -0.0295    |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0452    |\n",
      "|    n_updates            | 15523      |\n",
      "|    policy_gradient_loss | -0.0485    |\n",
      "|    value_loss           | 0.0455     |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=209500, episode_reward=44.76 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 230        |\n",
      "|    mean_reward          | 44.8       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 209500     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.61594063 |\n",
      "|    clip_fraction        | 0.133      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.068     |\n",
      "|    explained_variance   | 0.713      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.105     |\n",
      "|    n_updates            | 15542      |\n",
      "|    policy_gradient_loss | -0.0622    |\n",
      "|    value_loss           | 0.132      |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 77.2     |\n",
      "| time/              |          |\n",
      "|    fps             | 774      |\n",
      "|    iterations      | 819      |\n",
      "|    time_elapsed    | 270      |\n",
      "|    total_timesteps | 209664   |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 76.9      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 775       |\n",
      "|    iterations           | 820       |\n",
      "|    time_elapsed         | 270       |\n",
      "|    total_timesteps      | 209920    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.5926237 |\n",
      "|    clip_fraction        | 0.18      |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0576   |\n",
      "|    explained_variance   | 0.689     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0529   |\n",
      "|    n_updates            | 15561     |\n",
      "|    policy_gradient_loss | -0.0809   |\n",
      "|    value_loss           | 0.0914    |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=210000, episode_reward=69.14 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 69.1      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 210000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.8775824 |\n",
      "|    clip_fraction        | 0.276     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.135    |\n",
      "|    explained_variance   | 0.674     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.00312  |\n",
      "|    n_updates            | 15580     |\n",
      "|    policy_gradient_loss | -0.0645   |\n",
      "|    value_loss           | 0.182     |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 76.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 774      |\n",
      "|    iterations      | 821      |\n",
      "|    time_elapsed    | 271      |\n",
      "|    total_timesteps | 210176   |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 76.9       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 774        |\n",
      "|    iterations           | 822        |\n",
      "|    time_elapsed         | 271        |\n",
      "|    total_timesteps      | 210432     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.59320724 |\n",
      "|    clip_fraction        | 0.148      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0684    |\n",
      "|    explained_variance   | 0.637      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.112     |\n",
      "|    n_updates            | 15599      |\n",
      "|    policy_gradient_loss | -0.0563    |\n",
      "|    value_loss           | 0.0355     |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=210500, episode_reward=31.26 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "--------------------------------------\n",
      "| eval/                   |          |\n",
      "|    mean_ep_length       | 230      |\n",
      "|    mean_reward          | 31.3     |\n",
      "| time/                   |          |\n",
      "|    total_timesteps      | 210500   |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 0.644466 |\n",
      "|    clip_fraction        | 0.169    |\n",
      "|    clip_range           | 0.4      |\n",
      "|    entropy_loss         | -0.0805  |\n",
      "|    explained_variance   | 0.514    |\n",
      "|    learning_rate        | 0.000389 |\n",
      "|    loss                 | -0.119   |\n",
      "|    n_updates            | 15618    |\n",
      "|    policy_gradient_loss | -0.0596  |\n",
      "|    value_loss           | 0.256    |\n",
      "--------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 76.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 774      |\n",
      "|    iterations      | 823      |\n",
      "|    time_elapsed    | 271      |\n",
      "|    total_timesteps | 210688   |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 76.9      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 775       |\n",
      "|    iterations           | 824       |\n",
      "|    time_elapsed         | 272       |\n",
      "|    total_timesteps      | 210944    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.3443163 |\n",
      "|    clip_fraction        | 0.106     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0669   |\n",
      "|    explained_variance   | 0.573     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | 0.000679  |\n",
      "|    n_updates            | 15637     |\n",
      "|    policy_gradient_loss | -0.048    |\n",
      "|    value_loss           | 0.313     |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=211000, episode_reward=78.16 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 230        |\n",
      "|    mean_reward          | 78.2       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 211000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.66417646 |\n",
      "|    clip_fraction        | 0.22       |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.12      |\n",
      "|    explained_variance   | 0.281      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.106     |\n",
      "|    n_updates            | 15656      |\n",
      "|    policy_gradient_loss | -0.0809    |\n",
      "|    value_loss           | 0.035      |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 76.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 774      |\n",
      "|    iterations      | 825      |\n",
      "|    time_elapsed    | 272      |\n",
      "|    total_timesteps | 211200   |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 76.9       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 775        |\n",
      "|    iterations           | 826        |\n",
      "|    time_elapsed         | 272        |\n",
      "|    total_timesteps      | 211456     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.68735635 |\n",
      "|    clip_fraction        | 0.156      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0742    |\n",
      "|    explained_variance   | 0.624      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0839    |\n",
      "|    n_updates            | 15675      |\n",
      "|    policy_gradient_loss | -0.0701    |\n",
      "|    value_loss           | 0.169      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=211500, episode_reward=72.05 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 72        |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 211500    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.8773435 |\n",
      "|    clip_fraction        | 0.191     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0819   |\n",
      "|    explained_variance   | 0.741     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.054    |\n",
      "|    n_updates            | 15694     |\n",
      "|    policy_gradient_loss | -0.0742   |\n",
      "|    value_loss           | 0.0851    |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 77.1     |\n",
      "| time/              |          |\n",
      "|    fps             | 774      |\n",
      "|    iterations      | 827      |\n",
      "|    time_elapsed    | 273      |\n",
      "|    total_timesteps | 211712   |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 77.1       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 774        |\n",
      "|    iterations           | 828        |\n",
      "|    time_elapsed         | 273        |\n",
      "|    total_timesteps      | 211968     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.46634823 |\n",
      "|    clip_fraction        | 0.179      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.129     |\n",
      "|    explained_variance   | 0.838      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0531    |\n",
      "|    n_updates            | 15713      |\n",
      "|    policy_gradient_loss | -0.0599    |\n",
      "|    value_loss           | 0.124      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=212000, episode_reward=59.42 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 230        |\n",
      "|    mean_reward          | 59.4       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 212000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.59984386 |\n",
      "|    clip_fraction        | 0.167      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0704    |\n",
      "|    explained_variance   | 0.688      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0582    |\n",
      "|    n_updates            | 15732      |\n",
      "|    policy_gradient_loss | -0.0666    |\n",
      "|    value_loss           | 0.0322     |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 77.1     |\n",
      "| time/              |          |\n",
      "|    fps             | 774      |\n",
      "|    iterations      | 829      |\n",
      "|    time_elapsed    | 273      |\n",
      "|    total_timesteps | 212224   |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 77.3       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 775        |\n",
      "|    iterations           | 830        |\n",
      "|    time_elapsed         | 274        |\n",
      "|    total_timesteps      | 212480     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.92300403 |\n",
      "|    clip_fraction        | 0.206      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.08      |\n",
      "|    explained_variance   | 0.68       |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0826    |\n",
      "|    n_updates            | 15751      |\n",
      "|    policy_gradient_loss | -0.0887    |\n",
      "|    value_loss           | 0.136      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=212500, episode_reward=83.23 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 83.2      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 212500    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.6115769 |\n",
      "|    clip_fraction        | 0.179     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0906   |\n",
      "|    explained_variance   | 0.83      |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.069    |\n",
      "|    n_updates            | 15770     |\n",
      "|    policy_gradient_loss | -0.0603   |\n",
      "|    value_loss           | 0.154     |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 77.3     |\n",
      "| time/              |          |\n",
      "|    fps             | 775      |\n",
      "|    iterations      | 831      |\n",
      "|    time_elapsed    | 274      |\n",
      "|    total_timesteps | 212736   |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 77.3      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 775       |\n",
      "|    iterations           | 832       |\n",
      "|    time_elapsed         | 274       |\n",
      "|    total_timesteps      | 212992    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.9410398 |\n",
      "|    clip_fraction        | 0.225     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.137    |\n",
      "|    explained_variance   | 0.145     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0729   |\n",
      "|    n_updates            | 15789     |\n",
      "|    policy_gradient_loss | -0.0758   |\n",
      "|    value_loss           | 0.0421    |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=213000, episode_reward=57.95 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 57.9      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 213000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.8794322 |\n",
      "|    clip_fraction        | 0.171     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0726   |\n",
      "|    explained_variance   | 0.724     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | 0.00298   |\n",
      "|    n_updates            | 15808     |\n",
      "|    policy_gradient_loss | -0.0583   |\n",
      "|    value_loss           | 0.158     |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 77.3     |\n",
      "| time/              |          |\n",
      "|    fps             | 774      |\n",
      "|    iterations      | 833      |\n",
      "|    time_elapsed    | 275      |\n",
      "|    total_timesteps | 213248   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=213500, episode_reward=63.24 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 63.2      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 213500    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.9908677 |\n",
      "|    clip_fraction        | 0.176     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0737   |\n",
      "|    explained_variance   | 0.579     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.00818  |\n",
      "|    n_updates            | 15827     |\n",
      "|    policy_gradient_loss | -0.0581   |\n",
      "|    value_loss           | 0.0955    |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 77.4     |\n",
      "| time/              |          |\n",
      "|    fps             | 774      |\n",
      "|    iterations      | 834      |\n",
      "|    time_elapsed    | 275      |\n",
      "|    total_timesteps | 213504   |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 77.4      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 774       |\n",
      "|    iterations           | 835       |\n",
      "|    time_elapsed         | 275       |\n",
      "|    total_timesteps      | 213760    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.7404705 |\n",
      "|    clip_fraction        | 0.228     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.123    |\n",
      "|    explained_variance   | 0.676     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0205   |\n",
      "|    n_updates            | 15846     |\n",
      "|    policy_gradient_loss | -0.0457   |\n",
      "|    value_loss           | 0.502     |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=214000, episode_reward=71.47 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 71.5      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 214000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.5280615 |\n",
      "|    clip_fraction        | 0.128     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0551   |\n",
      "|    explained_variance   | 0.742     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0551   |\n",
      "|    n_updates            | 15865     |\n",
      "|    policy_gradient_loss | -0.0414   |\n",
      "|    value_loss           | 0.04      |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 77.4     |\n",
      "| time/              |          |\n",
      "|    fps             | 774      |\n",
      "|    iterations      | 836      |\n",
      "|    time_elapsed    | 276      |\n",
      "|    total_timesteps | 214016   |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 77.5      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 774       |\n",
      "|    iterations           | 837       |\n",
      "|    time_elapsed         | 276       |\n",
      "|    total_timesteps      | 214272    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.5258843 |\n",
      "|    clip_fraction        | 0.144     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0612   |\n",
      "|    explained_variance   | 0.746     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.124    |\n",
      "|    n_updates            | 15884     |\n",
      "|    policy_gradient_loss | -0.0687   |\n",
      "|    value_loss           | 0.135     |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=214500, episode_reward=50.55 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 50.5      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 214500    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.5891999 |\n",
      "|    clip_fraction        | 0.183     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0963   |\n",
      "|    explained_variance   | 0.669     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0608   |\n",
      "|    n_updates            | 15903     |\n",
      "|    policy_gradient_loss | -0.07     |\n",
      "|    value_loss           | 0.354     |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 77.5     |\n",
      "| time/              |          |\n",
      "|    fps             | 774      |\n",
      "|    iterations      | 838      |\n",
      "|    time_elapsed    | 277      |\n",
      "|    total_timesteps | 214528   |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 77.5      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 774       |\n",
      "|    iterations           | 839       |\n",
      "|    time_elapsed         | 277       |\n",
      "|    total_timesteps      | 214784    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.8432413 |\n",
      "|    clip_fraction        | 0.192     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.122    |\n",
      "|    explained_variance   | 0.271     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0336   |\n",
      "|    n_updates            | 15922     |\n",
      "|    policy_gradient_loss | -0.0653   |\n",
      "|    value_loss           | 0.0401    |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=215000, episode_reward=62.17 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 230        |\n",
      "|    mean_reward          | 62.2       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 215000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.65667677 |\n",
      "|    clip_fraction        | 0.142      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0592    |\n",
      "|    explained_variance   | 0.86       |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.105     |\n",
      "|    n_updates            | 15941      |\n",
      "|    policy_gradient_loss | -0.0637    |\n",
      "|    value_loss           | 0.0988     |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 77.5     |\n",
      "| time/              |          |\n",
      "|    fps             | 774      |\n",
      "|    iterations      | 840      |\n",
      "|    time_elapsed    | 277      |\n",
      "|    total_timesteps | 215040   |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 77.7      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 774       |\n",
      "|    iterations           | 841       |\n",
      "|    time_elapsed         | 277       |\n",
      "|    total_timesteps      | 215296    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.1129986 |\n",
      "|    clip_fraction        | 0.188     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0699   |\n",
      "|    explained_variance   | 0.744     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.107    |\n",
      "|    n_updates            | 15960     |\n",
      "|    policy_gradient_loss | -0.0793   |\n",
      "|    value_loss           | 0.0739    |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=215500, episode_reward=54.62 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 230        |\n",
      "|    mean_reward          | 54.6       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 215500     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.64250755 |\n",
      "|    clip_fraction        | 0.182      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.139     |\n",
      "|    explained_variance   | 0.643      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0542    |\n",
      "|    n_updates            | 15979      |\n",
      "|    policy_gradient_loss | -0.0547    |\n",
      "|    value_loss           | 0.295      |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 77.7     |\n",
      "| time/              |          |\n",
      "|    fps             | 774      |\n",
      "|    iterations      | 842      |\n",
      "|    time_elapsed    | 278      |\n",
      "|    total_timesteps | 215552   |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 77.7       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 774        |\n",
      "|    iterations           | 843        |\n",
      "|    time_elapsed         | 278        |\n",
      "|    total_timesteps      | 215808     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.87794554 |\n",
      "|    clip_fraction        | 0.148      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0484    |\n",
      "|    explained_variance   | 0.918      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.135     |\n",
      "|    n_updates            | 15998      |\n",
      "|    policy_gradient_loss | -0.0634    |\n",
      "|    value_loss           | 0.037      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=216000, episode_reward=45.51 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 45.5      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 216000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.4530697 |\n",
      "|    clip_fraction        | 0.129     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0717   |\n",
      "|    explained_variance   | 0.495     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0273   |\n",
      "|    n_updates            | 16017     |\n",
      "|    policy_gradient_loss | -0.0494   |\n",
      "|    value_loss           | 0.113     |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 77.7     |\n",
      "| time/              |          |\n",
      "|    fps             | 774      |\n",
      "|    iterations      | 844      |\n",
      "|    time_elapsed    | 278      |\n",
      "|    total_timesteps | 216064   |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 77.7      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 774       |\n",
      "|    iterations           | 845       |\n",
      "|    time_elapsed         | 279       |\n",
      "|    total_timesteps      | 216320    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.3974675 |\n",
      "|    clip_fraction        | 0.145     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0745   |\n",
      "|    explained_variance   | 0.57      |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0365   |\n",
      "|    n_updates            | 16036     |\n",
      "|    policy_gradient_loss | -0.0565   |\n",
      "|    value_loss           | 0.292     |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=216500, episode_reward=48.37 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 230        |\n",
      "|    mean_reward          | 48.4       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 216500     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.67903596 |\n",
      "|    clip_fraction        | 0.199      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0953    |\n",
      "|    explained_variance   | -0.0418    |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0777    |\n",
      "|    n_updates            | 16055      |\n",
      "|    policy_gradient_loss | -0.0648    |\n",
      "|    value_loss           | 0.0399     |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 77.7     |\n",
      "| time/              |          |\n",
      "|    fps             | 774      |\n",
      "|    iterations      | 846      |\n",
      "|    time_elapsed    | 279      |\n",
      "|    total_timesteps | 216576   |\n",
      "---------------------------------\n",
      "--------------------------------------\n",
      "| rollout/                |          |\n",
      "|    ep_len_mean          | 889      |\n",
      "|    ep_rew_mean          | 77.7     |\n",
      "| time/                   |          |\n",
      "|    fps                  | 774      |\n",
      "|    iterations           | 847      |\n",
      "|    time_elapsed         | 279      |\n",
      "|    total_timesteps      | 216832   |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 0.826215 |\n",
      "|    clip_fraction        | 0.143    |\n",
      "|    clip_range           | 0.4      |\n",
      "|    entropy_loss         | -0.0657  |\n",
      "|    explained_variance   | 0.756    |\n",
      "|    learning_rate        | 0.000389 |\n",
      "|    loss                 | -0.0758  |\n",
      "|    n_updates            | 16074    |\n",
      "|    policy_gradient_loss | -0.0612  |\n",
      "|    value_loss           | 0.151    |\n",
      "--------------------------------------\n",
      "Eval num_timesteps=217000, episode_reward=29.52 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 29.5      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 217000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.3289319 |\n",
      "|    clip_fraction        | 0.162     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0597   |\n",
      "|    explained_variance   | 0.76      |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0599   |\n",
      "|    n_updates            | 16093     |\n",
      "|    policy_gradient_loss | -0.0619   |\n",
      "|    value_loss           | 0.106     |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 77.8     |\n",
      "| time/              |          |\n",
      "|    fps             | 774      |\n",
      "|    iterations      | 848      |\n",
      "|    time_elapsed    | 280      |\n",
      "|    total_timesteps | 217088   |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 77.8       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 775        |\n",
      "|    iterations           | 849        |\n",
      "|    time_elapsed         | 280        |\n",
      "|    total_timesteps      | 217344     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.37447396 |\n",
      "|    clip_fraction        | 0.151      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.102     |\n",
      "|    explained_variance   | 0.357      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | 0.0672     |\n",
      "|    n_updates            | 16112      |\n",
      "|    policy_gradient_loss | -0.0286    |\n",
      "|    value_loss           | 0.2        |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=217500, episode_reward=17.84 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 17.8      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 217500    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.6347661 |\n",
      "|    clip_fraction        | 0.135     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0473   |\n",
      "|    explained_variance   | 0.875     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.071    |\n",
      "|    n_updates            | 16131     |\n",
      "|    policy_gradient_loss | -0.0574   |\n",
      "|    value_loss           | 0.0553    |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 77.8     |\n",
      "| time/              |          |\n",
      "|    fps             | 774      |\n",
      "|    iterations      | 850      |\n",
      "|    time_elapsed    | 280      |\n",
      "|    total_timesteps | 217600   |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 77.8      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 775       |\n",
      "|    iterations           | 851       |\n",
      "|    time_elapsed         | 281       |\n",
      "|    total_timesteps      | 217856    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.3659636 |\n",
      "|    clip_fraction        | 0.107     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0643   |\n",
      "|    explained_variance   | 0.535     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0662   |\n",
      "|    n_updates            | 16150     |\n",
      "|    policy_gradient_loss | -0.0523   |\n",
      "|    value_loss           | 0.186     |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=218000, episode_reward=15.81 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 230        |\n",
      "|    mean_reward          | 15.8       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 218000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.65517825 |\n",
      "|    clip_fraction        | 0.147      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0881    |\n",
      "|    explained_variance   | 0.566      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | 0.0297     |\n",
      "|    n_updates            | 16169      |\n",
      "|    policy_gradient_loss | -0.0543    |\n",
      "|    value_loss           | 0.405      |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 77.8     |\n",
      "| time/              |          |\n",
      "|    fps             | 774      |\n",
      "|    iterations      | 852      |\n",
      "|    time_elapsed    | 281      |\n",
      "|    total_timesteps | 218112   |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 77.8       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 774        |\n",
      "|    iterations           | 853        |\n",
      "|    time_elapsed         | 281        |\n",
      "|    total_timesteps      | 218368     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.33633494 |\n",
      "|    clip_fraction        | 0.138      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.083     |\n",
      "|    explained_variance   | 0.45       |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0213    |\n",
      "|    n_updates            | 16188      |\n",
      "|    policy_gradient_loss | -0.0584    |\n",
      "|    value_loss           | 0.0524     |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=218500, episode_reward=22.96 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 230        |\n",
      "|    mean_reward          | 23         |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 218500     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.67557096 |\n",
      "|    clip_fraction        | 0.135      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0826    |\n",
      "|    explained_variance   | 0.74       |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.086     |\n",
      "|    n_updates            | 16207      |\n",
      "|    policy_gradient_loss | -0.0475    |\n",
      "|    value_loss           | 0.212      |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 77.8     |\n",
      "| time/              |          |\n",
      "|    fps             | 774      |\n",
      "|    iterations      | 854      |\n",
      "|    time_elapsed    | 282      |\n",
      "|    total_timesteps | 218624   |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 77.8      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 774       |\n",
      "|    iterations           | 855       |\n",
      "|    time_elapsed         | 282       |\n",
      "|    total_timesteps      | 218880    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.5504987 |\n",
      "|    clip_fraction        | 0.139     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0682   |\n",
      "|    explained_variance   | 0.759     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0816   |\n",
      "|    n_updates            | 16226     |\n",
      "|    policy_gradient_loss | -0.0485   |\n",
      "|    value_loss           | 0.134     |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=219000, episode_reward=43.77 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 230        |\n",
      "|    mean_reward          | 43.8       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 219000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.57825184 |\n",
      "|    clip_fraction        | 0.143      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0692    |\n",
      "|    explained_variance   | -0.332     |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0284    |\n",
      "|    n_updates            | 16245      |\n",
      "|    policy_gradient_loss | -0.046     |\n",
      "|    value_loss           | 0.105      |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 77.8     |\n",
      "| time/              |          |\n",
      "|    fps             | 774      |\n",
      "|    iterations      | 856      |\n",
      "|    time_elapsed    | 282      |\n",
      "|    total_timesteps | 219136   |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 77.8       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 774        |\n",
      "|    iterations           | 857        |\n",
      "|    time_elapsed         | 283        |\n",
      "|    total_timesteps      | 219392     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.39945364 |\n",
      "|    clip_fraction        | 0.132      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0619    |\n",
      "|    explained_variance   | 0.663      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.038     |\n",
      "|    n_updates            | 16264      |\n",
      "|    policy_gradient_loss | -0.0402    |\n",
      "|    value_loss           | 0.178      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=219500, episode_reward=-13.94 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 230        |\n",
      "|    mean_reward          | -13.9      |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 219500     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.39953798 |\n",
      "|    clip_fraction        | 0.14       |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0817    |\n",
      "|    explained_variance   | 0.544      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0728    |\n",
      "|    n_updates            | 16283      |\n",
      "|    policy_gradient_loss | -0.066     |\n",
      "|    value_loss           | 0.173      |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 77.7     |\n",
      "| time/              |          |\n",
      "|    fps             | 774      |\n",
      "|    iterations      | 858      |\n",
      "|    time_elapsed    | 283      |\n",
      "|    total_timesteps | 219648   |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 77.7       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 774        |\n",
      "|    iterations           | 859        |\n",
      "|    time_elapsed         | 283        |\n",
      "|    total_timesteps      | 219904     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.35241622 |\n",
      "|    clip_fraction        | 0.15       |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.089     |\n",
      "|    explained_variance   | 0.838      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.051     |\n",
      "|    n_updates            | 16302      |\n",
      "|    policy_gradient_loss | -0.0435    |\n",
      "|    value_loss           | 0.189      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=220000, episode_reward=5.25 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 230        |\n",
      "|    mean_reward          | 5.25       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 220000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.67258537 |\n",
      "|    clip_fraction        | 0.131      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0709    |\n",
      "|    explained_variance   | 0.0689     |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0887    |\n",
      "|    n_updates            | 16321      |\n",
      "|    policy_gradient_loss | -0.0481    |\n",
      "|    value_loss           | 0.0808     |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 77.7     |\n",
      "| time/              |          |\n",
      "|    fps             | 774      |\n",
      "|    iterations      | 860      |\n",
      "|    time_elapsed    | 284      |\n",
      "|    total_timesteps | 220160   |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 77.7       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 774        |\n",
      "|    iterations           | 861        |\n",
      "|    time_elapsed         | 284        |\n",
      "|    total_timesteps      | 220416     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.47740588 |\n",
      "|    clip_fraction        | 0.117      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0695    |\n",
      "|    explained_variance   | 0.69       |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0822    |\n",
      "|    n_updates            | 16340      |\n",
      "|    policy_gradient_loss | -0.0563    |\n",
      "|    value_loss           | 0.227      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=220500, episode_reward=14.85 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 230        |\n",
      "|    mean_reward          | 14.9       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 220500     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.59587646 |\n",
      "|    clip_fraction        | 0.138      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0589    |\n",
      "|    explained_variance   | 0.786      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0663    |\n",
      "|    n_updates            | 16359      |\n",
      "|    policy_gradient_loss | -0.0502    |\n",
      "|    value_loss           | 0.214      |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 77.7     |\n",
      "| time/              |          |\n",
      "|    fps             | 774      |\n",
      "|    iterations      | 862      |\n",
      "|    time_elapsed    | 284      |\n",
      "|    total_timesteps | 220672   |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 77.7      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 774       |\n",
      "|    iterations           | 863       |\n",
      "|    time_elapsed         | 285       |\n",
      "|    total_timesteps      | 220928    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.5050233 |\n",
      "|    clip_fraction        | 0.161     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.116    |\n",
      "|    explained_variance   | -0.747    |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0288   |\n",
      "|    n_updates            | 16378     |\n",
      "|    policy_gradient_loss | -0.0483   |\n",
      "|    value_loss           | 0.282     |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=221000, episode_reward=33.96 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 34        |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 221000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.9342611 |\n",
      "|    clip_fraction        | 0.13      |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0558   |\n",
      "|    explained_variance   | 0.756     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0362   |\n",
      "|    n_updates            | 16397     |\n",
      "|    policy_gradient_loss | -0.0568   |\n",
      "|    value_loss           | 0.127     |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 77.7     |\n",
      "| time/              |          |\n",
      "|    fps             | 774      |\n",
      "|    iterations      | 864      |\n",
      "|    time_elapsed    | 285      |\n",
      "|    total_timesteps | 221184   |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 77.6      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 774       |\n",
      "|    iterations           | 865       |\n",
      "|    time_elapsed         | 285       |\n",
      "|    total_timesteps      | 221440    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.0165117 |\n",
      "|    clip_fraction        | 0.166     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0652   |\n",
      "|    explained_variance   | 0.644     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0583   |\n",
      "|    n_updates            | 16416     |\n",
      "|    policy_gradient_loss | -0.0692   |\n",
      "|    value_loss           | 0.0859    |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=221500, episode_reward=28.05 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 28        |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 221500    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.8240399 |\n",
      "|    clip_fraction        | 0.167     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.135    |\n",
      "|    explained_variance   | 0.782     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.00157  |\n",
      "|    n_updates            | 16435     |\n",
      "|    policy_gradient_loss | -0.0514   |\n",
      "|    value_loss           | 0.434     |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 77.6     |\n",
      "| time/              |          |\n",
      "|    fps             | 774      |\n",
      "|    iterations      | 866      |\n",
      "|    time_elapsed    | 286      |\n",
      "|    total_timesteps | 221696   |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 77.6       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 774        |\n",
      "|    iterations           | 867        |\n",
      "|    time_elapsed         | 286        |\n",
      "|    total_timesteps      | 221952     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.86486745 |\n",
      "|    clip_fraction        | 0.159      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0876    |\n",
      "|    explained_variance   | 0.0592     |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0698    |\n",
      "|    n_updates            | 16454      |\n",
      "|    policy_gradient_loss | -0.0574    |\n",
      "|    value_loss           | 0.072      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=222000, episode_reward=29.26 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 230        |\n",
      "|    mean_reward          | 29.3       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 222000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.36877522 |\n",
      "|    clip_fraction        | 0.107      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0638    |\n",
      "|    explained_variance   | 0.664      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0838    |\n",
      "|    n_updates            | 16473      |\n",
      "|    policy_gradient_loss | -0.0473    |\n",
      "|    value_loss           | 0.128      |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 77.6     |\n",
      "| time/              |          |\n",
      "|    fps             | 774      |\n",
      "|    iterations      | 868      |\n",
      "|    time_elapsed    | 286      |\n",
      "|    total_timesteps | 222208   |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 77.7       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 774        |\n",
      "|    iterations           | 869        |\n",
      "|    time_elapsed         | 287        |\n",
      "|    total_timesteps      | 222464     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.70678115 |\n",
      "|    clip_fraction        | 0.168      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0726    |\n",
      "|    explained_variance   | 0.822      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0257    |\n",
      "|    n_updates            | 16492      |\n",
      "|    policy_gradient_loss | -0.0514    |\n",
      "|    value_loss           | 0.15       |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=222500, episode_reward=35.65 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 230        |\n",
      "|    mean_reward          | 35.6       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 222500     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.49638817 |\n",
      "|    clip_fraction        | 0.182      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.137     |\n",
      "|    explained_variance   | 0.724      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0119    |\n",
      "|    n_updates            | 16511      |\n",
      "|    policy_gradient_loss | -0.0477    |\n",
      "|    value_loss           | 0.128      |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 77.7     |\n",
      "| time/              |          |\n",
      "|    fps             | 774      |\n",
      "|    iterations      | 870      |\n",
      "|    time_elapsed    | 287      |\n",
      "|    total_timesteps | 222720   |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 77.7       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 775        |\n",
      "|    iterations           | 871        |\n",
      "|    time_elapsed         | 287        |\n",
      "|    total_timesteps      | 222976     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.44407892 |\n",
      "|    clip_fraction        | 0.126      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0495    |\n",
      "|    explained_variance   | 0.824      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0366    |\n",
      "|    n_updates            | 16530      |\n",
      "|    policy_gradient_loss | -0.053     |\n",
      "|    value_loss           | 0.065      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=223000, episode_reward=45.04 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "--------------------------------------\n",
      "| eval/                   |          |\n",
      "|    mean_ep_length       | 230      |\n",
      "|    mean_reward          | 45       |\n",
      "| time/                   |          |\n",
      "|    total_timesteps      | 223000   |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 0.775661 |\n",
      "|    clip_fraction        | 0.156    |\n",
      "|    clip_range           | 0.4      |\n",
      "|    entropy_loss         | -0.0678  |\n",
      "|    explained_variance   | 0.726    |\n",
      "|    learning_rate        | 0.000389 |\n",
      "|    loss                 | -0.0846  |\n",
      "|    n_updates            | 16549    |\n",
      "|    policy_gradient_loss | -0.0597  |\n",
      "|    value_loss           | 0.0935   |\n",
      "--------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 77.7     |\n",
      "| time/              |          |\n",
      "|    fps             | 774      |\n",
      "|    iterations      | 872      |\n",
      "|    time_elapsed    | 288      |\n",
      "|    total_timesteps | 223232   |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 77.7       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 774        |\n",
      "|    iterations           | 873        |\n",
      "|    time_elapsed         | 288        |\n",
      "|    total_timesteps      | 223488     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.60509455 |\n",
      "|    clip_fraction        | 0.225      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.148     |\n",
      "|    explained_variance   | 0.837      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | 0.0189     |\n",
      "|    n_updates            | 16568      |\n",
      "|    policy_gradient_loss | -0.065     |\n",
      "|    value_loss           | 0.25       |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=223500, episode_reward=16.05 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 230        |\n",
      "|    mean_reward          | 16.1       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 223500     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.71373075 |\n",
      "|    clip_fraction        | 0.152      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0687    |\n",
      "|    explained_variance   | 0.6        |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.106     |\n",
      "|    n_updates            | 16587      |\n",
      "|    policy_gradient_loss | -0.0576    |\n",
      "|    value_loss           | 0.0536     |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 77.7     |\n",
      "| time/              |          |\n",
      "|    fps             | 774      |\n",
      "|    iterations      | 874      |\n",
      "|    time_elapsed    | 288      |\n",
      "|    total_timesteps | 223744   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=224000, episode_reward=4.57 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 230        |\n",
      "|    mean_reward          | 4.57       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 224000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.93590087 |\n",
      "|    clip_fraction        | 0.13       |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0511    |\n",
      "|    explained_variance   | 0.772      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0533    |\n",
      "|    n_updates            | 16606      |\n",
      "|    policy_gradient_loss | -0.0548    |\n",
      "|    value_loss           | 0.139      |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 77.7     |\n",
      "| time/              |          |\n",
      "|    fps             | 774      |\n",
      "|    iterations      | 875      |\n",
      "|    time_elapsed    | 289      |\n",
      "|    total_timesteps | 224000   |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 77.7      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 774       |\n",
      "|    iterations           | 876       |\n",
      "|    time_elapsed         | 289       |\n",
      "|    total_timesteps      | 224256    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.1904061 |\n",
      "|    clip_fraction        | 0.169     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0678   |\n",
      "|    explained_variance   | 0.839     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0451   |\n",
      "|    n_updates            | 16625     |\n",
      "|    policy_gradient_loss | -0.0722   |\n",
      "|    value_loss           | 0.187     |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=224500, episode_reward=20.88 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 230        |\n",
      "|    mean_reward          | 20.9       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 224500     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.43877575 |\n",
      "|    clip_fraction        | 0.164      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.11      |\n",
      "|    explained_variance   | 0.444      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0556    |\n",
      "|    n_updates            | 16644      |\n",
      "|    policy_gradient_loss | -0.0488    |\n",
      "|    value_loss           | 0.0858     |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 77.7     |\n",
      "| time/              |          |\n",
      "|    fps             | 774      |\n",
      "|    iterations      | 877      |\n",
      "|    time_elapsed    | 290      |\n",
      "|    total_timesteps | 224512   |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 77.7      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 774       |\n",
      "|    iterations           | 878       |\n",
      "|    time_elapsed         | 290       |\n",
      "|    total_timesteps      | 224768    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.5809679 |\n",
      "|    clip_fraction        | 0.125     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0577   |\n",
      "|    explained_variance   | 0.721     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0564   |\n",
      "|    n_updates            | 16663     |\n",
      "|    policy_gradient_loss | -0.0564   |\n",
      "|    value_loss           | 0.0822    |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=225000, episode_reward=39.74 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 39.7      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 225000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.3436455 |\n",
      "|    clip_fraction        | 0.129     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0562   |\n",
      "|    explained_variance   | 0.771     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0534   |\n",
      "|    n_updates            | 16682     |\n",
      "|    policy_gradient_loss | -0.0414   |\n",
      "|    value_loss           | 0.153     |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 77.7     |\n",
      "| time/              |          |\n",
      "|    fps             | 773      |\n",
      "|    iterations      | 879      |\n",
      "|    time_elapsed    | 290      |\n",
      "|    total_timesteps | 225024   |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 77.7      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 773       |\n",
      "|    iterations           | 880       |\n",
      "|    time_elapsed         | 291       |\n",
      "|    total_timesteps      | 225280    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.6499402 |\n",
      "|    clip_fraction        | 0.205     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.136    |\n",
      "|    explained_variance   | 0.795     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | 0.178     |\n",
      "|    n_updates            | 16701     |\n",
      "|    policy_gradient_loss | -0.0551   |\n",
      "|    value_loss           | 0.305     |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=225500, episode_reward=23.14 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 23.1      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 225500    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.7645637 |\n",
      "|    clip_fraction        | 0.137     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0476   |\n",
      "|    explained_variance   | 0.715     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0501   |\n",
      "|    n_updates            | 16720     |\n",
      "|    policy_gradient_loss | -0.0485   |\n",
      "|    value_loss           | 0.034     |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 77.7     |\n",
      "| time/              |          |\n",
      "|    fps             | 773      |\n",
      "|    iterations      | 881      |\n",
      "|    time_elapsed    | 291      |\n",
      "|    total_timesteps | 225536   |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 77.7       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 773        |\n",
      "|    iterations           | 882        |\n",
      "|    time_elapsed         | 291        |\n",
      "|    total_timesteps      | 225792     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.59042776 |\n",
      "|    clip_fraction        | 0.125      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0433    |\n",
      "|    explained_variance   | 0.673      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.072     |\n",
      "|    n_updates            | 16739      |\n",
      "|    policy_gradient_loss | -0.0542    |\n",
      "|    value_loss           | 0.21       |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=226000, episode_reward=36.91 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 36.9      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 226000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.3645269 |\n",
      "|    clip_fraction        | 0.113     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0684   |\n",
      "|    explained_variance   | 0.871     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0106   |\n",
      "|    n_updates            | 16758     |\n",
      "|    policy_gradient_loss | -0.0347   |\n",
      "|    value_loss           | 0.113     |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 77.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 772      |\n",
      "|    iterations      | 883      |\n",
      "|    time_elapsed    | 292      |\n",
      "|    total_timesteps | 226048   |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 77.9       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 772        |\n",
      "|    iterations           | 884        |\n",
      "|    time_elapsed         | 292        |\n",
      "|    total_timesteps      | 226304     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.45048365 |\n",
      "|    clip_fraction        | 0.217      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.142     |\n",
      "|    explained_variance   | 0.32       |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.111     |\n",
      "|    n_updates            | 16777      |\n",
      "|    policy_gradient_loss | -0.0583    |\n",
      "|    value_loss           | 0.04       |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=226500, episode_reward=23.46 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 23.5      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 226500    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.0854814 |\n",
      "|    clip_fraction        | 0.184     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0683   |\n",
      "|    explained_variance   | 0.76      |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0779   |\n",
      "|    n_updates            | 16796     |\n",
      "|    policy_gradient_loss | -0.0717   |\n",
      "|    value_loss           | 0.136     |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 77.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 772      |\n",
      "|    iterations      | 885      |\n",
      "|    time_elapsed    | 293      |\n",
      "|    total_timesteps | 226560   |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 78        |\n",
      "| time/                   |           |\n",
      "|    fps                  | 772       |\n",
      "|    iterations           | 886       |\n",
      "|    time_elapsed         | 293       |\n",
      "|    total_timesteps      | 226816    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.7110261 |\n",
      "|    clip_fraction        | 0.112     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0494   |\n",
      "|    explained_variance   | 0.763     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0205   |\n",
      "|    n_updates            | 16815     |\n",
      "|    policy_gradient_loss | -0.0442   |\n",
      "|    value_loss           | 0.129     |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=227000, episode_reward=12.54 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 230        |\n",
      "|    mean_reward          | 12.5       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 227000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.68166363 |\n",
      "|    clip_fraction        | 0.269      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.18      |\n",
      "|    explained_variance   | 0.908      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.129     |\n",
      "|    n_updates            | 16834      |\n",
      "|    policy_gradient_loss | -0.0708    |\n",
      "|    value_loss           | 0.0661     |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 78       |\n",
      "| time/              |          |\n",
      "|    fps             | 772      |\n",
      "|    iterations      | 887      |\n",
      "|    time_elapsed    | 293      |\n",
      "|    total_timesteps | 227072   |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 78        |\n",
      "| time/                   |           |\n",
      "|    fps                  | 772       |\n",
      "|    iterations           | 888       |\n",
      "|    time_elapsed         | 294       |\n",
      "|    total_timesteps      | 227328    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.9212947 |\n",
      "|    clip_fraction        | 0.186     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0526   |\n",
      "|    explained_variance   | 0.301     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0718   |\n",
      "|    n_updates            | 16853     |\n",
      "|    policy_gradient_loss | -0.0601   |\n",
      "|    value_loss           | 0.0682    |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=227500, episode_reward=25.40 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 230        |\n",
      "|    mean_reward          | 25.4       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 227500     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.66114765 |\n",
      "|    clip_fraction        | 0.135      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0717    |\n",
      "|    explained_variance   | 0.825      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0836    |\n",
      "|    n_updates            | 16872      |\n",
      "|    policy_gradient_loss | -0.0515    |\n",
      "|    value_loss           | 0.153      |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 78.2     |\n",
      "| time/              |          |\n",
      "|    fps             | 772      |\n",
      "|    iterations      | 889      |\n",
      "|    time_elapsed    | 294      |\n",
      "|    total_timesteps | 227584   |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 78.2      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 772       |\n",
      "|    iterations           | 890       |\n",
      "|    time_elapsed         | 294       |\n",
      "|    total_timesteps      | 227840    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.4083506 |\n",
      "|    clip_fraction        | 0.117     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0596   |\n",
      "|    explained_variance   | 0.719     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.044    |\n",
      "|    n_updates            | 16891     |\n",
      "|    policy_gradient_loss | -0.0544   |\n",
      "|    value_loss           | 0.176     |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=228000, episode_reward=58.95 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 59        |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 228000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.9139674 |\n",
      "|    clip_fraction        | 0.168     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.137    |\n",
      "|    explained_variance   | -0.102    |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0858   |\n",
      "|    n_updates            | 16910     |\n",
      "|    policy_gradient_loss | -0.0665   |\n",
      "|    value_loss           | 0.0457    |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 78.2     |\n",
      "| time/              |          |\n",
      "|    fps             | 772      |\n",
      "|    iterations      | 891      |\n",
      "|    time_elapsed    | 295      |\n",
      "|    total_timesteps | 228096   |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 78.2       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 773        |\n",
      "|    iterations           | 892        |\n",
      "|    time_elapsed         | 295        |\n",
      "|    total_timesteps      | 228352     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.67455626 |\n",
      "|    clip_fraction        | 0.118      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0619    |\n",
      "|    explained_variance   | 0.698      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0878    |\n",
      "|    n_updates            | 16929      |\n",
      "|    policy_gradient_loss | -0.0378    |\n",
      "|    value_loss           | 0.276      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=228500, episode_reward=28.67 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 230        |\n",
      "|    mean_reward          | 28.7       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 228500     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.48450017 |\n",
      "|    clip_fraction        | 0.13       |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0501    |\n",
      "|    explained_variance   | 0.747      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0861    |\n",
      "|    n_updates            | 16948      |\n",
      "|    policy_gradient_loss | -0.0581    |\n",
      "|    value_loss           | 0.0728     |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 78.3     |\n",
      "| time/              |          |\n",
      "|    fps             | 772      |\n",
      "|    iterations      | 893      |\n",
      "|    time_elapsed    | 295      |\n",
      "|    total_timesteps | 228608   |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 78.3       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 773        |\n",
      "|    iterations           | 894        |\n",
      "|    time_elapsed         | 296        |\n",
      "|    total_timesteps      | 228864     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.63597953 |\n",
      "|    clip_fraction        | 0.181      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.142     |\n",
      "|    explained_variance   | 0.891      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | 0.0429     |\n",
      "|    n_updates            | 16967      |\n",
      "|    policy_gradient_loss | -0.0518    |\n",
      "|    value_loss           | 0.159      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=229000, episode_reward=54.02 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "--------------------------------------\n",
      "| eval/                   |          |\n",
      "|    mean_ep_length       | 230      |\n",
      "|    mean_reward          | 54       |\n",
      "| time/                   |          |\n",
      "|    total_timesteps      | 229000   |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 0.566906 |\n",
      "|    clip_fraction        | 0.138    |\n",
      "|    clip_range           | 0.4      |\n",
      "|    entropy_loss         | -0.058   |\n",
      "|    explained_variance   | 0.702    |\n",
      "|    learning_rate        | 0.000389 |\n",
      "|    loss                 | -0.0602  |\n",
      "|    n_updates            | 16986    |\n",
      "|    policy_gradient_loss | -0.0551  |\n",
      "|    value_loss           | 0.044    |\n",
      "--------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 78.3     |\n",
      "| time/              |          |\n",
      "|    fps             | 773      |\n",
      "|    iterations      | 895      |\n",
      "|    time_elapsed    | 296      |\n",
      "|    total_timesteps | 229120   |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 78.4       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 773        |\n",
      "|    iterations           | 896        |\n",
      "|    time_elapsed         | 296        |\n",
      "|    total_timesteps      | 229376     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.49568588 |\n",
      "|    clip_fraction        | 0.0975     |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0437    |\n",
      "|    explained_variance   | 0.754      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0331    |\n",
      "|    n_updates            | 17005      |\n",
      "|    policy_gradient_loss | -0.0445    |\n",
      "|    value_loss           | 0.194      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=229500, episode_reward=77.05 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 77.1      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 229500    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.0639126 |\n",
      "|    clip_fraction        | 0.166     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0683   |\n",
      "|    explained_variance   | 0.821     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0878   |\n",
      "|    n_updates            | 17024     |\n",
      "|    policy_gradient_loss | -0.065    |\n",
      "|    value_loss           | 0.151     |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 78.4     |\n",
      "| time/              |          |\n",
      "|    fps             | 772      |\n",
      "|    iterations      | 897      |\n",
      "|    time_elapsed    | 297      |\n",
      "|    total_timesteps | 229632   |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 78.4       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 772        |\n",
      "|    iterations           | 898        |\n",
      "|    time_elapsed         | 297        |\n",
      "|    total_timesteps      | 229888     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.38846892 |\n",
      "|    clip_fraction        | 0.143      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.135     |\n",
      "|    explained_variance   | 0.0779     |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0222    |\n",
      "|    n_updates            | 17043      |\n",
      "|    policy_gradient_loss | -0.0455    |\n",
      "|    value_loss           | 0.0471     |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=230000, episode_reward=63.34 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 230        |\n",
      "|    mean_reward          | 63.3       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 230000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.34812155 |\n",
      "|    clip_fraction        | 0.101      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0601    |\n",
      "|    explained_variance   | 0.824      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0574    |\n",
      "|    n_updates            | 17062      |\n",
      "|    policy_gradient_loss | -0.0382    |\n",
      "|    value_loss           | 0.109      |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 78.4     |\n",
      "| time/              |          |\n",
      "|    fps             | 772      |\n",
      "|    iterations      | 899      |\n",
      "|    time_elapsed    | 297      |\n",
      "|    total_timesteps | 230144   |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 78.4      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 773       |\n",
      "|    iterations           | 900       |\n",
      "|    time_elapsed         | 297       |\n",
      "|    total_timesteps      | 230400    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.2934182 |\n",
      "|    clip_fraction        | 0.152     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0509   |\n",
      "|    explained_variance   | 0.675     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0297   |\n",
      "|    n_updates            | 17081     |\n",
      "|    policy_gradient_loss | -0.0572   |\n",
      "|    value_loss           | 0.0689    |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=230500, episode_reward=57.99 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "--------------------------------------\n",
      "| eval/                   |          |\n",
      "|    mean_ep_length       | 230      |\n",
      "|    mean_reward          | 58       |\n",
      "| time/                   |          |\n",
      "|    total_timesteps      | 230500   |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 0.715966 |\n",
      "|    clip_fraction        | 0.215    |\n",
      "|    clip_range           | 0.4      |\n",
      "|    entropy_loss         | -0.145   |\n",
      "|    explained_variance   | 0.845    |\n",
      "|    learning_rate        | 0.000389 |\n",
      "|    loss                 | -0.043   |\n",
      "|    n_updates            | 17100    |\n",
      "|    policy_gradient_loss | -0.0509  |\n",
      "|    value_loss           | 0.228    |\n",
      "--------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 78.4     |\n",
      "| time/              |          |\n",
      "|    fps             | 773      |\n",
      "|    iterations      | 901      |\n",
      "|    time_elapsed    | 298      |\n",
      "|    total_timesteps | 230656   |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 78.4      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 773       |\n",
      "|    iterations           | 902       |\n",
      "|    time_elapsed         | 298       |\n",
      "|    total_timesteps      | 230912    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.8088259 |\n",
      "|    clip_fraction        | 0.162     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.066    |\n",
      "|    explained_variance   | 0.846     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0532   |\n",
      "|    n_updates            | 17119     |\n",
      "|    policy_gradient_loss | -0.0581   |\n",
      "|    value_loss           | 0.051     |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=231000, episode_reward=66.21 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 230        |\n",
      "|    mean_reward          | 66.2       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 231000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.50815856 |\n",
      "|    clip_fraction        | 0.113      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0593    |\n",
      "|    explained_variance   | 0.562      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0404    |\n",
      "|    n_updates            | 17138      |\n",
      "|    policy_gradient_loss | -0.0412    |\n",
      "|    value_loss           | 0.151      |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 78.2     |\n",
      "| time/              |          |\n",
      "|    fps             | 773      |\n",
      "|    iterations      | 903      |\n",
      "|    time_elapsed    | 298      |\n",
      "|    total_timesteps | 231168   |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 78.2       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 773        |\n",
      "|    iterations           | 904        |\n",
      "|    time_elapsed         | 299        |\n",
      "|    total_timesteps      | 231424     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.34729254 |\n",
      "|    clip_fraction        | 0.105      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0558    |\n",
      "|    explained_variance   | 0.751      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0374    |\n",
      "|    n_updates            | 17157      |\n",
      "|    policy_gradient_loss | -0.0303    |\n",
      "|    value_loss           | 0.3        |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=231500, episode_reward=45.46 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 230        |\n",
      "|    mean_reward          | 45.5       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 231500     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.41586623 |\n",
      "|    clip_fraction        | 0.202      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.148     |\n",
      "|    explained_variance   | 0.166      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0799    |\n",
      "|    n_updates            | 17176      |\n",
      "|    policy_gradient_loss | -0.0514    |\n",
      "|    value_loss           | 0.0595     |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 78.2     |\n",
      "| time/              |          |\n",
      "|    fps             | 773      |\n",
      "|    iterations      | 905      |\n",
      "|    time_elapsed    | 299      |\n",
      "|    total_timesteps | 231680   |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 78.2       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 773        |\n",
      "|    iterations           | 906        |\n",
      "|    time_elapsed         | 299        |\n",
      "|    total_timesteps      | 231936     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.31702268 |\n",
      "|    clip_fraction        | 0.0964     |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0572    |\n",
      "|    explained_variance   | 0.654      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0931    |\n",
      "|    n_updates            | 17195      |\n",
      "|    policy_gradient_loss | -0.0426    |\n",
      "|    value_loss           | 0.115      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=232000, episode_reward=74.84 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 230        |\n",
      "|    mean_reward          | 74.8       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 232000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.60482574 |\n",
      "|    clip_fraction        | 0.102      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0451    |\n",
      "|    explained_variance   | 0.751      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0621    |\n",
      "|    n_updates            | 17214      |\n",
      "|    policy_gradient_loss | -0.0402    |\n",
      "|    value_loss           | 0.0836     |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 78       |\n",
      "| time/              |          |\n",
      "|    fps             | 773      |\n",
      "|    iterations      | 907      |\n",
      "|    time_elapsed    | 300      |\n",
      "|    total_timesteps | 232192   |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 78         |\n",
      "| time/                   |            |\n",
      "|    fps                  | 773        |\n",
      "|    iterations           | 908        |\n",
      "|    time_elapsed         | 300        |\n",
      "|    total_timesteps      | 232448     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.49036187 |\n",
      "|    clip_fraction        | 0.226      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.187     |\n",
      "|    explained_variance   | 0.76       |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | 0.0138     |\n",
      "|    n_updates            | 17233      |\n",
      "|    policy_gradient_loss | -0.0449    |\n",
      "|    value_loss           | 0.206      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=232500, episode_reward=69.16 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 230        |\n",
      "|    mean_reward          | 69.2       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 232500     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.44364488 |\n",
      "|    clip_fraction        | 0.102      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0411    |\n",
      "|    explained_variance   | 0.85       |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0802    |\n",
      "|    n_updates            | 17252      |\n",
      "|    policy_gradient_loss | -0.0369    |\n",
      "|    value_loss           | 0.0375     |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 78       |\n",
      "| time/              |          |\n",
      "|    fps             | 773      |\n",
      "|    iterations      | 909      |\n",
      "|    time_elapsed    | 300      |\n",
      "|    total_timesteps | 232704   |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 78        |\n",
      "| time/                   |           |\n",
      "|    fps                  | 773       |\n",
      "|    iterations           | 910       |\n",
      "|    time_elapsed         | 301       |\n",
      "|    total_timesteps      | 232960    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.6361829 |\n",
      "|    clip_fraction        | 0.138     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0593   |\n",
      "|    explained_variance   | 0.619     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.103    |\n",
      "|    n_updates            | 17271     |\n",
      "|    policy_gradient_loss | -0.056    |\n",
      "|    value_loss           | 0.13      |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=233000, episode_reward=30.33 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 30.3      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 233000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.6363282 |\n",
      "|    clip_fraction        | 0.145     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0833   |\n",
      "|    explained_variance   | 0.878     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.00967  |\n",
      "|    n_updates            | 17290     |\n",
      "|    policy_gradient_loss | -0.0517   |\n",
      "|    value_loss           | 0.121     |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 78       |\n",
      "| time/              |          |\n",
      "|    fps             | 772      |\n",
      "|    iterations      | 911      |\n",
      "|    time_elapsed    | 301      |\n",
      "|    total_timesteps | 233216   |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 78         |\n",
      "| time/                   |            |\n",
      "|    fps                  | 772        |\n",
      "|    iterations           | 912        |\n",
      "|    time_elapsed         | 302        |\n",
      "|    total_timesteps      | 233472     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.65941787 |\n",
      "|    clip_fraction        | 0.202      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.109     |\n",
      "|    explained_variance   | 0.653      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0607    |\n",
      "|    n_updates            | 17309      |\n",
      "|    policy_gradient_loss | -0.0647    |\n",
      "|    value_loss           | 0.0305     |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=233500, episode_reward=5.02 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 230        |\n",
      "|    mean_reward          | 5.02       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 233500     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.24087761 |\n",
      "|    clip_fraction        | 0.0933     |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0593    |\n",
      "|    explained_variance   | 0.724      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.036     |\n",
      "|    n_updates            | 17328      |\n",
      "|    policy_gradient_loss | -0.027     |\n",
      "|    value_loss           | 0.154      |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 78       |\n",
      "| time/              |          |\n",
      "|    fps             | 772      |\n",
      "|    iterations      | 913      |\n",
      "|    time_elapsed    | 302      |\n",
      "|    total_timesteps | 233728   |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 77.9      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 773       |\n",
      "|    iterations           | 914       |\n",
      "|    time_elapsed         | 302       |\n",
      "|    total_timesteps      | 233984    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.4568426 |\n",
      "|    clip_fraction        | 0.0938    |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.037    |\n",
      "|    explained_variance   | 0.933     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.07     |\n",
      "|    n_updates            | 17347     |\n",
      "|    policy_gradient_loss | -0.0309   |\n",
      "|    value_loss           | 0.0688    |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=234000, episode_reward=51.90 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 230        |\n",
      "|    mean_reward          | 51.9       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 234000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.87224853 |\n",
      "|    clip_fraction        | 0.251      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.139     |\n",
      "|    explained_variance   | 0.721      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0116    |\n",
      "|    n_updates            | 17366      |\n",
      "|    policy_gradient_loss | 0.0205     |\n",
      "|    value_loss           | 0.109      |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 77.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 772      |\n",
      "|    iterations      | 915      |\n",
      "|    time_elapsed    | 303      |\n",
      "|    total_timesteps | 234240   |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 77.9       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 772        |\n",
      "|    iterations           | 916        |\n",
      "|    time_elapsed         | 303        |\n",
      "|    total_timesteps      | 234496     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.66257775 |\n",
      "|    clip_fraction        | 0.141      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0449    |\n",
      "|    explained_variance   | 0.832      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0537    |\n",
      "|    n_updates            | 17385      |\n",
      "|    policy_gradient_loss | -0.0594    |\n",
      "|    value_loss           | 0.0433     |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=234500, episode_reward=68.66 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 230        |\n",
      "|    mean_reward          | 68.7       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 234500     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.45260656 |\n",
      "|    clip_fraction        | 0.15       |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0753    |\n",
      "|    explained_variance   | 0.51       |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0513    |\n",
      "|    n_updates            | 17404      |\n",
      "|    policy_gradient_loss | -0.0633    |\n",
      "|    value_loss           | 0.128      |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 77.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 772      |\n",
      "|    iterations      | 917      |\n",
      "|    time_elapsed    | 303      |\n",
      "|    total_timesteps | 234752   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=235000, episode_reward=79.43 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 79.4      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 235000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.5199487 |\n",
      "|    clip_fraction        | 0.119     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0786   |\n",
      "|    explained_variance   | 0.789     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0106   |\n",
      "|    n_updates            | 17423     |\n",
      "|    policy_gradient_loss | -0.039    |\n",
      "|    value_loss           | 0.21      |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 77.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 771      |\n",
      "|    iterations      | 918      |\n",
      "|    time_elapsed    | 304      |\n",
      "|    total_timesteps | 235008   |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 77.9      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 772       |\n",
      "|    iterations           | 919       |\n",
      "|    time_elapsed         | 304       |\n",
      "|    total_timesteps      | 235264    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.0342622 |\n",
      "|    clip_fraction        | 0.181     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0788   |\n",
      "|    explained_variance   | 0.499     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0885   |\n",
      "|    n_updates            | 17442     |\n",
      "|    policy_gradient_loss | -0.0599   |\n",
      "|    value_loss           | 0.0496    |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=235500, episode_reward=50.48 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 50.5      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 235500    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.3182888 |\n",
      "|    clip_fraction        | 0.14      |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0505   |\n",
      "|    explained_variance   | 0.82      |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0234   |\n",
      "|    n_updates            | 17461     |\n",
      "|    policy_gradient_loss | -0.065    |\n",
      "|    value_loss           | 0.0575    |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 77.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 771      |\n",
      "|    iterations      | 920      |\n",
      "|    time_elapsed    | 305      |\n",
      "|    total_timesteps | 235520   |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 77.9      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 772       |\n",
      "|    iterations           | 921       |\n",
      "|    time_elapsed         | 305       |\n",
      "|    total_timesteps      | 235776    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.2999438 |\n",
      "|    clip_fraction        | 0.155     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0549   |\n",
      "|    explained_variance   | 0.839     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0357   |\n",
      "|    n_updates            | 17480     |\n",
      "|    policy_gradient_loss | -0.0557   |\n",
      "|    value_loss           | 0.225     |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=236000, episode_reward=66.88 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 230        |\n",
      "|    mean_reward          | 66.9       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 236000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.58275884 |\n",
      "|    clip_fraction        | 0.187      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.12      |\n",
      "|    explained_variance   | 0.602      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.026     |\n",
      "|    n_updates            | 17499      |\n",
      "|    policy_gradient_loss | -0.0507    |\n",
      "|    value_loss           | 0.281      |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 77.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 771      |\n",
      "|    iterations      | 922      |\n",
      "|    time_elapsed    | 305      |\n",
      "|    total_timesteps | 236032   |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 77.9       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 772        |\n",
      "|    iterations           | 923        |\n",
      "|    time_elapsed         | 306        |\n",
      "|    total_timesteps      | 236288     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.86913925 |\n",
      "|    clip_fraction        | 0.145      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0642    |\n",
      "|    explained_variance   | 0.788      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0462    |\n",
      "|    n_updates            | 17518      |\n",
      "|    policy_gradient_loss | -0.0614    |\n",
      "|    value_loss           | 0.0628     |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=236500, episode_reward=14.45 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 14.4      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 236500    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.4895605 |\n",
      "|    clip_fraction        | 0.0798    |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0354   |\n",
      "|    explained_variance   | 0.5       |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0573   |\n",
      "|    n_updates            | 17537     |\n",
      "|    policy_gradient_loss | -0.0346   |\n",
      "|    value_loss           | 0.155     |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 78       |\n",
      "| time/              |          |\n",
      "|    fps             | 772      |\n",
      "|    iterations      | 924      |\n",
      "|    time_elapsed    | 306      |\n",
      "|    total_timesteps | 236544   |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 78        |\n",
      "| time/                   |           |\n",
      "|    fps                  | 772       |\n",
      "|    iterations           | 925       |\n",
      "|    time_elapsed         | 306       |\n",
      "|    total_timesteps      | 236800    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.9242359 |\n",
      "|    clip_fraction        | 0.174     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0894   |\n",
      "|    explained_variance   | 0.505     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.119    |\n",
      "|    n_updates            | 17556     |\n",
      "|    policy_gradient_loss | -0.0475   |\n",
      "|    value_loss           | 0.234     |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=237000, episode_reward=28.35 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 28.4      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 237000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.1246552 |\n",
      "|    clip_fraction        | 0.171     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0697   |\n",
      "|    explained_variance   | 0.635     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.121    |\n",
      "|    n_updates            | 17575     |\n",
      "|    policy_gradient_loss | -0.0612   |\n",
      "|    value_loss           | 0.0567    |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 78       |\n",
      "| time/              |          |\n",
      "|    fps             | 772      |\n",
      "|    iterations      | 926      |\n",
      "|    time_elapsed    | 306      |\n",
      "|    total_timesteps | 237056   |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 78        |\n",
      "| time/                   |           |\n",
      "|    fps                  | 772       |\n",
      "|    iterations           | 927       |\n",
      "|    time_elapsed         | 307       |\n",
      "|    total_timesteps      | 237312    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.6587646 |\n",
      "|    clip_fraction        | 0.109     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0487   |\n",
      "|    explained_variance   | 0.804     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0802   |\n",
      "|    n_updates            | 17594     |\n",
      "|    policy_gradient_loss | -0.0401   |\n",
      "|    value_loss           | 0.0736    |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=237500, episode_reward=51.59 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 51.6      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 237500    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.7405492 |\n",
      "|    clip_fraction        | 0.168     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.059    |\n",
      "|    explained_variance   | 0.664     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0752   |\n",
      "|    n_updates            | 17613     |\n",
      "|    policy_gradient_loss | -0.0571   |\n",
      "|    value_loss           | 0.257     |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 78.1     |\n",
      "| time/              |          |\n",
      "|    fps             | 772      |\n",
      "|    iterations      | 928      |\n",
      "|    time_elapsed    | 307      |\n",
      "|    total_timesteps | 237568   |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 78.1       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 772        |\n",
      "|    iterations           | 929        |\n",
      "|    time_elapsed         | 307        |\n",
      "|    total_timesteps      | 237824     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.58455634 |\n",
      "|    clip_fraction        | 0.144      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0717    |\n",
      "|    explained_variance   | 0.481      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.00492   |\n",
      "|    n_updates            | 17632      |\n",
      "|    policy_gradient_loss | -0.0513    |\n",
      "|    value_loss           | 0.143      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=238000, episode_reward=60.99 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 61        |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 238000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.7652701 |\n",
      "|    clip_fraction        | 0.117     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0525   |\n",
      "|    explained_variance   | 0.802     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0377   |\n",
      "|    n_updates            | 17651     |\n",
      "|    policy_gradient_loss | -0.0465   |\n",
      "|    value_loss           | 0.0638    |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 78.1     |\n",
      "| time/              |          |\n",
      "|    fps             | 772      |\n",
      "|    iterations      | 930      |\n",
      "|    time_elapsed    | 308      |\n",
      "|    total_timesteps | 238080   |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 78.2       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 772        |\n",
      "|    iterations           | 931        |\n",
      "|    time_elapsed         | 308        |\n",
      "|    total_timesteps      | 238336     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.83966506 |\n",
      "|    clip_fraction        | 0.133      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0415    |\n",
      "|    explained_variance   | 0.817      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.081     |\n",
      "|    n_updates            | 17670      |\n",
      "|    policy_gradient_loss | -0.0447    |\n",
      "|    value_loss           | 0.0762     |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=238500, episode_reward=62.88 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 62.9      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 238500    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.5118828 |\n",
      "|    clip_fraction        | 0.176     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.099    |\n",
      "|    explained_variance   | 0.762     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0755   |\n",
      "|    n_updates            | 17689     |\n",
      "|    policy_gradient_loss | -0.0575   |\n",
      "|    value_loss           | 0.246     |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 78.2     |\n",
      "| time/              |          |\n",
      "|    fps             | 772      |\n",
      "|    iterations      | 932      |\n",
      "|    time_elapsed    | 308      |\n",
      "|    total_timesteps | 238592   |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 78.2      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 772       |\n",
      "|    iterations           | 933       |\n",
      "|    time_elapsed         | 309       |\n",
      "|    total_timesteps      | 238848    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.0106087 |\n",
      "|    clip_fraction        | 0.166     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0725   |\n",
      "|    explained_variance   | 0.507     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0955   |\n",
      "|    n_updates            | 17708     |\n",
      "|    policy_gradient_loss | -0.0786   |\n",
      "|    value_loss           | 0.064     |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=239000, episode_reward=45.75 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 230        |\n",
      "|    mean_reward          | 45.7       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 239000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.49890625 |\n",
      "|    clip_fraction        | 0.103      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0484    |\n",
      "|    explained_variance   | 0.759      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0349    |\n",
      "|    n_updates            | 17727      |\n",
      "|    policy_gradient_loss | -0.0432    |\n",
      "|    value_loss           | 0.0831     |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 78.2     |\n",
      "| time/              |          |\n",
      "|    fps             | 772      |\n",
      "|    iterations      | 934      |\n",
      "|    time_elapsed    | 309      |\n",
      "|    total_timesteps | 239104   |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 78.1       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 772        |\n",
      "|    iterations           | 935        |\n",
      "|    time_elapsed         | 309        |\n",
      "|    total_timesteps      | 239360     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.61080205 |\n",
      "|    clip_fraction        | 0.128      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0575    |\n",
      "|    explained_variance   | 0.797      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.075     |\n",
      "|    n_updates            | 17746      |\n",
      "|    policy_gradient_loss | -0.0547    |\n",
      "|    value_loss           | 0.171      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=239500, episode_reward=43.30 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 43.3      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 239500    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.5785036 |\n",
      "|    clip_fraction        | 0.195     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.105    |\n",
      "|    explained_variance   | 0.749     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.067    |\n",
      "|    n_updates            | 17765     |\n",
      "|    policy_gradient_loss | -0.0623   |\n",
      "|    value_loss           | 0.0811    |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 78.1     |\n",
      "| time/              |          |\n",
      "|    fps             | 772      |\n",
      "|    iterations      | 936      |\n",
      "|    time_elapsed    | 310      |\n",
      "|    total_timesteps | 239616   |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 78.1       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 773        |\n",
      "|    iterations           | 937        |\n",
      "|    time_elapsed         | 310        |\n",
      "|    total_timesteps      | 239872     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.68654346 |\n",
      "|    clip_fraction        | 0.133      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0695    |\n",
      "|    explained_variance   | 0.829      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0751    |\n",
      "|    n_updates            | 17784      |\n",
      "|    policy_gradient_loss | -0.0599    |\n",
      "|    value_loss           | 0.057      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=240000, episode_reward=39.81 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 39.8      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 240000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.3243712 |\n",
      "|    clip_fraction        | 0.0944    |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0432   |\n",
      "|    explained_variance   | 0.56      |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0206   |\n",
      "|    n_updates            | 17803     |\n",
      "|    policy_gradient_loss | -0.0497   |\n",
      "|    value_loss           | 0.149     |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 77.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 772      |\n",
      "|    iterations      | 938      |\n",
      "|    time_elapsed    | 310      |\n",
      "|    total_timesteps | 240128   |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 77.9      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 773       |\n",
      "|    iterations           | 939       |\n",
      "|    time_elapsed         | 310       |\n",
      "|    total_timesteps      | 240384    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.2734884 |\n",
      "|    clip_fraction        | 0.118     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.094    |\n",
      "|    explained_variance   | 0.454     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.00444  |\n",
      "|    n_updates            | 17822     |\n",
      "|    policy_gradient_loss | -0.0475   |\n",
      "|    value_loss           | 0.263     |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=240500, episode_reward=50.46 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 50.5      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 240500    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.5724008 |\n",
      "|    clip_fraction        | 0.12      |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0512   |\n",
      "|    explained_variance   | 0.595     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0391   |\n",
      "|    n_updates            | 17841     |\n",
      "|    policy_gradient_loss | -0.0551   |\n",
      "|    value_loss           | 0.048     |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 77.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 772      |\n",
      "|    iterations      | 940      |\n",
      "|    time_elapsed    | 311      |\n",
      "|    total_timesteps | 240640   |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 77.9      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 773       |\n",
      "|    iterations           | 941       |\n",
      "|    time_elapsed         | 311       |\n",
      "|    total_timesteps      | 240896    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.2355623 |\n",
      "|    clip_fraction        | 0.135     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0524   |\n",
      "|    explained_variance   | 0.525     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0454   |\n",
      "|    n_updates            | 17860     |\n",
      "|    policy_gradient_loss | -0.0631   |\n",
      "|    value_loss           | 0.0983    |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=241000, episode_reward=59.08 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 59.1      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 241000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.6842396 |\n",
      "|    clip_fraction        | 0.132     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0581   |\n",
      "|    explained_variance   | 0.524     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0545   |\n",
      "|    n_updates            | 17879     |\n",
      "|    policy_gradient_loss | -0.0464   |\n",
      "|    value_loss           | 0.312     |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 77.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 772      |\n",
      "|    iterations      | 942      |\n",
      "|    time_elapsed    | 311      |\n",
      "|    total_timesteps | 241152   |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 77.9      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 773       |\n",
      "|    iterations           | 943       |\n",
      "|    time_elapsed         | 312       |\n",
      "|    total_timesteps      | 241408    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.6658925 |\n",
      "|    clip_fraction        | 0.171     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0926   |\n",
      "|    explained_variance   | 0.0586    |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0209   |\n",
      "|    n_updates            | 17898     |\n",
      "|    policy_gradient_loss | -0.0499   |\n",
      "|    value_loss           | 0.0723    |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=241500, episode_reward=77.66 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 77.7      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 241500    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.0156708 |\n",
      "|    clip_fraction        | 0.169     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0728   |\n",
      "|    explained_variance   | 0.652     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0996   |\n",
      "|    n_updates            | 17917     |\n",
      "|    policy_gradient_loss | -0.0713   |\n",
      "|    value_loss           | 0.11      |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 77.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 772      |\n",
      "|    iterations      | 944      |\n",
      "|    time_elapsed    | 312      |\n",
      "|    total_timesteps | 241664   |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 77.9      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 773       |\n",
      "|    iterations           | 945       |\n",
      "|    time_elapsed         | 312       |\n",
      "|    total_timesteps      | 241920    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.3845045 |\n",
      "|    clip_fraction        | 0.132     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0549   |\n",
      "|    explained_variance   | 0.785     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0509   |\n",
      "|    n_updates            | 17936     |\n",
      "|    policy_gradient_loss | -0.0656   |\n",
      "|    value_loss           | 0.0792    |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=242000, episode_reward=75.55 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 230        |\n",
      "|    mean_reward          | 75.6       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 242000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.43970835 |\n",
      "|    clip_fraction        | 0.177      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.147     |\n",
      "|    explained_variance   | 0.643      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0599    |\n",
      "|    n_updates            | 17955      |\n",
      "|    policy_gradient_loss | -0.0576    |\n",
      "|    value_loss           | 0.49       |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 77.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 773      |\n",
      "|    iterations      | 946      |\n",
      "|    time_elapsed    | 313      |\n",
      "|    total_timesteps | 242176   |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 77.9       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 773        |\n",
      "|    iterations           | 947        |\n",
      "|    time_elapsed         | 313        |\n",
      "|    total_timesteps      | 242432     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.67875826 |\n",
      "|    clip_fraction        | 0.169      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0753    |\n",
      "|    explained_variance   | 0.522      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.105     |\n",
      "|    n_updates            | 17974      |\n",
      "|    policy_gradient_loss | -0.0735    |\n",
      "|    value_loss           | 0.0595     |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=242500, episode_reward=76.89 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "--------------------------------------\n",
      "| eval/                   |          |\n",
      "|    mean_ep_length       | 230      |\n",
      "|    mean_reward          | 76.9     |\n",
      "| time/                   |          |\n",
      "|    total_timesteps      | 242500   |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 0.560372 |\n",
      "|    clip_fraction        | 0.138    |\n",
      "|    clip_range           | 0.4      |\n",
      "|    entropy_loss         | -0.0802  |\n",
      "|    explained_variance   | 0.724    |\n",
      "|    learning_rate        | 0.000389 |\n",
      "|    loss                 | -0.0579  |\n",
      "|    n_updates            | 17993    |\n",
      "|    policy_gradient_loss | -0.0628  |\n",
      "|    value_loss           | 0.156    |\n",
      "--------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 77.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 773      |\n",
      "|    iterations      | 948      |\n",
      "|    time_elapsed    | 313      |\n",
      "|    total_timesteps | 242688   |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 78         |\n",
      "| time/                   |            |\n",
      "|    fps                  | 773        |\n",
      "|    iterations           | 949        |\n",
      "|    time_elapsed         | 314        |\n",
      "|    total_timesteps      | 242944     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.68793035 |\n",
      "|    clip_fraction        | 0.18       |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0898    |\n",
      "|    explained_variance   | 0.789      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | 0.0376     |\n",
      "|    n_updates            | 18012      |\n",
      "|    policy_gradient_loss | -0.0711    |\n",
      "|    value_loss           | 0.344      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=243000, episode_reward=55.92 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 55.9      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 243000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.5931493 |\n",
      "|    clip_fraction        | 0.206     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.125    |\n",
      "|    explained_variance   | 0.23      |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | 0.103     |\n",
      "|    n_updates            | 18031     |\n",
      "|    policy_gradient_loss | -0.0536   |\n",
      "|    value_loss           | 0.0604    |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 78       |\n",
      "| time/              |          |\n",
      "|    fps             | 773      |\n",
      "|    iterations      | 950      |\n",
      "|    time_elapsed    | 314      |\n",
      "|    total_timesteps | 243200   |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 78         |\n",
      "| time/                   |            |\n",
      "|    fps                  | 773        |\n",
      "|    iterations           | 951        |\n",
      "|    time_elapsed         | 314        |\n",
      "|    total_timesteps      | 243456     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.32565224 |\n",
      "|    clip_fraction        | 0.118      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0888    |\n",
      "|    explained_variance   | 0.653      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0379    |\n",
      "|    n_updates            | 18050      |\n",
      "|    policy_gradient_loss | -0.051     |\n",
      "|    value_loss           | 0.159      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=243500, episode_reward=32.42 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 32.4      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 243500    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.1226745 |\n",
      "|    clip_fraction        | 0.179     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0546   |\n",
      "|    explained_variance   | 0.755     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0661   |\n",
      "|    n_updates            | 18069     |\n",
      "|    policy_gradient_loss | -0.0786   |\n",
      "|    value_loss           | 0.0863    |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 78       |\n",
      "| time/              |          |\n",
      "|    fps             | 773      |\n",
      "|    iterations      | 952      |\n",
      "|    time_elapsed    | 315      |\n",
      "|    total_timesteps | 243712   |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 78        |\n",
      "| time/                   |           |\n",
      "|    fps                  | 773       |\n",
      "|    iterations           | 953       |\n",
      "|    time_elapsed         | 315       |\n",
      "|    total_timesteps      | 243968    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.5601942 |\n",
      "|    clip_fraction        | 0.161     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.124    |\n",
      "|    explained_variance   | 0.723     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | 0.191     |\n",
      "|    n_updates            | 18088     |\n",
      "|    policy_gradient_loss | -0.0653   |\n",
      "|    value_loss           | 0.43      |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=244000, episode_reward=14.26 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 14.3      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 244000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.9242617 |\n",
      "|    clip_fraction        | 0.153     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0573   |\n",
      "|    explained_variance   | 0.615     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0195   |\n",
      "|    n_updates            | 18107     |\n",
      "|    policy_gradient_loss | -0.0525   |\n",
      "|    value_loss           | 0.0424    |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 78       |\n",
      "| time/              |          |\n",
      "|    fps             | 772      |\n",
      "|    iterations      | 954      |\n",
      "|    time_elapsed    | 315      |\n",
      "|    total_timesteps | 244224   |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 77.9       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 773        |\n",
      "|    iterations           | 955        |\n",
      "|    time_elapsed         | 316        |\n",
      "|    total_timesteps      | 244480     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.42012548 |\n",
      "|    clip_fraction        | 0.115      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0691    |\n",
      "|    explained_variance   | 0.785      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0665    |\n",
      "|    n_updates            | 18126      |\n",
      "|    policy_gradient_loss | -0.0508    |\n",
      "|    value_loss           | 0.154      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=244500, episode_reward=26.20 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 230        |\n",
      "|    mean_reward          | 26.2       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 244500     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.34006608 |\n",
      "|    clip_fraction        | 0.112      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0696    |\n",
      "|    explained_variance   | 0.0395     |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0312    |\n",
      "|    n_updates            | 18145      |\n",
      "|    policy_gradient_loss | -0.0545    |\n",
      "|    value_loss           | 0.409      |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 77.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 773      |\n",
      "|    iterations      | 956      |\n",
      "|    time_elapsed    | 316      |\n",
      "|    total_timesteps | 244736   |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 77.9       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 773        |\n",
      "|    iterations           | 957        |\n",
      "|    time_elapsed         | 316        |\n",
      "|    total_timesteps      | 244992     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.68182254 |\n",
      "|    clip_fraction        | 0.164      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0889    |\n",
      "|    explained_variance   | 0.296      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.105     |\n",
      "|    n_updates            | 18164      |\n",
      "|    policy_gradient_loss | -0.0556    |\n",
      "|    value_loss           | 0.049      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=245000, episode_reward=47.37 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 230        |\n",
      "|    mean_reward          | 47.4       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 245000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.37727508 |\n",
      "|    clip_fraction        | 0.102      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0775    |\n",
      "|    explained_variance   | 0.757      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.042     |\n",
      "|    n_updates            | 18183      |\n",
      "|    policy_gradient_loss | -0.043     |\n",
      "|    value_loss           | 0.179      |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 77.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 773      |\n",
      "|    iterations      | 958      |\n",
      "|    time_elapsed    | 317      |\n",
      "|    total_timesteps | 245248   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=245500, episode_reward=59.05 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 59        |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 245500    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.5117539 |\n",
      "|    clip_fraction        | 0.167     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0497   |\n",
      "|    explained_variance   | 0.787     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0892   |\n",
      "|    n_updates            | 18202     |\n",
      "|    policy_gradient_loss | -0.0519   |\n",
      "|    value_loss           | 0.0724    |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 77.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 773      |\n",
      "|    iterations      | 959      |\n",
      "|    time_elapsed    | 317      |\n",
      "|    total_timesteps | 245504   |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 77.9       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 773        |\n",
      "|    iterations           | 960        |\n",
      "|    time_elapsed         | 317        |\n",
      "|    total_timesteps      | 245760     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.59054005 |\n",
      "|    clip_fraction        | 0.168      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.124     |\n",
      "|    explained_variance   | 0.369      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0291    |\n",
      "|    n_updates            | 18221      |\n",
      "|    policy_gradient_loss | -0.0475    |\n",
      "|    value_loss           | 0.648      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=246000, episode_reward=75.71 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 230        |\n",
      "|    mean_reward          | 75.7       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 246000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.68880594 |\n",
      "|    clip_fraction        | 0.164      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0669    |\n",
      "|    explained_variance   | 0.694      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0907    |\n",
      "|    n_updates            | 18240      |\n",
      "|    policy_gradient_loss | -0.0698    |\n",
      "|    value_loss           | 0.0557     |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 77.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 773      |\n",
      "|    iterations      | 961      |\n",
      "|    time_elapsed    | 318      |\n",
      "|    total_timesteps | 246016   |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 78        |\n",
      "| time/                   |           |\n",
      "|    fps                  | 773       |\n",
      "|    iterations           | 962       |\n",
      "|    time_elapsed         | 318       |\n",
      "|    total_timesteps      | 246272    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.6723238 |\n",
      "|    clip_fraction        | 0.143     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0657   |\n",
      "|    explained_variance   | 0.665     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0333   |\n",
      "|    n_updates            | 18259     |\n",
      "|    policy_gradient_loss | -0.0561   |\n",
      "|    value_loss           | 0.209     |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=246500, episode_reward=76.06 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 230        |\n",
      "|    mean_reward          | 76.1       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 246500     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.25521505 |\n",
      "|    clip_fraction        | 0.112      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.092     |\n",
      "|    explained_variance   | 0.636      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | 0.0493     |\n",
      "|    n_updates            | 18278      |\n",
      "|    policy_gradient_loss | -0.0321    |\n",
      "|    value_loss           | 0.535      |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 78       |\n",
      "| time/              |          |\n",
      "|    fps             | 773      |\n",
      "|    iterations      | 963      |\n",
      "|    time_elapsed    | 318      |\n",
      "|    total_timesteps | 246528   |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 78         |\n",
      "| time/                   |            |\n",
      "|    fps                  | 773        |\n",
      "|    iterations           | 964        |\n",
      "|    time_elapsed         | 319        |\n",
      "|    total_timesteps      | 246784     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.66597235 |\n",
      "|    clip_fraction        | 0.199      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.106     |\n",
      "|    explained_variance   | 0.313      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0265    |\n",
      "|    n_updates            | 18297      |\n",
      "|    policy_gradient_loss | -0.0716    |\n",
      "|    value_loss           | 0.0595     |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=247000, episode_reward=75.32 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 230        |\n",
      "|    mean_reward          | 75.3       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 247000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.38484055 |\n",
      "|    clip_fraction        | 0.0933     |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0433    |\n",
      "|    explained_variance   | 0.714      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0409    |\n",
      "|    n_updates            | 18316      |\n",
      "|    policy_gradient_loss | -0.045     |\n",
      "|    value_loss           | 0.17       |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 78       |\n",
      "| time/              |          |\n",
      "|    fps             | 773      |\n",
      "|    iterations      | 965      |\n",
      "|    time_elapsed    | 319      |\n",
      "|    total_timesteps | 247040   |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 78.2       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 773        |\n",
      "|    iterations           | 966        |\n",
      "|    time_elapsed         | 319        |\n",
      "|    total_timesteps      | 247296     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.94522226 |\n",
      "|    clip_fraction        | 0.158      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0619    |\n",
      "|    explained_variance   | 0.754      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.111     |\n",
      "|    n_updates            | 18335      |\n",
      "|    policy_gradient_loss | -0.0491    |\n",
      "|    value_loss           | 0.0665     |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=247500, episode_reward=63.59 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 230        |\n",
      "|    mean_reward          | 63.6       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 247500     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.75768286 |\n",
      "|    clip_fraction        | 0.181      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.128     |\n",
      "|    explained_variance   | 0.441      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0182    |\n",
      "|    n_updates            | 18354      |\n",
      "|    policy_gradient_loss | -0.056     |\n",
      "|    value_loss           | 0.451      |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 78.2     |\n",
      "| time/              |          |\n",
      "|    fps             | 773      |\n",
      "|    iterations      | 967      |\n",
      "|    time_elapsed    | 320      |\n",
      "|    total_timesteps | 247552   |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 78.2       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 773        |\n",
      "|    iterations           | 968        |\n",
      "|    time_elapsed         | 320        |\n",
      "|    total_timesteps      | 247808     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.41882914 |\n",
      "|    clip_fraction        | 0.112      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0544    |\n",
      "|    explained_variance   | 0.917      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.049     |\n",
      "|    n_updates            | 18373      |\n",
      "|    policy_gradient_loss | -0.0434    |\n",
      "|    value_loss           | 0.0538     |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=248000, episode_reward=85.43 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 230        |\n",
      "|    mean_reward          | 85.4       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 248000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.70744014 |\n",
      "|    clip_fraction        | 0.126      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.071     |\n",
      "|    explained_variance   | 0.408      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0832    |\n",
      "|    n_updates            | 18392      |\n",
      "|    policy_gradient_loss | -0.0528    |\n",
      "|    value_loss           | 0.181      |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 78       |\n",
      "| time/              |          |\n",
      "|    fps             | 773      |\n",
      "|    iterations      | 969      |\n",
      "|    time_elapsed    | 320      |\n",
      "|    total_timesteps | 248064   |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 78        |\n",
      "| time/                   |           |\n",
      "|    fps                  | 773       |\n",
      "|    iterations           | 970       |\n",
      "|    time_elapsed         | 320       |\n",
      "|    total_timesteps      | 248320    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.6971912 |\n",
      "|    clip_fraction        | 0.153     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.106    |\n",
      "|    explained_variance   | 0.809     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0949   |\n",
      "|    n_updates            | 18411     |\n",
      "|    policy_gradient_loss | -0.0685   |\n",
      "|    value_loss           | 0.241     |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=248500, episode_reward=76.95 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 77        |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 248500    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.7572373 |\n",
      "|    clip_fraction        | 0.216     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.111    |\n",
      "|    explained_variance   | 0.273     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0825   |\n",
      "|    n_updates            | 18430     |\n",
      "|    policy_gradient_loss | -0.0804   |\n",
      "|    value_loss           | 0.0509    |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 78       |\n",
      "| time/              |          |\n",
      "|    fps             | 773      |\n",
      "|    iterations      | 971      |\n",
      "|    time_elapsed    | 321      |\n",
      "|    total_timesteps | 248576   |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 78        |\n",
      "| time/                   |           |\n",
      "|    fps                  | 773       |\n",
      "|    iterations           | 972       |\n",
      "|    time_elapsed         | 321       |\n",
      "|    total_timesteps      | 248832    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.3989364 |\n",
      "|    clip_fraction        | 0.12      |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0623   |\n",
      "|    explained_variance   | 0.657     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0556   |\n",
      "|    n_updates            | 18449     |\n",
      "|    policy_gradient_loss | -0.0611   |\n",
      "|    value_loss           | 0.131     |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=249000, episode_reward=103.77 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 230        |\n",
      "|    mean_reward          | 104        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 249000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.45942047 |\n",
      "|    clip_fraction        | 0.114      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0517    |\n",
      "|    explained_variance   | 0.852      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0303    |\n",
      "|    n_updates            | 18468      |\n",
      "|    policy_gradient_loss | -0.0478    |\n",
      "|    value_loss           | 0.0724     |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 78.1     |\n",
      "| time/              |          |\n",
      "|    fps             | 773      |\n",
      "|    iterations      | 973      |\n",
      "|    time_elapsed    | 322      |\n",
      "|    total_timesteps | 249088   |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 78.1      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 773       |\n",
      "|    iterations           | 974       |\n",
      "|    time_elapsed         | 322       |\n",
      "|    total_timesteps      | 249344    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.5858426 |\n",
      "|    clip_fraction        | 0.203     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.16     |\n",
      "|    explained_variance   | 0.475     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | 0.0385    |\n",
      "|    n_updates            | 18487     |\n",
      "|    policy_gradient_loss | -0.0514   |\n",
      "|    value_loss           | 0.63      |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=249500, episode_reward=64.31 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 64.3      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 249500    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.5569782 |\n",
      "|    clip_fraction        | 0.14      |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0551   |\n",
      "|    explained_variance   | 0.709     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.058    |\n",
      "|    n_updates            | 18506     |\n",
      "|    policy_gradient_loss | -0.0454   |\n",
      "|    value_loss           | 0.14      |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 78.1     |\n",
      "| time/              |          |\n",
      "|    fps             | 773      |\n",
      "|    iterations      | 975      |\n",
      "|    time_elapsed    | 322      |\n",
      "|    total_timesteps | 249600   |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 78        |\n",
      "| time/                   |           |\n",
      "|    fps                  | 773       |\n",
      "|    iterations           | 976       |\n",
      "|    time_elapsed         | 322       |\n",
      "|    total_timesteps      | 249856    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.5975709 |\n",
      "|    clip_fraction        | 0.166     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.07     |\n",
      "|    explained_variance   | 0.508     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0766   |\n",
      "|    n_updates            | 18525     |\n",
      "|    policy_gradient_loss | -0.0814   |\n",
      "|    value_loss           | 0.124     |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=250000, episode_reward=64.04 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 64        |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 250000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.4895569 |\n",
      "|    clip_fraction        | 0.132     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0899   |\n",
      "|    explained_variance   | 0.618     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | 0.0526    |\n",
      "|    n_updates            | 18544     |\n",
      "|    policy_gradient_loss | -0.0461   |\n",
      "|    value_loss           | 0.529     |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 78       |\n",
      "| time/              |          |\n",
      "|    fps             | 773      |\n",
      "|    iterations      | 977      |\n",
      "|    time_elapsed    | 323      |\n",
      "|    total_timesteps | 250112   |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 78        |\n",
      "| time/                   |           |\n",
      "|    fps                  | 773       |\n",
      "|    iterations           | 978       |\n",
      "|    time_elapsed         | 323       |\n",
      "|    total_timesteps      | 250368    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.5116569 |\n",
      "|    clip_fraction        | 0.177     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0885   |\n",
      "|    explained_variance   | -0.0757   |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0497   |\n",
      "|    n_updates            | 18563     |\n",
      "|    policy_gradient_loss | -0.0651   |\n",
      "|    value_loss           | 0.12      |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=250500, episode_reward=53.61 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 230        |\n",
      "|    mean_reward          | 53.6       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 250500     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.56618786 |\n",
      "|    clip_fraction        | 0.127      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0688    |\n",
      "|    explained_variance   | 0.626      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0264    |\n",
      "|    n_updates            | 18582      |\n",
      "|    policy_gradient_loss | -0.0525    |\n",
      "|    value_loss           | 0.262      |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 78       |\n",
      "| time/              |          |\n",
      "|    fps             | 773      |\n",
      "|    iterations      | 979      |\n",
      "|    time_elapsed    | 324      |\n",
      "|    total_timesteps | 250624   |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 77.8      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 773       |\n",
      "|    iterations           | 980       |\n",
      "|    time_elapsed         | 324       |\n",
      "|    total_timesteps      | 250880    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.8160743 |\n",
      "|    clip_fraction        | 0.14      |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0518   |\n",
      "|    explained_variance   | 0.857     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0179   |\n",
      "|    n_updates            | 18601     |\n",
      "|    policy_gradient_loss | -0.0498   |\n",
      "|    value_loss           | 0.12      |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=251000, episode_reward=80.36 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 80.4      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 251000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.8715905 |\n",
      "|    clip_fraction        | 0.198     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.102    |\n",
      "|    explained_variance   | 0.295     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | 0.0171    |\n",
      "|    n_updates            | 18620     |\n",
      "|    policy_gradient_loss | -0.0514   |\n",
      "|    value_loss           | 0.268     |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 77.8     |\n",
      "| time/              |          |\n",
      "|    fps             | 773      |\n",
      "|    iterations      | 981      |\n",
      "|    time_elapsed    | 324      |\n",
      "|    total_timesteps | 251136   |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 77.8       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 773        |\n",
      "|    iterations           | 982        |\n",
      "|    time_elapsed         | 324        |\n",
      "|    total_timesteps      | 251392     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.85561633 |\n",
      "|    clip_fraction        | 0.164      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0626    |\n",
      "|    explained_variance   | 0.805      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0474    |\n",
      "|    n_updates            | 18639      |\n",
      "|    policy_gradient_loss | -0.0622    |\n",
      "|    value_loss           | 0.0868     |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=251500, episode_reward=56.74 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 56.7      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 251500    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.5540952 |\n",
      "|    clip_fraction        | 0.134     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0633   |\n",
      "|    explained_variance   | 0.728     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0658   |\n",
      "|    n_updates            | 18658     |\n",
      "|    policy_gradient_loss | -0.0467   |\n",
      "|    value_loss           | 0.0822    |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 77.8     |\n",
      "| time/              |          |\n",
      "|    fps             | 773      |\n",
      "|    iterations      | 983      |\n",
      "|    time_elapsed    | 325      |\n",
      "|    total_timesteps | 251648   |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 77.8       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 773        |\n",
      "|    iterations           | 984        |\n",
      "|    time_elapsed         | 325        |\n",
      "|    total_timesteps      | 251904     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.55283254 |\n",
      "|    clip_fraction        | 0.135      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.102     |\n",
      "|    explained_variance   | 0.848      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0249    |\n",
      "|    n_updates            | 18677      |\n",
      "|    policy_gradient_loss | -0.0432    |\n",
      "|    value_loss           | 0.214      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=252000, episode_reward=40.89 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 40.9      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 252000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.8323261 |\n",
      "|    clip_fraction        | 0.112     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0374   |\n",
      "|    explained_variance   | 0.186     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | 0.00286   |\n",
      "|    n_updates            | 18696     |\n",
      "|    policy_gradient_loss | -0.033    |\n",
      "|    value_loss           | 0.0553    |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 77.8     |\n",
      "| time/              |          |\n",
      "|    fps             | 773      |\n",
      "|    iterations      | 985      |\n",
      "|    time_elapsed    | 326      |\n",
      "|    total_timesteps | 252160   |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 77.8      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 773       |\n",
      "|    iterations           | 986       |\n",
      "|    time_elapsed         | 326       |\n",
      "|    total_timesteps      | 252416    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.9472555 |\n",
      "|    clip_fraction        | 0.13      |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0487   |\n",
      "|    explained_variance   | 0.736     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0983   |\n",
      "|    n_updates            | 18715     |\n",
      "|    policy_gradient_loss | -0.0626   |\n",
      "|    value_loss           | 0.127     |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=252500, episode_reward=79.88 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 230        |\n",
      "|    mean_reward          | 79.9       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 252500     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.34448326 |\n",
      "|    clip_fraction        | 0.106      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.043     |\n",
      "|    explained_variance   | 0.922      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.045     |\n",
      "|    n_updates            | 18734      |\n",
      "|    policy_gradient_loss | -0.0507    |\n",
      "|    value_loss           | 0.0992     |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 77.8     |\n",
      "| time/              |          |\n",
      "|    fps             | 773      |\n",
      "|    iterations      | 987      |\n",
      "|    time_elapsed    | 326      |\n",
      "|    total_timesteps | 252672   |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 77.8      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 773       |\n",
      "|    iterations           | 988       |\n",
      "|    time_elapsed         | 327       |\n",
      "|    total_timesteps      | 252928    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.7116457 |\n",
      "|    clip_fraction        | 0.181     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.11     |\n",
      "|    explained_variance   | 0.607     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0561   |\n",
      "|    n_updates            | 18753     |\n",
      "|    policy_gradient_loss | -0.0377   |\n",
      "|    value_loss           | 0.154     |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=253000, episode_reward=80.10 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 230        |\n",
      "|    mean_reward          | 80.1       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 253000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.42221466 |\n",
      "|    clip_fraction        | 0.0775     |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0391    |\n",
      "|    explained_variance   | 0.827      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.00694   |\n",
      "|    n_updates            | 18772      |\n",
      "|    policy_gradient_loss | -0.0275    |\n",
      "|    value_loss           | 0.0712     |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 77.8     |\n",
      "| time/              |          |\n",
      "|    fps             | 773      |\n",
      "|    iterations      | 989      |\n",
      "|    time_elapsed    | 327      |\n",
      "|    total_timesteps | 253184   |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 77.9      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 773       |\n",
      "|    iterations           | 990       |\n",
      "|    time_elapsed         | 327       |\n",
      "|    total_timesteps      | 253440    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.3094496 |\n",
      "|    clip_fraction        | 0.0831    |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0381   |\n",
      "|    explained_variance   | 0.749     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0288   |\n",
      "|    n_updates            | 18791     |\n",
      "|    policy_gradient_loss | -0.0396   |\n",
      "|    value_loss           | 0.0804    |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=253500, episode_reward=95.67 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 230        |\n",
      "|    mean_reward          | 95.7       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 253500     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.67014027 |\n",
      "|    clip_fraction        | 0.152      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0836    |\n",
      "|    explained_variance   | 0.879      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0254    |\n",
      "|    n_updates            | 18810      |\n",
      "|    policy_gradient_loss | -0.0505    |\n",
      "|    value_loss           | 0.245      |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 77.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 773      |\n",
      "|    iterations      | 991      |\n",
      "|    time_elapsed    | 328      |\n",
      "|    total_timesteps | 253696   |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 77.9      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 773       |\n",
      "|    iterations           | 992       |\n",
      "|    time_elapsed         | 328       |\n",
      "|    total_timesteps      | 253952    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.6835747 |\n",
      "|    clip_fraction        | 0.183     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0734   |\n",
      "|    explained_variance   | 0.411     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.068    |\n",
      "|    n_updates            | 18829     |\n",
      "|    policy_gradient_loss | -0.0669   |\n",
      "|    value_loss           | 0.0489    |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=254000, episode_reward=115.76 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 230        |\n",
      "|    mean_reward          | 116        |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 254000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.82431114 |\n",
      "|    clip_fraction        | 0.113      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0509    |\n",
      "|    explained_variance   | 0.619      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0427    |\n",
      "|    n_updates            | 18848      |\n",
      "|    policy_gradient_loss | -0.0649    |\n",
      "|    value_loss           | 0.125      |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 77.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 773      |\n",
      "|    iterations      | 993      |\n",
      "|    time_elapsed    | 328      |\n",
      "|    total_timesteps | 254208   |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 77.8      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 773       |\n",
      "|    iterations           | 994       |\n",
      "|    time_elapsed         | 328       |\n",
      "|    total_timesteps      | 254464    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.2836754 |\n",
      "|    clip_fraction        | 0.149     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0519   |\n",
      "|    explained_variance   | 0.882     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0216   |\n",
      "|    n_updates            | 18867     |\n",
      "|    policy_gradient_loss | -0.0543   |\n",
      "|    value_loss           | 0.148     |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=254500, episode_reward=93.24 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 230        |\n",
      "|    mean_reward          | 93.2       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 254500     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.42297426 |\n",
      "|    clip_fraction        | 0.18       |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.104     |\n",
      "|    explained_variance   | 0.57       |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.15      |\n",
      "|    n_updates            | 18886      |\n",
      "|    policy_gradient_loss | -0.0661    |\n",
      "|    value_loss           | 0.126      |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 77.8     |\n",
      "| time/              |          |\n",
      "|    fps             | 773      |\n",
      "|    iterations      | 995      |\n",
      "|    time_elapsed    | 329      |\n",
      "|    total_timesteps | 254720   |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 77.8       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 773        |\n",
      "|    iterations           | 996        |\n",
      "|    time_elapsed         | 329        |\n",
      "|    total_timesteps      | 254976     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.46358126 |\n",
      "|    clip_fraction        | 0.0761     |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0276    |\n",
      "|    explained_variance   | 0.778      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0782    |\n",
      "|    n_updates            | 18905      |\n",
      "|    policy_gradient_loss | -0.048     |\n",
      "|    value_loss           | 0.092      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=255000, episode_reward=94.56 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 230        |\n",
      "|    mean_reward          | 94.6       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 255000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.51339823 |\n",
      "|    clip_fraction        | 0.134      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0558    |\n",
      "|    explained_variance   | 0.623      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0781    |\n",
      "|    n_updates            | 18924      |\n",
      "|    policy_gradient_loss | -0.0568    |\n",
      "|    value_loss           | 0.146      |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 77.6     |\n",
      "| time/              |          |\n",
      "|    fps             | 773      |\n",
      "|    iterations      | 997      |\n",
      "|    time_elapsed    | 329      |\n",
      "|    total_timesteps | 255232   |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 77.6       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 773        |\n",
      "|    iterations           | 998        |\n",
      "|    time_elapsed         | 330        |\n",
      "|    total_timesteps      | 255488     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.44134858 |\n",
      "|    clip_fraction        | 0.117      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0802    |\n",
      "|    explained_variance   | 0.842      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0591    |\n",
      "|    n_updates            | 18943      |\n",
      "|    policy_gradient_loss | -0.0513    |\n",
      "|    value_loss           | 0.216      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=255500, episode_reward=80.72 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "--------------------------------------\n",
      "| eval/                   |          |\n",
      "|    mean_ep_length       | 230      |\n",
      "|    mean_reward          | 80.7     |\n",
      "| time/                   |          |\n",
      "|    total_timesteps      | 255500   |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 0.626345 |\n",
      "|    clip_fraction        | 0.128    |\n",
      "|    clip_range           | 0.4      |\n",
      "|    entropy_loss         | -0.0575  |\n",
      "|    explained_variance   | 0.535    |\n",
      "|    learning_rate        | 0.000389 |\n",
      "|    loss                 | -0.0384  |\n",
      "|    n_updates            | 18962    |\n",
      "|    policy_gradient_loss | -0.0489  |\n",
      "|    value_loss           | 0.049    |\n",
      "--------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 77.6     |\n",
      "| time/              |          |\n",
      "|    fps             | 773      |\n",
      "|    iterations      | 999      |\n",
      "|    time_elapsed    | 330      |\n",
      "|    total_timesteps | 255744   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=256000, episode_reward=93.41 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 230        |\n",
      "|    mean_reward          | 93.4       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 256000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.75724286 |\n",
      "|    clip_fraction        | 0.124      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0394    |\n",
      "|    explained_variance   | 0.733      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.015     |\n",
      "|    n_updates            | 18981      |\n",
      "|    policy_gradient_loss | -0.0479    |\n",
      "|    value_loss           | 0.129      |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 77.6     |\n",
      "| time/              |          |\n",
      "|    fps             | 773      |\n",
      "|    iterations      | 1000     |\n",
      "|    time_elapsed    | 330      |\n",
      "|    total_timesteps | 256000   |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 77.5      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 773       |\n",
      "|    iterations           | 1001      |\n",
      "|    time_elapsed         | 331       |\n",
      "|    total_timesteps      | 256256    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.5328254 |\n",
      "|    clip_fraction        | 0.138     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0604   |\n",
      "|    explained_variance   | 0.869     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0564   |\n",
      "|    n_updates            | 19000     |\n",
      "|    policy_gradient_loss | -0.0523   |\n",
      "|    value_loss           | 0.159     |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=256500, episode_reward=99.62 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 99.6      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 256500    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.6369076 |\n",
      "|    clip_fraction        | 0.131     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0779   |\n",
      "|    explained_variance   | 0.335     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.055    |\n",
      "|    n_updates            | 19019     |\n",
      "|    policy_gradient_loss | -0.0603   |\n",
      "|    value_loss           | 0.156     |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 77.5     |\n",
      "| time/              |          |\n",
      "|    fps             | 773      |\n",
      "|    iterations      | 1002     |\n",
      "|    time_elapsed    | 331      |\n",
      "|    total_timesteps | 256512   |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 77.5       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 773        |\n",
      "|    iterations           | 1003       |\n",
      "|    time_elapsed         | 331        |\n",
      "|    total_timesteps      | 256768     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.62743115 |\n",
      "|    clip_fraction        | 0.09       |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0293    |\n",
      "|    explained_variance   | 0.727      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0253    |\n",
      "|    n_updates            | 19038      |\n",
      "|    policy_gradient_loss | -0.0423    |\n",
      "|    value_loss           | 0.0762     |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=257000, episode_reward=75.10 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 75.1      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 257000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.3380043 |\n",
      "|    clip_fraction        | 0.0983    |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0416   |\n",
      "|    explained_variance   | 0.431     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0208   |\n",
      "|    n_updates            | 19057     |\n",
      "|    policy_gradient_loss | -0.0369   |\n",
      "|    value_loss           | 0.23      |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 77.6     |\n",
      "| time/              |          |\n",
      "|    fps             | 773      |\n",
      "|    iterations      | 1004     |\n",
      "|    time_elapsed    | 332      |\n",
      "|    total_timesteps | 257024   |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 77.6      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 773       |\n",
      "|    iterations           | 1005      |\n",
      "|    time_elapsed         | 332       |\n",
      "|    total_timesteps      | 257280    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.4422922 |\n",
      "|    clip_fraction        | 0.11      |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0508   |\n",
      "|    explained_variance   | 0.906     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0221   |\n",
      "|    n_updates            | 19076     |\n",
      "|    policy_gradient_loss | -0.0418   |\n",
      "|    value_loss           | 0.132     |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=257500, episode_reward=73.32 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 73.3      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 257500    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.8477168 |\n",
      "|    clip_fraction        | 0.141     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0391   |\n",
      "|    explained_variance   | 0.522     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0707   |\n",
      "|    n_updates            | 19095     |\n",
      "|    policy_gradient_loss | -0.0561   |\n",
      "|    value_loss           | 0.0544    |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 77.6     |\n",
      "| time/              |          |\n",
      "|    fps             | 773      |\n",
      "|    iterations      | 1006     |\n",
      "|    time_elapsed    | 333      |\n",
      "|    total_timesteps | 257536   |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 77.6       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 773        |\n",
      "|    iterations           | 1007       |\n",
      "|    time_elapsed         | 333        |\n",
      "|    total_timesteps      | 257792     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.54166484 |\n",
      "|    clip_fraction        | 0.0789     |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0271    |\n",
      "|    explained_variance   | 0.499      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0641    |\n",
      "|    n_updates            | 19114      |\n",
      "|    policy_gradient_loss | -0.0253    |\n",
      "|    value_loss           | 0.107      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=258000, episode_reward=77.79 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 77.8      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 258000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.3516056 |\n",
      "|    clip_fraction        | 0.101     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0533   |\n",
      "|    explained_variance   | 0.392     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0488   |\n",
      "|    n_updates            | 19133     |\n",
      "|    policy_gradient_loss | -0.0465   |\n",
      "|    value_loss           | 0.466     |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 77.4     |\n",
      "| time/              |          |\n",
      "|    fps             | 773      |\n",
      "|    iterations      | 1008     |\n",
      "|    time_elapsed    | 333      |\n",
      "|    total_timesteps | 258048   |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 77.4       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 773        |\n",
      "|    iterations           | 1009       |\n",
      "|    time_elapsed         | 334        |\n",
      "|    total_timesteps      | 258304     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.87148845 |\n",
      "|    clip_fraction        | 0.166      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.07      |\n",
      "|    explained_variance   | 0.408      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0587    |\n",
      "|    n_updates            | 19152      |\n",
      "|    policy_gradient_loss | -0.062     |\n",
      "|    value_loss           | 0.0627     |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=258500, episode_reward=62.31 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 62.3      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 258500    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.0670254 |\n",
      "|    clip_fraction        | 0.104     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0263   |\n",
      "|    explained_variance   | 0.305     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0622   |\n",
      "|    n_updates            | 19171     |\n",
      "|    policy_gradient_loss | -0.0498   |\n",
      "|    value_loss           | 0.0803    |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 77.4     |\n",
      "| time/              |          |\n",
      "|    fps             | 772      |\n",
      "|    iterations      | 1010     |\n",
      "|    time_elapsed    | 334      |\n",
      "|    total_timesteps | 258560   |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 77.5       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 773        |\n",
      "|    iterations           | 1011       |\n",
      "|    time_elapsed         | 334        |\n",
      "|    total_timesteps      | 258816     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.46911037 |\n",
      "|    clip_fraction        | 0.113      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0556    |\n",
      "|    explained_variance   | 0.665      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0339    |\n",
      "|    n_updates            | 19190      |\n",
      "|    policy_gradient_loss | -0.0368    |\n",
      "|    value_loss           | 0.17       |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=259000, episode_reward=101.55 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 102       |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 259000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.8562455 |\n",
      "|    clip_fraction        | 0.139     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0921   |\n",
      "|    explained_variance   | 0.746     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0151   |\n",
      "|    n_updates            | 19209     |\n",
      "|    policy_gradient_loss | -0.044    |\n",
      "|    value_loss           | 0.181     |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 77.5     |\n",
      "| time/              |          |\n",
      "|    fps             | 772      |\n",
      "|    iterations      | 1012     |\n",
      "|    time_elapsed    | 335      |\n",
      "|    total_timesteps | 259072   |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 77.5      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 773       |\n",
      "|    iterations           | 1013      |\n",
      "|    time_elapsed         | 335       |\n",
      "|    total_timesteps      | 259328    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.8168447 |\n",
      "|    clip_fraction        | 0.111     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0372   |\n",
      "|    explained_variance   | 0.428     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0445   |\n",
      "|    n_updates            | 19228     |\n",
      "|    policy_gradient_loss | -0.0401   |\n",
      "|    value_loss           | 0.0304    |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=259500, episode_reward=72.68 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 230        |\n",
      "|    mean_reward          | 72.7       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 259500     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.35287037 |\n",
      "|    clip_fraction        | 0.0903     |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0464    |\n",
      "|    explained_variance   | 0.728      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0583    |\n",
      "|    n_updates            | 19247      |\n",
      "|    policy_gradient_loss | -0.04      |\n",
      "|    value_loss           | 0.106      |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 77.5     |\n",
      "| time/              |          |\n",
      "|    fps             | 773      |\n",
      "|    iterations      | 1014     |\n",
      "|    time_elapsed    | 335      |\n",
      "|    total_timesteps | 259584   |\n",
      "---------------------------------\n",
      "--------------------------------------\n",
      "| rollout/                |          |\n",
      "|    ep_len_mean          | 889      |\n",
      "|    ep_rew_mean          | 77.4     |\n",
      "| time/                   |          |\n",
      "|    fps                  | 773      |\n",
      "|    iterations           | 1015     |\n",
      "|    time_elapsed         | 336      |\n",
      "|    total_timesteps      | 259840   |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 0.470072 |\n",
      "|    clip_fraction        | 0.116    |\n",
      "|    clip_range           | 0.4      |\n",
      "|    entropy_loss         | -0.0645  |\n",
      "|    explained_variance   | 0.722    |\n",
      "|    learning_rate        | 0.000389 |\n",
      "|    loss                 | -0.0105  |\n",
      "|    n_updates            | 19266    |\n",
      "|    policy_gradient_loss | -0.039   |\n",
      "|    value_loss           | 0.275    |\n",
      "--------------------------------------\n",
      "Eval num_timesteps=260000, episode_reward=47.27 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "--------------------------------------\n",
      "| eval/                   |          |\n",
      "|    mean_ep_length       | 230      |\n",
      "|    mean_reward          | 47.3     |\n",
      "| time/                   |          |\n",
      "|    total_timesteps      | 260000   |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 0.76845  |\n",
      "|    clip_fraction        | 0.131    |\n",
      "|    clip_range           | 0.4      |\n",
      "|    entropy_loss         | -0.0598  |\n",
      "|    explained_variance   | -0.139   |\n",
      "|    learning_rate        | 0.000389 |\n",
      "|    loss                 | -0.03    |\n",
      "|    n_updates            | 19285    |\n",
      "|    policy_gradient_loss | -0.0556  |\n",
      "|    value_loss           | 0.0835   |\n",
      "--------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 77.4     |\n",
      "| time/              |          |\n",
      "|    fps             | 773      |\n",
      "|    iterations      | 1016     |\n",
      "|    time_elapsed    | 336      |\n",
      "|    total_timesteps | 260096   |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 77.4       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 773        |\n",
      "|    iterations           | 1017       |\n",
      "|    time_elapsed         | 336        |\n",
      "|    total_timesteps      | 260352     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.43626228 |\n",
      "|    clip_fraction        | 0.0779     |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0296    |\n",
      "|    explained_variance   | 0.662      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0607    |\n",
      "|    n_updates            | 19304      |\n",
      "|    policy_gradient_loss | -0.0398    |\n",
      "|    value_loss           | 0.112      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=260500, episode_reward=45.07 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 230        |\n",
      "|    mean_reward          | 45.1       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 260500     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.53850144 |\n",
      "|    clip_fraction        | 0.102      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0478    |\n",
      "|    explained_variance   | 0.779      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0441    |\n",
      "|    n_updates            | 19323      |\n",
      "|    policy_gradient_loss | -0.0442    |\n",
      "|    value_loss           | 0.0964     |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 77.3     |\n",
      "| time/              |          |\n",
      "|    fps             | 773      |\n",
      "|    iterations      | 1018     |\n",
      "|    time_elapsed    | 337      |\n",
      "|    total_timesteps | 260608   |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 77.3      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 773       |\n",
      "|    iterations           | 1019      |\n",
      "|    time_elapsed         | 337       |\n",
      "|    total_timesteps      | 260864    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.6415012 |\n",
      "|    clip_fraction        | 0.164     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.079    |\n",
      "|    explained_variance   | 0.844     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0313   |\n",
      "|    n_updates            | 19342     |\n",
      "|    policy_gradient_loss | -0.0525   |\n",
      "|    value_loss           | 0.186     |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=261000, episode_reward=62.41 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 230        |\n",
      "|    mean_reward          | 62.4       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 261000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.70119345 |\n",
      "|    clip_fraction        | 0.0818     |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0195    |\n",
      "|    explained_variance   | 0.428      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0342    |\n",
      "|    n_updates            | 19361      |\n",
      "|    policy_gradient_loss | -0.0363    |\n",
      "|    value_loss           | 0.0367     |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 77.3     |\n",
      "| time/              |          |\n",
      "|    fps             | 773      |\n",
      "|    iterations      | 1020     |\n",
      "|    time_elapsed    | 337      |\n",
      "|    total_timesteps | 261120   |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 77.3      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 773       |\n",
      "|    iterations           | 1021      |\n",
      "|    time_elapsed         | 337       |\n",
      "|    total_timesteps      | 261376    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.5055249 |\n",
      "|    clip_fraction        | 0.0853    |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0374   |\n",
      "|    explained_variance   | 0.772     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0633   |\n",
      "|    n_updates            | 19380     |\n",
      "|    policy_gradient_loss | -0.0326   |\n",
      "|    value_loss           | 0.131     |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=261500, episode_reward=76.57 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 76.6      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 261500    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.5127325 |\n",
      "|    clip_fraction        | 0.12      |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0545   |\n",
      "|    explained_variance   | 0.387     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0738   |\n",
      "|    n_updates            | 19399     |\n",
      "|    policy_gradient_loss | -0.0547   |\n",
      "|    value_loss           | 0.257     |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 77.3     |\n",
      "| time/              |          |\n",
      "|    fps             | 773      |\n",
      "|    iterations      | 1022     |\n",
      "|    time_elapsed    | 338      |\n",
      "|    total_timesteps | 261632   |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 77.3       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 773        |\n",
      "|    iterations           | 1023       |\n",
      "|    time_elapsed         | 338        |\n",
      "|    total_timesteps      | 261888     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.58352876 |\n",
      "|    clip_fraction        | 0.144      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0647    |\n",
      "|    explained_variance   | 0.272      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0178    |\n",
      "|    n_updates            | 19418      |\n",
      "|    policy_gradient_loss | -0.044     |\n",
      "|    value_loss           | 0.0698     |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=262000, episode_reward=55.81 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 55.8      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 262000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.5064565 |\n",
      "|    clip_fraction        | 0.0915    |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.037    |\n",
      "|    explained_variance   | 0.662     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0383   |\n",
      "|    n_updates            | 19437     |\n",
      "|    policy_gradient_loss | -0.0358   |\n",
      "|    value_loss           | 0.0972    |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 77.3     |\n",
      "| time/              |          |\n",
      "|    fps             | 773      |\n",
      "|    iterations      | 1024     |\n",
      "|    time_elapsed    | 338      |\n",
      "|    total_timesteps | 262144   |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 77.4      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 773       |\n",
      "|    iterations           | 1025      |\n",
      "|    time_elapsed         | 339       |\n",
      "|    total_timesteps      | 262400    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.4691478 |\n",
      "|    clip_fraction        | 0.102     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0371   |\n",
      "|    explained_variance   | 0.836     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0382   |\n",
      "|    n_updates            | 19456     |\n",
      "|    policy_gradient_loss | -0.0456   |\n",
      "|    value_loss           | 0.056     |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=262500, episode_reward=69.33 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 69.3      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 262500    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.8988962 |\n",
      "|    clip_fraction        | 0.168     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0802   |\n",
      "|    explained_variance   | 0.646     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | 0.0125    |\n",
      "|    n_updates            | 19475     |\n",
      "|    policy_gradient_loss | -0.0438   |\n",
      "|    value_loss           | 0.357     |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 77.4     |\n",
      "| time/              |          |\n",
      "|    fps             | 773      |\n",
      "|    iterations      | 1026     |\n",
      "|    time_elapsed    | 339      |\n",
      "|    total_timesteps | 262656   |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 77.4      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 773       |\n",
      "|    iterations           | 1027      |\n",
      "|    time_elapsed         | 339       |\n",
      "|    total_timesteps      | 262912    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.5784513 |\n",
      "|    clip_fraction        | 0.108     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0395   |\n",
      "|    explained_variance   | 0.646     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0658   |\n",
      "|    n_updates            | 19494     |\n",
      "|    policy_gradient_loss | -0.0405   |\n",
      "|    value_loss           | 0.0364    |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=263000, episode_reward=71.10 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 230        |\n",
      "|    mean_reward          | 71.1       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 263000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.42562562 |\n",
      "|    clip_fraction        | 0.13       |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0473    |\n",
      "|    explained_variance   | 0.69       |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0551    |\n",
      "|    n_updates            | 19513      |\n",
      "|    policy_gradient_loss | -0.0535    |\n",
      "|    value_loss           | 0.123      |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 77.5     |\n",
      "| time/              |          |\n",
      "|    fps             | 773      |\n",
      "|    iterations      | 1028     |\n",
      "|    time_elapsed    | 340      |\n",
      "|    total_timesteps | 263168   |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 77.5      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 773       |\n",
      "|    iterations           | 1029      |\n",
      "|    time_elapsed         | 340       |\n",
      "|    total_timesteps      | 263424    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.4202445 |\n",
      "|    clip_fraction        | 0.138     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0788   |\n",
      "|    explained_variance   | 0.765     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | 0.103     |\n",
      "|    n_updates            | 19532     |\n",
      "|    policy_gradient_loss | -0.0414   |\n",
      "|    value_loss           | 0.326     |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=263500, episode_reward=58.11 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "--------------------------------------\n",
      "| eval/                   |          |\n",
      "|    mean_ep_length       | 230      |\n",
      "|    mean_reward          | 58.1     |\n",
      "| time/                   |          |\n",
      "|    total_timesteps      | 263500   |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 0.975108 |\n",
      "|    clip_fraction        | 0.177    |\n",
      "|    clip_range           | 0.4      |\n",
      "|    entropy_loss         | -0.0639  |\n",
      "|    explained_variance   | 0.156    |\n",
      "|    learning_rate        | 0.000389 |\n",
      "|    loss                 | -0.0736  |\n",
      "|    n_updates            | 19551    |\n",
      "|    policy_gradient_loss | -0.0581  |\n",
      "|    value_loss           | 0.0599   |\n",
      "--------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 77.5     |\n",
      "| time/              |          |\n",
      "|    fps             | 773      |\n",
      "|    iterations      | 1030     |\n",
      "|    time_elapsed    | 340      |\n",
      "|    total_timesteps | 263680   |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 77.5      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 773       |\n",
      "|    iterations           | 1031      |\n",
      "|    time_elapsed         | 341       |\n",
      "|    total_timesteps      | 263936    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.0976975 |\n",
      "|    clip_fraction        | 0.124     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0538   |\n",
      "|    explained_variance   | 0.695     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0251   |\n",
      "|    n_updates            | 19570     |\n",
      "|    policy_gradient_loss | -0.0519   |\n",
      "|    value_loss           | 0.129     |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=264000, episode_reward=62.89 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 62.9      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 264000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.6059673 |\n",
      "|    clip_fraction        | 0.137     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0447   |\n",
      "|    explained_variance   | 0.833     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.00444  |\n",
      "|    n_updates            | 19589     |\n",
      "|    policy_gradient_loss | -0.0586   |\n",
      "|    value_loss           | 0.114     |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 77.5     |\n",
      "| time/              |          |\n",
      "|    fps             | 773      |\n",
      "|    iterations      | 1032     |\n",
      "|    time_elapsed    | 341      |\n",
      "|    total_timesteps | 264192   |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 77.5       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 773        |\n",
      "|    iterations           | 1033       |\n",
      "|    time_elapsed         | 341        |\n",
      "|    total_timesteps      | 264448     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.52979684 |\n",
      "|    clip_fraction        | 0.145      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0907    |\n",
      "|    explained_variance   | 0.78       |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | 0.0225     |\n",
      "|    n_updates            | 19608      |\n",
      "|    policy_gradient_loss | -0.0433    |\n",
      "|    value_loss           | 0.568      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=264500, episode_reward=53.58 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 53.6      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 264500    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.6724646 |\n",
      "|    clip_fraction        | 0.121     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0471   |\n",
      "|    explained_variance   | 0.688     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0665   |\n",
      "|    n_updates            | 19627     |\n",
      "|    policy_gradient_loss | -0.0467   |\n",
      "|    value_loss           | 0.0589    |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 77.5     |\n",
      "| time/              |          |\n",
      "|    fps             | 773      |\n",
      "|    iterations      | 1034     |\n",
      "|    time_elapsed    | 342      |\n",
      "|    total_timesteps | 264704   |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 77.4       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 773        |\n",
      "|    iterations           | 1035       |\n",
      "|    time_elapsed         | 342        |\n",
      "|    total_timesteps      | 264960     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.57678616 |\n",
      "|    clip_fraction        | 0.12       |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0576    |\n",
      "|    explained_variance   | 0.472      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0214    |\n",
      "|    n_updates            | 19646      |\n",
      "|    policy_gradient_loss | -0.0473    |\n",
      "|    value_loss           | 0.157      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=265000, episode_reward=82.06 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 82.1      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 265000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.7858661 |\n",
      "|    clip_fraction        | 0.202     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0581   |\n",
      "|    explained_variance   | 0.894     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0671   |\n",
      "|    n_updates            | 19665     |\n",
      "|    policy_gradient_loss | -0.0651   |\n",
      "|    value_loss           | 0.176     |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 77.4     |\n",
      "| time/              |          |\n",
      "|    fps             | 773      |\n",
      "|    iterations      | 1036     |\n",
      "|    time_elapsed    | 342      |\n",
      "|    total_timesteps | 265216   |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 77.4       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 773        |\n",
      "|    iterations           | 1037       |\n",
      "|    time_elapsed         | 342        |\n",
      "|    total_timesteps      | 265472     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.85382277 |\n",
      "|    clip_fraction        | 0.15       |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0512    |\n",
      "|    explained_variance   | 0.0392     |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0947    |\n",
      "|    n_updates            | 19684      |\n",
      "|    policy_gradient_loss | -0.0701    |\n",
      "|    value_loss           | 0.0602     |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=265500, episode_reward=92.77 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "--------------------------------------\n",
      "| eval/                   |          |\n",
      "|    mean_ep_length       | 230      |\n",
      "|    mean_reward          | 92.8     |\n",
      "| time/                   |          |\n",
      "|    total_timesteps      | 265500   |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 1.020216 |\n",
      "|    clip_fraction        | 0.132    |\n",
      "|    clip_range           | 0.4      |\n",
      "|    entropy_loss         | -0.0455  |\n",
      "|    explained_variance   | 0.643    |\n",
      "|    learning_rate        | 0.000389 |\n",
      "|    loss                 | -0.0556  |\n",
      "|    n_updates            | 19703    |\n",
      "|    policy_gradient_loss | -0.0417  |\n",
      "|    value_loss           | 0.171    |\n",
      "--------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 77.4     |\n",
      "| time/              |          |\n",
      "|    fps             | 773      |\n",
      "|    iterations      | 1038     |\n",
      "|    time_elapsed    | 343      |\n",
      "|    total_timesteps | 265728   |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 77.3      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 773       |\n",
      "|    iterations           | 1039      |\n",
      "|    time_elapsed         | 343       |\n",
      "|    total_timesteps      | 265984    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.6688663 |\n",
      "|    clip_fraction        | 0.0946    |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0481   |\n",
      "|    explained_variance   | 0.934     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.032    |\n",
      "|    n_updates            | 19722     |\n",
      "|    policy_gradient_loss | -0.0466   |\n",
      "|    value_loss           | 0.0809    |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=266000, episode_reward=67.88 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 67.9      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 266000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.3744251 |\n",
      "|    clip_fraction        | 0.11      |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0696   |\n",
      "|    explained_variance   | 0.21      |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.058    |\n",
      "|    n_updates            | 19741     |\n",
      "|    policy_gradient_loss | -0.0315   |\n",
      "|    value_loss           | 0.125     |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 77.3     |\n",
      "| time/              |          |\n",
      "|    fps             | 773      |\n",
      "|    iterations      | 1040     |\n",
      "|    time_elapsed    | 344      |\n",
      "|    total_timesteps | 266240   |\n",
      "---------------------------------\n",
      "--------------------------------------\n",
      "| rollout/                |          |\n",
      "|    ep_len_mean          | 889      |\n",
      "|    ep_rew_mean          | 77.3     |\n",
      "| time/                   |          |\n",
      "|    fps                  | 773      |\n",
      "|    iterations           | 1041     |\n",
      "|    time_elapsed         | 344      |\n",
      "|    total_timesteps      | 266496   |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 1.719476 |\n",
      "|    clip_fraction        | 0.113    |\n",
      "|    clip_range           | 0.4      |\n",
      "|    entropy_loss         | -0.0295  |\n",
      "|    explained_variance   | 0.874    |\n",
      "|    learning_rate        | 0.000389 |\n",
      "|    loss                 | -0.0444  |\n",
      "|    n_updates            | 19760    |\n",
      "|    policy_gradient_loss | -0.052   |\n",
      "|    value_loss           | 0.0368   |\n",
      "--------------------------------------\n",
      "Eval num_timesteps=266500, episode_reward=64.01 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 230        |\n",
      "|    mean_reward          | 64         |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 266500     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.18515655 |\n",
      "|    clip_fraction        | 0.0715     |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0445    |\n",
      "|    explained_variance   | 0.548      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.00784   |\n",
      "|    n_updates            | 19779      |\n",
      "|    policy_gradient_loss | -0.0347    |\n",
      "|    value_loss           | 0.272      |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 77.2     |\n",
      "| time/              |          |\n",
      "|    fps             | 773      |\n",
      "|    iterations      | 1042     |\n",
      "|    time_elapsed    | 344      |\n",
      "|    total_timesteps | 266752   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=267000, episode_reward=52.97 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 53        |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 267000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.0465465 |\n",
      "|    clip_fraction        | 0.163     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0674   |\n",
      "|    explained_variance   | 0.823     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0588   |\n",
      "|    n_updates            | 19798     |\n",
      "|    policy_gradient_loss | -0.0554   |\n",
      "|    value_loss           | 0.318     |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 77.2     |\n",
      "| time/              |          |\n",
      "|    fps             | 773      |\n",
      "|    iterations      | 1043     |\n",
      "|    time_elapsed    | 345      |\n",
      "|    total_timesteps | 267008   |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 77.2       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 773        |\n",
      "|    iterations           | 1044       |\n",
      "|    time_elapsed         | 345        |\n",
      "|    total_timesteps      | 267264     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.38663682 |\n",
      "|    clip_fraction        | 0.124      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0515    |\n",
      "|    explained_variance   | -0.0578    |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.00614   |\n",
      "|    n_updates            | 19817      |\n",
      "|    policy_gradient_loss | -0.0412    |\n",
      "|    value_loss           | 0.119      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=267500, episode_reward=84.16 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 84.2      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 267500    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.5509399 |\n",
      "|    clip_fraction        | 0.0769    |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0381   |\n",
      "|    explained_variance   | 0.808     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0755   |\n",
      "|    n_updates            | 19836     |\n",
      "|    policy_gradient_loss | -0.0289   |\n",
      "|    value_loss           | 0.107     |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 77.2     |\n",
      "| time/              |          |\n",
      "|    fps             | 773      |\n",
      "|    iterations      | 1045     |\n",
      "|    time_elapsed    | 345      |\n",
      "|    total_timesteps | 267520   |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 77.1      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 773       |\n",
      "|    iterations           | 1046      |\n",
      "|    time_elapsed         | 346       |\n",
      "|    total_timesteps      | 267776    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.0563157 |\n",
      "|    clip_fraction        | 0.147     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0492   |\n",
      "|    explained_variance   | 0.73      |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0863   |\n",
      "|    n_updates            | 19855     |\n",
      "|    policy_gradient_loss | -0.0529   |\n",
      "|    value_loss           | 0.255     |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=268000, episode_reward=87.74 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 230        |\n",
      "|    mean_reward          | 87.7       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 268000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.56205106 |\n",
      "|    clip_fraction        | 0.15       |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0906    |\n",
      "|    explained_variance   | 0.218      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | 0.0336     |\n",
      "|    n_updates            | 19874      |\n",
      "|    policy_gradient_loss | -0.062     |\n",
      "|    value_loss           | 0.256      |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 77.1     |\n",
      "| time/              |          |\n",
      "|    fps             | 773      |\n",
      "|    iterations      | 1047     |\n",
      "|    time_elapsed    | 346      |\n",
      "|    total_timesteps | 268032   |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 77.1       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 773        |\n",
      "|    iterations           | 1048       |\n",
      "|    time_elapsed         | 346        |\n",
      "|    total_timesteps      | 268288     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.26991504 |\n",
      "|    clip_fraction        | 0.0734     |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0346    |\n",
      "|    explained_variance   | 0.685      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.00502   |\n",
      "|    n_updates            | 19893      |\n",
      "|    policy_gradient_loss | -0.0235    |\n",
      "|    value_loss           | 0.134      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=268500, episode_reward=73.90 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 230        |\n",
      "|    mean_reward          | 73.9       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 268500     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.44717735 |\n",
      "|    clip_fraction        | 0.133      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0613    |\n",
      "|    explained_variance   | 0.718      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0144    |\n",
      "|    n_updates            | 19912      |\n",
      "|    policy_gradient_loss | -0.0532    |\n",
      "|    value_loss           | 0.0934     |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 77       |\n",
      "| time/              |          |\n",
      "|    fps             | 773      |\n",
      "|    iterations      | 1049     |\n",
      "|    time_elapsed    | 347      |\n",
      "|    total_timesteps | 268544   |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 77         |\n",
      "| time/                   |            |\n",
      "|    fps                  | 773        |\n",
      "|    iterations           | 1050       |\n",
      "|    time_elapsed         | 347        |\n",
      "|    total_timesteps      | 268800     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.58618164 |\n",
      "|    clip_fraction        | 0.152      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0779    |\n",
      "|    explained_variance   | 0.847      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0476    |\n",
      "|    n_updates            | 19931      |\n",
      "|    policy_gradient_loss | -0.0595    |\n",
      "|    value_loss           | 0.261      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=269000, episode_reward=55.95 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 230        |\n",
      "|    mean_reward          | 56         |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 269000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.84716594 |\n",
      "|    clip_fraction        | 0.119      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0421    |\n",
      "|    explained_variance   | 0.594      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0748    |\n",
      "|    n_updates            | 19950      |\n",
      "|    policy_gradient_loss | -0.0456    |\n",
      "|    value_loss           | 0.0464     |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 77       |\n",
      "| time/              |          |\n",
      "|    fps             | 773      |\n",
      "|    iterations      | 1051     |\n",
      "|    time_elapsed    | 347      |\n",
      "|    total_timesteps | 269056   |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 77        |\n",
      "| time/                   |           |\n",
      "|    fps                  | 773       |\n",
      "|    iterations           | 1052      |\n",
      "|    time_elapsed         | 348       |\n",
      "|    total_timesteps      | 269312    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.2963154 |\n",
      "|    clip_fraction        | 0.0773    |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0437   |\n",
      "|    explained_variance   | 0.852     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0583   |\n",
      "|    n_updates            | 19969     |\n",
      "|    policy_gradient_loss | -0.0379   |\n",
      "|    value_loss           | 0.114     |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=269500, episode_reward=52.01 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 52        |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 269500    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.1500841 |\n",
      "|    clip_fraction        | 0.131     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0497   |\n",
      "|    explained_variance   | 0.903     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0856   |\n",
      "|    n_updates            | 19988     |\n",
      "|    policy_gradient_loss | -0.0494   |\n",
      "|    value_loss           | 0.152     |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 77       |\n",
      "| time/              |          |\n",
      "|    fps             | 773      |\n",
      "|    iterations      | 1053     |\n",
      "|    time_elapsed    | 348      |\n",
      "|    total_timesteps | 269568   |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 77        |\n",
      "| time/                   |           |\n",
      "|    fps                  | 773       |\n",
      "|    iterations           | 1054      |\n",
      "|    time_elapsed         | 348       |\n",
      "|    total_timesteps      | 269824    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.9547204 |\n",
      "|    clip_fraction        | 0.177     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.081    |\n",
      "|    explained_variance   | 0.692     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.087    |\n",
      "|    n_updates            | 20007     |\n",
      "|    policy_gradient_loss | -0.0608   |\n",
      "|    value_loss           | 0.142     |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=270000, episode_reward=48.07 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 230        |\n",
      "|    mean_reward          | 48.1       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 270000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.64446855 |\n",
      "|    clip_fraction        | 0.0972     |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0348    |\n",
      "|    explained_variance   | 0.814      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.053     |\n",
      "|    n_updates            | 20026      |\n",
      "|    policy_gradient_loss | -0.0424    |\n",
      "|    value_loss           | 0.128      |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 77       |\n",
      "| time/              |          |\n",
      "|    fps             | 773      |\n",
      "|    iterations      | 1055     |\n",
      "|    time_elapsed    | 349      |\n",
      "|    total_timesteps | 270080   |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 76.9       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 773        |\n",
      "|    iterations           | 1056       |\n",
      "|    time_elapsed         | 349        |\n",
      "|    total_timesteps      | 270336     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.54379463 |\n",
      "|    clip_fraction        | 0.087      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0531    |\n",
      "|    explained_variance   | 0.695      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.022     |\n",
      "|    n_updates            | 20045      |\n",
      "|    policy_gradient_loss | -0.0363    |\n",
      "|    value_loss           | 0.127      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=270500, episode_reward=74.48 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 74.5      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 270500    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.4277961 |\n",
      "|    clip_fraction        | 0.122     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0801   |\n",
      "|    explained_variance   | 0.708     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0409   |\n",
      "|    n_updates            | 20064     |\n",
      "|    policy_gradient_loss | -0.044    |\n",
      "|    value_loss           | 0.478     |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 76.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 773      |\n",
      "|    iterations      | 1057     |\n",
      "|    time_elapsed    | 349      |\n",
      "|    total_timesteps | 270592   |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 76.9      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 774       |\n",
      "|    iterations           | 1058      |\n",
      "|    time_elapsed         | 349       |\n",
      "|    total_timesteps      | 270848    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.2209553 |\n",
      "|    clip_fraction        | 0.0701    |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0445   |\n",
      "|    explained_variance   | 0.58      |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0669   |\n",
      "|    n_updates            | 20083     |\n",
      "|    policy_gradient_loss | -0.0298   |\n",
      "|    value_loss           | 0.0545    |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=271000, episode_reward=66.12 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 230        |\n",
      "|    mean_reward          | 66.1       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 271000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.68594265 |\n",
      "|    clip_fraction        | 0.111      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.048     |\n",
      "|    explained_variance   | 0.861      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.00968   |\n",
      "|    n_updates            | 20102      |\n",
      "|    policy_gradient_loss | -0.045     |\n",
      "|    value_loss           | 0.0837     |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 76.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 773      |\n",
      "|    iterations      | 1059     |\n",
      "|    time_elapsed    | 350      |\n",
      "|    total_timesteps | 271104   |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 76.8      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 774       |\n",
      "|    iterations           | 1060      |\n",
      "|    time_elapsed         | 350       |\n",
      "|    total_timesteps      | 271360    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.0062644 |\n",
      "|    clip_fraction        | 0.185     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0775   |\n",
      "|    explained_variance   | 0.862     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.119    |\n",
      "|    n_updates            | 20121     |\n",
      "|    policy_gradient_loss | -0.0636   |\n",
      "|    value_loss           | 0.135     |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=271500, episode_reward=60.58 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 230        |\n",
      "|    mean_reward          | 60.6       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 271500     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.44199067 |\n",
      "|    clip_fraction        | 0.141      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0636    |\n",
      "|    explained_variance   | 0.044      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.102     |\n",
      "|    n_updates            | 20140      |\n",
      "|    policy_gradient_loss | -0.0628    |\n",
      "|    value_loss           | 0.0621     |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 76.8     |\n",
      "| time/              |          |\n",
      "|    fps             | 773      |\n",
      "|    iterations      | 1061     |\n",
      "|    time_elapsed    | 350      |\n",
      "|    total_timesteps | 271616   |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 76.8       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 774        |\n",
      "|    iterations           | 1062       |\n",
      "|    time_elapsed         | 351        |\n",
      "|    total_timesteps      | 271872     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.60291517 |\n",
      "|    clip_fraction        | 0.102      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.039     |\n",
      "|    explained_variance   | 0.713      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.00423   |\n",
      "|    n_updates            | 20159      |\n",
      "|    policy_gradient_loss | -0.036     |\n",
      "|    value_loss           | 0.137      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=272000, episode_reward=62.09 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 230        |\n",
      "|    mean_reward          | 62.1       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 272000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.54472494 |\n",
      "|    clip_fraction        | 0.129      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0472    |\n",
      "|    explained_variance   | 0.752      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0459    |\n",
      "|    n_updates            | 20178      |\n",
      "|    policy_gradient_loss | -0.0467    |\n",
      "|    value_loss           | 0.122      |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 76.8     |\n",
      "| time/              |          |\n",
      "|    fps             | 773      |\n",
      "|    iterations      | 1063     |\n",
      "|    time_elapsed    | 351      |\n",
      "|    total_timesteps | 272128   |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 76.8      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 774       |\n",
      "|    iterations           | 1064      |\n",
      "|    time_elapsed         | 351       |\n",
      "|    total_timesteps      | 272384    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.8953638 |\n",
      "|    clip_fraction        | 0.194     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.102    |\n",
      "|    explained_variance   | 0.854     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | 0.0157    |\n",
      "|    n_updates            | 20197     |\n",
      "|    policy_gradient_loss | -0.0619   |\n",
      "|    value_loss           | 0.21      |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=272500, episode_reward=83.65 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 83.7      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 272500    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.7916095 |\n",
      "|    clip_fraction        | 0.159     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0607   |\n",
      "|    explained_variance   | 0.504     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0677   |\n",
      "|    n_updates            | 20216     |\n",
      "|    policy_gradient_loss | -0.0627   |\n",
      "|    value_loss           | 0.0375    |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 76.8     |\n",
      "| time/              |          |\n",
      "|    fps             | 773      |\n",
      "|    iterations      | 1065     |\n",
      "|    time_elapsed    | 352      |\n",
      "|    total_timesteps | 272640   |\n",
      "---------------------------------\n",
      "--------------------------------------\n",
      "| rollout/                |          |\n",
      "|    ep_len_mean          | 889      |\n",
      "|    ep_rew_mean          | 76.8     |\n",
      "| time/                   |          |\n",
      "|    fps                  | 774      |\n",
      "|    iterations           | 1066     |\n",
      "|    time_elapsed         | 352      |\n",
      "|    total_timesteps      | 272896   |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 0.980541 |\n",
      "|    clip_fraction        | 0.1      |\n",
      "|    clip_range           | 0.4      |\n",
      "|    entropy_loss         | -0.0456  |\n",
      "|    explained_variance   | 0.804    |\n",
      "|    learning_rate        | 0.000389 |\n",
      "|    loss                 | -0.0376  |\n",
      "|    n_updates            | 20235    |\n",
      "|    policy_gradient_loss | -0.0365  |\n",
      "|    value_loss           | 0.124    |\n",
      "--------------------------------------\n",
      "Eval num_timesteps=273000, episode_reward=82.29 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 82.3      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 273000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.1207602 |\n",
      "|    clip_fraction        | 0.186     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0762   |\n",
      "|    explained_variance   | 0.927     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0379   |\n",
      "|    n_updates            | 20254     |\n",
      "|    policy_gradient_loss | -0.0767   |\n",
      "|    value_loss           | 0.0979    |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 76.8     |\n",
      "| time/              |          |\n",
      "|    fps             | 773      |\n",
      "|    iterations      | 1067     |\n",
      "|    time_elapsed    | 352      |\n",
      "|    total_timesteps | 273152   |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 76.8      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 774       |\n",
      "|    iterations           | 1068      |\n",
      "|    time_elapsed         | 353       |\n",
      "|    total_timesteps      | 273408    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.5240894 |\n",
      "|    clip_fraction        | 0.123     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0704   |\n",
      "|    explained_variance   | 0.218     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0796   |\n",
      "|    n_updates            | 20273     |\n",
      "|    policy_gradient_loss | -0.0597   |\n",
      "|    value_loss           | 0.0927    |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=273500, episode_reward=66.67 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 230        |\n",
      "|    mean_reward          | 66.7       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 273500     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.46425265 |\n",
      "|    clip_fraction        | 0.117      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0574    |\n",
      "|    explained_variance   | 0.538      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.022     |\n",
      "|    n_updates            | 20292      |\n",
      "|    policy_gradient_loss | -0.0437    |\n",
      "|    value_loss           | 0.217      |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 76.8     |\n",
      "| time/              |          |\n",
      "|    fps             | 774      |\n",
      "|    iterations      | 1069     |\n",
      "|    time_elapsed    | 353      |\n",
      "|    total_timesteps | 273664   |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 76.9      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 774       |\n",
      "|    iterations           | 1070      |\n",
      "|    time_elapsed         | 353       |\n",
      "|    total_timesteps      | 273920    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.7422739 |\n",
      "|    clip_fraction        | 0.156     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.058    |\n",
      "|    explained_variance   | 0.804     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0496   |\n",
      "|    n_updates            | 20311     |\n",
      "|    policy_gradient_loss | -0.0602   |\n",
      "|    value_loss           | 0.0789    |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=274000, episode_reward=79.63 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 230        |\n",
      "|    mean_reward          | 79.6       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 274000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.88926744 |\n",
      "|    clip_fraction        | 0.177      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0959    |\n",
      "|    explained_variance   | 0.936      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0827    |\n",
      "|    n_updates            | 20330      |\n",
      "|    policy_gradient_loss | -0.0773    |\n",
      "|    value_loss           | 0.132      |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 76.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 774      |\n",
      "|    iterations      | 1071     |\n",
      "|    time_elapsed    | 354      |\n",
      "|    total_timesteps | 274176   |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 76.9       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 774        |\n",
      "|    iterations           | 1072       |\n",
      "|    time_elapsed         | 354        |\n",
      "|    total_timesteps      | 274432     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.58539355 |\n",
      "|    clip_fraction        | 0.142      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0469    |\n",
      "|    explained_variance   | 0.374      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0717    |\n",
      "|    n_updates            | 20349      |\n",
      "|    policy_gradient_loss | -0.0656    |\n",
      "|    value_loss           | 0.0527     |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=274500, episode_reward=57.52 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 57.5      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 274500    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.5132822 |\n",
      "|    clip_fraction        | 0.117     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0511   |\n",
      "|    explained_variance   | 0.816     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.11     |\n",
      "|    n_updates            | 20368     |\n",
      "|    policy_gradient_loss | -0.0629   |\n",
      "|    value_loss           | 0.135     |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 76.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 773      |\n",
      "|    iterations      | 1073     |\n",
      "|    time_elapsed    | 354      |\n",
      "|    total_timesteps | 274688   |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 77.1      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 774       |\n",
      "|    iterations           | 1074      |\n",
      "|    time_elapsed         | 355       |\n",
      "|    total_timesteps      | 274944    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.5382277 |\n",
      "|    clip_fraction        | 0.134     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0776   |\n",
      "|    explained_variance   | 0.464     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | 0.00897   |\n",
      "|    n_updates            | 20387     |\n",
      "|    policy_gradient_loss | -0.0559   |\n",
      "|    value_loss           | 0.893     |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=275000, episode_reward=32.80 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 32.8      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 275000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.6983946 |\n",
      "|    clip_fraction        | 0.145     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0744   |\n",
      "|    explained_variance   | -0.284    |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0458   |\n",
      "|    n_updates            | 20406     |\n",
      "|    policy_gradient_loss | -0.0514   |\n",
      "|    value_loss           | 0.0643    |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 77.1     |\n",
      "| time/              |          |\n",
      "|    fps             | 773      |\n",
      "|    iterations      | 1075     |\n",
      "|    time_elapsed    | 355      |\n",
      "|    total_timesteps | 275200   |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 77.1       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 774        |\n",
      "|    iterations           | 1076       |\n",
      "|    time_elapsed         | 355        |\n",
      "|    total_timesteps      | 275456     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.67293656 |\n",
      "|    clip_fraction        | 0.0905     |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0457    |\n",
      "|    explained_variance   | 0.745      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0305    |\n",
      "|    n_updates            | 20425      |\n",
      "|    policy_gradient_loss | -0.0348    |\n",
      "|    value_loss           | 0.086      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=275500, episode_reward=74.05 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 74        |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 275500    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.4099761 |\n",
      "|    clip_fraction        | 0.131     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0704   |\n",
      "|    explained_variance   | 0.674     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.047    |\n",
      "|    n_updates            | 20444     |\n",
      "|    policy_gradient_loss | -0.0431   |\n",
      "|    value_loss           | 0.11      |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 77.1     |\n",
      "| time/              |          |\n",
      "|    fps             | 774      |\n",
      "|    iterations      | 1077     |\n",
      "|    time_elapsed    | 356      |\n",
      "|    total_timesteps | 275712   |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 77.1       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 774        |\n",
      "|    iterations           | 1078       |\n",
      "|    time_elapsed         | 356        |\n",
      "|    total_timesteps      | 275968     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.44954413 |\n",
      "|    clip_fraction        | 0.155      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0925    |\n",
      "|    explained_variance   | 0.724      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0897    |\n",
      "|    n_updates            | 20463      |\n",
      "|    policy_gradient_loss | -0.0696    |\n",
      "|    value_loss           | 0.349      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=276000, episode_reward=52.97 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 53        |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 276000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.6543772 |\n",
      "|    clip_fraction        | 0.097     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0465   |\n",
      "|    explained_variance   | 0.0147    |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.075    |\n",
      "|    n_updates            | 20482     |\n",
      "|    policy_gradient_loss | -0.0403   |\n",
      "|    value_loss           | 0.0475    |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 77.1     |\n",
      "| time/              |          |\n",
      "|    fps             | 773      |\n",
      "|    iterations      | 1079     |\n",
      "|    time_elapsed    | 356      |\n",
      "|    total_timesteps | 276224   |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 77.2      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 774       |\n",
      "|    iterations           | 1080      |\n",
      "|    time_elapsed         | 357       |\n",
      "|    total_timesteps      | 276480    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.2844141 |\n",
      "|    clip_fraction        | 0.088     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0513   |\n",
      "|    explained_variance   | 0.692     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.00505  |\n",
      "|    n_updates            | 20501     |\n",
      "|    policy_gradient_loss | -0.0341   |\n",
      "|    value_loss           | 0.129     |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=276500, episode_reward=64.44 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "--------------------------------------\n",
      "| eval/                   |          |\n",
      "|    mean_ep_length       | 230      |\n",
      "|    mean_reward          | 64.4     |\n",
      "| time/                   |          |\n",
      "|    total_timesteps      | 276500   |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 0.595041 |\n",
      "|    clip_fraction        | 0.167    |\n",
      "|    clip_range           | 0.4      |\n",
      "|    entropy_loss         | -0.102   |\n",
      "|    explained_variance   | 0.852    |\n",
      "|    learning_rate        | 0.000389 |\n",
      "|    loss                 | -0.073   |\n",
      "|    n_updates            | 20520    |\n",
      "|    policy_gradient_loss | -0.0694  |\n",
      "|    value_loss           | 0.248    |\n",
      "--------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 77.2     |\n",
      "| time/              |          |\n",
      "|    fps             | 774      |\n",
      "|    iterations      | 1081     |\n",
      "|    time_elapsed    | 357      |\n",
      "|    total_timesteps | 276736   |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 77.2      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 774       |\n",
      "|    iterations           | 1082      |\n",
      "|    time_elapsed         | 357       |\n",
      "|    total_timesteps      | 276992    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.6024388 |\n",
      "|    clip_fraction        | 0.17      |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0858   |\n",
      "|    explained_variance   | -0.0476   |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0615   |\n",
      "|    n_updates            | 20539     |\n",
      "|    policy_gradient_loss | -0.0608   |\n",
      "|    value_loss           | 0.0418    |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=277000, episode_reward=53.01 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 230        |\n",
      "|    mean_reward          | 53         |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 277000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.49292016 |\n",
      "|    clip_fraction        | 0.101      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0451    |\n",
      "|    explained_variance   | 0.634      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0575    |\n",
      "|    n_updates            | 20558      |\n",
      "|    policy_gradient_loss | -0.0427    |\n",
      "|    value_loss           | 0.155      |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 77.2     |\n",
      "| time/              |          |\n",
      "|    fps             | 774      |\n",
      "|    iterations      | 1083     |\n",
      "|    time_elapsed    | 358      |\n",
      "|    total_timesteps | 277248   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=277500, episode_reward=47.65 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 230        |\n",
      "|    mean_reward          | 47.7       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 277500     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.54437506 |\n",
      "|    clip_fraction        | 0.113      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0588    |\n",
      "|    explained_variance   | 0.691      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0552    |\n",
      "|    n_updates            | 20577      |\n",
      "|    policy_gradient_loss | -0.0547    |\n",
      "|    value_loss           | 0.114      |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 77.2     |\n",
      "| time/              |          |\n",
      "|    fps             | 774      |\n",
      "|    iterations      | 1084     |\n",
      "|    time_elapsed    | 358      |\n",
      "|    total_timesteps | 277504   |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 77.2      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 774       |\n",
      "|    iterations           | 1085      |\n",
      "|    time_elapsed         | 358       |\n",
      "|    total_timesteps      | 277760    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.2430184 |\n",
      "|    clip_fraction        | 0.126     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.094    |\n",
      "|    explained_variance   | 0.657     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0914   |\n",
      "|    n_updates            | 20596     |\n",
      "|    policy_gradient_loss | -0.0544   |\n",
      "|    value_loss           | 0.35      |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=278000, episode_reward=65.94 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "--------------------------------------\n",
      "| eval/                   |          |\n",
      "|    mean_ep_length       | 230      |\n",
      "|    mean_reward          | 65.9     |\n",
      "| time/                   |          |\n",
      "|    total_timesteps      | 278000   |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 0.505552 |\n",
      "|    clip_fraction        | 0.13     |\n",
      "|    clip_range           | 0.4      |\n",
      "|    entropy_loss         | -0.0556  |\n",
      "|    explained_variance   | 0.473    |\n",
      "|    learning_rate        | 0.000389 |\n",
      "|    loss                 | -0.0843  |\n",
      "|    n_updates            | 20615    |\n",
      "|    policy_gradient_loss | -0.0576  |\n",
      "|    value_loss           | 0.035    |\n",
      "--------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 77.2     |\n",
      "| time/              |          |\n",
      "|    fps             | 774      |\n",
      "|    iterations      | 1086     |\n",
      "|    time_elapsed    | 358      |\n",
      "|    total_timesteps | 278016   |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 77.2       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 774        |\n",
      "|    iterations           | 1087       |\n",
      "|    time_elapsed         | 359        |\n",
      "|    total_timesteps      | 278272     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.49729842 |\n",
      "|    clip_fraction        | 0.112      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0579    |\n",
      "|    explained_variance   | 0.679      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0103    |\n",
      "|    n_updates            | 20634      |\n",
      "|    policy_gradient_loss | -0.0525    |\n",
      "|    value_loss           | 0.225      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=278500, episode_reward=56.76 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 56.8      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 278500    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.6565961 |\n",
      "|    clip_fraction        | 0.17      |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0828   |\n",
      "|    explained_variance   | 0.798     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0706   |\n",
      "|    n_updates            | 20653     |\n",
      "|    policy_gradient_loss | -0.0642   |\n",
      "|    value_loss           | 0.195     |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 77.2     |\n",
      "| time/              |          |\n",
      "|    fps             | 774      |\n",
      "|    iterations      | 1088     |\n",
      "|    time_elapsed    | 359      |\n",
      "|    total_timesteps | 278528   |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 77.2      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 774       |\n",
      "|    iterations           | 1089      |\n",
      "|    time_elapsed         | 359       |\n",
      "|    total_timesteps      | 278784    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.8893134 |\n",
      "|    clip_fraction        | 0.177     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0759   |\n",
      "|    explained_variance   | -0.000421 |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0861   |\n",
      "|    n_updates            | 20672     |\n",
      "|    policy_gradient_loss | -0.0598   |\n",
      "|    value_loss           | 0.0406    |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=279000, episode_reward=80.73 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 80.7      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 279000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.6827335 |\n",
      "|    clip_fraction        | 0.132     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0607   |\n",
      "|    explained_variance   | 0.83      |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0494   |\n",
      "|    n_updates            | 20691     |\n",
      "|    policy_gradient_loss | -0.0527   |\n",
      "|    value_loss           | 0.0973    |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 77.2     |\n",
      "| time/              |          |\n",
      "|    fps             | 774      |\n",
      "|    iterations      | 1090     |\n",
      "|    time_elapsed    | 360      |\n",
      "|    total_timesteps | 279040   |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 77.3      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 774       |\n",
      "|    iterations           | 1091      |\n",
      "|    time_elapsed         | 360       |\n",
      "|    total_timesteps      | 279296    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.7542031 |\n",
      "|    clip_fraction        | 0.154     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0595   |\n",
      "|    explained_variance   | 0.774     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0373   |\n",
      "|    n_updates            | 20710     |\n",
      "|    policy_gradient_loss | -0.0592   |\n",
      "|    value_loss           | 0.103     |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=279500, episode_reward=47.61 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 47.6      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 279500    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.4702394 |\n",
      "|    clip_fraction        | 0.156     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.115    |\n",
      "|    explained_variance   | 0.662     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0854   |\n",
      "|    n_updates            | 20729     |\n",
      "|    policy_gradient_loss | -0.0448   |\n",
      "|    value_loss           | 0.368     |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 77.3     |\n",
      "| time/              |          |\n",
      "|    fps             | 774      |\n",
      "|    iterations      | 1092     |\n",
      "|    time_elapsed    | 360      |\n",
      "|    total_timesteps | 279552   |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 77.3      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 774       |\n",
      "|    iterations           | 1093      |\n",
      "|    time_elapsed         | 361       |\n",
      "|    total_timesteps      | 279808    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.6997283 |\n",
      "|    clip_fraction        | 0.122     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0425   |\n",
      "|    explained_variance   | 0.861     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.077    |\n",
      "|    n_updates            | 20748     |\n",
      "|    policy_gradient_loss | -0.0535   |\n",
      "|    value_loss           | 0.041     |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=280000, episode_reward=72.63 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 72.6      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 280000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.7937385 |\n",
      "|    clip_fraction        | 0.142     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0607   |\n",
      "|    explained_variance   | 0.793     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0776   |\n",
      "|    n_updates            | 20767     |\n",
      "|    policy_gradient_loss | -0.0649   |\n",
      "|    value_loss           | 0.0899    |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 77.3     |\n",
      "| time/              |          |\n",
      "|    fps             | 774      |\n",
      "|    iterations      | 1094     |\n",
      "|    time_elapsed    | 361      |\n",
      "|    total_timesteps | 280064   |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 77.3      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 774       |\n",
      "|    iterations           | 1095      |\n",
      "|    time_elapsed         | 361       |\n",
      "|    total_timesteps      | 280320    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.7683822 |\n",
      "|    clip_fraction        | 0.156     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0929   |\n",
      "|    explained_variance   | 0.596     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0637   |\n",
      "|    n_updates            | 20786     |\n",
      "|    policy_gradient_loss | -0.0619   |\n",
      "|    value_loss           | 0.352     |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=280500, episode_reward=64.05 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 230        |\n",
      "|    mean_reward          | 64.1       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 280500     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.61615753 |\n",
      "|    clip_fraction        | 0.18       |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0861    |\n",
      "|    explained_variance   | 0.0208     |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.13      |\n",
      "|    n_updates            | 20805      |\n",
      "|    policy_gradient_loss | -0.0551    |\n",
      "|    value_loss           | 0.0586     |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 77.3     |\n",
      "| time/              |          |\n",
      "|    fps             | 774      |\n",
      "|    iterations      | 1096     |\n",
      "|    time_elapsed    | 362      |\n",
      "|    total_timesteps | 280576   |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 77.3      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 775       |\n",
      "|    iterations           | 1097      |\n",
      "|    time_elapsed         | 362       |\n",
      "|    total_timesteps      | 280832    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.3554966 |\n",
      "|    clip_fraction        | 0.0987    |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0539   |\n",
      "|    explained_variance   | 0.323     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0687   |\n",
      "|    n_updates            | 20824     |\n",
      "|    policy_gradient_loss | -0.0392   |\n",
      "|    value_loss           | 0.247     |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=281000, episode_reward=65.66 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 230        |\n",
      "|    mean_reward          | 65.7       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 281000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.52462184 |\n",
      "|    clip_fraction        | 0.123      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0513    |\n",
      "|    explained_variance   | 0.799      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0525    |\n",
      "|    n_updates            | 20843      |\n",
      "|    policy_gradient_loss | -0.0396    |\n",
      "|    value_loss           | 0.141      |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 77.2     |\n",
      "| time/              |          |\n",
      "|    fps             | 774      |\n",
      "|    iterations      | 1098     |\n",
      "|    time_elapsed    | 362      |\n",
      "|    total_timesteps | 281088   |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 77.2      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 775       |\n",
      "|    iterations           | 1099      |\n",
      "|    time_elapsed         | 363       |\n",
      "|    total_timesteps      | 281344    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.5512639 |\n",
      "|    clip_fraction        | 0.153     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.121    |\n",
      "|    explained_variance   | 0.618     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.124    |\n",
      "|    n_updates            | 20862     |\n",
      "|    policy_gradient_loss | -0.0656   |\n",
      "|    value_loss           | 0.283     |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=281500, episode_reward=56.69 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 56.7      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 281500    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.6780222 |\n",
      "|    clip_fraction        | 0.143     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0497   |\n",
      "|    explained_variance   | 0.824     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.102    |\n",
      "|    n_updates            | 20881     |\n",
      "|    policy_gradient_loss | -0.0582   |\n",
      "|    value_loss           | 0.0508    |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 77.2     |\n",
      "| time/              |          |\n",
      "|    fps             | 774      |\n",
      "|    iterations      | 1100     |\n",
      "|    time_elapsed    | 363      |\n",
      "|    total_timesteps | 281600   |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 77.1       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 774        |\n",
      "|    iterations           | 1101       |\n",
      "|    time_elapsed         | 363        |\n",
      "|    total_timesteps      | 281856     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.34338397 |\n",
      "|    clip_fraction        | 0.111      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0654    |\n",
      "|    explained_variance   | 0.612      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0182    |\n",
      "|    n_updates            | 20900      |\n",
      "|    policy_gradient_loss | -0.0484    |\n",
      "|    value_loss           | 0.103      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=282000, episode_reward=59.44 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 59.4      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 282000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.2807052 |\n",
      "|    clip_fraction        | 0.0851    |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0789   |\n",
      "|    explained_variance   | 0.814     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0515   |\n",
      "|    n_updates            | 20919     |\n",
      "|    policy_gradient_loss | -0.035    |\n",
      "|    value_loss           | 0.324     |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 77.1     |\n",
      "| time/              |          |\n",
      "|    fps             | 774      |\n",
      "|    iterations      | 1102     |\n",
      "|    time_elapsed    | 364      |\n",
      "|    total_timesteps | 282112   |\n",
      "---------------------------------\n",
      "--------------------------------------\n",
      "| rollout/                |          |\n",
      "|    ep_len_mean          | 889      |\n",
      "|    ep_rew_mean          | 77.1     |\n",
      "| time/                   |          |\n",
      "|    fps                  | 775      |\n",
      "|    iterations           | 1103     |\n",
      "|    time_elapsed         | 364      |\n",
      "|    total_timesteps      | 282368   |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 0.550979 |\n",
      "|    clip_fraction        | 0.174    |\n",
      "|    clip_range           | 0.4      |\n",
      "|    entropy_loss         | -0.0914  |\n",
      "|    explained_variance   | 0.107    |\n",
      "|    learning_rate        | 0.000389 |\n",
      "|    loss                 | -0.0802  |\n",
      "|    n_updates            | 20938    |\n",
      "|    policy_gradient_loss | -0.0573  |\n",
      "|    value_loss           | 0.047    |\n",
      "--------------------------------------\n",
      "Eval num_timesteps=282500, episode_reward=58.83 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 230        |\n",
      "|    mean_reward          | 58.8       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 282500     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.45279187 |\n",
      "|    clip_fraction        | 0.0785     |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0446    |\n",
      "|    explained_variance   | 0.628      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0763    |\n",
      "|    n_updates            | 20957      |\n",
      "|    policy_gradient_loss | -0.0421    |\n",
      "|    value_loss           | 0.117      |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 77.1     |\n",
      "| time/              |          |\n",
      "|    fps             | 774      |\n",
      "|    iterations      | 1104     |\n",
      "|    time_elapsed    | 364      |\n",
      "|    total_timesteps | 282624   |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 77.2       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 775        |\n",
      "|    iterations           | 1105       |\n",
      "|    time_elapsed         | 364        |\n",
      "|    total_timesteps      | 282880     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.93544424 |\n",
      "|    clip_fraction        | 0.136      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0599    |\n",
      "|    explained_variance   | 0.895      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0667    |\n",
      "|    n_updates            | 20976      |\n",
      "|    policy_gradient_loss | -0.0473    |\n",
      "|    value_loss           | 0.0902     |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=283000, episode_reward=82.45 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 82.5      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 283000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.3107587 |\n",
      "|    clip_fraction        | 0.117     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.101    |\n",
      "|    explained_variance   | 0.118     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | 0.00684   |\n",
      "|    n_updates            | 20995     |\n",
      "|    policy_gradient_loss | -0.0427   |\n",
      "|    value_loss           | 0.147     |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 77.2     |\n",
      "| time/              |          |\n",
      "|    fps             | 774      |\n",
      "|    iterations      | 1106     |\n",
      "|    time_elapsed    | 365      |\n",
      "|    total_timesteps | 283136   |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 77.2       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 774        |\n",
      "|    iterations           | 1107       |\n",
      "|    time_elapsed         | 365        |\n",
      "|    total_timesteps      | 283392     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.48063353 |\n",
      "|    clip_fraction        | 0.103      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0585    |\n",
      "|    explained_variance   | 0.459      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0523    |\n",
      "|    n_updates            | 21014      |\n",
      "|    policy_gradient_loss | -0.0331    |\n",
      "|    value_loss           | 0.0674     |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=283500, episode_reward=53.58 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 230        |\n",
      "|    mean_reward          | 53.6       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 283500     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.38855112 |\n",
      "|    clip_fraction        | 0.132      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.071     |\n",
      "|    explained_variance   | 0.394      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.109     |\n",
      "|    n_updates            | 21033      |\n",
      "|    policy_gradient_loss | -0.0476    |\n",
      "|    value_loss           | 0.0919     |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 77.1     |\n",
      "| time/              |          |\n",
      "|    fps             | 774      |\n",
      "|    iterations      | 1108     |\n",
      "|    time_elapsed    | 366      |\n",
      "|    total_timesteps | 283648   |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 77.1      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 774       |\n",
      "|    iterations           | 1109      |\n",
      "|    time_elapsed         | 366       |\n",
      "|    total_timesteps      | 283904    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.6254562 |\n",
      "|    clip_fraction        | 0.162     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.104    |\n",
      "|    explained_variance   | 0.832     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0597   |\n",
      "|    n_updates            | 21052     |\n",
      "|    policy_gradient_loss | -0.0548   |\n",
      "|    value_loss           | 0.372     |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=284000, episode_reward=40.39 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 230        |\n",
      "|    mean_reward          | 40.4       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 284000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.64108336 |\n",
      "|    clip_fraction        | 0.154      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.069     |\n",
      "|    explained_variance   | 0.334      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0671    |\n",
      "|    n_updates            | 21071      |\n",
      "|    policy_gradient_loss | -0.0467    |\n",
      "|    value_loss           | 0.0388     |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 77.1     |\n",
      "| time/              |          |\n",
      "|    fps             | 774      |\n",
      "|    iterations      | 1110     |\n",
      "|    time_elapsed    | 366      |\n",
      "|    total_timesteps | 284160   |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 77.1       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 774        |\n",
      "|    iterations           | 1111       |\n",
      "|    time_elapsed         | 367        |\n",
      "|    total_timesteps      | 284416     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.61494493 |\n",
      "|    clip_fraction        | 0.139      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0581    |\n",
      "|    explained_variance   | 0.556      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0142    |\n",
      "|    n_updates            | 21090      |\n",
      "|    policy_gradient_loss | -0.0494    |\n",
      "|    value_loss           | 0.156      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=284500, episode_reward=52.13 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 52.1      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 284500    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.8848313 |\n",
      "|    clip_fraction        | 0.154     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0506   |\n",
      "|    explained_variance   | 0.865     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0777   |\n",
      "|    n_updates            | 21109     |\n",
      "|    policy_gradient_loss | -0.0665   |\n",
      "|    value_loss           | 0.131     |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 77       |\n",
      "| time/              |          |\n",
      "|    fps             | 774      |\n",
      "|    iterations      | 1112     |\n",
      "|    time_elapsed    | 367      |\n",
      "|    total_timesteps | 284672   |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 77         |\n",
      "| time/                   |            |\n",
      "|    fps                  | 775        |\n",
      "|    iterations           | 1113       |\n",
      "|    time_elapsed         | 367        |\n",
      "|    total_timesteps      | 284928     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.47752044 |\n",
      "|    clip_fraction        | 0.133      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0906    |\n",
      "|    explained_variance   | 0.433      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0386    |\n",
      "|    n_updates            | 21128      |\n",
      "|    policy_gradient_loss | -0.0584    |\n",
      "|    value_loss           | 0.135      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=285000, episode_reward=46.05 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 46.1      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 285000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.3545422 |\n",
      "|    clip_fraction        | 0.111     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0619   |\n",
      "|    explained_variance   | 0.798     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0402   |\n",
      "|    n_updates            | 21147     |\n",
      "|    policy_gradient_loss | -0.0364   |\n",
      "|    value_loss           | 0.0708    |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 77       |\n",
      "| time/              |          |\n",
      "|    fps             | 774      |\n",
      "|    iterations      | 1114     |\n",
      "|    time_elapsed    | 368      |\n",
      "|    total_timesteps | 285184   |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 76.8       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 775        |\n",
      "|    iterations           | 1115       |\n",
      "|    time_elapsed         | 368        |\n",
      "|    total_timesteps      | 285440     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.87287164 |\n",
      "|    clip_fraction        | 0.14       |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0623    |\n",
      "|    explained_variance   | 0.541      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.112     |\n",
      "|    n_updates            | 21166      |\n",
      "|    policy_gradient_loss | -0.0604    |\n",
      "|    value_loss           | 0.11       |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=285500, episode_reward=68.60 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 230        |\n",
      "|    mean_reward          | 68.6       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 285500     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.23103309 |\n",
      "|    clip_fraction        | 0.104      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0668    |\n",
      "|    explained_variance   | 0.26       |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0362    |\n",
      "|    n_updates            | 21185      |\n",
      "|    policy_gradient_loss | -0.0418    |\n",
      "|    value_loss           | 0.3        |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 76.8     |\n",
      "| time/              |          |\n",
      "|    fps             | 775      |\n",
      "|    iterations      | 1116     |\n",
      "|    time_elapsed    | 368      |\n",
      "|    total_timesteps | 285696   |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 76.8      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 775       |\n",
      "|    iterations           | 1117      |\n",
      "|    time_elapsed         | 368       |\n",
      "|    total_timesteps      | 285952    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.8492854 |\n",
      "|    clip_fraction        | 0.167     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0772   |\n",
      "|    explained_variance   | 0.828     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0985   |\n",
      "|    n_updates            | 21204     |\n",
      "|    policy_gradient_loss | -0.0574   |\n",
      "|    value_loss           | 0.0354    |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=286000, episode_reward=52.23 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 52.2      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 286000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.0367594 |\n",
      "|    clip_fraction        | 0.111     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0503   |\n",
      "|    explained_variance   | 0.584     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0892   |\n",
      "|    n_updates            | 21223     |\n",
      "|    policy_gradient_loss | -0.0328   |\n",
      "|    value_loss           | 0.11      |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 76.8     |\n",
      "| time/              |          |\n",
      "|    fps             | 775      |\n",
      "|    iterations      | 1118     |\n",
      "|    time_elapsed    | 369      |\n",
      "|    total_timesteps | 286208   |\n",
      "---------------------------------\n",
      "--------------------------------------\n",
      "| rollout/                |          |\n",
      "|    ep_len_mean          | 889      |\n",
      "|    ep_rew_mean          | 76.6     |\n",
      "| time/                   |          |\n",
      "|    fps                  | 775      |\n",
      "|    iterations           | 1119     |\n",
      "|    time_elapsed         | 369      |\n",
      "|    total_timesteps      | 286464   |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 0.637655 |\n",
      "|    clip_fraction        | 0.144    |\n",
      "|    clip_range           | 0.4      |\n",
      "|    entropy_loss         | -0.078   |\n",
      "|    explained_variance   | 0.639    |\n",
      "|    learning_rate        | 0.000389 |\n",
      "|    loss                 | -0.0356  |\n",
      "|    n_updates            | 21242    |\n",
      "|    policy_gradient_loss | -0.0733  |\n",
      "|    value_loss           | 0.157    |\n",
      "--------------------------------------\n",
      "Eval num_timesteps=286500, episode_reward=71.84 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 230        |\n",
      "|    mean_reward          | 71.8       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 286500     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.39335197 |\n",
      "|    clip_fraction        | 0.136      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0834    |\n",
      "|    explained_variance   | -0.218     |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.052     |\n",
      "|    n_updates            | 21261      |\n",
      "|    policy_gradient_loss | -0.0342    |\n",
      "|    value_loss           | 0.0747     |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 76.6     |\n",
      "| time/              |          |\n",
      "|    fps             | 775      |\n",
      "|    iterations      | 1120     |\n",
      "|    time_elapsed    | 369      |\n",
      "|    total_timesteps | 286720   |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 76.6      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 775       |\n",
      "|    iterations           | 1121      |\n",
      "|    time_elapsed         | 369       |\n",
      "|    total_timesteps      | 286976    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.7575538 |\n",
      "|    clip_fraction        | 0.128     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0456   |\n",
      "|    explained_variance   | 0.753     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.017    |\n",
      "|    n_updates            | 21280     |\n",
      "|    policy_gradient_loss | -0.0522   |\n",
      "|    value_loss           | 0.0976    |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=287000, episode_reward=88.47 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 230        |\n",
      "|    mean_reward          | 88.5       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 287000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.79723024 |\n",
      "|    clip_fraction        | 0.166      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0618    |\n",
      "|    explained_variance   | 0.581      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0583    |\n",
      "|    n_updates            | 21299      |\n",
      "|    policy_gradient_loss | -0.0552    |\n",
      "|    value_loss           | 0.123      |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 76.6     |\n",
      "| time/              |          |\n",
      "|    fps             | 775      |\n",
      "|    iterations      | 1122     |\n",
      "|    time_elapsed    | 370      |\n",
      "|    total_timesteps | 287232   |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 76.6       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 775        |\n",
      "|    iterations           | 1123       |\n",
      "|    time_elapsed         | 370        |\n",
      "|    total_timesteps      | 287488     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.61857677 |\n",
      "|    clip_fraction        | 0.154      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.103     |\n",
      "|    explained_variance   | 0.529      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.00926   |\n",
      "|    n_updates            | 21318      |\n",
      "|    policy_gradient_loss | -0.0617    |\n",
      "|    value_loss           | 0.343      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=287500, episode_reward=44.81 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 230        |\n",
      "|    mean_reward          | 44.8       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 287500     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.58420676 |\n",
      "|    clip_fraction        | 0.157      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0535    |\n",
      "|    explained_variance   | 0.633      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0359    |\n",
      "|    n_updates            | 21337      |\n",
      "|    policy_gradient_loss | -0.0573    |\n",
      "|    value_loss           | 0.0618     |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 76.6     |\n",
      "| time/              |          |\n",
      "|    fps             | 775      |\n",
      "|    iterations      | 1124     |\n",
      "|    time_elapsed    | 371      |\n",
      "|    total_timesteps | 287744   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=288000, episode_reward=45.90 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 230        |\n",
      "|    mean_reward          | 45.9       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 288000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.88018274 |\n",
      "|    clip_fraction        | 0.128      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0413    |\n",
      "|    explained_variance   | 0.573      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.00446   |\n",
      "|    n_updates            | 21356      |\n",
      "|    policy_gradient_loss | -0.047     |\n",
      "|    value_loss           | 0.22       |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 76.6     |\n",
      "| time/              |          |\n",
      "|    fps             | 775      |\n",
      "|    iterations      | 1125     |\n",
      "|    time_elapsed    | 371      |\n",
      "|    total_timesteps | 288000   |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 76.4      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 775       |\n",
      "|    iterations           | 1126      |\n",
      "|    time_elapsed         | 371       |\n",
      "|    total_timesteps      | 288256    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.6265115 |\n",
      "|    clip_fraction        | 0.137     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0764   |\n",
      "|    explained_variance   | 0.618     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0596   |\n",
      "|    n_updates            | 21375     |\n",
      "|    policy_gradient_loss | -0.0578   |\n",
      "|    value_loss           | 0.316     |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=288500, episode_reward=57.76 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 57.8      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 288500    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.5128192 |\n",
      "|    clip_fraction        | 0.122     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0741   |\n",
      "|    explained_variance   | 0.202     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0343   |\n",
      "|    n_updates            | 21394     |\n",
      "|    policy_gradient_loss | -0.0325   |\n",
      "|    value_loss           | 0.136     |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 76.4     |\n",
      "| time/              |          |\n",
      "|    fps             | 775      |\n",
      "|    iterations      | 1127     |\n",
      "|    time_elapsed    | 372      |\n",
      "|    total_timesteps | 288512   |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 76.4       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 775        |\n",
      "|    iterations           | 1128       |\n",
      "|    time_elapsed         | 372        |\n",
      "|    total_timesteps      | 288768     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.54112685 |\n",
      "|    clip_fraction        | 0.126      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0535    |\n",
      "|    explained_variance   | 0.816      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0452    |\n",
      "|    n_updates            | 21413      |\n",
      "|    policy_gradient_loss | -0.0512    |\n",
      "|    value_loss           | 0.0831     |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=289000, episode_reward=81.92 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 81.9      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 289000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.8038975 |\n",
      "|    clip_fraction        | 0.139     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0526   |\n",
      "|    explained_variance   | 0.676     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0588   |\n",
      "|    n_updates            | 21432     |\n",
      "|    policy_gradient_loss | -0.0547   |\n",
      "|    value_loss           | 0.118     |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 76.3     |\n",
      "| time/              |          |\n",
      "|    fps             | 774      |\n",
      "|    iterations      | 1129     |\n",
      "|    time_elapsed    | 372      |\n",
      "|    total_timesteps | 289024   |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 76.3       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 775        |\n",
      "|    iterations           | 1130       |\n",
      "|    time_elapsed         | 373        |\n",
      "|    total_timesteps      | 289280     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.45291167 |\n",
      "|    clip_fraction        | 0.126      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0861    |\n",
      "|    explained_variance   | 0.467      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | 0.0453     |\n",
      "|    n_updates            | 21451      |\n",
      "|    policy_gradient_loss | -0.0394    |\n",
      "|    value_loss           | 0.284      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=289500, episode_reward=58.59 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 230        |\n",
      "|    mean_reward          | 58.6       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 289500     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.72621787 |\n",
      "|    clip_fraction        | 0.167      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0659    |\n",
      "|    explained_variance   | 0.466      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0568    |\n",
      "|    n_updates            | 21470      |\n",
      "|    policy_gradient_loss | -0.0694    |\n",
      "|    value_loss           | 0.0531     |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 76.3     |\n",
      "| time/              |          |\n",
      "|    fps             | 774      |\n",
      "|    iterations      | 1131     |\n",
      "|    time_elapsed    | 373      |\n",
      "|    total_timesteps | 289536   |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 76.3       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 775        |\n",
      "|    iterations           | 1132       |\n",
      "|    time_elapsed         | 373        |\n",
      "|    total_timesteps      | 289792     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.75427866 |\n",
      "|    clip_fraction        | 0.13       |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.056     |\n",
      "|    explained_variance   | 0.7        |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0619    |\n",
      "|    n_updates            | 21489      |\n",
      "|    policy_gradient_loss | -0.0502    |\n",
      "|    value_loss           | 0.2        |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=290000, episode_reward=74.82 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 230        |\n",
      "|    mean_reward          | 74.8       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 290000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.45865262 |\n",
      "|    clip_fraction        | 0.111      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0801    |\n",
      "|    explained_variance   | 0.321      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | 0.0258     |\n",
      "|    n_updates            | 21508      |\n",
      "|    policy_gradient_loss | -0.0476    |\n",
      "|    value_loss           | 0.468      |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 76.4     |\n",
      "| time/              |          |\n",
      "|    fps             | 774      |\n",
      "|    iterations      | 1133     |\n",
      "|    time_elapsed    | 374      |\n",
      "|    total_timesteps | 290048   |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 76.4      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 774       |\n",
      "|    iterations           | 1134      |\n",
      "|    time_elapsed         | 374       |\n",
      "|    total_timesteps      | 290304    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.8688737 |\n",
      "|    clip_fraction        | 0.226     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.117    |\n",
      "|    explained_variance   | 0.384     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.114    |\n",
      "|    n_updates            | 21527     |\n",
      "|    policy_gradient_loss | -0.0746   |\n",
      "|    value_loss           | 0.0742    |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=290500, episode_reward=76.06 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 230        |\n",
      "|    mean_reward          | 76.1       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 290500     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.48791435 |\n",
      "|    clip_fraction        | 0.115      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0607    |\n",
      "|    explained_variance   | 0.646      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.124     |\n",
      "|    n_updates            | 21546      |\n",
      "|    policy_gradient_loss | -0.0555    |\n",
      "|    value_loss           | 0.13       |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 76.4     |\n",
      "| time/              |          |\n",
      "|    fps             | 774      |\n",
      "|    iterations      | 1135     |\n",
      "|    time_elapsed    | 375      |\n",
      "|    total_timesteps | 290560   |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 76.4      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 774       |\n",
      "|    iterations           | 1136      |\n",
      "|    time_elapsed         | 375       |\n",
      "|    total_timesteps      | 290816    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.7449774 |\n",
      "|    clip_fraction        | 0.13      |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0495   |\n",
      "|    explained_variance   | 0.554     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0704   |\n",
      "|    n_updates            | 21565     |\n",
      "|    policy_gradient_loss | -0.0622   |\n",
      "|    value_loss           | 0.0975    |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=291000, episode_reward=109.89 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 110       |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 291000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.0935173 |\n",
      "|    clip_fraction        | 0.207     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.112    |\n",
      "|    explained_variance   | 0.733     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0736   |\n",
      "|    n_updates            | 21584     |\n",
      "|    policy_gradient_loss | -0.0563   |\n",
      "|    value_loss           | 0.197     |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 76.4     |\n",
      "| time/              |          |\n",
      "|    fps             | 774      |\n",
      "|    iterations      | 1137     |\n",
      "|    time_elapsed    | 375      |\n",
      "|    total_timesteps | 291072   |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 76.4      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 774       |\n",
      "|    iterations           | 1138      |\n",
      "|    time_elapsed         | 375       |\n",
      "|    total_timesteps      | 291328    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.2519066 |\n",
      "|    clip_fraction        | 0.116     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0692   |\n",
      "|    explained_variance   | 0.566     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | 0.0108    |\n",
      "|    n_updates            | 21603     |\n",
      "|    policy_gradient_loss | -0.0352   |\n",
      "|    value_loss           | 0.0461    |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=291500, episode_reward=88.56 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 88.6      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 291500    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.5410234 |\n",
      "|    clip_fraction        | 0.111     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0468   |\n",
      "|    explained_variance   | 0.693     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0058   |\n",
      "|    n_updates            | 21622     |\n",
      "|    policy_gradient_loss | -0.0467   |\n",
      "|    value_loss           | 0.201     |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 76.4     |\n",
      "| time/              |          |\n",
      "|    fps             | 774      |\n",
      "|    iterations      | 1139     |\n",
      "|    time_elapsed    | 376      |\n",
      "|    total_timesteps | 291584   |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 76.4      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 775       |\n",
      "|    iterations           | 1140      |\n",
      "|    time_elapsed         | 376       |\n",
      "|    total_timesteps      | 291840    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.5225155 |\n",
      "|    clip_fraction        | 0.118     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0786   |\n",
      "|    explained_variance   | 0.813     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0203   |\n",
      "|    n_updates            | 21641     |\n",
      "|    policy_gradient_loss | -0.0573   |\n",
      "|    value_loss           | 0.213     |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=292000, episode_reward=83.79 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 83.8      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 292000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.9580837 |\n",
      "|    clip_fraction        | 0.174     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.093    |\n",
      "|    explained_variance   | -0.0661   |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0198   |\n",
      "|    n_updates            | 21660     |\n",
      "|    policy_gradient_loss | -0.0519   |\n",
      "|    value_loss           | 0.0642    |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 76.4     |\n",
      "| time/              |          |\n",
      "|    fps             | 774      |\n",
      "|    iterations      | 1141     |\n",
      "|    time_elapsed    | 376      |\n",
      "|    total_timesteps | 292096   |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 76.4      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 775       |\n",
      "|    iterations           | 1142      |\n",
      "|    time_elapsed         | 377       |\n",
      "|    total_timesteps      | 292352    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.4229067 |\n",
      "|    clip_fraction        | 0.0808    |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0461   |\n",
      "|    explained_variance   | 0.0564    |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0529   |\n",
      "|    n_updates            | 21679     |\n",
      "|    policy_gradient_loss | -0.0379   |\n",
      "|    value_loss           | 0.146     |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=292500, episode_reward=53.11 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 53.1      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 292500    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.3782604 |\n",
      "|    clip_fraction        | 0.103     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0675   |\n",
      "|    explained_variance   | 0.186     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.037    |\n",
      "|    n_updates            | 21698     |\n",
      "|    policy_gradient_loss | -0.0455   |\n",
      "|    value_loss           | 0.216     |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 76.6     |\n",
      "| time/              |          |\n",
      "|    fps             | 775      |\n",
      "|    iterations      | 1143     |\n",
      "|    time_elapsed    | 377      |\n",
      "|    total_timesteps | 292608   |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 76.6      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 775       |\n",
      "|    iterations           | 1144      |\n",
      "|    time_elapsed         | 377       |\n",
      "|    total_timesteps      | 292864    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.0055435 |\n",
      "|    clip_fraction        | 0.188     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.115    |\n",
      "|    explained_variance   | 0.752     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0874   |\n",
      "|    n_updates            | 21717     |\n",
      "|    policy_gradient_loss | -0.0495   |\n",
      "|    value_loss           | 0.207     |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=293000, episode_reward=34.05 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 230        |\n",
      "|    mean_reward          | 34.1       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 293000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.76966006 |\n",
      "|    clip_fraction        | 0.131      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0481    |\n",
      "|    explained_variance   | 0.569      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.118     |\n",
      "|    n_updates            | 21736      |\n",
      "|    policy_gradient_loss | -0.0558    |\n",
      "|    value_loss           | 0.0364     |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 76.6     |\n",
      "| time/              |          |\n",
      "|    fps             | 775      |\n",
      "|    iterations      | 1145     |\n",
      "|    time_elapsed    | 378      |\n",
      "|    total_timesteps | 293120   |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 76.6       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 775        |\n",
      "|    iterations           | 1146       |\n",
      "|    time_elapsed         | 378        |\n",
      "|    total_timesteps      | 293376     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.38120073 |\n",
      "|    clip_fraction        | 0.0715     |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0443    |\n",
      "|    explained_variance   | 0.752      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0246    |\n",
      "|    n_updates            | 21755      |\n",
      "|    policy_gradient_loss | -0.0257    |\n",
      "|    value_loss           | 0.1        |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=293500, episode_reward=29.94 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 230        |\n",
      "|    mean_reward          | 29.9       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 293500     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.44242582 |\n",
      "|    clip_fraction        | 0.134      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0707    |\n",
      "|    explained_variance   | 0.502      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0449    |\n",
      "|    n_updates            | 21774      |\n",
      "|    policy_gradient_loss | -0.0501    |\n",
      "|    value_loss           | 0.574      |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 76.6     |\n",
      "| time/              |          |\n",
      "|    fps             | 775      |\n",
      "|    iterations      | 1147     |\n",
      "|    time_elapsed    | 378      |\n",
      "|    total_timesteps | 293632   |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 76.6      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 775       |\n",
      "|    iterations           | 1148      |\n",
      "|    time_elapsed         | 378       |\n",
      "|    total_timesteps      | 293888    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.9511803 |\n",
      "|    clip_fraction        | 0.168     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0891   |\n",
      "|    explained_variance   | 0.292     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0912   |\n",
      "|    n_updates            | 21793     |\n",
      "|    policy_gradient_loss | -0.0678   |\n",
      "|    value_loss           | 0.0657    |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=294000, episode_reward=36.23 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 36.2      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 294000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.6061993 |\n",
      "|    clip_fraction        | 0.11      |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0525   |\n",
      "|    explained_variance   | 0.694     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0504   |\n",
      "|    n_updates            | 21812     |\n",
      "|    policy_gradient_loss | -0.0541   |\n",
      "|    value_loss           | 0.0937    |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 76.6     |\n",
      "| time/              |          |\n",
      "|    fps             | 775      |\n",
      "|    iterations      | 1149     |\n",
      "|    time_elapsed    | 379      |\n",
      "|    total_timesteps | 294144   |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 76.6       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 775        |\n",
      "|    iterations           | 1150       |\n",
      "|    time_elapsed         | 379        |\n",
      "|    total_timesteps      | 294400     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.44731832 |\n",
      "|    clip_fraction        | 0.119      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0443    |\n",
      "|    explained_variance   | 0.682      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0677    |\n",
      "|    n_updates            | 21831      |\n",
      "|    policy_gradient_loss | -0.0542    |\n",
      "|    value_loss           | 0.0613     |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=294500, episode_reward=47.24 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 230        |\n",
      "|    mean_reward          | 47.2       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 294500     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.35654494 |\n",
      "|    clip_fraction        | 0.127      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0872    |\n",
      "|    explained_variance   | 0.724      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.00125   |\n",
      "|    n_updates            | 21850      |\n",
      "|    policy_gradient_loss | 0.3        |\n",
      "|    value_loss           | 0.21       |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 76.6     |\n",
      "| time/              |          |\n",
      "|    fps             | 775      |\n",
      "|    iterations      | 1151     |\n",
      "|    time_elapsed    | 379      |\n",
      "|    total_timesteps | 294656   |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 76.6      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 775       |\n",
      "|    iterations           | 1152      |\n",
      "|    time_elapsed         | 380       |\n",
      "|    total_timesteps      | 294912    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.1023803 |\n",
      "|    clip_fraction        | 0.147     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0458   |\n",
      "|    explained_variance   | 0.767     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0637   |\n",
      "|    n_updates            | 21869     |\n",
      "|    policy_gradient_loss | -0.05     |\n",
      "|    value_loss           | 0.0349    |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=295000, episode_reward=43.82 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 43.8      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 295000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.8188566 |\n",
      "|    clip_fraction        | 0.111     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.045    |\n",
      "|    explained_variance   | 0.609     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.115    |\n",
      "|    n_updates            | 21888     |\n",
      "|    policy_gradient_loss | -0.0482   |\n",
      "|    value_loss           | 0.149     |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 76.8     |\n",
      "| time/              |          |\n",
      "|    fps             | 775      |\n",
      "|    iterations      | 1153     |\n",
      "|    time_elapsed    | 380      |\n",
      "|    total_timesteps | 295168   |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 76.8       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 775        |\n",
      "|    iterations           | 1154       |\n",
      "|    time_elapsed         | 380        |\n",
      "|    total_timesteps      | 295424     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.48582876 |\n",
      "|    clip_fraction        | 0.137      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0755    |\n",
      "|    explained_variance   | 0.642      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.029     |\n",
      "|    n_updates            | 21907      |\n",
      "|    policy_gradient_loss | -0.0446    |\n",
      "|    value_loss           | 0.34       |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=295500, episode_reward=63.76 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 230        |\n",
      "|    mean_reward          | 63.8       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 295500     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.78096986 |\n",
      "|    clip_fraction        | 0.156      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0756    |\n",
      "|    explained_variance   | 0.565      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0623    |\n",
      "|    n_updates            | 21926      |\n",
      "|    policy_gradient_loss | -0.0531    |\n",
      "|    value_loss           | 0.0348     |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 76.8     |\n",
      "| time/              |          |\n",
      "|    fps             | 775      |\n",
      "|    iterations      | 1155     |\n",
      "|    time_elapsed    | 381      |\n",
      "|    total_timesteps | 295680   |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 76.8      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 775       |\n",
      "|    iterations           | 1156      |\n",
      "|    time_elapsed         | 381       |\n",
      "|    total_timesteps      | 295936    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.9501002 |\n",
      "|    clip_fraction        | 0.127     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0416   |\n",
      "|    explained_variance   | 0.832     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0467   |\n",
      "|    n_updates            | 21945     |\n",
      "|    policy_gradient_loss | -0.0602   |\n",
      "|    value_loss           | 0.0683    |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=296000, episode_reward=63.39 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 230        |\n",
      "|    mean_reward          | 63.4       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 296000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.75253236 |\n",
      "|    clip_fraction        | 0.169      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0619    |\n",
      "|    explained_variance   | 0.683      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0764    |\n",
      "|    n_updates            | 21964      |\n",
      "|    policy_gradient_loss | -0.067     |\n",
      "|    value_loss           | 0.0869     |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 76.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 775      |\n",
      "|    iterations      | 1157     |\n",
      "|    time_elapsed    | 381      |\n",
      "|    total_timesteps | 296192   |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 76.9      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 775       |\n",
      "|    iterations           | 1158      |\n",
      "|    time_elapsed         | 382       |\n",
      "|    total_timesteps      | 296448    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.7481861 |\n",
      "|    clip_fraction        | 0.174     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.106    |\n",
      "|    explained_variance   | 0.658     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | 0.0318    |\n",
      "|    n_updates            | 21983     |\n",
      "|    policy_gradient_loss | -0.0632   |\n",
      "|    value_loss           | 0.354     |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=296500, episode_reward=43.51 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 230        |\n",
      "|    mean_reward          | 43.5       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 296500     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.48523396 |\n",
      "|    clip_fraction        | 0.122      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0576    |\n",
      "|    explained_variance   | 0.357      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0674    |\n",
      "|    n_updates            | 22002      |\n",
      "|    policy_gradient_loss | -0.0459    |\n",
      "|    value_loss           | 0.0645     |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 76.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 775      |\n",
      "|    iterations      | 1159     |\n",
      "|    time_elapsed    | 382      |\n",
      "|    total_timesteps | 296704   |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 77.3       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 775        |\n",
      "|    iterations           | 1160       |\n",
      "|    time_elapsed         | 382        |\n",
      "|    total_timesteps      | 296960     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.74326825 |\n",
      "|    clip_fraction        | 0.126      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0495    |\n",
      "|    explained_variance   | 0.745      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0584    |\n",
      "|    n_updates            | 22021      |\n",
      "|    policy_gradient_loss | -0.0498    |\n",
      "|    value_loss           | 0.13       |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=297000, episode_reward=70.96 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 71        |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 297000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.4621108 |\n",
      "|    clip_fraction        | 0.142     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0875   |\n",
      "|    explained_variance   | 0.621     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | 0.0408    |\n",
      "|    n_updates            | 22040     |\n",
      "|    policy_gradient_loss | -0.04     |\n",
      "|    value_loss           | 0.374     |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 77.3     |\n",
      "| time/              |          |\n",
      "|    fps             | 775      |\n",
      "|    iterations      | 1161     |\n",
      "|    time_elapsed    | 383      |\n",
      "|    total_timesteps | 297216   |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 77.3      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 775       |\n",
      "|    iterations           | 1162      |\n",
      "|    time_elapsed         | 383       |\n",
      "|    total_timesteps      | 297472    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.5209071 |\n",
      "|    clip_fraction        | 0.157     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0807   |\n",
      "|    explained_variance   | 0.43      |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0684   |\n",
      "|    n_updates            | 22059     |\n",
      "|    policy_gradient_loss | -0.0625   |\n",
      "|    value_loss           | 0.0543    |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=297500, episode_reward=74.07 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 230        |\n",
      "|    mean_reward          | 74.1       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 297500     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.54112744 |\n",
      "|    clip_fraction        | 0.0907     |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0445    |\n",
      "|    explained_variance   | 0.493      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.091     |\n",
      "|    n_updates            | 22078      |\n",
      "|    policy_gradient_loss | -0.0414    |\n",
      "|    value_loss           | 0.146      |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 77.3     |\n",
      "| time/              |          |\n",
      "|    fps             | 775      |\n",
      "|    iterations      | 1163     |\n",
      "|    time_elapsed    | 383      |\n",
      "|    total_timesteps | 297728   |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 77.5      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 775       |\n",
      "|    iterations           | 1164      |\n",
      "|    time_elapsed         | 384       |\n",
      "|    total_timesteps      | 297984    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.8547757 |\n",
      "|    clip_fraction        | 0.203     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0822   |\n",
      "|    explained_variance   | 0.857     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.124    |\n",
      "|    n_updates            | 22097     |\n",
      "|    policy_gradient_loss | -0.0741   |\n",
      "|    value_loss           | 0.0939    |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=298000, episode_reward=62.05 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 230        |\n",
      "|    mean_reward          | 62         |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 298000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.30417454 |\n",
      "|    clip_fraction        | 0.118      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0881    |\n",
      "|    explained_variance   | 0.674      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.00029   |\n",
      "|    n_updates            | 22116      |\n",
      "|    policy_gradient_loss | -0.0341    |\n",
      "|    value_loss           | 0.136      |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 77.5     |\n",
      "| time/              |          |\n",
      "|    fps             | 775      |\n",
      "|    iterations      | 1165     |\n",
      "|    time_elapsed    | 384      |\n",
      "|    total_timesteps | 298240   |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 77.5      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 775       |\n",
      "|    iterations           | 1166      |\n",
      "|    time_elapsed         | 384       |\n",
      "|    total_timesteps      | 298496    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.5730724 |\n",
      "|    clip_fraction        | 0.158     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0576   |\n",
      "|    explained_variance   | 0.706     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.15     |\n",
      "|    n_updates            | 22135     |\n",
      "|    policy_gradient_loss | -0.0648   |\n",
      "|    value_loss           | 0.0656    |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=298500, episode_reward=52.62 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 52.6      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 298500    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.0524957 |\n",
      "|    clip_fraction        | 0.147     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0558   |\n",
      "|    explained_variance   | 0.303     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.146    |\n",
      "|    n_updates            | 22154     |\n",
      "|    policy_gradient_loss | -0.0578   |\n",
      "|    value_loss           | 0.219     |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 77.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 775      |\n",
      "|    iterations      | 1167     |\n",
      "|    time_elapsed    | 385      |\n",
      "|    total_timesteps | 298752   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=299000, episode_reward=45.84 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 230        |\n",
      "|    mean_reward          | 45.8       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 299000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.52097255 |\n",
      "|    clip_fraction        | 0.102      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0657    |\n",
      "|    explained_variance   | 0.767      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0603    |\n",
      "|    n_updates            | 22173      |\n",
      "|    policy_gradient_loss | -0.054     |\n",
      "|    value_loss           | 0.247      |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 77.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 774      |\n",
      "|    iterations      | 1168     |\n",
      "|    time_elapsed    | 385      |\n",
      "|    total_timesteps | 299008   |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 77.9      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 775       |\n",
      "|    iterations           | 1169      |\n",
      "|    time_elapsed         | 386       |\n",
      "|    total_timesteps      | 299264    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.8255021 |\n",
      "|    clip_fraction        | 0.135     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0472   |\n",
      "|    explained_variance   | 0.529     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0261   |\n",
      "|    n_updates            | 22192     |\n",
      "|    policy_gradient_loss | -0.0544   |\n",
      "|    value_loss           | 0.0528    |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=299500, episode_reward=63.52 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 230        |\n",
      "|    mean_reward          | 63.5       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 299500     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.81728846 |\n",
      "|    clip_fraction        | 0.124      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.053     |\n",
      "|    explained_variance   | 0.703      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0247    |\n",
      "|    n_updates            | 22211      |\n",
      "|    policy_gradient_loss | -0.0538    |\n",
      "|    value_loss           | 0.082      |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 77.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 775      |\n",
      "|    iterations      | 1170     |\n",
      "|    time_elapsed    | 386      |\n",
      "|    total_timesteps | 299520   |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 78        |\n",
      "| time/                   |           |\n",
      "|    fps                  | 775       |\n",
      "|    iterations           | 1171      |\n",
      "|    time_elapsed         | 386       |\n",
      "|    total_timesteps      | 299776    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.1907996 |\n",
      "|    clip_fraction        | 0.141     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.04     |\n",
      "|    explained_variance   | 0.878     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0794   |\n",
      "|    n_updates            | 22230     |\n",
      "|    policy_gradient_loss | -0.0582   |\n",
      "|    value_loss           | 0.0971    |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=300000, episode_reward=53.20 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 230        |\n",
      "|    mean_reward          | 53.2       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 300000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.85309875 |\n",
      "|    clip_fraction        | 0.164      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0749    |\n",
      "|    explained_variance   | 0.705      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0108    |\n",
      "|    n_updates            | 22249      |\n",
      "|    policy_gradient_loss | -0.039     |\n",
      "|    value_loss           | 0.117      |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 78       |\n",
      "| time/              |          |\n",
      "|    fps             | 775      |\n",
      "|    iterations      | 1172     |\n",
      "|    time_elapsed    | 387      |\n",
      "|    total_timesteps | 300032   |\n",
      "---------------------------------\n",
      "Final model evaluation on validation set: Mean Reward = 114.36 ± 7.77\n"
     ]
    }
   ],
   "source": [
    "#Train the final model\n",
    "best_params = study.best_params\n",
    "\n",
    "\n",
    "train_env = CustomStocksEnv(df=train_df, window_size=window_size, frame_bound=(window_size, len(train_df)))\n",
    "val_env = CustomStocksEnv(df=val_df, window_size=window_size, frame_bound=(window_size, len(val_df)))\n",
    "val_env = Monitor(val_env)\n",
    "\n",
    "eval_callback = EvalCallback(\n",
    "        val_env,\n",
    "        best_model_save_path='./logs/best_model/',\n",
    "        log_path='./logs/results/',\n",
    "        eval_freq=500,  # Evaluate every 500 timesteps.\n",
    "        deterministic=True,\n",
    "        render=False\n",
    "    )\n",
    "\n",
    "final_model = PPO(\n",
    "    \"MlpPolicy\",\n",
    "    train_env,\n",
    "    learning_rate=best_params[\"learning_rate\"],\n",
    "    n_steps=best_params[\"n_steps\"],\n",
    "    gamma=best_params[\"gamma\"],\n",
    "    ent_coef=best_params[\"ent_coef\"],\n",
    "    clip_range=best_params[\"clip_range\"],\n",
    "    batch_size=best_params[\"batch_size\"],\n",
    "    gae_lambda=best_params[\"gae_lambda\"],\n",
    "    n_epochs=best_params[\"n_epochs\"],\n",
    "    verbose=1,\n",
    ")\n",
    "\n",
    "final_model.learn(total_timesteps=300_000, callback=eval_callback)\n",
    "final_model.save(\"ppo_stocks_model\")\n",
    "\n",
    "#Load best model\n",
    "final_model = PPO.load(\"./logs/best_model/best_model.zip\")\n",
    "\n",
    "\n",
    "#val_env = CustomStocksEnv(df=val_df, window_size=window_size, frame_bound=(window_size, len(val_df)))\n",
    "mean_reward, std_reward = evaluate_policy(final_model, val_env, n_eval_episodes=20, deterministic=False)\n",
    "\n",
    "print(f\"Final model evaluation on validation set: Mean Reward = {mean_reward:.2f} ± {std_reward:.2f}\")\n",
    "\n",
    "train_env.close()\n",
    "val_env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7136cf19-5e02-43bc-8941-4062687ab01e",
   "metadata": {},
   "source": [
    "## c. Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3025335f-539f-4d0f-9a0e-2f74d2eeb0ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final info: {'total_reward': np.float32(-2.4900055), 'total_profit': np.float32(0.88610405), 'position': <Positions.Long: 1>}\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_env = CustomStocksEnv(df=test_df, window_size=window_size, frame_bound=(window_size, len(test_df)))\n",
    "final_model = PPO.load(\"./logs/best_model/best_model.zip\")\n",
    "\n",
    "obs, info = test_env.reset()\n",
    "rewards = []\n",
    "while True:\n",
    "    action, _states = final_model.predict(obs, deterministic=True)\n",
    "    \n",
    "    obs, reward, terminated, truncated, info = test_env.step(action)\n",
    "    done = terminated or truncated\n",
    "    if done:\n",
    "        print(\"Final info:\", info)\n",
    "        break\n",
    "\n",
    "plt.figure(figsize=(15, 6))\n",
    "plt.cla()\n",
    "test_env.unwrapped.render_all()\n",
    "plt.show()\n",
    "\n",
    "test_env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6ea825b-7740-428a-9bb8-2be0b0849fc9",
   "metadata": {},
   "source": [
    "# 4. RecurrentPPO Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a8ca9eb-0ccb-4980-8e63-de61b9effdf9",
   "metadata": {},
   "source": [
    "Because RecurrentPPO is an extension of PPO, we can reused hyperparameters from PPO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4514c584-085e-4518-a0d2-589faca20e13",
   "metadata": {},
   "source": [
    "## a. Fine tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a6985aec-4932-4f24-a5c0-8872613ba4be",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"ppo_best_params.pkl\", \"rb\") as f:\n",
    "    ppo_best_params = pickle.load(f)\n",
    "\n",
    "def recurrent_ppo_objective(trial: optuna.Trial):\n",
    "    \"\"\"\n",
    "    Fine tuning using Bayesian Optimization from optuna\n",
    "    \"\"\"\n",
    "    if ppo_best_params is not None:\n",
    "        # Sample LSTM hyperparams\n",
    "        n_lstm_layers = trial.suggest_int(\"n_lstm_layers\", 1, 3)\n",
    "        hidden_size  = trial.suggest_categorical(\"hidden_size\", [32, 64, 128, 256])\n",
    "        dropout      = trial.suggest_float(\"dropout\", 0.0, 0.5, step=0.1)\n",
    "    \n",
    "        # PPO hyperparams\n",
    "        learning_rate = ppo_best_params['learning_rate']\n",
    "        n_steps       = ppo_best_params['n_steps']\n",
    "        gamma         = ppo_best_params['gamma']\n",
    "        ent_coef      = ppo_best_params['ent_coef']\n",
    "        clip_range    = ppo_best_params['clip_range']\n",
    "        batch_size       = ppo_best_params['batch_size']\n",
    "        gae_lambda       = ppo_best_params['gae_lambda']\n",
    "        n_epochs       = ppo_best_params['n_epochs']\n",
    "        \n",
    "    else:\n",
    "        # Sample LSTM hyperparams\n",
    "        n_lstm_layers = trial.suggest_int(\"n_lstm_layers\", 1, 3)\n",
    "        hidden_size  = trial.suggest_categorical(\"hidden_size\", [32, 64, 128, 256])\n",
    "        dropout      = trial.suggest_float(\"dropout\", 0.0, 0.5, step=0.1)\n",
    "    \n",
    "        # PPO hyperparams\n",
    "        learning_rate = trial.suggest_float(\"learning_rate\", 1e-5, 1e-2,log=True)\n",
    "        n_steps       = trial.suggest_int(\"n_steps\", 128, 2048, step=128)\n",
    "        gamma         = trial.suggest_float(\"gamma\", 0.90, 0.99, step=0.01)\n",
    "        ent_coef      = trial.suggest_float(\"ent_coef\", 1e-8, 0.1, log=True)\n",
    "        clip_range    = trial.suggest_float(\"clip_range\", 0.1, 0.4, step=0.05)\n",
    "    \n",
    "        #Less important hyperparameters:\n",
    "        batch_size       = trial.suggest_categorical(\"batch_size\", [32,64])\n",
    "        gae_lambda       = trial.suggest_float(\"gae_lambda\", 0.8, 0.98)\n",
    "        n_epochs       = trial.suggest_int(\"n_epochs\", 5, 20)\n",
    "\n",
    "\n",
    "    # Create the environment\n",
    "    train_env = CustomStocksEnv(df=train_df, window_size=window_size, frame_bound=(window_size, len(train_df)))\n",
    "    val_env = CustomStocksEnv(df=val_df, window_size=window_size, frame_bound=(window_size, len(val_df)))\n",
    "    val_env = Monitor(val_env)\n",
    "\n",
    "    eval_callback_ft = EvalCallback(\n",
    "        val_env,\n",
    "        best_model_save_path='./logs/best_model_ft_lstm/',\n",
    "        log_path='./logs/results_ft/',\n",
    "        eval_freq=500,  # Evaluate every 500 timesteps.\n",
    "        deterministic=True,\n",
    "        render=False,\n",
    "        verbose=0\n",
    "    )\n",
    "    \n",
    "    # Build policy_kwargs with a LSTM\n",
    "    policy_kwargs = dict(\n",
    "        n_lstm_layers=n_lstm_layers,\n",
    "        lstm_hidden_size = hidden_size,\n",
    "        lstm_kwargs=dict(\n",
    "            dropout=dropout\n",
    "        ),\n",
    "        # after LSTM, we add a small MLP\n",
    "        net_arch=dict(pi=[64], vf=[64])\n",
    "    )\n",
    "\n",
    "    # Build the RecurrentPPO model\n",
    "    model = RecurrentPPO( \n",
    "        policy=\"MlpLstmPolicy\", \n",
    "        env=train_env, \n",
    "        verbose=0, \n",
    "        learning_rate=learning_rate,\n",
    "        n_steps=n_steps,\n",
    "        batch_size=batch_size,\n",
    "        gae_lambda=gae_lambda,\n",
    "        gamma=gamma,\n",
    "        n_epochs=n_epochs, \n",
    "        ent_coef=ent_coef,\n",
    "        clip_range=clip_range,\n",
    "        policy_kwargs=policy_kwargs,\n",
    "    )\n",
    "\n",
    "\n",
    "    #Keep it short - 50,000 steps also make sense\n",
    "    model.learn(total_timesteps=100_000, callback=eval_callback_ft)\n",
    "\n",
    "    model = RecurrentPPO.load(\"./logs/best_model_ft_lstm/best_model.zip\")\n",
    "    mean_reward, std_reward = evaluate_policy(model, val_env, n_eval_episodes=20, deterministic=False)\n",
    "\n",
    "    # Cleanup the environments\n",
    "    train_env.close()\n",
    "    val_env.close()\n",
    "\n",
    "    return mean_reward\n",
    "\n",
    "def run_lstm_optimization():\n",
    "    n_trials = 50 if ppo_best_params is not None else 100\n",
    "    study = optuna.create_study(direction=\"maximize\")  \n",
    "    study.optimize(recurrent_ppo_objective, n_trials=n_trials)\n",
    "\n",
    "    print(\"Best hyperparameters:\", study.best_params)\n",
    "    print(\"Best value (objective):\", study.best_value)\n",
    "\n",
    "    return study\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "76e0113f-b97a-4718-80df-5d03e2a8fedd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-06 07:29:31,656] A new study created in memory with name: no-name-91ac7bd8-9f8c-43cc-92f0-6e2f93b198ee\n",
      "[I 2025-02-06 07:43:25,182] Trial 0 finished with value: 70.49236205 and parameters: {'n_lstm_layers': 2, 'hidden_size': 64, 'dropout': 0.4}. Best is trial 0 with value: 70.49236205.\n",
      "[I 2025-02-06 08:25:04,335] Trial 1 finished with value: 73.20287060000001 and parameters: {'n_lstm_layers': 2, 'hidden_size': 256, 'dropout': 0.2}. Best is trial 1 with value: 73.20287060000001.\n",
      "[I 2025-02-06 08:44:07,973] Trial 2 finished with value: 68.94015734999999 and parameters: {'n_lstm_layers': 3, 'hidden_size': 64, 'dropout': 0.2}. Best is trial 1 with value: 73.20287060000001.\n",
      "[I 2025-02-06 08:58:27,756] Trial 3 finished with value: 89.43671144999999 and parameters: {'n_lstm_layers': 1, 'hidden_size': 128, 'dropout': 0.0}. Best is trial 3 with value: 89.43671144999999.\n",
      "[I 2025-02-06 09:12:06,294] Trial 4 finished with value: 99.77510149999999 and parameters: {'n_lstm_layers': 2, 'hidden_size': 64, 'dropout': 0.0}. Best is trial 4 with value: 99.77510149999999.\n",
      "[I 2025-02-06 09:24:06,816] Trial 5 finished with value: 62.06810705 and parameters: {'n_lstm_layers': 2, 'hidden_size': 32, 'dropout': 0.1}. Best is trial 4 with value: 99.77510149999999.\n",
      "[I 2025-02-06 09:38:21,926] Trial 6 finished with value: 53.62809804999999 and parameters: {'n_lstm_layers': 2, 'hidden_size': 64, 'dropout': 0.1}. Best is trial 4 with value: 99.77510149999999.\n",
      "[I 2025-02-06 10:41:16,485] Trial 7 finished with value: 64.39037195 and parameters: {'n_lstm_layers': 3, 'hidden_size': 256, 'dropout': 0.2}. Best is trial 4 with value: 99.77510149999999.\n",
      "[I 2025-02-06 11:44:30,396] Trial 8 finished with value: 66.8877999 and parameters: {'n_lstm_layers': 3, 'hidden_size': 256, 'dropout': 0.5}. Best is trial 4 with value: 99.77510149999999.\n",
      "/opt/anaconda3/envs/self-learning/lib/python3.9/site-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "  warnings.warn(\n",
      "[I 2025-02-06 12:01:32,624] Trial 9 finished with value: 85.32706285 and parameters: {'n_lstm_layers': 1, 'hidden_size': 128, 'dropout': 0.1}. Best is trial 4 with value: 99.77510149999999.\n",
      "[I 2025-02-06 12:09:06,071] Trial 10 finished with value: 87.37412785 and parameters: {'n_lstm_layers': 1, 'hidden_size': 32, 'dropout': 0.0}. Best is trial 4 with value: 99.77510149999999.\n",
      "[I 2025-02-06 12:23:59,965] Trial 11 finished with value: 103.35086989999999 and parameters: {'n_lstm_layers': 1, 'hidden_size': 128, 'dropout': 0.0}. Best is trial 11 with value: 103.35086989999999.\n",
      "[I 2025-02-06 12:39:00,188] Trial 12 finished with value: 101.4057235 and parameters: {'n_lstm_layers': 1, 'hidden_size': 128, 'dropout': 0.0}. Best is trial 11 with value: 103.35086989999999.\n",
      "/opt/anaconda3/envs/self-learning/lib/python3.9/site-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.30000000000000004 and num_layers=1\n",
      "  warnings.warn(\n",
      "[I 2025-02-06 12:54:31,924] Trial 13 finished with value: 72.61569014999999 and parameters: {'n_lstm_layers': 1, 'hidden_size': 128, 'dropout': 0.30000000000000004}. Best is trial 11 with value: 103.35086989999999.\n",
      "[I 2025-02-06 13:10:22,232] Trial 14 finished with value: 92.51505605 and parameters: {'n_lstm_layers': 1, 'hidden_size': 128, 'dropout': 0.0}. Best is trial 11 with value: 103.35086989999999.\n",
      "/opt/anaconda3/envs/self-learning/lib/python3.9/site-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "  warnings.warn(\n",
      "[I 2025-02-06 13:25:36,378] Trial 15 finished with value: 77.3330181 and parameters: {'n_lstm_layers': 1, 'hidden_size': 128, 'dropout': 0.1}. Best is trial 11 with value: 103.35086989999999.\n",
      "/opt/anaconda3/envs/self-learning/lib/python3.9/site-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.30000000000000004 and num_layers=1\n",
      "  warnings.warn(\n",
      "[I 2025-02-06 13:40:22,878] Trial 16 finished with value: 64.19837559999999 and parameters: {'n_lstm_layers': 1, 'hidden_size': 128, 'dropout': 0.30000000000000004}. Best is trial 11 with value: 103.35086989999999.\n",
      "[I 2025-02-06 13:56:42,544] Trial 17 finished with value: 75.9573002 and parameters: {'n_lstm_layers': 1, 'hidden_size': 128, 'dropout': 0.0}. Best is trial 11 with value: 103.35086989999999.\n",
      "[I 2025-02-06 14:23:32,923] Trial 18 finished with value: 80.8562642 and parameters: {'n_lstm_layers': 2, 'hidden_size': 128, 'dropout': 0.1}. Best is trial 11 with value: 103.35086989999999.\n",
      "/opt/anaconda3/envs/self-learning/lib/python3.9/site-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
      "  warnings.warn(\n",
      "[I 2025-02-06 14:31:24,846] Trial 19 finished with value: 99.1903535 and parameters: {'n_lstm_layers': 1, 'hidden_size': 32, 'dropout': 0.5}. Best is trial 11 with value: 103.35086989999999.\n",
      "/opt/anaconda3/envs/self-learning/lib/python3.9/site-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.30000000000000004 and num_layers=1\n",
      "  warnings.warn(\n",
      "[I 2025-02-06 14:47:12,031] Trial 20 finished with value: 75.20846125 and parameters: {'n_lstm_layers': 1, 'hidden_size': 128, 'dropout': 0.30000000000000004}. Best is trial 11 with value: 103.35086989999999.\n",
      "[I 2025-02-06 15:01:26,482] Trial 21 finished with value: 89.0910839 and parameters: {'n_lstm_layers': 2, 'hidden_size': 64, 'dropout': 0.0}. Best is trial 11 with value: 103.35086989999999.\n",
      "[I 2025-02-06 15:23:35,742] Trial 22 finished with value: 67.03329655 and parameters: {'n_lstm_layers': 3, 'hidden_size': 64, 'dropout': 0.0}. Best is trial 11 with value: 103.35086989999999.\n",
      "[I 2025-02-06 15:39:23,817] Trial 23 finished with value: 62.72814485 and parameters: {'n_lstm_layers': 2, 'hidden_size': 64, 'dropout': 0.1}. Best is trial 11 with value: 103.35086989999999.\n",
      "[I 2025-02-06 15:56:21,464] Trial 24 finished with value: 83.75605519999999 and parameters: {'n_lstm_layers': 1, 'hidden_size': 128, 'dropout': 0.0}. Best is trial 11 with value: 103.35086989999999.\n",
      "[I 2025-02-06 16:11:25,670] Trial 25 finished with value: 98.4559046 and parameters: {'n_lstm_layers': 2, 'hidden_size': 64, 'dropout': 0.1}. Best is trial 11 with value: 103.35086989999999.\n",
      "[I 2025-02-06 16:19:08,242] Trial 26 finished with value: 86.48513215 and parameters: {'n_lstm_layers': 1, 'hidden_size': 32, 'dropout': 0.0}. Best is trial 11 with value: 103.35086989999999.\n",
      "[I 2025-02-06 17:18:36,989] Trial 27 finished with value: 69.9528505 and parameters: {'n_lstm_layers': 3, 'hidden_size': 256, 'dropout': 0.2}. Best is trial 11 with value: 103.35086989999999.\n",
      "[I 2025-02-06 17:44:25,152] Trial 28 finished with value: 73.6361173 and parameters: {'n_lstm_layers': 2, 'hidden_size': 128, 'dropout': 0.4}. Best is trial 11 with value: 103.35086989999999.\n",
      "[I 2025-02-06 17:58:02,232] Trial 29 finished with value: 80.43722515 and parameters: {'n_lstm_layers': 2, 'hidden_size': 64, 'dropout': 0.4}. Best is trial 11 with value: 103.35086989999999.\n",
      "[I 2025-02-06 18:06:31,363] Trial 30 finished with value: 78.82531435 and parameters: {'n_lstm_layers': 1, 'hidden_size': 64, 'dropout': 0.0}. Best is trial 11 with value: 103.35086989999999.\n",
      "/opt/anaconda3/envs/self-learning/lib/python3.9/site-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
      "  warnings.warn(\n",
      "[I 2025-02-06 18:13:44,165] Trial 31 finished with value: 90.3694991 and parameters: {'n_lstm_layers': 1, 'hidden_size': 32, 'dropout': 0.5}. Best is trial 11 with value: 103.35086989999999.\n",
      "/opt/anaconda3/envs/self-learning/lib/python3.9/site-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
      "  warnings.warn(\n",
      "[I 2025-02-06 18:20:59,168] Trial 32 finished with value: 78.6763433 and parameters: {'n_lstm_layers': 1, 'hidden_size': 32, 'dropout': 0.5}. Best is trial 11 with value: 103.35086989999999.\n",
      "/opt/anaconda3/envs/self-learning/lib/python3.9/site-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.4 and num_layers=1\n",
      "  warnings.warn(\n",
      "[I 2025-02-06 18:28:12,132] Trial 33 finished with value: 83.4847498 and parameters: {'n_lstm_layers': 1, 'hidden_size': 32, 'dropout': 0.4}. Best is trial 11 with value: 103.35086989999999.\n",
      "/opt/anaconda3/envs/self-learning/lib/python3.9/site-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
      "  warnings.warn(\n",
      "[I 2025-02-06 18:35:23,232] Trial 34 finished with value: 73.49322649999999 and parameters: {'n_lstm_layers': 1, 'hidden_size': 32, 'dropout': 0.2}. Best is trial 11 with value: 103.35086989999999.\n",
      "[I 2025-02-06 19:15:53,974] Trial 35 finished with value: 48.199371199999995 and parameters: {'n_lstm_layers': 2, 'hidden_size': 256, 'dropout': 0.1}. Best is trial 11 with value: 103.35086989999999.\n",
      "/opt/anaconda3/envs/self-learning/lib/python3.9/site-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
      "  warnings.warn(\n",
      "[I 2025-02-06 19:23:15,915] Trial 36 finished with value: 86.0647416 and parameters: {'n_lstm_layers': 1, 'hidden_size': 32, 'dropout': 0.2}. Best is trial 11 with value: 103.35086989999999.\n",
      "[I 2025-02-06 19:49:00,370] Trial 37 finished with value: 72.83574395000001 and parameters: {'n_lstm_layers': 2, 'hidden_size': 128, 'dropout': 0.0}. Best is trial 11 with value: 103.35086989999999.\n",
      "[I 2025-02-06 20:07:38,154] Trial 38 finished with value: 79.42805795 and parameters: {'n_lstm_layers': 3, 'hidden_size': 64, 'dropout': 0.1}. Best is trial 11 with value: 103.35086989999999.\n",
      "/opt/anaconda3/envs/self-learning/lib/python3.9/site-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.30000000000000004 and num_layers=1\n",
      "  warnings.warn(\n",
      "[I 2025-02-06 20:30:15,777] Trial 39 finished with value: 71.02488904999998 and parameters: {'n_lstm_layers': 1, 'hidden_size': 256, 'dropout': 0.30000000000000004}. Best is trial 11 with value: 103.35086989999999.\n",
      "[I 2025-02-06 20:41:55,535] Trial 40 finished with value: 59.51806144999999 and parameters: {'n_lstm_layers': 2, 'hidden_size': 32, 'dropout': 0.5}. Best is trial 11 with value: 103.35086989999999.\n",
      "[I 2025-02-06 20:55:32,780] Trial 41 finished with value: 79.94485655 and parameters: {'n_lstm_layers': 2, 'hidden_size': 64, 'dropout': 0.1}. Best is trial 11 with value: 103.35086989999999.\n",
      "[I 2025-02-06 21:09:09,925] Trial 42 finished with value: 88.27908895 and parameters: {'n_lstm_layers': 2, 'hidden_size': 64, 'dropout': 0.0}. Best is trial 11 with value: 103.35086989999999.\n",
      "[I 2025-02-06 21:29:26,231] Trial 43 finished with value: 41.96997965 and parameters: {'n_lstm_layers': 3, 'hidden_size': 64, 'dropout': 0.0}. Best is trial 11 with value: 103.35086989999999.\n",
      "[I 2025-02-06 21:44:00,309] Trial 44 finished with value: 81.06112115 and parameters: {'n_lstm_layers': 2, 'hidden_size': 64, 'dropout': 0.1}. Best is trial 11 with value: 103.35086989999999.\n",
      "[I 2025-02-06 21:59:54,596] Trial 45 finished with value: 76.43395915 and parameters: {'n_lstm_layers': 1, 'hidden_size': 128, 'dropout': 0.0}. Best is trial 11 with value: 103.35086989999999.\n",
      "/opt/anaconda3/envs/self-learning/lib/python3.9/site-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
      "  warnings.warn(\n",
      "[I 2025-02-06 22:08:56,044] Trial 46 finished with value: 105.10637735 and parameters: {'n_lstm_layers': 1, 'hidden_size': 64, 'dropout': 0.2}. Best is trial 46 with value: 105.10637735.\n",
      "/opt/anaconda3/envs/self-learning/lib/python3.9/site-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
      "  warnings.warn(\n",
      "[I 2025-02-06 22:24:43,504] Trial 47 finished with value: 76.57728345 and parameters: {'n_lstm_layers': 1, 'hidden_size': 128, 'dropout': 0.2}. Best is trial 46 with value: 105.10637735.\n",
      "/opt/anaconda3/envs/self-learning/lib/python3.9/site-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.30000000000000004 and num_layers=1\n",
      "  warnings.warn(\n",
      "[I 2025-02-06 22:40:24,646] Trial 48 finished with value: 83.607191 and parameters: {'n_lstm_layers': 1, 'hidden_size': 128, 'dropout': 0.30000000000000004}. Best is trial 46 with value: 105.10637735.\n",
      "/opt/anaconda3/envs/self-learning/lib/python3.9/site-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.4 and num_layers=1\n",
      "  warnings.warn(\n",
      "[I 2025-02-06 22:50:38,959] Trial 49 finished with value: 95.1035578 and parameters: {'n_lstm_layers': 1, 'hidden_size': 64, 'dropout': 0.4}. Best is trial 46 with value: 105.10637735.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters: {'n_lstm_layers': 1, 'hidden_size': 64, 'dropout': 0.2}\n",
      "Best value (objective): 105.10637735\n"
     ]
    }
   ],
   "source": [
    "#Run fine tuning\n",
    "lstm_study = run_lstm_optimization()\n",
    "\n",
    "lstm_best_params = lstm_study.best_params\n",
    "\n",
    "if ppo_best_params is not None:\n",
    "    lstm_best_params.update(ppo_best_params)\n",
    "\n",
    "with open(\"lstm_best_params.pkl\", \"wb\") as f:\n",
    "    pickle.dump(lstm_best_params, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3388c1ed-7dcd-493f-a568-da472a383196",
   "metadata": {},
   "source": [
    "## b. Training final model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8efaa9cc-69f1-441f-834a-a989bfdba4c5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 3297 |\n",
      "|    iterations      | 1    |\n",
      "|    time_elapsed    | 0    |\n",
      "|    total_timesteps | 256  |\n",
      "-----------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/self-learning/lib/python3.9/site-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=500, episode_reward=2.72 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 2.72      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 500       |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0757671 |\n",
      "|    clip_fraction        | 0.158     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.656    |\n",
      "|    explained_variance   | 0.257     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.172    |\n",
      "|    n_updates            | 19        |\n",
      "|    policy_gradient_loss | -0.0631   |\n",
      "|    value_loss           | 0.0626    |\n",
      "---------------------------------------\n",
      "New best mean reward!\n",
      "----------------------------\n",
      "| time/              |     |\n",
      "|    fps             | 225 |\n",
      "|    iterations      | 2   |\n",
      "|    time_elapsed    | 2   |\n",
      "|    total_timesteps | 512 |\n",
      "----------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 210        |\n",
      "|    iterations           | 3          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 768        |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08886457 |\n",
      "|    clip_fraction        | 0.21       |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.612     |\n",
      "|    explained_variance   | -0.409     |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.125     |\n",
      "|    n_updates            | 38         |\n",
      "|    policy_gradient_loss | -0.0996    |\n",
      "|    value_loss           | 0.198      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=1000, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 230         |\n",
      "|    mean_reward          | 0           |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 1000        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.100162074 |\n",
      "|    clip_fraction        | 0.202       |\n",
      "|    clip_range           | 0.4         |\n",
      "|    entropy_loss         | -0.544      |\n",
      "|    explained_variance   | 0.111       |\n",
      "|    learning_rate        | 0.000389    |\n",
      "|    loss                 | -0.111      |\n",
      "|    n_updates            | 57          |\n",
      "|    policy_gradient_loss | -0.102      |\n",
      "|    value_loss           | 0.234       |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 35.4     |\n",
      "| time/              |          |\n",
      "|    fps             | 194      |\n",
      "|    iterations      | 4        |\n",
      "|    time_elapsed    | 5        |\n",
      "|    total_timesteps | 1024     |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 35.4       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 189        |\n",
      "|    iterations           | 5          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 1280       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08794424 |\n",
      "|    clip_fraction        | 0.185      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.479     |\n",
      "|    explained_variance   | -0.00916   |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.113     |\n",
      "|    n_updates            | 76         |\n",
      "|    policy_gradient_loss | -0.064     |\n",
      "|    value_loss           | 1.35       |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=1500, episode_reward=61.54 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 230        |\n",
      "|    mean_reward          | 61.5       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 1500       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.09929703 |\n",
      "|    clip_fraction        | 0.235      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.429     |\n",
      "|    explained_variance   | 0.00809    |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.133     |\n",
      "|    n_updates            | 95         |\n",
      "|    policy_gradient_loss | -0.112     |\n",
      "|    value_loss           | 0.0611     |\n",
      "----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 35.4     |\n",
      "| time/              |          |\n",
      "|    fps             | 189      |\n",
      "|    iterations      | 6        |\n",
      "|    time_elapsed    | 8        |\n",
      "|    total_timesteps | 1536     |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 29.6       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 193        |\n",
      "|    iterations           | 7          |\n",
      "|    time_elapsed         | 9          |\n",
      "|    total_timesteps      | 1792       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.18120489 |\n",
      "|    clip_fraction        | 0.293      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.481     |\n",
      "|    explained_variance   | 0.105      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | 0.0248     |\n",
      "|    n_updates            | 114        |\n",
      "|    policy_gradient_loss | -0.118     |\n",
      "|    value_loss           | 0.245      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=2000, episode_reward=54.92 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 230        |\n",
      "|    mean_reward          | 54.9       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 2000       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11823708 |\n",
      "|    clip_fraction        | 0.185      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.38      |\n",
      "|    explained_variance   | 0.231      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | 2.82       |\n",
      "|    n_updates            | 133        |\n",
      "|    policy_gradient_loss | -0.0922    |\n",
      "|    value_loss           | 1.57       |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 29.6     |\n",
      "| time/              |          |\n",
      "|    fps             | 184      |\n",
      "|    iterations      | 8        |\n",
      "|    time_elapsed    | 11       |\n",
      "|    total_timesteps | 2048     |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 29.6       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 188        |\n",
      "|    iterations           | 9          |\n",
      "|    time_elapsed         | 12         |\n",
      "|    total_timesteps      | 2304       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12345174 |\n",
      "|    clip_fraction        | 0.26       |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.403     |\n",
      "|    explained_variance   | 0.169      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.134     |\n",
      "|    n_updates            | 152        |\n",
      "|    policy_gradient_loss | -0.098     |\n",
      "|    value_loss           | 0.0388     |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=2500, episode_reward=41.64 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 230        |\n",
      "|    mean_reward          | 41.6       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 2500       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.19456682 |\n",
      "|    clip_fraction        | 0.278      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.415     |\n",
      "|    explained_variance   | 0.442      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0668    |\n",
      "|    n_updates            | 171        |\n",
      "|    policy_gradient_loss | -0.11      |\n",
      "|    value_loss           | 0.189      |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 29.6     |\n",
      "| time/              |          |\n",
      "|    fps             | 188      |\n",
      "|    iterations      | 10       |\n",
      "|    time_elapsed    | 13       |\n",
      "|    total_timesteps | 2560     |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 29.7       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 187        |\n",
      "|    iterations           | 11         |\n",
      "|    time_elapsed         | 15         |\n",
      "|    total_timesteps      | 2816       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.13902095 |\n",
      "|    clip_fraction        | 0.264      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.395     |\n",
      "|    explained_variance   | 0.375      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0936    |\n",
      "|    n_updates            | 190        |\n",
      "|    policy_gradient_loss | -0.105     |\n",
      "|    value_loss           | 0.187      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=3000, episode_reward=27.37 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 230        |\n",
      "|    mean_reward          | 27.4       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 3000       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.23686156 |\n",
      "|    clip_fraction        | 0.281      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.383     |\n",
      "|    explained_variance   | 0.0973     |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0517    |\n",
      "|    n_updates            | 209        |\n",
      "|    policy_gradient_loss | -0.0876    |\n",
      "|    value_loss           | 1.47       |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 29.7     |\n",
      "| time/              |          |\n",
      "|    fps             | 184      |\n",
      "|    iterations      | 12       |\n",
      "|    time_elapsed    | 16       |\n",
      "|    total_timesteps | 3072     |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 29.7      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 183       |\n",
      "|    iterations           | 13        |\n",
      "|    time_elapsed         | 18        |\n",
      "|    total_timesteps      | 3328      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.3478219 |\n",
      "|    clip_fraction        | 0.341     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.359    |\n",
      "|    explained_variance   | 0.541     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.213    |\n",
      "|    n_updates            | 228       |\n",
      "|    policy_gradient_loss | -0.139    |\n",
      "|    value_loss           | 0.0778    |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=3500, episode_reward=24.04 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 230        |\n",
      "|    mean_reward          | 24         |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 3500       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.26831618 |\n",
      "|    clip_fraction        | 0.301      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.399     |\n",
      "|    explained_variance   | 0.495      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | 0.26       |\n",
      "|    n_updates            | 247        |\n",
      "|    policy_gradient_loss | -0.13      |\n",
      "|    value_loss           | 0.286      |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 31.8     |\n",
      "| time/              |          |\n",
      "|    fps             | 182      |\n",
      "|    iterations      | 14       |\n",
      "|    time_elapsed    | 19       |\n",
      "|    total_timesteps | 3584     |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 31.8       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 179        |\n",
      "|    iterations           | 15         |\n",
      "|    time_elapsed         | 21         |\n",
      "|    total_timesteps      | 3840       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.19358644 |\n",
      "|    clip_fraction        | 0.275      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.339     |\n",
      "|    explained_variance   | 0.776      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.196     |\n",
      "|    n_updates            | 266        |\n",
      "|    policy_gradient_loss | -0.103     |\n",
      "|    value_loss           | 0.325      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=4000, episode_reward=24.57 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 230        |\n",
      "|    mean_reward          | 24.6       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 4000       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.22408706 |\n",
      "|    clip_fraction        | 0.324      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.359     |\n",
      "|    explained_variance   | -0.155     |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0912    |\n",
      "|    n_updates            | 285        |\n",
      "|    policy_gradient_loss | -0.101     |\n",
      "|    value_loss           | 0.0428     |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 31.8     |\n",
      "| time/              |          |\n",
      "|    fps             | 179      |\n",
      "|    iterations      | 16       |\n",
      "|    time_elapsed    | 22       |\n",
      "|    total_timesteps | 4096     |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 31.8       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 181        |\n",
      "|    iterations           | 17         |\n",
      "|    time_elapsed         | 23         |\n",
      "|    total_timesteps      | 4352       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.30274314 |\n",
      "|    clip_fraction        | 0.265      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.307     |\n",
      "|    explained_variance   | 0.7        |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0539    |\n",
      "|    n_updates            | 304        |\n",
      "|    policy_gradient_loss | -0.105     |\n",
      "|    value_loss           | 0.146      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=4500, episode_reward=20.63 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 230        |\n",
      "|    mean_reward          | 20.6       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 4500       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.32615626 |\n",
      "|    clip_fraction        | 0.293      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.329     |\n",
      "|    explained_variance   | 0.44       |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.162     |\n",
      "|    n_updates            | 323        |\n",
      "|    policy_gradient_loss | -0.12      |\n",
      "|    value_loss           | 0.159      |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 36.8     |\n",
      "| time/              |          |\n",
      "|    fps             | 180      |\n",
      "|    iterations      | 18       |\n",
      "|    time_elapsed    | 25       |\n",
      "|    total_timesteps | 4608     |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 36.8       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 180        |\n",
      "|    iterations           | 19         |\n",
      "|    time_elapsed         | 26         |\n",
      "|    total_timesteps      | 4864       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.18586329 |\n",
      "|    clip_fraction        | 0.325      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.393     |\n",
      "|    explained_variance   | 0.157      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.138     |\n",
      "|    n_updates            | 342        |\n",
      "|    policy_gradient_loss | -0.0933    |\n",
      "|    value_loss           | 0.54       |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=5000, episode_reward=29.56 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 29.6      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 5000      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.2676272 |\n",
      "|    clip_fraction        | 0.304     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.302    |\n",
      "|    explained_variance   | 0.744     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.147    |\n",
      "|    n_updates            | 361       |\n",
      "|    policy_gradient_loss | -0.111    |\n",
      "|    value_loss           | 0.104     |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 36.8     |\n",
      "| time/              |          |\n",
      "|    fps             | 180      |\n",
      "|    iterations      | 20       |\n",
      "|    time_elapsed    | 28       |\n",
      "|    total_timesteps | 5120     |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 39         |\n",
      "| time/                   |            |\n",
      "|    fps                  | 182        |\n",
      "|    iterations           | 21         |\n",
      "|    time_elapsed         | 29         |\n",
      "|    total_timesteps      | 5376       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.23156479 |\n",
      "|    clip_fraction        | 0.273      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.31      |\n",
      "|    explained_variance   | -0.0448    |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.155     |\n",
      "|    n_updates            | 380        |\n",
      "|    policy_gradient_loss | -0.0956    |\n",
      "|    value_loss           | 0.25       |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=5500, episode_reward=43.93 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 230        |\n",
      "|    mean_reward          | 43.9       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 5500       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.43647623 |\n",
      "|    clip_fraction        | 0.309      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.294     |\n",
      "|    explained_variance   | 0.708      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0152    |\n",
      "|    n_updates            | 399        |\n",
      "|    policy_gradient_loss | -0.117     |\n",
      "|    value_loss           | 0.345      |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 39       |\n",
      "| time/              |          |\n",
      "|    fps             | 181      |\n",
      "|    iterations      | 22       |\n",
      "|    time_elapsed    | 31       |\n",
      "|    total_timesteps | 5632     |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 39         |\n",
      "| time/                   |            |\n",
      "|    fps                  | 182        |\n",
      "|    iterations           | 23         |\n",
      "|    time_elapsed         | 32         |\n",
      "|    total_timesteps      | 5888       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.35837832 |\n",
      "|    clip_fraction        | 0.285      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.31      |\n",
      "|    explained_variance   | 0.082      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.137     |\n",
      "|    n_updates            | 418        |\n",
      "|    policy_gradient_loss | -0.118     |\n",
      "|    value_loss           | 0.0466     |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=6000, episode_reward=84.93 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 230        |\n",
      "|    mean_reward          | 84.9       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 6000       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.28524965 |\n",
      "|    clip_fraction        | 0.281      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.267     |\n",
      "|    explained_variance   | 0.401      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | 0.111      |\n",
      "|    n_updates            | 437        |\n",
      "|    policy_gradient_loss | -0.0973    |\n",
      "|    value_loss           | 0.211      |\n",
      "----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 39       |\n",
      "| time/              |          |\n",
      "|    fps             | 182      |\n",
      "|    iterations      | 24       |\n",
      "|    time_elapsed    | 33       |\n",
      "|    total_timesteps | 6144     |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 40.2       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 182        |\n",
      "|    iterations           | 25         |\n",
      "|    time_elapsed         | 35         |\n",
      "|    total_timesteps      | 6400       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.29677755 |\n",
      "|    clip_fraction        | 0.225      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.248     |\n",
      "|    explained_variance   | 0.227      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.175     |\n",
      "|    n_updates            | 456        |\n",
      "|    policy_gradient_loss | -0.104     |\n",
      "|    value_loss           | 0.173      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=6500, episode_reward=34.91 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 230        |\n",
      "|    mean_reward          | 34.9       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 6500       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.24001434 |\n",
      "|    clip_fraction        | 0.243      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.285     |\n",
      "|    explained_variance   | -0.448     |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.121     |\n",
      "|    n_updates            | 475        |\n",
      "|    policy_gradient_loss | -0.0748    |\n",
      "|    value_loss           | 1.17       |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 40.2     |\n",
      "| time/              |          |\n",
      "|    fps             | 180      |\n",
      "|    iterations      | 26       |\n",
      "|    time_elapsed    | 36       |\n",
      "|    total_timesteps | 6656     |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 40.2       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 179        |\n",
      "|    iterations           | 27         |\n",
      "|    time_elapsed         | 38         |\n",
      "|    total_timesteps      | 6912       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.49597257 |\n",
      "|    clip_fraction        | 0.339      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.261     |\n",
      "|    explained_variance   | 0.708      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.205     |\n",
      "|    n_updates            | 494        |\n",
      "|    policy_gradient_loss | -0.123     |\n",
      "|    value_loss           | 0.0716     |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=7000, episode_reward=64.80 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 64.8      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 7000      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.3196108 |\n",
      "|    clip_fraction        | 0.263     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.234    |\n",
      "|    explained_variance   | 0.0368    |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | 0.217     |\n",
      "|    n_updates            | 513       |\n",
      "|    policy_gradient_loss | -0.0839   |\n",
      "|    value_loss           | 0.342     |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 40.8     |\n",
      "| time/              |          |\n",
      "|    fps             | 178      |\n",
      "|    iterations      | 28       |\n",
      "|    time_elapsed    | 40       |\n",
      "|    total_timesteps | 7168     |\n",
      "---------------------------------\n",
      "--------------------------------------\n",
      "| rollout/                |          |\n",
      "|    ep_len_mean          | 889      |\n",
      "|    ep_rew_mean          | 40.8     |\n",
      "| time/                   |          |\n",
      "|    fps                  | 178      |\n",
      "|    iterations           | 29       |\n",
      "|    time_elapsed         | 41       |\n",
      "|    total_timesteps      | 7424     |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 0.606748 |\n",
      "|    clip_fraction        | 0.253    |\n",
      "|    clip_range           | 0.4      |\n",
      "|    entropy_loss         | -0.21    |\n",
      "|    explained_variance   | 0.225    |\n",
      "|    learning_rate        | 0.000389 |\n",
      "|    loss                 | 2.82     |\n",
      "|    n_updates            | 532      |\n",
      "|    policy_gradient_loss | -0.0892  |\n",
      "|    value_loss           | 1.16     |\n",
      "--------------------------------------\n",
      "Eval num_timesteps=7500, episode_reward=2.53 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 2.53      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 7500      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.4496333 |\n",
      "|    clip_fraction        | 0.366     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.316    |\n",
      "|    explained_variance   | 0.139     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.198    |\n",
      "|    n_updates            | 551       |\n",
      "|    policy_gradient_loss | -0.141    |\n",
      "|    value_loss           | 0.0664    |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 40.8     |\n",
      "| time/              |          |\n",
      "|    fps             | 178      |\n",
      "|    iterations      | 30       |\n",
      "|    time_elapsed    | 42       |\n",
      "|    total_timesteps | 7680     |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 40.8      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 180       |\n",
      "|    iterations           | 31        |\n",
      "|    time_elapsed         | 44        |\n",
      "|    total_timesteps      | 7936      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.9005282 |\n",
      "|    clip_fraction        | 0.241     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.177    |\n",
      "|    explained_variance   | 0.358     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.131    |\n",
      "|    n_updates            | 570       |\n",
      "|    policy_gradient_loss | -0.085    |\n",
      "|    value_loss           | 0.16      |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=8000, episode_reward=8.72 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 8.72      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 8000      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.5490035 |\n",
      "|    clip_fraction        | 0.284     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.206    |\n",
      "|    explained_variance   | 0.0368    |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.149    |\n",
      "|    n_updates            | 589       |\n",
      "|    policy_gradient_loss | -0.109    |\n",
      "|    value_loss           | 0.119     |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 40.1     |\n",
      "| time/              |          |\n",
      "|    fps             | 179      |\n",
      "|    iterations      | 32       |\n",
      "|    time_elapsed    | 45       |\n",
      "|    total_timesteps | 8192     |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 40.1       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 179        |\n",
      "|    iterations           | 33         |\n",
      "|    time_elapsed         | 47         |\n",
      "|    total_timesteps      | 8448       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.36234587 |\n",
      "|    clip_fraction        | 0.3        |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.272     |\n",
      "|    explained_variance   | 0.169      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.16      |\n",
      "|    n_updates            | 608        |\n",
      "|    policy_gradient_loss | -0.0498    |\n",
      "|    value_loss           | 0.074      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=8500, episode_reward=6.39 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 230        |\n",
      "|    mean_reward          | 6.39       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 8500       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.33972836 |\n",
      "|    clip_fraction        | 0.25       |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.168     |\n",
      "|    explained_variance   | 0.202      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.118     |\n",
      "|    n_updates            | 627        |\n",
      "|    policy_gradient_loss | -0.0897    |\n",
      "|    value_loss           | 0.144      |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 40.1     |\n",
      "| time/              |          |\n",
      "|    fps             | 179      |\n",
      "|    iterations      | 34       |\n",
      "|    time_elapsed    | 48       |\n",
      "|    total_timesteps | 8704     |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 39.1      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 180       |\n",
      "|    iterations           | 35        |\n",
      "|    time_elapsed         | 49        |\n",
      "|    total_timesteps      | 8960      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.4497175 |\n",
      "|    clip_fraction        | 0.269     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.206    |\n",
      "|    explained_variance   | 0.495     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0865   |\n",
      "|    n_updates            | 646       |\n",
      "|    policy_gradient_loss | -0.0917   |\n",
      "|    value_loss           | 0.122     |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=9000, episode_reward=1.34 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 230        |\n",
      "|    mean_reward          | 1.34       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 9000       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.49547306 |\n",
      "|    clip_fraction        | 0.286      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.257     |\n",
      "|    explained_variance   | 0.428      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.123     |\n",
      "|    n_updates            | 665        |\n",
      "|    policy_gradient_loss | -0.0907    |\n",
      "|    value_loss           | 0.213      |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 39.1     |\n",
      "| time/              |          |\n",
      "|    fps             | 179      |\n",
      "|    iterations      | 36       |\n",
      "|    time_elapsed    | 51       |\n",
      "|    total_timesteps | 9216     |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 39.1      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 179       |\n",
      "|    iterations           | 37        |\n",
      "|    time_elapsed         | 52        |\n",
      "|    total_timesteps      | 9472      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.9709941 |\n",
      "|    clip_fraction        | 0.312     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.256    |\n",
      "|    explained_variance   | 0.247     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.209    |\n",
      "|    n_updates            | 684       |\n",
      "|    policy_gradient_loss | -0.117    |\n",
      "|    value_loss           | 0.0609    |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=9500, episode_reward=21.06 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 21.1      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 9500      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.8420832 |\n",
      "|    clip_fraction        | 0.288     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.199    |\n",
      "|    explained_variance   | 0.502     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.15     |\n",
      "|    n_updates            | 703       |\n",
      "|    policy_gradient_loss | -0.0963   |\n",
      "|    value_loss           | 0.24      |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 39.1     |\n",
      "| time/              |          |\n",
      "|    fps             | 179      |\n",
      "|    iterations      | 38       |\n",
      "|    time_elapsed    | 54       |\n",
      "|    total_timesteps | 9728     |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 40.6       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 180        |\n",
      "|    iterations           | 39         |\n",
      "|    time_elapsed         | 55         |\n",
      "|    total_timesteps      | 9984       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.44631445 |\n",
      "|    clip_fraction        | 0.297      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.258     |\n",
      "|    explained_variance   | 0.177      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.229     |\n",
      "|    n_updates            | 722        |\n",
      "|    policy_gradient_loss | -0.112     |\n",
      "|    value_loss           | 0.533      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=10000, episode_reward=-2.19 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | -2.19     |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 10000     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.6345126 |\n",
      "|    clip_fraction        | 0.308     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.234    |\n",
      "|    explained_variance   | -0.0572   |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.11     |\n",
      "|    n_updates            | 741       |\n",
      "|    policy_gradient_loss | -0.0953   |\n",
      "|    value_loss           | 0.294     |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 40.6     |\n",
      "| time/              |          |\n",
      "|    fps             | 180      |\n",
      "|    iterations      | 40       |\n",
      "|    time_elapsed    | 56       |\n",
      "|    total_timesteps | 10240    |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 40.6      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 179       |\n",
      "|    iterations           | 41        |\n",
      "|    time_elapsed         | 58        |\n",
      "|    total_timesteps      | 10496     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.7641328 |\n",
      "|    clip_fraction        | 0.284     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.189    |\n",
      "|    explained_variance   | 0.677     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.159    |\n",
      "|    n_updates            | 760       |\n",
      "|    policy_gradient_loss | -0.117    |\n",
      "|    value_loss           | 0.11      |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=10500, episode_reward=18.85 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 230        |\n",
      "|    mean_reward          | 18.9       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 10500      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.57921505 |\n",
      "|    clip_fraction        | 0.286      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.197     |\n",
      "|    explained_variance   | 0.588      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.123     |\n",
      "|    n_updates            | 779        |\n",
      "|    policy_gradient_loss | -0.0926    |\n",
      "|    value_loss           | 0.152      |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 42.3     |\n",
      "| time/              |          |\n",
      "|    fps             | 180      |\n",
      "|    iterations      | 42       |\n",
      "|    time_elapsed    | 59       |\n",
      "|    total_timesteps | 10752    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=11000, episode_reward=35.85 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 35.8      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 11000     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.7519165 |\n",
      "|    clip_fraction        | 0.31      |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.278    |\n",
      "|    explained_variance   | 0.564     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | 0.157     |\n",
      "|    n_updates            | 798       |\n",
      "|    policy_gradient_loss | -0.0914   |\n",
      "|    value_loss           | 0.284     |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 42.3     |\n",
      "| time/              |          |\n",
      "|    fps             | 178      |\n",
      "|    iterations      | 43       |\n",
      "|    time_elapsed    | 61       |\n",
      "|    total_timesteps | 11008    |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 42.3       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 178        |\n",
      "|    iterations           | 44         |\n",
      "|    time_elapsed         | 62         |\n",
      "|    total_timesteps      | 11264      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.63726413 |\n",
      "|    clip_fraction        | 0.274      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.199     |\n",
      "|    explained_variance   | 0.382      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0973    |\n",
      "|    n_updates            | 817        |\n",
      "|    policy_gradient_loss | -0.102     |\n",
      "|    value_loss           | 0.0602     |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=11500, episode_reward=71.02 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 230        |\n",
      "|    mean_reward          | 71         |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 11500      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.61168206 |\n",
      "|    clip_fraction        | 0.212      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.143     |\n",
      "|    explained_variance   | 0.661      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0638    |\n",
      "|    n_updates            | 836        |\n",
      "|    policy_gradient_loss | -0.0838    |\n",
      "|    value_loss           | 0.321      |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 42.3     |\n",
      "| time/              |          |\n",
      "|    fps             | 178      |\n",
      "|    iterations      | 45       |\n",
      "|    time_elapsed    | 64       |\n",
      "|    total_timesteps | 11520    |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 44.3      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 179       |\n",
      "|    iterations           | 46        |\n",
      "|    time_elapsed         | 65        |\n",
      "|    total_timesteps      | 11776     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.5155387 |\n",
      "|    clip_fraction        | 0.213     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.155    |\n",
      "|    explained_variance   | 0.733     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | 0.0746    |\n",
      "|    n_updates            | 855       |\n",
      "|    policy_gradient_loss | -0.0756   |\n",
      "|    value_loss           | 0.182     |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=12000, episode_reward=38.93 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 230        |\n",
      "|    mean_reward          | 38.9       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 12000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.66126704 |\n",
      "|    clip_fraction        | 0.255      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.214     |\n",
      "|    explained_variance   | -0.842     |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0549    |\n",
      "|    n_updates            | 874        |\n",
      "|    policy_gradient_loss | -0.0729    |\n",
      "|    value_loss           | 0.109      |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 44.3     |\n",
      "| time/              |          |\n",
      "|    fps             | 179      |\n",
      "|    iterations      | 47       |\n",
      "|    time_elapsed    | 67       |\n",
      "|    total_timesteps | 12032    |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 44.3      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 180       |\n",
      "|    iterations           | 48        |\n",
      "|    time_elapsed         | 68        |\n",
      "|    total_timesteps      | 12288     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.5215261 |\n",
      "|    clip_fraction        | 0.225     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.154    |\n",
      "|    explained_variance   | 0.676     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0834   |\n",
      "|    n_updates            | 893       |\n",
      "|    policy_gradient_loss | -0.0872   |\n",
      "|    value_loss           | 0.165     |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=12500, episode_reward=42.13 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 230        |\n",
      "|    mean_reward          | 42.1       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 12500      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.82489955 |\n",
      "|    clip_fraction        | 0.263      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.138     |\n",
      "|    explained_variance   | 0.577      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.168     |\n",
      "|    n_updates            | 912        |\n",
      "|    policy_gradient_loss | -0.0965    |\n",
      "|    value_loss           | 0.135      |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 44.8     |\n",
      "| time/              |          |\n",
      "|    fps             | 180      |\n",
      "|    iterations      | 49       |\n",
      "|    time_elapsed    | 69       |\n",
      "|    total_timesteps | 12544    |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 44.8       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 180        |\n",
      "|    iterations           | 50         |\n",
      "|    time_elapsed         | 71         |\n",
      "|    total_timesteps      | 12800      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.35911584 |\n",
      "|    clip_fraction        | 0.326      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.265     |\n",
      "|    explained_variance   | 0.674      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0608    |\n",
      "|    n_updates            | 931        |\n",
      "|    policy_gradient_loss | -0.0814    |\n",
      "|    value_loss           | 0.224      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=13000, episode_reward=34.12 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 34.1      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 13000     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.7333718 |\n",
      "|    clip_fraction        | 0.27      |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.17     |\n",
      "|    explained_variance   | 0.437     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.213    |\n",
      "|    n_updates            | 950       |\n",
      "|    policy_gradient_loss | -0.109    |\n",
      "|    value_loss           | 0.0723    |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 44.8     |\n",
      "| time/              |          |\n",
      "|    fps             | 180      |\n",
      "|    iterations      | 51       |\n",
      "|    time_elapsed    | 72       |\n",
      "|    total_timesteps | 13056    |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 44.8       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 180        |\n",
      "|    iterations           | 52         |\n",
      "|    time_elapsed         | 73         |\n",
      "|    total_timesteps      | 13312      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.50998294 |\n",
      "|    clip_fraction        | 0.234      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.167     |\n",
      "|    explained_variance   | 0.76       |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0591    |\n",
      "|    n_updates            | 969        |\n",
      "|    policy_gradient_loss | -0.0787    |\n",
      "|    value_loss           | 0.196      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=13500, episode_reward=66.51 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 66.5      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 13500     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.5551779 |\n",
      "|    clip_fraction        | 0.22      |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.167    |\n",
      "|    explained_variance   | 0.725     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.103    |\n",
      "|    n_updates            | 988       |\n",
      "|    policy_gradient_loss | -0.085    |\n",
      "|    value_loss           | 0.236     |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 46.4     |\n",
      "| time/              |          |\n",
      "|    fps             | 180      |\n",
      "|    iterations      | 53       |\n",
      "|    time_elapsed    | 75       |\n",
      "|    total_timesteps | 13568    |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 46.4       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 180        |\n",
      "|    iterations           | 54         |\n",
      "|    time_elapsed         | 76         |\n",
      "|    total_timesteps      | 13824      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.58998555 |\n",
      "|    clip_fraction        | 0.296      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.211     |\n",
      "|    explained_variance   | 0.105      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.134     |\n",
      "|    n_updates            | 1007       |\n",
      "|    policy_gradient_loss | -0.0888    |\n",
      "|    value_loss           | 0.0596     |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=14000, episode_reward=11.68 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 11.7      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 14000     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.6642065 |\n",
      "|    clip_fraction        | 0.244     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.165    |\n",
      "|    explained_variance   | 0.592     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0995   |\n",
      "|    n_updates            | 1026      |\n",
      "|    policy_gradient_loss | -0.104    |\n",
      "|    value_loss           | 0.184     |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 46.4     |\n",
      "| time/              |          |\n",
      "|    fps             | 180      |\n",
      "|    iterations      | 55       |\n",
      "|    time_elapsed    | 77       |\n",
      "|    total_timesteps | 14080    |\n",
      "---------------------------------\n",
      "--------------------------------------\n",
      "| rollout/                |          |\n",
      "|    ep_len_mean          | 889      |\n",
      "|    ep_rew_mean          | 47.5     |\n",
      "| time/                   |          |\n",
      "|    fps                  | 181      |\n",
      "|    iterations           | 56       |\n",
      "|    time_elapsed         | 79       |\n",
      "|    total_timesteps      | 14336    |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 0.561712 |\n",
      "|    clip_fraction        | 0.263    |\n",
      "|    clip_range           | 0.4      |\n",
      "|    entropy_loss         | -0.162   |\n",
      "|    explained_variance   | 0.73     |\n",
      "|    learning_rate        | 0.000389 |\n",
      "|    loss                 | -0.0786  |\n",
      "|    n_updates            | 1045     |\n",
      "|    policy_gradient_loss | -0.103   |\n",
      "|    value_loss           | 0.11     |\n",
      "--------------------------------------\n",
      "Eval num_timesteps=14500, episode_reward=22.26 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 230        |\n",
      "|    mean_reward          | 22.3       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 14500      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.57483006 |\n",
      "|    clip_fraction        | 0.278      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.211     |\n",
      "|    explained_variance   | 0.817      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0723    |\n",
      "|    n_updates            | 1064       |\n",
      "|    policy_gradient_loss | -0.0817    |\n",
      "|    value_loss           | 0.199      |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 47.5     |\n",
      "| time/              |          |\n",
      "|    fps             | 180      |\n",
      "|    iterations      | 57       |\n",
      "|    time_elapsed    | 80       |\n",
      "|    total_timesteps | 14592    |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 47.5      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 181       |\n",
      "|    iterations           | 58        |\n",
      "|    time_elapsed         | 81        |\n",
      "|    total_timesteps      | 14848     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.8920388 |\n",
      "|    clip_fraction        | 0.281     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.149    |\n",
      "|    explained_variance   | 0.554     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0582   |\n",
      "|    n_updates            | 1083      |\n",
      "|    policy_gradient_loss | -0.107    |\n",
      "|    value_loss           | 0.0499    |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=15000, episode_reward=16.22 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 230        |\n",
      "|    mean_reward          | 16.2       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 15000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.71472144 |\n",
      "|    clip_fraction        | 0.279      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.158     |\n",
      "|    explained_variance   | 0.735      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | 0.0194     |\n",
      "|    n_updates            | 1102       |\n",
      "|    policy_gradient_loss | -0.0906    |\n",
      "|    value_loss           | 0.412      |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 47.5     |\n",
      "| time/              |          |\n",
      "|    fps             | 180      |\n",
      "|    iterations      | 59       |\n",
      "|    time_elapsed    | 83       |\n",
      "|    total_timesteps | 15104    |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 48.9      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 180       |\n",
      "|    iterations           | 60        |\n",
      "|    time_elapsed         | 84        |\n",
      "|    total_timesteps      | 15360     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.8667108 |\n",
      "|    clip_fraction        | 0.36      |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.209    |\n",
      "|    explained_variance   | 0.795     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0733   |\n",
      "|    n_updates            | 1121      |\n",
      "|    policy_gradient_loss | -0.118    |\n",
      "|    value_loss           | 0.198     |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=15500, episode_reward=36.90 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 230        |\n",
      "|    mean_reward          | 36.9       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 15500      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.57341325 |\n",
      "|    clip_fraction        | 0.276      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.215     |\n",
      "|    explained_variance   | 0.0013     |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.078     |\n",
      "|    n_updates            | 1140       |\n",
      "|    policy_gradient_loss | -0.0803    |\n",
      "|    value_loss           | 0.0835     |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 48.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 180      |\n",
      "|    iterations      | 61       |\n",
      "|    time_elapsed    | 86       |\n",
      "|    total_timesteps | 15616    |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 48.9      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 180       |\n",
      "|    iterations           | 62        |\n",
      "|    time_elapsed         | 87        |\n",
      "|    total_timesteps      | 15872     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.1507955 |\n",
      "|    clip_fraction        | 0.263     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.13     |\n",
      "|    explained_variance   | 0.752     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0553   |\n",
      "|    n_updates            | 1159      |\n",
      "|    policy_gradient_loss | -0.109    |\n",
      "|    value_loss           | 0.163     |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=16000, episode_reward=-18.74 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 230        |\n",
      "|    mean_reward          | -18.7      |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 16000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.93412495 |\n",
      "|    clip_fraction        | 0.197      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.128     |\n",
      "|    explained_variance   | 0.783      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.097     |\n",
      "|    n_updates            | 1178       |\n",
      "|    policy_gradient_loss | -0.0666    |\n",
      "|    value_loss           | 0.175      |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 49.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 180      |\n",
      "|    iterations      | 63       |\n",
      "|    time_elapsed    | 89       |\n",
      "|    total_timesteps | 16128    |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 49.9       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 180        |\n",
      "|    iterations           | 64         |\n",
      "|    time_elapsed         | 90         |\n",
      "|    total_timesteps      | 16384      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.35155308 |\n",
      "|    clip_fraction        | 0.34       |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.29      |\n",
      "|    explained_variance   | 0.81       |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0439    |\n",
      "|    n_updates            | 1197       |\n",
      "|    policy_gradient_loss | -0.0862    |\n",
      "|    value_loss           | 0.227      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=16500, episode_reward=21.56 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 21.6      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 16500     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.8051535 |\n",
      "|    clip_fraction        | 0.259     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.155    |\n",
      "|    explained_variance   | 0.396     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0576   |\n",
      "|    n_updates            | 1216      |\n",
      "|    policy_gradient_loss | -0.0767   |\n",
      "|    value_loss           | 0.0754    |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 49.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 180      |\n",
      "|    iterations      | 65       |\n",
      "|    time_elapsed    | 92       |\n",
      "|    total_timesteps | 16640    |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 50.3      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 180       |\n",
      "|    iterations           | 66        |\n",
      "|    time_elapsed         | 93        |\n",
      "|    total_timesteps      | 16896     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.8682336 |\n",
      "|    clip_fraction        | 0.219     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.139    |\n",
      "|    explained_variance   | 0.764     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.119    |\n",
      "|    n_updates            | 1235      |\n",
      "|    policy_gradient_loss | -0.0916   |\n",
      "|    value_loss           | 0.286     |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=17000, episode_reward=-39.29 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | -39.3     |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 17000     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.8824328 |\n",
      "|    clip_fraction        | 0.258     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.219    |\n",
      "|    explained_variance   | 0.781     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0576   |\n",
      "|    n_updates            | 1254      |\n",
      "|    policy_gradient_loss | -0.0985   |\n",
      "|    value_loss           | 0.368     |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 50.3     |\n",
      "| time/              |          |\n",
      "|    fps             | 180      |\n",
      "|    iterations      | 67       |\n",
      "|    time_elapsed    | 95       |\n",
      "|    total_timesteps | 17152    |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 50.3      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 180       |\n",
      "|    iterations           | 68        |\n",
      "|    time_elapsed         | 96        |\n",
      "|    total_timesteps      | 17408     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.5048268 |\n",
      "|    clip_fraction        | 0.28      |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.219    |\n",
      "|    explained_variance   | 0.0323    |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.081    |\n",
      "|    n_updates            | 1273      |\n",
      "|    policy_gradient_loss | -0.0917   |\n",
      "|    value_loss           | 0.0295    |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=17500, episode_reward=10.65 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 230        |\n",
      "|    mean_reward          | 10.7       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 17500      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.49088955 |\n",
      "|    clip_fraction        | 0.159      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0979    |\n",
      "|    explained_variance   | 0.881      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | 0.0259     |\n",
      "|    n_updates            | 1292       |\n",
      "|    policy_gradient_loss | -0.0564    |\n",
      "|    value_loss           | 0.114      |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 50.3     |\n",
      "| time/              |          |\n",
      "|    fps             | 180      |\n",
      "|    iterations      | 69       |\n",
      "|    time_elapsed    | 98       |\n",
      "|    total_timesteps | 17664    |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 50.2      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 180       |\n",
      "|    iterations           | 70        |\n",
      "|    time_elapsed         | 99        |\n",
      "|    total_timesteps      | 17920     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.6867862 |\n",
      "|    clip_fraction        | 0.285     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.157    |\n",
      "|    explained_variance   | 0.291     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0535   |\n",
      "|    n_updates            | 1311      |\n",
      "|    policy_gradient_loss | -0.0853   |\n",
      "|    value_loss           | 0.219     |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=18000, episode_reward=-10.45 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | -10.5     |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 18000     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.4382222 |\n",
      "|    clip_fraction        | 0.226     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.195    |\n",
      "|    explained_variance   | 0.0197    |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0746   |\n",
      "|    n_updates            | 1330      |\n",
      "|    policy_gradient_loss | -0.029    |\n",
      "|    value_loss           | 0.206     |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 50.2     |\n",
      "| time/              |          |\n",
      "|    fps             | 180      |\n",
      "|    iterations      | 71       |\n",
      "|    time_elapsed    | 100      |\n",
      "|    total_timesteps | 18176    |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 50.2      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 181       |\n",
      "|    iterations           | 72        |\n",
      "|    time_elapsed         | 101       |\n",
      "|    total_timesteps      | 18432     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.2415454 |\n",
      "|    clip_fraction        | 0.257     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.124    |\n",
      "|    explained_variance   | 0.614     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.19     |\n",
      "|    n_updates            | 1349      |\n",
      "|    policy_gradient_loss | -0.0918   |\n",
      "|    value_loss           | 0.0546    |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=18500, episode_reward=0.46 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 230        |\n",
      "|    mean_reward          | 0.458      |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 18500      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.71735084 |\n",
      "|    clip_fraction        | 0.28       |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.191     |\n",
      "|    explained_variance   | 0.377      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.177     |\n",
      "|    n_updates            | 1368       |\n",
      "|    policy_gradient_loss | -0.0876    |\n",
      "|    value_loss           | 0.476      |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 50.7     |\n",
      "| time/              |          |\n",
      "|    fps             | 180      |\n",
      "|    iterations      | 73       |\n",
      "|    time_elapsed    | 103      |\n",
      "|    total_timesteps | 18688    |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 50.7      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 181       |\n",
      "|    iterations           | 74        |\n",
      "|    time_elapsed         | 104       |\n",
      "|    total_timesteps      | 18944     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.5695279 |\n",
      "|    clip_fraction        | 0.284     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.209    |\n",
      "|    explained_variance   | 0.425     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0872   |\n",
      "|    n_updates            | 1387      |\n",
      "|    policy_gradient_loss | -0.0772   |\n",
      "|    value_loss           | 0.271     |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=19000, episode_reward=16.11 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 16.1      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 19000     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.6741216 |\n",
      "|    clip_fraction        | 0.281     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.199    |\n",
      "|    explained_variance   | -0.541    |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.135    |\n",
      "|    n_updates            | 1406      |\n",
      "|    policy_gradient_loss | -0.0839   |\n",
      "|    value_loss           | 0.0668    |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 50.7     |\n",
      "| time/              |          |\n",
      "|    fps             | 181      |\n",
      "|    iterations      | 75       |\n",
      "|    time_elapsed    | 105      |\n",
      "|    total_timesteps | 19200    |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 50.7      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 181       |\n",
      "|    iterations           | 76        |\n",
      "|    time_elapsed         | 107       |\n",
      "|    total_timesteps      | 19456     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.1453001 |\n",
      "|    clip_fraction        | 0.225     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.134    |\n",
      "|    explained_variance   | 0.181     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.151    |\n",
      "|    n_updates            | 1425      |\n",
      "|    policy_gradient_loss | -0.08     |\n",
      "|    value_loss           | 0.242     |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=19500, episode_reward=6.16 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 230        |\n",
      "|    mean_reward          | 6.16       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 19500      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.51900727 |\n",
      "|    clip_fraction        | 0.27       |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.149     |\n",
      "|    explained_variance   | 0.706      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0492    |\n",
      "|    n_updates            | 1444       |\n",
      "|    policy_gradient_loss | -0.0915    |\n",
      "|    value_loss           | 0.0741     |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 51.2     |\n",
      "| time/              |          |\n",
      "|    fps             | 181      |\n",
      "|    iterations      | 77       |\n",
      "|    time_elapsed    | 108      |\n",
      "|    total_timesteps | 19712    |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 51.2      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 181       |\n",
      "|    iterations           | 78        |\n",
      "|    time_elapsed         | 109       |\n",
      "|    total_timesteps      | 19968     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.0536219 |\n",
      "|    clip_fraction        | 0.403     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.243    |\n",
      "|    explained_variance   | 0.71      |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.17     |\n",
      "|    n_updates            | 1463      |\n",
      "|    policy_gradient_loss | -0.107    |\n",
      "|    value_loss           | 0.187     |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=20000, episode_reward=-7.43 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | -7.43     |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 20000     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.9361469 |\n",
      "|    clip_fraction        | 0.212     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.117    |\n",
      "|    explained_variance   | 0.69      |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.105    |\n",
      "|    n_updates            | 1482      |\n",
      "|    policy_gradient_loss | -0.0716   |\n",
      "|    value_loss           | 0.0653    |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 51.2     |\n",
      "| time/              |          |\n",
      "|    fps             | 181      |\n",
      "|    iterations      | 79       |\n",
      "|    time_elapsed    | 111      |\n",
      "|    total_timesteps | 20224    |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 51        |\n",
      "| time/                   |           |\n",
      "|    fps                  | 181       |\n",
      "|    iterations           | 80        |\n",
      "|    time_elapsed         | 112       |\n",
      "|    total_timesteps      | 20480     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.7421954 |\n",
      "|    clip_fraction        | 0.181     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.122    |\n",
      "|    explained_variance   | -0.0777   |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.105    |\n",
      "|    n_updates            | 1501      |\n",
      "|    policy_gradient_loss | -0.0568   |\n",
      "|    value_loss           | 0.244     |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=20500, episode_reward=19.01 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 230        |\n",
      "|    mean_reward          | 19         |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 20500      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.55959237 |\n",
      "|    clip_fraction        | 0.243      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.195     |\n",
      "|    explained_variance   | 0.733      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0956    |\n",
      "|    n_updates            | 1520       |\n",
      "|    policy_gradient_loss | -0.0791    |\n",
      "|    value_loss           | 0.168      |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 51       |\n",
      "| time/              |          |\n",
      "|    fps             | 181      |\n",
      "|    iterations      | 81       |\n",
      "|    time_elapsed    | 114      |\n",
      "|    total_timesteps | 20736    |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 51         |\n",
      "| time/                   |            |\n",
      "|    fps                  | 181        |\n",
      "|    iterations           | 82         |\n",
      "|    time_elapsed         | 115        |\n",
      "|    total_timesteps      | 20992      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.81747085 |\n",
      "|    clip_fraction        | 0.273      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.17      |\n",
      "|    explained_variance   | 0.0615     |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.104     |\n",
      "|    n_updates            | 1539       |\n",
      "|    policy_gradient_loss | -0.0934    |\n",
      "|    value_loss           | 0.04       |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=21000, episode_reward=59.69 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 230        |\n",
      "|    mean_reward          | 59.7       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 21000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.71651816 |\n",
      "|    clip_fraction        | 0.156      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0807    |\n",
      "|    explained_variance   | 0.616      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0106    |\n",
      "|    n_updates            | 1558       |\n",
      "|    policy_gradient_loss | -0.056     |\n",
      "|    value_loss           | 0.146      |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 51       |\n",
      "| time/              |          |\n",
      "|    fps             | 181      |\n",
      "|    iterations      | 83       |\n",
      "|    time_elapsed    | 117      |\n",
      "|    total_timesteps | 21248    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=21500, episode_reward=28.99 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 29        |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 21500     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.0438342 |\n",
      "|    clip_fraction        | 0.217     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.138    |\n",
      "|    explained_variance   | 0.739     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0491   |\n",
      "|    n_updates            | 1577      |\n",
      "|    policy_gradient_loss | -0.0698   |\n",
      "|    value_loss           | 0.159     |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 51.2     |\n",
      "| time/              |          |\n",
      "|    fps             | 181      |\n",
      "|    iterations      | 84       |\n",
      "|    time_elapsed    | 118      |\n",
      "|    total_timesteps | 21504    |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 51.2      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 181       |\n",
      "|    iterations           | 85        |\n",
      "|    time_elapsed         | 119       |\n",
      "|    total_timesteps      | 21760     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.4468347 |\n",
      "|    clip_fraction        | 0.248     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.201    |\n",
      "|    explained_variance   | 0.506     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0703   |\n",
      "|    n_updates            | 1596      |\n",
      "|    policy_gradient_loss | -0.0589   |\n",
      "|    value_loss           | 0.141     |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=22000, episode_reward=2.15 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 230        |\n",
      "|    mean_reward          | 2.15       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 22000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.97184813 |\n",
      "|    clip_fraction        | 0.189      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0988    |\n",
      "|    explained_variance   | 0.384      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.136     |\n",
      "|    n_updates            | 1615       |\n",
      "|    policy_gradient_loss | -0.0768    |\n",
      "|    value_loss           | 0.127      |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 51.2     |\n",
      "| time/              |          |\n",
      "|    fps             | 181      |\n",
      "|    iterations      | 86       |\n",
      "|    time_elapsed    | 121      |\n",
      "|    total_timesteps | 22016    |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 51.1      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 182       |\n",
      "|    iterations           | 87        |\n",
      "|    time_elapsed         | 122       |\n",
      "|    total_timesteps      | 22272     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.5406705 |\n",
      "|    clip_fraction        | 0.216     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.148    |\n",
      "|    explained_variance   | 0.383     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.147    |\n",
      "|    n_updates            | 1634      |\n",
      "|    policy_gradient_loss | -0.0577   |\n",
      "|    value_loss           | 0.15      |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=22500, episode_reward=-4.88 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 230        |\n",
      "|    mean_reward          | -4.88      |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 22500      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.74921346 |\n",
      "|    clip_fraction        | 0.256      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.16      |\n",
      "|    explained_variance   | 0.755      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0739    |\n",
      "|    n_updates            | 1653       |\n",
      "|    policy_gradient_loss | -0.079     |\n",
      "|    value_loss           | 0.248      |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 51.1     |\n",
      "| time/              |          |\n",
      "|    fps             | 181      |\n",
      "|    iterations      | 88       |\n",
      "|    time_elapsed    | 124      |\n",
      "|    total_timesteps | 22528    |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 51.1      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 182       |\n",
      "|    iterations           | 89        |\n",
      "|    time_elapsed         | 125       |\n",
      "|    total_timesteps      | 22784     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.2107503 |\n",
      "|    clip_fraction        | 0.279     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.131    |\n",
      "|    explained_variance   | 0.341     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.103    |\n",
      "|    n_updates            | 1672      |\n",
      "|    policy_gradient_loss | -0.0857   |\n",
      "|    value_loss           | 0.0494    |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=23000, episode_reward=17.69 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 17.7      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 23000     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.8281474 |\n",
      "|    clip_fraction        | 0.189     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.124    |\n",
      "|    explained_variance   | 0.431     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0435   |\n",
      "|    n_updates            | 1691      |\n",
      "|    policy_gradient_loss | -0.0701   |\n",
      "|    value_loss           | 0.219     |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 51.1     |\n",
      "| time/              |          |\n",
      "|    fps             | 182      |\n",
      "|    iterations      | 90       |\n",
      "|    time_elapsed    | 126      |\n",
      "|    total_timesteps | 23040    |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 51.6      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 182       |\n",
      "|    iterations           | 91        |\n",
      "|    time_elapsed         | 127       |\n",
      "|    total_timesteps      | 23296     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.9260765 |\n",
      "|    clip_fraction        | 0.244     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.151    |\n",
      "|    explained_variance   | 0.76      |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0977   |\n",
      "|    n_updates            | 1710      |\n",
      "|    policy_gradient_loss | -0.0917   |\n",
      "|    value_loss           | 0.166     |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=23500, episode_reward=34.92 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 34.9      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 23500     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.9758914 |\n",
      "|    clip_fraction        | 0.121     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0845   |\n",
      "|    explained_variance   | 0.407     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | 0.215     |\n",
      "|    n_updates            | 1729      |\n",
      "|    policy_gradient_loss | -0.0494   |\n",
      "|    value_loss           | 0.452     |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 51.6     |\n",
      "| time/              |          |\n",
      "|    fps             | 181      |\n",
      "|    iterations      | 92       |\n",
      "|    time_elapsed    | 129      |\n",
      "|    total_timesteps | 23552    |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 51.6       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 182        |\n",
      "|    iterations           | 93         |\n",
      "|    time_elapsed         | 130        |\n",
      "|    total_timesteps      | 23808      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.82977223 |\n",
      "|    clip_fraction        | 0.194      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0786    |\n",
      "|    explained_variance   | 0.538      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.149     |\n",
      "|    n_updates            | 1748       |\n",
      "|    policy_gradient_loss | -0.0712    |\n",
      "|    value_loss           | 0.092      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=24000, episode_reward=7.99 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 230        |\n",
      "|    mean_reward          | 7.99       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 24000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.67098796 |\n",
      "|    clip_fraction        | 0.211      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.143     |\n",
      "|    explained_variance   | 0.408      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.123     |\n",
      "|    n_updates            | 1767       |\n",
      "|    policy_gradient_loss | -0.0729    |\n",
      "|    value_loss           | 0.267      |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 52       |\n",
      "| time/              |          |\n",
      "|    fps             | 181      |\n",
      "|    iterations      | 94       |\n",
      "|    time_elapsed    | 132      |\n",
      "|    total_timesteps | 24064    |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 52         |\n",
      "| time/                   |            |\n",
      "|    fps                  | 181        |\n",
      "|    iterations           | 95         |\n",
      "|    time_elapsed         | 133        |\n",
      "|    total_timesteps      | 24320      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.41407925 |\n",
      "|    clip_fraction        | 0.208      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.132     |\n",
      "|    explained_variance   | 0.787      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0602    |\n",
      "|    n_updates            | 1786       |\n",
      "|    policy_gradient_loss | -0.0726    |\n",
      "|    value_loss           | 0.426      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=24500, episode_reward=12.77 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 230        |\n",
      "|    mean_reward          | 12.8       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 24500      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.94365597 |\n",
      "|    clip_fraction        | 0.293      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.143     |\n",
      "|    explained_variance   | 0.492      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.111     |\n",
      "|    n_updates            | 1805       |\n",
      "|    policy_gradient_loss | -0.0892    |\n",
      "|    value_loss           | 0.0805     |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 52       |\n",
      "| time/              |          |\n",
      "|    fps             | 181      |\n",
      "|    iterations      | 96       |\n",
      "|    time_elapsed    | 135      |\n",
      "|    total_timesteps | 24576    |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 52         |\n",
      "| time/                   |            |\n",
      "|    fps                  | 181        |\n",
      "|    iterations           | 97         |\n",
      "|    time_elapsed         | 136        |\n",
      "|    total_timesteps      | 24832      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.90864456 |\n",
      "|    clip_fraction        | 0.229      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.133     |\n",
      "|    explained_variance   | 0.491      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.115     |\n",
      "|    n_updates            | 1824       |\n",
      "|    policy_gradient_loss | -0.0905    |\n",
      "|    value_loss           | 0.169      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=25000, episode_reward=54.15 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 54.2      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 25000     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.4708096 |\n",
      "|    clip_fraction        | 0.155     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0969   |\n",
      "|    explained_variance   | 0.89      |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.126    |\n",
      "|    n_updates            | 1843      |\n",
      "|    policy_gradient_loss | -0.0672   |\n",
      "|    value_loss           | 0.149     |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 52.5     |\n",
      "| time/              |          |\n",
      "|    fps             | 181      |\n",
      "|    iterations      | 98       |\n",
      "|    time_elapsed    | 137      |\n",
      "|    total_timesteps | 25088    |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 52.5       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 181        |\n",
      "|    iterations           | 99         |\n",
      "|    time_elapsed         | 139        |\n",
      "|    total_timesteps      | 25344      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.66822314 |\n",
      "|    clip_fraction        | 0.17       |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.113     |\n",
      "|    explained_variance   | -0.379     |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.158     |\n",
      "|    n_updates            | 1862       |\n",
      "|    policy_gradient_loss | -0.0551    |\n",
      "|    value_loss           | 0.469      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=25500, episode_reward=22.94 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 22.9      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 25500     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.7946032 |\n",
      "|    clip_fraction        | 0.209     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.109    |\n",
      "|    explained_variance   | 0.5       |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.132    |\n",
      "|    n_updates            | 1881      |\n",
      "|    policy_gradient_loss | -0.0783   |\n",
      "|    value_loss           | 0.196     |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 52.5     |\n",
      "| time/              |          |\n",
      "|    fps             | 181      |\n",
      "|    iterations      | 100      |\n",
      "|    time_elapsed    | 140      |\n",
      "|    total_timesteps | 25600    |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 52.7       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 182        |\n",
      "|    iterations           | 101        |\n",
      "|    time_elapsed         | 142        |\n",
      "|    total_timesteps      | 25856      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.74130714 |\n",
      "|    clip_fraction        | 0.22       |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.112     |\n",
      "|    explained_variance   | 0.526      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0601    |\n",
      "|    n_updates            | 1900       |\n",
      "|    policy_gradient_loss | -0.0872    |\n",
      "|    value_loss           | 0.177      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=26000, episode_reward=14.53 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 230        |\n",
      "|    mean_reward          | 14.5       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 26000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.67165637 |\n",
      "|    clip_fraction        | 0.245      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.174     |\n",
      "|    explained_variance   | 0.606      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0839    |\n",
      "|    n_updates            | 1919       |\n",
      "|    policy_gradient_loss | -0.0625    |\n",
      "|    value_loss           | 0.303      |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 52.7     |\n",
      "| time/              |          |\n",
      "|    fps             | 181      |\n",
      "|    iterations      | 102      |\n",
      "|    time_elapsed    | 143      |\n",
      "|    total_timesteps | 26112    |\n",
      "---------------------------------\n",
      "--------------------------------------\n",
      "| rollout/                |          |\n",
      "|    ep_len_mean          | 889      |\n",
      "|    ep_rew_mean          | 52.7     |\n",
      "| time/                   |          |\n",
      "|    fps                  | 181      |\n",
      "|    iterations           | 103      |\n",
      "|    time_elapsed         | 145      |\n",
      "|    total_timesteps      | 26368    |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 0.895462 |\n",
      "|    clip_fraction        | 0.251    |\n",
      "|    clip_range           | 0.4      |\n",
      "|    entropy_loss         | -0.131   |\n",
      "|    explained_variance   | 0.392    |\n",
      "|    learning_rate        | 0.000389 |\n",
      "|    loss                 | -0.106   |\n",
      "|    n_updates            | 1938     |\n",
      "|    policy_gradient_loss | -0.0821  |\n",
      "|    value_loss           | 0.134    |\n",
      "--------------------------------------\n",
      "Eval num_timesteps=26500, episode_reward=-1.66 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | -1.66     |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 26500     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.7632665 |\n",
      "|    clip_fraction        | 0.234     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.13     |\n",
      "|    explained_variance   | 0.452     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.162    |\n",
      "|    n_updates            | 1957      |\n",
      "|    policy_gradient_loss | -0.0794   |\n",
      "|    value_loss           | 0.208     |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 52.7     |\n",
      "| time/              |          |\n",
      "|    fps             | 181      |\n",
      "|    iterations      | 104      |\n",
      "|    time_elapsed    | 146      |\n",
      "|    total_timesteps | 26624    |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 52.6       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 181        |\n",
      "|    iterations           | 105        |\n",
      "|    time_elapsed         | 147        |\n",
      "|    total_timesteps      | 26880      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.58952695 |\n",
      "|    clip_fraction        | 0.243      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.157     |\n",
      "|    explained_variance   | 0.501      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.142     |\n",
      "|    n_updates            | 1976       |\n",
      "|    policy_gradient_loss | -0.0896    |\n",
      "|    value_loss           | 0.215      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=27000, episode_reward=9.41 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 9.41      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 27000     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.9854197 |\n",
      "|    clip_fraction        | 0.252     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.163    |\n",
      "|    explained_variance   | 0.262     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | 0.0912    |\n",
      "|    n_updates            | 1995      |\n",
      "|    policy_gradient_loss | -0.0659   |\n",
      "|    value_loss           | 0.149     |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 52.6     |\n",
      "| time/              |          |\n",
      "|    fps             | 181      |\n",
      "|    iterations      | 106      |\n",
      "|    time_elapsed    | 149      |\n",
      "|    total_timesteps | 27136    |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 52.6      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 181       |\n",
      "|    iterations           | 107       |\n",
      "|    time_elapsed         | 150       |\n",
      "|    total_timesteps      | 27392     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.9715555 |\n",
      "|    clip_fraction        | 0.227     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.105    |\n",
      "|    explained_variance   | 0.411     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0552   |\n",
      "|    n_updates            | 2014      |\n",
      "|    policy_gradient_loss | -0.0767   |\n",
      "|    value_loss           | 0.117     |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=27500, episode_reward=36.43 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 36.4      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 27500     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.0910081 |\n",
      "|    clip_fraction        | 0.288     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.146    |\n",
      "|    explained_variance   | 0.671     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0552   |\n",
      "|    n_updates            | 2033      |\n",
      "|    policy_gradient_loss | -0.0905   |\n",
      "|    value_loss           | 0.115     |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 52.8     |\n",
      "| time/              |          |\n",
      "|    fps             | 181      |\n",
      "|    iterations      | 108      |\n",
      "|    time_elapsed    | 152      |\n",
      "|    total_timesteps | 27648    |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 52.8       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 181        |\n",
      "|    iterations           | 109        |\n",
      "|    time_elapsed         | 153        |\n",
      "|    total_timesteps      | 27904      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.62257344 |\n",
      "|    clip_fraction        | 0.332      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.248     |\n",
      "|    explained_variance   | -0.0422    |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | 0.0305     |\n",
      "|    n_updates            | 2052       |\n",
      "|    policy_gradient_loss | -0.0804    |\n",
      "|    value_loss           | 0.363      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=28000, episode_reward=63.69 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 230        |\n",
      "|    mean_reward          | 63.7       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 28000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.79260105 |\n",
      "|    clip_fraction        | 0.256      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.127     |\n",
      "|    explained_variance   | 0.204      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.139     |\n",
      "|    n_updates            | 2071       |\n",
      "|    policy_gradient_loss | -0.0833    |\n",
      "|    value_loss           | 0.0685     |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 52.8     |\n",
      "| time/              |          |\n",
      "|    fps             | 181      |\n",
      "|    iterations      | 110      |\n",
      "|    time_elapsed    | 155      |\n",
      "|    total_timesteps | 28160    |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 52.8       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 181        |\n",
      "|    iterations           | 111        |\n",
      "|    time_elapsed         | 156        |\n",
      "|    total_timesteps      | 28416      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.60897243 |\n",
      "|    clip_fraction        | 0.234      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.136     |\n",
      "|    explained_variance   | 0.343      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | 0.0222     |\n",
      "|    n_updates            | 2090       |\n",
      "|    policy_gradient_loss | -0.0822    |\n",
      "|    value_loss           | 0.177      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=28500, episode_reward=25.36 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 25.4      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 28500     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.6567997 |\n",
      "|    clip_fraction        | 0.269     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.199    |\n",
      "|    explained_variance   | 0.779     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.126    |\n",
      "|    n_updates            | 2109      |\n",
      "|    policy_gradient_loss | -0.0952   |\n",
      "|    value_loss           | 0.198     |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 52.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 181      |\n",
      "|    iterations      | 112      |\n",
      "|    time_elapsed    | 157      |\n",
      "|    total_timesteps | 28672    |\n",
      "---------------------------------\n",
      "--------------------------------------\n",
      "| rollout/                |          |\n",
      "|    ep_len_mean          | 889      |\n",
      "|    ep_rew_mean          | 52.9     |\n",
      "| time/                   |          |\n",
      "|    fps                  | 181      |\n",
      "|    iterations           | 113      |\n",
      "|    time_elapsed         | 159      |\n",
      "|    total_timesteps      | 28928    |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 0.68345  |\n",
      "|    clip_fraction        | 0.267    |\n",
      "|    clip_range           | 0.4      |\n",
      "|    entropy_loss         | -0.196   |\n",
      "|    explained_variance   | -0.574   |\n",
      "|    learning_rate        | 0.000389 |\n",
      "|    loss                 | -0.0334  |\n",
      "|    n_updates            | 2128     |\n",
      "|    policy_gradient_loss | -0.0704  |\n",
      "|    value_loss           | 0.314    |\n",
      "--------------------------------------\n",
      "Eval num_timesteps=29000, episode_reward=23.54 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 230        |\n",
      "|    mean_reward          | 23.5       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 29000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.78564644 |\n",
      "|    clip_fraction        | 0.212      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0872    |\n",
      "|    explained_variance   | 0.47       |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0599    |\n",
      "|    n_updates            | 2147       |\n",
      "|    policy_gradient_loss | -0.0689    |\n",
      "|    value_loss           | 0.111      |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 52.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 181      |\n",
      "|    iterations      | 114      |\n",
      "|    time_elapsed    | 160      |\n",
      "|    total_timesteps | 29184    |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 52.7      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 181       |\n",
      "|    iterations           | 115       |\n",
      "|    time_elapsed         | 162       |\n",
      "|    total_timesteps      | 29440     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.6057651 |\n",
      "|    clip_fraction        | 0.212     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.116    |\n",
      "|    explained_variance   | 0.636     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0279   |\n",
      "|    n_updates            | 2166      |\n",
      "|    policy_gradient_loss | -0.0726   |\n",
      "|    value_loss           | 0.13      |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=29500, episode_reward=6.12 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 6.12      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 29500     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.6668891 |\n",
      "|    clip_fraction        | 0.271     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.197    |\n",
      "|    explained_variance   | -0.0189   |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0571   |\n",
      "|    n_updates            | 2185      |\n",
      "|    policy_gradient_loss | -0.0888   |\n",
      "|    value_loss           | 0.259     |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 52.7     |\n",
      "| time/              |          |\n",
      "|    fps             | 181      |\n",
      "|    iterations      | 116      |\n",
      "|    time_elapsed    | 163      |\n",
      "|    total_timesteps | 29696    |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 52.7      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 181       |\n",
      "|    iterations           | 117       |\n",
      "|    time_elapsed         | 165       |\n",
      "|    total_timesteps      | 29952     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.1952152 |\n",
      "|    clip_fraction        | 0.235     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0972   |\n",
      "|    explained_variance   | 0.657     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0641   |\n",
      "|    n_updates            | 2204      |\n",
      "|    policy_gradient_loss | -0.0834   |\n",
      "|    value_loss           | 0.04      |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=30000, episode_reward=27.42 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 27.4      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 30000     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.6441901 |\n",
      "|    clip_fraction        | 0.194     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.096    |\n",
      "|    explained_variance   | 0.213     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.132    |\n",
      "|    n_updates            | 2223      |\n",
      "|    policy_gradient_loss | -0.0835   |\n",
      "|    value_loss           | 0.365     |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 52.7     |\n",
      "| time/              |          |\n",
      "|    fps             | 181      |\n",
      "|    iterations      | 118      |\n",
      "|    time_elapsed    | 166      |\n",
      "|    total_timesteps | 30208    |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 53.2      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 181       |\n",
      "|    iterations           | 119       |\n",
      "|    time_elapsed         | 167       |\n",
      "|    total_timesteps      | 30464     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.6472394 |\n",
      "|    clip_fraction        | 0.227     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.163    |\n",
      "|    explained_variance   | 0.675     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0524   |\n",
      "|    n_updates            | 2242      |\n",
      "|    policy_gradient_loss | -0.0879   |\n",
      "|    value_loss           | 0.208     |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=30500, episode_reward=33.63 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 33.6      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 30500     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.0625757 |\n",
      "|    clip_fraction        | 0.255     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.124    |\n",
      "|    explained_variance   | 0.566     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.13     |\n",
      "|    n_updates            | 2261      |\n",
      "|    policy_gradient_loss | -0.0724   |\n",
      "|    value_loss           | 0.0529    |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 53.2     |\n",
      "| time/              |          |\n",
      "|    fps             | 181      |\n",
      "|    iterations      | 120      |\n",
      "|    time_elapsed    | 168      |\n",
      "|    total_timesteps | 30720    |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 53.2      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 181       |\n",
      "|    iterations           | 121       |\n",
      "|    time_elapsed         | 170       |\n",
      "|    total_timesteps      | 30976     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.0520695 |\n",
      "|    clip_fraction        | 0.184     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0962   |\n",
      "|    explained_variance   | 0.234     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.137    |\n",
      "|    n_updates            | 2280      |\n",
      "|    policy_gradient_loss | -0.0648   |\n",
      "|    value_loss           | 0.185     |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=31000, episode_reward=29.52 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 29.5      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 31000     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.1283619 |\n",
      "|    clip_fraction        | 0.238     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.111    |\n",
      "|    explained_variance   | 0.654     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0909   |\n",
      "|    n_updates            | 2299      |\n",
      "|    policy_gradient_loss | -0.0835   |\n",
      "|    value_loss           | 0.12      |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 53.3     |\n",
      "| time/              |          |\n",
      "|    fps             | 182      |\n",
      "|    iterations      | 122      |\n",
      "|    time_elapsed    | 171      |\n",
      "|    total_timesteps | 31232    |\n",
      "---------------------------------\n",
      "--------------------------------------\n",
      "| rollout/                |          |\n",
      "|    ep_len_mean          | 889      |\n",
      "|    ep_rew_mean          | 53.3     |\n",
      "| time/                   |          |\n",
      "|    fps                  | 181      |\n",
      "|    iterations           | 123      |\n",
      "|    time_elapsed         | 173      |\n",
      "|    total_timesteps      | 31488    |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 0.832832 |\n",
      "|    clip_fraction        | 0.281    |\n",
      "|    clip_range           | 0.4      |\n",
      "|    entropy_loss         | -0.184   |\n",
      "|    explained_variance   | 0.618    |\n",
      "|    learning_rate        | 0.000389 |\n",
      "|    loss                 | -0.0874  |\n",
      "|    n_updates            | 2318     |\n",
      "|    policy_gradient_loss | -0.0862  |\n",
      "|    value_loss           | 0.25     |\n",
      "--------------------------------------\n",
      "Eval num_timesteps=31500, episode_reward=51.55 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 51.5      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 31500     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.0154545 |\n",
      "|    clip_fraction        | 0.203     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.106    |\n",
      "|    explained_variance   | 0.472     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.128    |\n",
      "|    n_updates            | 2337      |\n",
      "|    policy_gradient_loss | -0.0723   |\n",
      "|    value_loss           | 0.0772    |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 53.3     |\n",
      "| time/              |          |\n",
      "|    fps             | 181      |\n",
      "|    iterations      | 124      |\n",
      "|    time_elapsed    | 174      |\n",
      "|    total_timesteps | 31744    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=32000, episode_reward=63.69 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 230        |\n",
      "|    mean_reward          | 63.7       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 32000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.47736922 |\n",
      "|    clip_fraction        | 0.171      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.106     |\n",
      "|    explained_variance   | 0.704      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0811    |\n",
      "|    n_updates            | 2356       |\n",
      "|    policy_gradient_loss | -0.0663    |\n",
      "|    value_loss           | 0.143      |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 53.3     |\n",
      "| time/              |          |\n",
      "|    fps             | 181      |\n",
      "|    iterations      | 125      |\n",
      "|    time_elapsed    | 175      |\n",
      "|    total_timesteps | 32000    |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 53.6       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 182        |\n",
      "|    iterations           | 126        |\n",
      "|    time_elapsed         | 177        |\n",
      "|    total_timesteps      | 32256      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.72132003 |\n",
      "|    clip_fraction        | 0.266      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.158     |\n",
      "|    explained_variance   | 0.634      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.148     |\n",
      "|    n_updates            | 2375       |\n",
      "|    policy_gradient_loss | -0.0842    |\n",
      "|    value_loss           | 0.195      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=32500, episode_reward=9.46 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 9.46      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 32500     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.8574141 |\n",
      "|    clip_fraction        | 0.288     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.147    |\n",
      "|    explained_variance   | 0.507     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.133    |\n",
      "|    n_updates            | 2394      |\n",
      "|    policy_gradient_loss | -0.101    |\n",
      "|    value_loss           | 0.0378    |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 53.6     |\n",
      "| time/              |          |\n",
      "|    fps             | 182      |\n",
      "|    iterations      | 127      |\n",
      "|    time_elapsed    | 178      |\n",
      "|    total_timesteps | 32512    |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 53.6      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 182       |\n",
      "|    iterations           | 128       |\n",
      "|    time_elapsed         | 179       |\n",
      "|    total_timesteps      | 32768     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.6645552 |\n",
      "|    clip_fraction        | 0.205     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0978   |\n",
      "|    explained_variance   | 0.441     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.149    |\n",
      "|    n_updates            | 2413      |\n",
      "|    policy_gradient_loss | -0.0791   |\n",
      "|    value_loss           | 0.16      |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=33000, episode_reward=13.42 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 13.4      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 33000     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.6198151 |\n",
      "|    clip_fraction        | 0.189     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.111    |\n",
      "|    explained_variance   | 0.548     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.16     |\n",
      "|    n_updates            | 2432      |\n",
      "|    policy_gradient_loss | -0.0631   |\n",
      "|    value_loss           | 0.125     |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 53.8     |\n",
      "| time/              |          |\n",
      "|    fps             | 182      |\n",
      "|    iterations      | 129      |\n",
      "|    time_elapsed    | 181      |\n",
      "|    total_timesteps | 33024    |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 53.8       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 182        |\n",
      "|    iterations           | 130        |\n",
      "|    time_elapsed         | 182        |\n",
      "|    total_timesteps      | 33280      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.48742193 |\n",
      "|    clip_fraction        | 0.257      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.18      |\n",
      "|    explained_variance   | 0.483      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | 0.322      |\n",
      "|    n_updates            | 2451       |\n",
      "|    policy_gradient_loss | -0.0729    |\n",
      "|    value_loss           | 0.395      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=33500, episode_reward=13.41 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 13.4      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 33500     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 2.4567468 |\n",
      "|    clip_fraction        | 0.199     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0991   |\n",
      "|    explained_variance   | 0.244     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0779   |\n",
      "|    n_updates            | 2470      |\n",
      "|    policy_gradient_loss | -0.0745   |\n",
      "|    value_loss           | 0.1       |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 53.8     |\n",
      "| time/              |          |\n",
      "|    fps             | 182      |\n",
      "|    iterations      | 131      |\n",
      "|    time_elapsed    | 183      |\n",
      "|    total_timesteps | 33536    |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 54.3       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 182        |\n",
      "|    iterations           | 132        |\n",
      "|    time_elapsed         | 185        |\n",
      "|    total_timesteps      | 33792      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.49693838 |\n",
      "|    clip_fraction        | 0.161      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.104     |\n",
      "|    explained_variance   | 0.603      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.00972   |\n",
      "|    n_updates            | 2489       |\n",
      "|    policy_gradient_loss | -0.0597    |\n",
      "|    value_loss           | 0.194      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=34000, episode_reward=-2.60 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | -2.6      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 34000     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.8363006 |\n",
      "|    clip_fraction        | 0.261     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.155    |\n",
      "|    explained_variance   | 0.689     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0978   |\n",
      "|    n_updates            | 2508      |\n",
      "|    policy_gradient_loss | -0.0747   |\n",
      "|    value_loss           | 0.245     |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 54.3     |\n",
      "| time/              |          |\n",
      "|    fps             | 182      |\n",
      "|    iterations      | 133      |\n",
      "|    time_elapsed    | 186      |\n",
      "|    total_timesteps | 34048    |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 54.3       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 182        |\n",
      "|    iterations           | 134        |\n",
      "|    time_elapsed         | 187        |\n",
      "|    total_timesteps      | 34304      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.83699083 |\n",
      "|    clip_fraction        | 0.289      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.156     |\n",
      "|    explained_variance   | 0.568      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.133     |\n",
      "|    n_updates            | 2527       |\n",
      "|    policy_gradient_loss | -0.0908    |\n",
      "|    value_loss           | 0.0479     |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=34500, episode_reward=1.46 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "--------------------------------------\n",
      "| eval/                   |          |\n",
      "|    mean_ep_length       | 230      |\n",
      "|    mean_reward          | 1.46     |\n",
      "| time/                   |          |\n",
      "|    total_timesteps      | 34500    |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 0.905793 |\n",
      "|    clip_fraction        | 0.199    |\n",
      "|    clip_range           | 0.4      |\n",
      "|    entropy_loss         | -0.108   |\n",
      "|    explained_variance   | 0.73     |\n",
      "|    learning_rate        | 0.000389 |\n",
      "|    loss                 | -0.14    |\n",
      "|    n_updates            | 2546     |\n",
      "|    policy_gradient_loss | -0.0739  |\n",
      "|    value_loss           | 0.122    |\n",
      "--------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 54.3     |\n",
      "| time/              |          |\n",
      "|    fps             | 182      |\n",
      "|    iterations      | 135      |\n",
      "|    time_elapsed    | 189      |\n",
      "|    total_timesteps | 34560    |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 54.3      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 182       |\n",
      "|    iterations           | 136       |\n",
      "|    time_elapsed         | 190       |\n",
      "|    total_timesteps      | 34816     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.7029408 |\n",
      "|    clip_fraction        | 0.212     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.114    |\n",
      "|    explained_variance   | 0.706     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.113    |\n",
      "|    n_updates            | 2565      |\n",
      "|    policy_gradient_loss | -0.0853   |\n",
      "|    value_loss           | 0.0849    |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=35000, episode_reward=-1.83 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 230        |\n",
      "|    mean_reward          | -1.83      |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 35000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.64614975 |\n",
      "|    clip_fraction        | 0.239      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.129     |\n",
      "|    explained_variance   | 0.588      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.109     |\n",
      "|    n_updates            | 2584       |\n",
      "|    policy_gradient_loss | -0.0802    |\n",
      "|    value_loss           | 0.286      |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 54.3     |\n",
      "| time/              |          |\n",
      "|    fps             | 182      |\n",
      "|    iterations      | 137      |\n",
      "|    time_elapsed    | 192      |\n",
      "|    total_timesteps | 35072    |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 54.3      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 182       |\n",
      "|    iterations           | 138       |\n",
      "|    time_elapsed         | 193       |\n",
      "|    total_timesteps      | 35328     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.7126128 |\n",
      "|    clip_fraction        | 0.208     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0931   |\n",
      "|    explained_variance   | 0.454     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0379   |\n",
      "|    n_updates            | 2603      |\n",
      "|    policy_gradient_loss | -0.0778   |\n",
      "|    value_loss           | 0.0965    |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=35500, episode_reward=-2.56 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | -2.56     |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 35500     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.9983965 |\n",
      "|    clip_fraction        | 0.227     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.105    |\n",
      "|    explained_variance   | 0.666     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.129    |\n",
      "|    n_updates            | 2622      |\n",
      "|    policy_gradient_loss | -0.0824   |\n",
      "|    value_loss           | 0.17      |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 54.3     |\n",
      "| time/              |          |\n",
      "|    fps             | 182      |\n",
      "|    iterations      | 139      |\n",
      "|    time_elapsed    | 194      |\n",
      "|    total_timesteps | 35584    |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 54.3       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 182        |\n",
      "|    iterations           | 140        |\n",
      "|    time_elapsed         | 196        |\n",
      "|    total_timesteps      | 35840      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.84071475 |\n",
      "|    clip_fraction        | 0.207      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.14      |\n",
      "|    explained_variance   | 0.613      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | 0.0123     |\n",
      "|    n_updates            | 2641       |\n",
      "|    policy_gradient_loss | -0.0752    |\n",
      "|    value_loss           | 0.459      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=36000, episode_reward=8.76 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 8.76      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 36000     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.9029325 |\n",
      "|    clip_fraction        | 0.184     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.105    |\n",
      "|    explained_variance   | 0.419     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.11     |\n",
      "|    n_updates            | 2660      |\n",
      "|    policy_gradient_loss | -0.0623   |\n",
      "|    value_loss           | 0.0444    |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 54.3     |\n",
      "| time/              |          |\n",
      "|    fps             | 182      |\n",
      "|    iterations      | 141      |\n",
      "|    time_elapsed    | 197      |\n",
      "|    total_timesteps | 36096    |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 54.3      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 183       |\n",
      "|    iterations           | 142       |\n",
      "|    time_elapsed         | 198       |\n",
      "|    total_timesteps      | 36352     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.4493375 |\n",
      "|    clip_fraction        | 0.238     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.114    |\n",
      "|    explained_variance   | 0.682     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.103    |\n",
      "|    n_updates            | 2679      |\n",
      "|    policy_gradient_loss | -0.0862   |\n",
      "|    value_loss           | 0.166     |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=36500, episode_reward=-8.46 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "--------------------------------------\n",
      "| eval/                   |          |\n",
      "|    mean_ep_length       | 230      |\n",
      "|    mean_reward          | -8.46    |\n",
      "| time/                   |          |\n",
      "|    total_timesteps      | 36500    |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 0.6666   |\n",
      "|    clip_fraction        | 0.194    |\n",
      "|    clip_range           | 0.4      |\n",
      "|    entropy_loss         | -0.114   |\n",
      "|    explained_variance   | 0.765    |\n",
      "|    learning_rate        | 0.000389 |\n",
      "|    loss                 | -0.104   |\n",
      "|    n_updates            | 2698     |\n",
      "|    policy_gradient_loss | -0.0684  |\n",
      "|    value_loss           | 0.124    |\n",
      "--------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 54       |\n",
      "| time/              |          |\n",
      "|    fps             | 183      |\n",
      "|    iterations      | 143      |\n",
      "|    time_elapsed    | 199      |\n",
      "|    total_timesteps | 36608    |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 54         |\n",
      "| time/                   |            |\n",
      "|    fps                  | 183        |\n",
      "|    iterations           | 144        |\n",
      "|    time_elapsed         | 201        |\n",
      "|    total_timesteps      | 36864      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.75217813 |\n",
      "|    clip_fraction        | 0.293      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.217     |\n",
      "|    explained_variance   | -0.103     |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.104     |\n",
      "|    n_updates            | 2717       |\n",
      "|    policy_gradient_loss | -0.0776    |\n",
      "|    value_loss           | 0.222      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=37000, episode_reward=47.06 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 47.1      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 37000     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.0844865 |\n",
      "|    clip_fraction        | 0.236     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.121    |\n",
      "|    explained_variance   | 0.702     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.133    |\n",
      "|    n_updates            | 2736      |\n",
      "|    policy_gradient_loss | -0.0813   |\n",
      "|    value_loss           | 0.0665    |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 54       |\n",
      "| time/              |          |\n",
      "|    fps             | 183      |\n",
      "|    iterations      | 145      |\n",
      "|    time_elapsed    | 202      |\n",
      "|    total_timesteps | 37120    |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 53.9      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 183       |\n",
      "|    iterations           | 146       |\n",
      "|    time_elapsed         | 203       |\n",
      "|    total_timesteps      | 37376     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.9274291 |\n",
      "|    clip_fraction        | 0.234     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.118    |\n",
      "|    explained_variance   | 0.427     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.119    |\n",
      "|    n_updates            | 2755      |\n",
      "|    policy_gradient_loss | -0.0838   |\n",
      "|    value_loss           | 0.171     |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=37500, episode_reward=11.74 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 230        |\n",
      "|    mean_reward          | 11.7       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 37500      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.68170893 |\n",
      "|    clip_fraction        | 0.238      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.149     |\n",
      "|    explained_variance   | 0.179      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.1       |\n",
      "|    n_updates            | 2774       |\n",
      "|    policy_gradient_loss | -0.0877    |\n",
      "|    value_loss           | 0.418      |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 53.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 183      |\n",
      "|    iterations      | 147      |\n",
      "|    time_elapsed    | 205      |\n",
      "|    total_timesteps | 37632    |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 53.9      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 183       |\n",
      "|    iterations           | 148       |\n",
      "|    time_elapsed         | 206       |\n",
      "|    total_timesteps      | 37888     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.1935822 |\n",
      "|    clip_fraction        | 0.29      |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.128    |\n",
      "|    explained_variance   | 0.355     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.128    |\n",
      "|    n_updates            | 2793      |\n",
      "|    policy_gradient_loss | -0.105    |\n",
      "|    value_loss           | 0.0881    |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=38000, episode_reward=28.45 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 28.4      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 38000     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.5527968 |\n",
      "|    clip_fraction        | 0.268     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0978   |\n",
      "|    explained_variance   | 0.701     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.059    |\n",
      "|    n_updates            | 2812      |\n",
      "|    policy_gradient_loss | -0.097    |\n",
      "|    value_loss           | 0.0959    |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 53.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 183      |\n",
      "|    iterations      | 149      |\n",
      "|    time_elapsed    | 207      |\n",
      "|    total_timesteps | 38144    |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 54.3      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 183       |\n",
      "|    iterations           | 150       |\n",
      "|    time_elapsed         | 209       |\n",
      "|    total_timesteps      | 38400     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.9067811 |\n",
      "|    clip_fraction        | 0.249     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.134    |\n",
      "|    explained_variance   | 0.594     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.107    |\n",
      "|    n_updates            | 2831      |\n",
      "|    policy_gradient_loss | -0.0875   |\n",
      "|    value_loss           | 0.228     |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=38500, episode_reward=26.81 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 26.8      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 38500     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.6794006 |\n",
      "|    clip_fraction        | 0.251     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.189    |\n",
      "|    explained_variance   | 0.184     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.146    |\n",
      "|    n_updates            | 2850      |\n",
      "|    policy_gradient_loss | -0.0664   |\n",
      "|    value_loss           | 0.448     |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 54.3     |\n",
      "| time/              |          |\n",
      "|    fps             | 183      |\n",
      "|    iterations      | 151      |\n",
      "|    time_elapsed    | 210      |\n",
      "|    total_timesteps | 38656    |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 54.3      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 183       |\n",
      "|    iterations           | 152       |\n",
      "|    time_elapsed         | 211       |\n",
      "|    total_timesteps      | 38912     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.0455496 |\n",
      "|    clip_fraction        | 0.234     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.102    |\n",
      "|    explained_variance   | 0.651     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.116    |\n",
      "|    n_updates            | 2869      |\n",
      "|    policy_gradient_loss | -0.0858   |\n",
      "|    value_loss           | 0.0831    |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=39000, episode_reward=26.15 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 26.1      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 39000     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.8260057 |\n",
      "|    clip_fraction        | 0.195     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.108    |\n",
      "|    explained_variance   | 0.19      |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0043   |\n",
      "|    n_updates            | 2888      |\n",
      "|    policy_gradient_loss | -0.0745   |\n",
      "|    value_loss           | 0.11      |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 54.3     |\n",
      "| time/              |          |\n",
      "|    fps             | 183      |\n",
      "|    iterations      | 153      |\n",
      "|    time_elapsed    | 213      |\n",
      "|    total_timesteps | 39168    |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 54.3       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 183        |\n",
      "|    iterations           | 154        |\n",
      "|    time_elapsed         | 214        |\n",
      "|    total_timesteps      | 39424      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.78410715 |\n",
      "|    clip_fraction        | 0.263      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.182     |\n",
      "|    explained_variance   | 0.643      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.16      |\n",
      "|    n_updates            | 2907       |\n",
      "|    policy_gradient_loss | -0.0981    |\n",
      "|    value_loss           | 0.141      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=39500, episode_reward=3.39 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 3.39      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 39500     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.6517457 |\n",
      "|    clip_fraction        | 0.236     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.128    |\n",
      "|    explained_variance   | 0.722     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.119    |\n",
      "|    n_updates            | 2926      |\n",
      "|    policy_gradient_loss | -0.0911   |\n",
      "|    value_loss           | 0.0305    |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 54.3     |\n",
      "| time/              |          |\n",
      "|    fps             | 183      |\n",
      "|    iterations      | 155      |\n",
      "|    time_elapsed    | 216      |\n",
      "|    total_timesteps | 39680    |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 54.3      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 183       |\n",
      "|    iterations           | 156       |\n",
      "|    time_elapsed         | 217       |\n",
      "|    total_timesteps      | 39936     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.2190598 |\n",
      "|    clip_fraction        | 0.211     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.111    |\n",
      "|    explained_variance   | 0.645     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.138    |\n",
      "|    n_updates            | 2945      |\n",
      "|    policy_gradient_loss | -0.0768   |\n",
      "|    value_loss           | 0.0946    |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=40000, episode_reward=22.46 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 230        |\n",
      "|    mean_reward          | 22.5       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 40000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.75274545 |\n",
      "|    clip_fraction        | 0.206      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.116     |\n",
      "|    explained_variance   | 0.601      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.148     |\n",
      "|    n_updates            | 2964       |\n",
      "|    policy_gradient_loss | -0.0787    |\n",
      "|    value_loss           | 0.315      |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 54.6     |\n",
      "| time/              |          |\n",
      "|    fps             | 183      |\n",
      "|    iterations      | 157      |\n",
      "|    time_elapsed    | 218      |\n",
      "|    total_timesteps | 40192    |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 54.6       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 183        |\n",
      "|    iterations           | 158        |\n",
      "|    time_elapsed         | 220        |\n",
      "|    total_timesteps      | 40448      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.83009785 |\n",
      "|    clip_fraction        | 0.153      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.132     |\n",
      "|    explained_variance   | 0.211      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | 0.322      |\n",
      "|    n_updates            | 2983       |\n",
      "|    policy_gradient_loss | -0.0575    |\n",
      "|    value_loss           | 0.411      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=40500, episode_reward=16.68 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 230        |\n",
      "|    mean_reward          | 16.7       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 40500      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.72946197 |\n",
      "|    clip_fraction        | 0.226      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.126     |\n",
      "|    explained_variance   | 0.599      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0962    |\n",
      "|    n_updates            | 3002       |\n",
      "|    policy_gradient_loss | -0.0793    |\n",
      "|    value_loss           | 0.0741     |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 54.6     |\n",
      "| time/              |          |\n",
      "|    fps             | 183      |\n",
      "|    iterations      | 159      |\n",
      "|    time_elapsed    | 221      |\n",
      "|    total_timesteps | 40704    |\n",
      "---------------------------------\n",
      "--------------------------------------\n",
      "| rollout/                |          |\n",
      "|    ep_len_mean          | 889      |\n",
      "|    ep_rew_mean          | 54.5     |\n",
      "| time/                   |          |\n",
      "|    fps                  | 184      |\n",
      "|    iterations           | 160      |\n",
      "|    time_elapsed         | 222      |\n",
      "|    total_timesteps      | 40960    |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 0.76783  |\n",
      "|    clip_fraction        | 0.204    |\n",
      "|    clip_range           | 0.4      |\n",
      "|    entropy_loss         | -0.107   |\n",
      "|    explained_variance   | 0.628    |\n",
      "|    learning_rate        | 0.000389 |\n",
      "|    loss                 | -0.123   |\n",
      "|    n_updates            | 3021     |\n",
      "|    policy_gradient_loss | -0.0804  |\n",
      "|    value_loss           | 0.0732   |\n",
      "--------------------------------------\n",
      "Eval num_timesteps=41000, episode_reward=3.90 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 3.9       |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 41000     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.8040371 |\n",
      "|    clip_fraction        | 0.243     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.162    |\n",
      "|    explained_variance   | 0.823     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.112    |\n",
      "|    n_updates            | 3040      |\n",
      "|    policy_gradient_loss | -0.0734   |\n",
      "|    value_loss           | 0.145     |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 54.5     |\n",
      "| time/              |          |\n",
      "|    fps             | 183      |\n",
      "|    iterations      | 161      |\n",
      "|    time_elapsed    | 224      |\n",
      "|    total_timesteps | 41216    |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 54.5      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 183       |\n",
      "|    iterations           | 162       |\n",
      "|    time_elapsed         | 225       |\n",
      "|    total_timesteps      | 41472     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.0678196 |\n",
      "|    clip_fraction        | 0.275     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.169    |\n",
      "|    explained_variance   | 0.431     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.134    |\n",
      "|    n_updates            | 3059      |\n",
      "|    policy_gradient_loss | -0.102    |\n",
      "|    value_loss           | 0.0573    |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=41500, episode_reward=1.54 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 1.54      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 41500     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.4503607 |\n",
      "|    clip_fraction        | 0.183     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.104    |\n",
      "|    explained_variance   | 0.347     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.044    |\n",
      "|    n_updates            | 3078      |\n",
      "|    policy_gradient_loss | -0.0665   |\n",
      "|    value_loss           | 0.167     |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 54.5     |\n",
      "| time/              |          |\n",
      "|    fps             | 183      |\n",
      "|    iterations      | 163      |\n",
      "|    time_elapsed    | 227      |\n",
      "|    total_timesteps | 41728    |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 54.6       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 183        |\n",
      "|    iterations           | 164        |\n",
      "|    time_elapsed         | 228        |\n",
      "|    total_timesteps      | 41984      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.80657434 |\n",
      "|    clip_fraction        | 0.228      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.135     |\n",
      "|    explained_variance   | 0.762      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.149     |\n",
      "|    n_updates            | 3097       |\n",
      "|    policy_gradient_loss | -0.0833    |\n",
      "|    value_loss           | 0.161      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=42000, episode_reward=36.80 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 36.8      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 42000     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.7162311 |\n",
      "|    clip_fraction        | 0.269     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.214    |\n",
      "|    explained_variance   | -0.0158   |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0207   |\n",
      "|    n_updates            | 3116      |\n",
      "|    policy_gradient_loss | -0.0925   |\n",
      "|    value_loss           | 0.0674    |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 54.6     |\n",
      "| time/              |          |\n",
      "|    fps             | 183      |\n",
      "|    iterations      | 165      |\n",
      "|    time_elapsed    | 230      |\n",
      "|    total_timesteps | 42240    |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 54.6      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 183       |\n",
      "|    iterations           | 166       |\n",
      "|    time_elapsed         | 231       |\n",
      "|    total_timesteps      | 42496     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.0120403 |\n",
      "|    clip_fraction        | 0.222     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0986   |\n",
      "|    explained_variance   | 0.811     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0171   |\n",
      "|    n_updates            | 3135      |\n",
      "|    policy_gradient_loss | -0.0776   |\n",
      "|    value_loss           | 0.0363    |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=42500, episode_reward=51.79 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 51.8      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 42500     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.3409824 |\n",
      "|    clip_fraction        | 0.24      |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0877   |\n",
      "|    explained_variance   | 0.207     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.103    |\n",
      "|    n_updates            | 3154      |\n",
      "|    policy_gradient_loss | -0.0825   |\n",
      "|    value_loss           | 0.0919    |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 55.1     |\n",
      "| time/              |          |\n",
      "|    fps             | 183      |\n",
      "|    iterations      | 167      |\n",
      "|    time_elapsed    | 233      |\n",
      "|    total_timesteps | 42752    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=43000, episode_reward=51.93 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 230        |\n",
      "|    mean_reward          | 51.9       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 43000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.82416457 |\n",
      "|    clip_fraction        | 0.279      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.182     |\n",
      "|    explained_variance   | 0.81       |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.193     |\n",
      "|    n_updates            | 3173       |\n",
      "|    policy_gradient_loss | -0.105     |\n",
      "|    value_loss           | 0.296      |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 55.1     |\n",
      "| time/              |          |\n",
      "|    fps             | 182      |\n",
      "|    iterations      | 168      |\n",
      "|    time_elapsed    | 235      |\n",
      "|    total_timesteps | 43008    |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 55.1      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 182       |\n",
      "|    iterations           | 169       |\n",
      "|    time_elapsed         | 236       |\n",
      "|    total_timesteps      | 43264     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.0869976 |\n",
      "|    clip_fraction        | 0.288     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.133    |\n",
      "|    explained_variance   | 0.529     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0928   |\n",
      "|    n_updates            | 3192      |\n",
      "|    policy_gradient_loss | -0.101    |\n",
      "|    value_loss           | 0.0328    |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=43500, episode_reward=23.88 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 23.9      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 43500     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.8103656 |\n",
      "|    clip_fraction        | 0.203     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0878   |\n",
      "|    explained_variance   | 0.664     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.109    |\n",
      "|    n_updates            | 3211      |\n",
      "|    policy_gradient_loss | -0.0567   |\n",
      "|    value_loss           | 0.146     |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 55.1     |\n",
      "| time/              |          |\n",
      "|    fps             | 182      |\n",
      "|    iterations      | 170      |\n",
      "|    time_elapsed    | 238      |\n",
      "|    total_timesteps | 43520    |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 55.3       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 182        |\n",
      "|    iterations           | 171        |\n",
      "|    time_elapsed         | 239        |\n",
      "|    total_timesteps      | 43776      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.95241493 |\n",
      "|    clip_fraction        | 0.227      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.138     |\n",
      "|    explained_variance   | 0.83       |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.064     |\n",
      "|    n_updates            | 3230       |\n",
      "|    policy_gradient_loss | -0.0671    |\n",
      "|    value_loss           | 0.241      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=44000, episode_reward=85.07 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 85.1      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 44000     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.7427321 |\n",
      "|    clip_fraction        | 0.264     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.157    |\n",
      "|    explained_variance   | 0.146     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0817   |\n",
      "|    n_updates            | 3249      |\n",
      "|    policy_gradient_loss | -0.105    |\n",
      "|    value_loss           | 0.156     |\n",
      "---------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 55.3     |\n",
      "| time/              |          |\n",
      "|    fps             | 182      |\n",
      "|    iterations      | 172      |\n",
      "|    time_elapsed    | 241      |\n",
      "|    total_timesteps | 44032    |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 55.3      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 182       |\n",
      "|    iterations           | 173       |\n",
      "|    time_elapsed         | 242       |\n",
      "|    total_timesteps      | 44288     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.1001526 |\n",
      "|    clip_fraction        | 0.236     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.11     |\n",
      "|    explained_variance   | 0.855     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0914   |\n",
      "|    n_updates            | 3268      |\n",
      "|    policy_gradient_loss | -0.102    |\n",
      "|    value_loss           | 0.0939    |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=44500, episode_reward=7.74 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 7.74      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 44500     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.8455062 |\n",
      "|    clip_fraction        | 0.168     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0806   |\n",
      "|    explained_variance   | 0.526     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0145   |\n",
      "|    n_updates            | 3287      |\n",
      "|    policy_gradient_loss | -0.0603   |\n",
      "|    value_loss           | 0.113     |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 55.5     |\n",
      "| time/              |          |\n",
      "|    fps             | 182      |\n",
      "|    iterations      | 174      |\n",
      "|    time_elapsed    | 244      |\n",
      "|    total_timesteps | 44544    |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 55.5       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 182        |\n",
      "|    iterations           | 175        |\n",
      "|    time_elapsed         | 245        |\n",
      "|    total_timesteps      | 44800      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.52110225 |\n",
      "|    clip_fraction        | 0.203      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.174     |\n",
      "|    explained_variance   | 0.79       |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.126     |\n",
      "|    n_updates            | 3306       |\n",
      "|    policy_gradient_loss | -0.0832    |\n",
      "|    value_loss           | 0.329      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=45000, episode_reward=3.48 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "--------------------------------------\n",
      "| eval/                   |          |\n",
      "|    mean_ep_length       | 230      |\n",
      "|    mean_reward          | 3.48     |\n",
      "| time/                   |          |\n",
      "|    total_timesteps      | 45000    |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 0.768273 |\n",
      "|    clip_fraction        | 0.207    |\n",
      "|    clip_range           | 0.4      |\n",
      "|    entropy_loss         | -0.0994  |\n",
      "|    explained_variance   | 0.658    |\n",
      "|    learning_rate        | 0.000389 |\n",
      "|    loss                 | -0.0576  |\n",
      "|    n_updates            | 3325     |\n",
      "|    policy_gradient_loss | -0.0781  |\n",
      "|    value_loss           | 0.0591   |\n",
      "--------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 55.5     |\n",
      "| time/              |          |\n",
      "|    fps             | 182      |\n",
      "|    iterations      | 176      |\n",
      "|    time_elapsed    | 247      |\n",
      "|    total_timesteps | 45056    |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 55.5       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 182        |\n",
      "|    iterations           | 177        |\n",
      "|    time_elapsed         | 248        |\n",
      "|    total_timesteps      | 45312      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.75832903 |\n",
      "|    clip_fraction        | 0.186      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0924    |\n",
      "|    explained_variance   | 0.819      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0599    |\n",
      "|    n_updates            | 3344       |\n",
      "|    policy_gradient_loss | -0.0728    |\n",
      "|    value_loss           | 0.106      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=45500, episode_reward=77.85 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 77.8      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 45500     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.8823424 |\n",
      "|    clip_fraction        | 0.206     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.129    |\n",
      "|    explained_variance   | 0.844     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.109    |\n",
      "|    n_updates            | 3363      |\n",
      "|    policy_gradient_loss | -0.0732   |\n",
      "|    value_loss           | 0.209     |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 55.7     |\n",
      "| time/              |          |\n",
      "|    fps             | 182      |\n",
      "|    iterations      | 178      |\n",
      "|    time_elapsed    | 250      |\n",
      "|    total_timesteps | 45568    |\n",
      "---------------------------------\n",
      "--------------------------------------\n",
      "| rollout/                |          |\n",
      "|    ep_len_mean          | 889      |\n",
      "|    ep_rew_mean          | 55.7     |\n",
      "| time/                   |          |\n",
      "|    fps                  | 181      |\n",
      "|    iterations           | 179      |\n",
      "|    time_elapsed         | 251      |\n",
      "|    total_timesteps      | 45824    |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 0.897714 |\n",
      "|    clip_fraction        | 0.274    |\n",
      "|    clip_range           | 0.4      |\n",
      "|    entropy_loss         | -0.139   |\n",
      "|    explained_variance   | 0.427    |\n",
      "|    learning_rate        | 0.000389 |\n",
      "|    loss                 | -0.075   |\n",
      "|    n_updates            | 3382     |\n",
      "|    policy_gradient_loss | -0.0874  |\n",
      "|    value_loss           | 0.0511   |\n",
      "--------------------------------------\n",
      "Eval num_timesteps=46000, episode_reward=61.44 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 61.4      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 46000     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.0465853 |\n",
      "|    clip_fraction        | 0.222     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0949   |\n",
      "|    explained_variance   | 0.641     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.126    |\n",
      "|    n_updates            | 3401      |\n",
      "|    policy_gradient_loss | -0.0655   |\n",
      "|    value_loss           | 0.179     |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 55.7     |\n",
      "| time/              |          |\n",
      "|    fps             | 181      |\n",
      "|    iterations      | 180      |\n",
      "|    time_elapsed    | 253      |\n",
      "|    total_timesteps | 46080    |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 56.2      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 181       |\n",
      "|    iterations           | 181       |\n",
      "|    time_elapsed         | 255       |\n",
      "|    total_timesteps      | 46336     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.8252244 |\n",
      "|    clip_fraction        | 0.173     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0931   |\n",
      "|    explained_variance   | 0.625     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.164    |\n",
      "|    n_updates            | 3420      |\n",
      "|    policy_gradient_loss | -0.0608   |\n",
      "|    value_loss           | 0.0933    |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=46500, episode_reward=42.97 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 230        |\n",
      "|    mean_reward          | 43         |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 46500      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.48413187 |\n",
      "|    clip_fraction        | 0.213      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.15      |\n",
      "|    explained_variance   | 0.645      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.142     |\n",
      "|    n_updates            | 3439       |\n",
      "|    policy_gradient_loss | -0.0768    |\n",
      "|    value_loss           | 0.358      |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 56.2     |\n",
      "| time/              |          |\n",
      "|    fps             | 181      |\n",
      "|    iterations      | 182      |\n",
      "|    time_elapsed    | 256      |\n",
      "|    total_timesteps | 46592    |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 56.2      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 181       |\n",
      "|    iterations           | 183       |\n",
      "|    time_elapsed         | 258       |\n",
      "|    total_timesteps      | 46848     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.6682627 |\n",
      "|    clip_fraction        | 0.193     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0936   |\n",
      "|    explained_variance   | 0.417     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.112    |\n",
      "|    n_updates            | 3458      |\n",
      "|    policy_gradient_loss | -0.0619   |\n",
      "|    value_loss           | 0.0656    |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=47000, episode_reward=11.74 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 11.7      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 47000     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.0542929 |\n",
      "|    clip_fraction        | 0.172     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0853   |\n",
      "|    explained_variance   | 0.801     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.162    |\n",
      "|    n_updates            | 3477      |\n",
      "|    policy_gradient_loss | -0.0673   |\n",
      "|    value_loss           | 0.0676    |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 56.2     |\n",
      "| time/              |          |\n",
      "|    fps             | 181      |\n",
      "|    iterations      | 184      |\n",
      "|    time_elapsed    | 260      |\n",
      "|    total_timesteps | 47104    |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 56.1      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 181       |\n",
      "|    iterations           | 185       |\n",
      "|    time_elapsed         | 261       |\n",
      "|    total_timesteps      | 47360     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.6902565 |\n",
      "|    clip_fraction        | 0.248     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.134    |\n",
      "|    explained_variance   | 0.725     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.117    |\n",
      "|    n_updates            | 3496      |\n",
      "|    policy_gradient_loss | -0.0921   |\n",
      "|    value_loss           | 0.172     |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=47500, episode_reward=40.28 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 230        |\n",
      "|    mean_reward          | 40.3       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 47500      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.61844313 |\n",
      "|    clip_fraction        | 0.263      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.154     |\n",
      "|    explained_variance   | 0.57       |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.133     |\n",
      "|    n_updates            | 3515       |\n",
      "|    policy_gradient_loss | -0.0808    |\n",
      "|    value_loss           | 0.051      |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 56.1     |\n",
      "| time/              |          |\n",
      "|    fps             | 180      |\n",
      "|    iterations      | 186      |\n",
      "|    time_elapsed    | 263      |\n",
      "|    total_timesteps | 47616    |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 56.1       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 180        |\n",
      "|    iterations           | 187        |\n",
      "|    time_elapsed         | 264        |\n",
      "|    total_timesteps      | 47872      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.90072393 |\n",
      "|    clip_fraction        | 0.212      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0953    |\n",
      "|    explained_variance   | 0.502      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.064     |\n",
      "|    n_updates            | 3534       |\n",
      "|    policy_gradient_loss | -0.0532    |\n",
      "|    value_loss           | 0.129      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=48000, episode_reward=41.15 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 230        |\n",
      "|    mean_reward          | 41.2       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 48000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.75952816 |\n",
      "|    clip_fraction        | 0.259      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.125     |\n",
      "|    explained_variance   | 0.681      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.153     |\n",
      "|    n_updates            | 3553       |\n",
      "|    policy_gradient_loss | -0.0826    |\n",
      "|    value_loss           | 0.105      |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 56.3     |\n",
      "| time/              |          |\n",
      "|    fps             | 180      |\n",
      "|    iterations      | 188      |\n",
      "|    time_elapsed    | 266      |\n",
      "|    total_timesteps | 48128    |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 56.3       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 180        |\n",
      "|    iterations           | 189        |\n",
      "|    time_elapsed         | 267        |\n",
      "|    total_timesteps      | 48384      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.90743095 |\n",
      "|    clip_fraction        | 0.309      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.167     |\n",
      "|    explained_variance   | 0.65       |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.142     |\n",
      "|    n_updates            | 3572       |\n",
      "|    policy_gradient_loss | -0.0842    |\n",
      "|    value_loss           | 0.254      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=48500, episode_reward=27.63 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 27.6      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 48500     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.4736598 |\n",
      "|    clip_fraction        | 0.233     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.113    |\n",
      "|    explained_variance   | 0.421     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.176    |\n",
      "|    n_updates            | 3591      |\n",
      "|    policy_gradient_loss | -0.0906   |\n",
      "|    value_loss           | 0.0922    |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 56.3     |\n",
      "| time/              |          |\n",
      "|    fps             | 180      |\n",
      "|    iterations      | 190      |\n",
      "|    time_elapsed    | 269      |\n",
      "|    total_timesteps | 48640    |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 56.4      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 180       |\n",
      "|    iterations           | 191       |\n",
      "|    time_elapsed         | 270       |\n",
      "|    total_timesteps      | 48896     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.0848699 |\n",
      "|    clip_fraction        | 0.237     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.101    |\n",
      "|    explained_variance   | 0.528     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.142    |\n",
      "|    n_updates            | 3610      |\n",
      "|    policy_gradient_loss | -0.0846   |\n",
      "|    value_loss           | 0.237     |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=49000, episode_reward=54.05 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 54.1      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 49000     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.7181239 |\n",
      "|    clip_fraction        | 0.208     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.121    |\n",
      "|    explained_variance   | 0.726     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.117    |\n",
      "|    n_updates            | 3629      |\n",
      "|    policy_gradient_loss | -0.0985   |\n",
      "|    value_loss           | 0.132     |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 56.4     |\n",
      "| time/              |          |\n",
      "|    fps             | 180      |\n",
      "|    iterations      | 192      |\n",
      "|    time_elapsed    | 272      |\n",
      "|    total_timesteps | 49152    |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 56.4      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 180       |\n",
      "|    iterations           | 193       |\n",
      "|    time_elapsed         | 273       |\n",
      "|    total_timesteps      | 49408     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.1233714 |\n",
      "|    clip_fraction        | 0.318     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.171    |\n",
      "|    explained_variance   | 0.339     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.177    |\n",
      "|    n_updates            | 3648      |\n",
      "|    policy_gradient_loss | -0.0887   |\n",
      "|    value_loss           | 0.0417    |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=49500, episode_reward=44.13 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 44.1      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 49500     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.2998991 |\n",
      "|    clip_fraction        | 0.21      |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.09     |\n",
      "|    explained_variance   | 0.638     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0283   |\n",
      "|    n_updates            | 3667      |\n",
      "|    policy_gradient_loss | -0.0733   |\n",
      "|    value_loss           | 0.147     |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 56.4     |\n",
      "| time/              |          |\n",
      "|    fps             | 180      |\n",
      "|    iterations      | 194      |\n",
      "|    time_elapsed    | 275      |\n",
      "|    total_timesteps | 49664    |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 56.7      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 180       |\n",
      "|    iterations           | 195       |\n",
      "|    time_elapsed         | 276       |\n",
      "|    total_timesteps      | 49920     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.7371297 |\n",
      "|    clip_fraction        | 0.211     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0992   |\n",
      "|    explained_variance   | 0.599     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.199    |\n",
      "|    n_updates            | 3686      |\n",
      "|    policy_gradient_loss | -0.0848   |\n",
      "|    value_loss           | 0.154     |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=50000, episode_reward=54.45 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 54.5      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 50000     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.6144501 |\n",
      "|    clip_fraction        | 0.219     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.141    |\n",
      "|    explained_variance   | 0.632     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | 0.128     |\n",
      "|    n_updates            | 3705      |\n",
      "|    policy_gradient_loss | -0.0752   |\n",
      "|    value_loss           | 0.276     |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 56.7     |\n",
      "| time/              |          |\n",
      "|    fps             | 180      |\n",
      "|    iterations      | 196      |\n",
      "|    time_elapsed    | 278      |\n",
      "|    total_timesteps | 50176    |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 56.7       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 180        |\n",
      "|    iterations           | 197        |\n",
      "|    time_elapsed         | 279        |\n",
      "|    total_timesteps      | 50432      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.66699046 |\n",
      "|    clip_fraction        | 0.188      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0986    |\n",
      "|    explained_variance   | 0.684      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.112     |\n",
      "|    n_updates            | 3724       |\n",
      "|    policy_gradient_loss | -0.0705    |\n",
      "|    value_loss           | 0.0395     |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=50500, episode_reward=42.92 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 42.9      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 50500     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.5884173 |\n",
      "|    clip_fraction        | 0.192     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.12     |\n",
      "|    explained_variance   | 0.636     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.018    |\n",
      "|    n_updates            | 3743      |\n",
      "|    policy_gradient_loss | -0.0575   |\n",
      "|    value_loss           | 0.187     |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 56.7     |\n",
      "| time/              |          |\n",
      "|    fps             | 180      |\n",
      "|    iterations      | 198      |\n",
      "|    time_elapsed    | 281      |\n",
      "|    total_timesteps | 50688    |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 56.7      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 179       |\n",
      "|    iterations           | 199       |\n",
      "|    time_elapsed         | 283       |\n",
      "|    total_timesteps      | 50944     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.7198083 |\n",
      "|    clip_fraction        | 0.176     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.12     |\n",
      "|    explained_variance   | 0.679     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.112    |\n",
      "|    n_updates            | 3762      |\n",
      "|    policy_gradient_loss | -0.0657   |\n",
      "|    value_loss           | 0.232     |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=51000, episode_reward=65.46 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 230        |\n",
      "|    mean_reward          | 65.5       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 51000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.51409173 |\n",
      "|    clip_fraction        | 0.186      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.128     |\n",
      "|    explained_variance   | 0.263      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.101     |\n",
      "|    n_updates            | 3781       |\n",
      "|    policy_gradient_loss | -0.0766    |\n",
      "|    value_loss           | 0.0226     |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 56.7     |\n",
      "| time/              |          |\n",
      "|    fps             | 179      |\n",
      "|    iterations      | 200      |\n",
      "|    time_elapsed    | 284      |\n",
      "|    total_timesteps | 51200    |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 56.7      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 179       |\n",
      "|    iterations           | 201       |\n",
      "|    time_elapsed         | 286       |\n",
      "|    total_timesteps      | 51456     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.9399799 |\n",
      "|    clip_fraction        | 0.202     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0918   |\n",
      "|    explained_variance   | 0.673     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.046    |\n",
      "|    n_updates            | 3800      |\n",
      "|    policy_gradient_loss | -0.0713   |\n",
      "|    value_loss           | 0.193     |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=51500, episode_reward=12.39 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 12.4      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 51500     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.7888335 |\n",
      "|    clip_fraction        | 0.237     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.117    |\n",
      "|    explained_variance   | 0.591     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.163    |\n",
      "|    n_updates            | 3819      |\n",
      "|    policy_gradient_loss | -0.0841   |\n",
      "|    value_loss           | 0.0949    |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 56.7     |\n",
      "| time/              |          |\n",
      "|    fps             | 179      |\n",
      "|    iterations      | 202      |\n",
      "|    time_elapsed    | 287      |\n",
      "|    total_timesteps | 51712    |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 56.7       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 179        |\n",
      "|    iterations           | 203        |\n",
      "|    time_elapsed         | 289        |\n",
      "|    total_timesteps      | 51968      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.62067986 |\n",
      "|    clip_fraction        | 0.221      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.165     |\n",
      "|    explained_variance   | 0.303      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | 0.012      |\n",
      "|    n_updates            | 3838       |\n",
      "|    policy_gradient_loss | -0.0663    |\n",
      "|    value_loss           | 0.249      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=52000, episode_reward=81.48 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 81.5      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 52000     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.0097587 |\n",
      "|    clip_fraction        | 0.182     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0914   |\n",
      "|    explained_variance   | 0.79      |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.11     |\n",
      "|    n_updates            | 3857      |\n",
      "|    policy_gradient_loss | -0.0718   |\n",
      "|    value_loss           | 0.0396    |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 56.7     |\n",
      "| time/              |          |\n",
      "|    fps             | 179      |\n",
      "|    iterations      | 204      |\n",
      "|    time_elapsed    | 290      |\n",
      "|    total_timesteps | 52224    |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 56.9      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 179       |\n",
      "|    iterations           | 205       |\n",
      "|    time_elapsed         | 292       |\n",
      "|    total_timesteps      | 52480     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.0730121 |\n",
      "|    clip_fraction        | 0.263     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.135    |\n",
      "|    explained_variance   | 0.381     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.163    |\n",
      "|    n_updates            | 3876      |\n",
      "|    policy_gradient_loss | -0.102    |\n",
      "|    value_loss           | 0.282     |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=52500, episode_reward=75.02 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 75        |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 52500     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.0610739 |\n",
      "|    clip_fraction        | 0.262     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.157    |\n",
      "|    explained_variance   | 0.454     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.129    |\n",
      "|    n_updates            | 3895      |\n",
      "|    policy_gradient_loss | -0.0833   |\n",
      "|    value_loss           | 0.59      |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 56.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 179      |\n",
      "|    iterations      | 206      |\n",
      "|    time_elapsed    | 294      |\n",
      "|    total_timesteps | 52736    |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 56.9      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 179       |\n",
      "|    iterations           | 207       |\n",
      "|    time_elapsed         | 295       |\n",
      "|    total_timesteps      | 52992     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.5762801 |\n",
      "|    clip_fraction        | 0.274     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.167    |\n",
      "|    explained_variance   | 0.32      |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0946   |\n",
      "|    n_updates            | 3914      |\n",
      "|    policy_gradient_loss | -0.0901   |\n",
      "|    value_loss           | 0.0367    |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=53000, episode_reward=73.74 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 73.7      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 53000     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.7654439 |\n",
      "|    clip_fraction        | 0.19      |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.11     |\n",
      "|    explained_variance   | 0.471     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.127    |\n",
      "|    n_updates            | 3933      |\n",
      "|    policy_gradient_loss | -0.0826   |\n",
      "|    value_loss           | 0.206     |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 56.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 179      |\n",
      "|    iterations      | 208      |\n",
      "|    time_elapsed    | 297      |\n",
      "|    total_timesteps | 53248    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=53500, episode_reward=81.18 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 81.2      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 53500     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.4470997 |\n",
      "|    clip_fraction        | 0.232     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.111    |\n",
      "|    explained_variance   | 0.692     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0747   |\n",
      "|    n_updates            | 3952      |\n",
      "|    policy_gradient_loss | -0.0789   |\n",
      "|    value_loss           | 0.0993    |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 56.8     |\n",
      "| time/              |          |\n",
      "|    fps             | 179      |\n",
      "|    iterations      | 209      |\n",
      "|    time_elapsed    | 298      |\n",
      "|    total_timesteps | 53504    |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 56.8      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 178       |\n",
      "|    iterations           | 210       |\n",
      "|    time_elapsed         | 300       |\n",
      "|    total_timesteps      | 53760     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.5069631 |\n",
      "|    clip_fraction        | 0.231     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.166    |\n",
      "|    explained_variance   | 0.532     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.00482  |\n",
      "|    n_updates            | 3971      |\n",
      "|    policy_gradient_loss | -0.0686   |\n",
      "|    value_loss           | 0.266     |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=54000, episode_reward=67.84 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 67.8      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 54000     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.0423692 |\n",
      "|    clip_fraction        | 0.165     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.087    |\n",
      "|    explained_variance   | 0.621     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | 0.0296    |\n",
      "|    n_updates            | 3990      |\n",
      "|    policy_gradient_loss | -0.0544   |\n",
      "|    value_loss           | 0.0711    |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 56.8     |\n",
      "| time/              |          |\n",
      "|    fps             | 178      |\n",
      "|    iterations      | 211      |\n",
      "|    time_elapsed    | 302      |\n",
      "|    total_timesteps | 54016    |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 57        |\n",
      "| time/                   |           |\n",
      "|    fps                  | 178       |\n",
      "|    iterations           | 212       |\n",
      "|    time_elapsed         | 303       |\n",
      "|    total_timesteps      | 54272     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.7500247 |\n",
      "|    clip_fraction        | 0.211     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.124    |\n",
      "|    explained_variance   | 0.672     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.156    |\n",
      "|    n_updates            | 4009      |\n",
      "|    policy_gradient_loss | -0.0709   |\n",
      "|    value_loss           | 0.13      |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=54500, episode_reward=44.94 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 44.9      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 54500     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.8147254 |\n",
      "|    clip_fraction        | 0.251     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.15     |\n",
      "|    explained_variance   | 0.595     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.14     |\n",
      "|    n_updates            | 4028      |\n",
      "|    policy_gradient_loss | -0.0742   |\n",
      "|    value_loss           | 0.262     |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 57       |\n",
      "| time/              |          |\n",
      "|    fps             | 178      |\n",
      "|    iterations      | 213      |\n",
      "|    time_elapsed    | 306      |\n",
      "|    total_timesteps | 54528    |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 57        |\n",
      "| time/                   |           |\n",
      "|    fps                  | 178       |\n",
      "|    iterations           | 214       |\n",
      "|    time_elapsed         | 307       |\n",
      "|    total_timesteps      | 54784     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.8332665 |\n",
      "|    clip_fraction        | 0.229     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.143    |\n",
      "|    explained_variance   | 0.48      |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.118    |\n",
      "|    n_updates            | 4047      |\n",
      "|    policy_gradient_loss | -0.082    |\n",
      "|    value_loss           | 0.0343    |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=55000, episode_reward=72.28 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "--------------------------------------\n",
      "| eval/                   |          |\n",
      "|    mean_ep_length       | 230      |\n",
      "|    mean_reward          | 72.3     |\n",
      "| time/                   |          |\n",
      "|    total_timesteps      | 55000    |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 1.57001  |\n",
      "|    clip_fraction        | 0.22     |\n",
      "|    clip_range           | 0.4      |\n",
      "|    entropy_loss         | -0.104   |\n",
      "|    explained_variance   | 0.695    |\n",
      "|    learning_rate        | 0.000389 |\n",
      "|    loss                 | -0.17    |\n",
      "|    n_updates            | 4066     |\n",
      "|    policy_gradient_loss | -0.0897  |\n",
      "|    value_loss           | 0.0941   |\n",
      "--------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 57       |\n",
      "| time/              |          |\n",
      "|    fps             | 177      |\n",
      "|    iterations      | 215      |\n",
      "|    time_elapsed    | 309      |\n",
      "|    total_timesteps | 55040    |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 57.4      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 177       |\n",
      "|    iterations           | 216       |\n",
      "|    time_elapsed         | 310       |\n",
      "|    total_timesteps      | 55296     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.4134331 |\n",
      "|    clip_fraction        | 0.177     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0939   |\n",
      "|    explained_variance   | 0.725     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0746   |\n",
      "|    n_updates            | 4085      |\n",
      "|    policy_gradient_loss | -0.0736   |\n",
      "|    value_loss           | 0.262     |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=55500, episode_reward=45.50 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 230        |\n",
      "|    mean_reward          | 45.5       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 55500      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.58489096 |\n",
      "|    clip_fraction        | 0.205      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.165     |\n",
      "|    explained_variance   | 0.375      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0952    |\n",
      "|    n_updates            | 4104       |\n",
      "|    policy_gradient_loss | -0.0752    |\n",
      "|    value_loss           | 0.363      |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 57.4     |\n",
      "| time/              |          |\n",
      "|    fps             | 177      |\n",
      "|    iterations      | 217      |\n",
      "|    time_elapsed    | 312      |\n",
      "|    total_timesteps | 55552    |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 57.4      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 177       |\n",
      "|    iterations           | 218       |\n",
      "|    time_elapsed         | 314       |\n",
      "|    total_timesteps      | 55808     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.7222937 |\n",
      "|    clip_fraction        | 0.22      |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0927   |\n",
      "|    explained_variance   | 0.0266    |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.148    |\n",
      "|    n_updates            | 4123      |\n",
      "|    policy_gradient_loss | -0.0874   |\n",
      "|    value_loss           | 0.132     |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=56000, episode_reward=89.05 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 89        |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 56000     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.8652301 |\n",
      "|    clip_fraction        | 0.196     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0979   |\n",
      "|    explained_variance   | 0.598     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.132    |\n",
      "|    n_updates            | 4142      |\n",
      "|    policy_gradient_loss | -0.0725   |\n",
      "|    value_loss           | 0.27      |\n",
      "---------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 57.2     |\n",
      "| time/              |          |\n",
      "|    fps             | 177      |\n",
      "|    iterations      | 219      |\n",
      "|    time_elapsed    | 315      |\n",
      "|    total_timesteps | 56064    |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 57.2       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 177        |\n",
      "|    iterations           | 220        |\n",
      "|    time_elapsed         | 317        |\n",
      "|    total_timesteps      | 56320      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.33243483 |\n",
      "|    clip_fraction        | 0.209      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.168     |\n",
      "|    explained_variance   | 0.577      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0931    |\n",
      "|    n_updates            | 4161       |\n",
      "|    policy_gradient_loss | -0.0715    |\n",
      "|    value_loss           | 0.415      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=56500, episode_reward=71.13 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "--------------------------------------\n",
      "| eval/                   |          |\n",
      "|    mean_ep_length       | 230      |\n",
      "|    mean_reward          | 71.1     |\n",
      "| time/                   |          |\n",
      "|    total_timesteps      | 56500    |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 1.481446 |\n",
      "|    clip_fraction        | 0.258    |\n",
      "|    clip_range           | 0.4      |\n",
      "|    entropy_loss         | -0.132   |\n",
      "|    explained_variance   | 0.301    |\n",
      "|    learning_rate        | 0.000389 |\n",
      "|    loss                 | -0.0735  |\n",
      "|    n_updates            | 4180     |\n",
      "|    policy_gradient_loss | -0.0815  |\n",
      "|    value_loss           | 0.0634   |\n",
      "--------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 57.2     |\n",
      "| time/              |          |\n",
      "|    fps             | 177      |\n",
      "|    iterations      | 221      |\n",
      "|    time_elapsed    | 319      |\n",
      "|    total_timesteps | 56576    |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 57.2       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 177        |\n",
      "|    iterations           | 222        |\n",
      "|    time_elapsed         | 320        |\n",
      "|    total_timesteps      | 56832      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.80810475 |\n",
      "|    clip_fraction        | 0.174      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0983    |\n",
      "|    explained_variance   | 0.0473     |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0773    |\n",
      "|    n_updates            | 4199       |\n",
      "|    policy_gradient_loss | -0.0503    |\n",
      "|    value_loss           | 0.455      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=57000, episode_reward=66.06 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "--------------------------------------\n",
      "| eval/                   |          |\n",
      "|    mean_ep_length       | 230      |\n",
      "|    mean_reward          | 66.1     |\n",
      "| time/                   |          |\n",
      "|    total_timesteps      | 57000    |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 1.022793 |\n",
      "|    clip_fraction        | 0.213    |\n",
      "|    clip_range           | 0.4      |\n",
      "|    entropy_loss         | -0.0934  |\n",
      "|    explained_variance   | 0.467    |\n",
      "|    learning_rate        | 0.000389 |\n",
      "|    loss                 | -0.0318  |\n",
      "|    n_updates            | 4218     |\n",
      "|    policy_gradient_loss | -0.0789  |\n",
      "|    value_loss           | 0.227    |\n",
      "--------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 57.1     |\n",
      "| time/              |          |\n",
      "|    fps             | 177      |\n",
      "|    iterations      | 223      |\n",
      "|    time_elapsed    | 322      |\n",
      "|    total_timesteps | 57088    |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 57.1      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 176       |\n",
      "|    iterations           | 224       |\n",
      "|    time_elapsed         | 324       |\n",
      "|    total_timesteps      | 57344     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.1450635 |\n",
      "|    clip_fraction        | 0.261     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.155    |\n",
      "|    explained_variance   | 0.116     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.105    |\n",
      "|    n_updates            | 4237      |\n",
      "|    policy_gradient_loss | -0.0758   |\n",
      "|    value_loss           | 0.366     |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=57500, episode_reward=68.69 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 68.7      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 57500     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.7094474 |\n",
      "|    clip_fraction        | 0.177     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0695   |\n",
      "|    explained_variance   | -0.0742   |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.097    |\n",
      "|    n_updates            | 4256      |\n",
      "|    policy_gradient_loss | -0.0794   |\n",
      "|    value_loss           | 0.172     |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 57.1     |\n",
      "| time/              |          |\n",
      "|    fps             | 176      |\n",
      "|    iterations      | 225      |\n",
      "|    time_elapsed    | 326      |\n",
      "|    total_timesteps | 57600    |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 57.3      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 176       |\n",
      "|    iterations           | 226       |\n",
      "|    time_elapsed         | 327       |\n",
      "|    total_timesteps      | 57856     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.1247408 |\n",
      "|    clip_fraction        | 0.164     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0792   |\n",
      "|    explained_variance   | 0.364     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.134    |\n",
      "|    n_updates            | 4275      |\n",
      "|    policy_gradient_loss | -0.0607   |\n",
      "|    value_loss           | 0.2       |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=58000, episode_reward=58.22 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 230        |\n",
      "|    mean_reward          | 58.2       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 58000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.79438853 |\n",
      "|    clip_fraction        | 0.259      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.173     |\n",
      "|    explained_variance   | 0.516      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.115     |\n",
      "|    n_updates            | 4294       |\n",
      "|    policy_gradient_loss | -0.0736    |\n",
      "|    value_loss           | 0.352      |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 57.3     |\n",
      "| time/              |          |\n",
      "|    fps             | 176      |\n",
      "|    iterations      | 227      |\n",
      "|    time_elapsed    | 329      |\n",
      "|    total_timesteps | 58112    |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 57.3      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 176       |\n",
      "|    iterations           | 228       |\n",
      "|    time_elapsed         | 331       |\n",
      "|    total_timesteps      | 58368     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.0096154 |\n",
      "|    clip_fraction        | 0.214     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0908   |\n",
      "|    explained_variance   | 0.292     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.126    |\n",
      "|    n_updates            | 4313      |\n",
      "|    policy_gradient_loss | -0.0814   |\n",
      "|    value_loss           | 0.0376    |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=58500, episode_reward=46.67 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 46.7      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 58500     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.1095018 |\n",
      "|    clip_fraction        | 0.176     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0723   |\n",
      "|    explained_variance   | 0.514     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0751   |\n",
      "|    n_updates            | 4332      |\n",
      "|    policy_gradient_loss | -0.0554   |\n",
      "|    value_loss           | 0.17      |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 57.3     |\n",
      "| time/              |          |\n",
      "|    fps             | 176      |\n",
      "|    iterations      | 229      |\n",
      "|    time_elapsed    | 332      |\n",
      "|    total_timesteps | 58624    |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 57.4       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 176        |\n",
      "|    iterations           | 230        |\n",
      "|    time_elapsed         | 334        |\n",
      "|    total_timesteps      | 58880      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.76413655 |\n",
      "|    clip_fraction        | 0.223      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.123     |\n",
      "|    explained_variance   | 0.833      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0799    |\n",
      "|    n_updates            | 4351       |\n",
      "|    policy_gradient_loss | -0.0687    |\n",
      "|    value_loss           | 0.111      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=59000, episode_reward=58.36 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "--------------------------------------\n",
      "| eval/                   |          |\n",
      "|    mean_ep_length       | 230      |\n",
      "|    mean_reward          | 58.4     |\n",
      "| time/                   |          |\n",
      "|    total_timesteps      | 59000    |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 0.84967  |\n",
      "|    clip_fraction        | 0.314    |\n",
      "|    clip_range           | 0.4      |\n",
      "|    entropy_loss         | -0.167   |\n",
      "|    explained_variance   | -0.0602  |\n",
      "|    learning_rate        | 0.000389 |\n",
      "|    loss                 | -0.0826  |\n",
      "|    n_updates            | 4370     |\n",
      "|    policy_gradient_loss | -0.0815  |\n",
      "|    value_loss           | 0.111    |\n",
      "--------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 57.4     |\n",
      "| time/              |          |\n",
      "|    fps             | 175      |\n",
      "|    iterations      | 231      |\n",
      "|    time_elapsed    | 336      |\n",
      "|    total_timesteps | 59136    |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 57.4       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 175        |\n",
      "|    iterations           | 232        |\n",
      "|    time_elapsed         | 337        |\n",
      "|    total_timesteps      | 59392      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.72180426 |\n",
      "|    clip_fraction        | 0.141      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0678    |\n",
      "|    explained_variance   | 0.705      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0634    |\n",
      "|    n_updates            | 4389       |\n",
      "|    policy_gradient_loss | -0.0657    |\n",
      "|    value_loss           | 0.0826     |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=59500, episode_reward=47.47 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 230        |\n",
      "|    mean_reward          | 47.5       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 59500      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.95974565 |\n",
      "|    clip_fraction        | 0.191      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0952    |\n",
      "|    explained_variance   | 0.327      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.192     |\n",
      "|    n_updates            | 4408       |\n",
      "|    policy_gradient_loss | -0.0822    |\n",
      "|    value_loss           | 0.158      |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 57.4     |\n",
      "| time/              |          |\n",
      "|    fps             | 175      |\n",
      "|    iterations      | 233      |\n",
      "|    time_elapsed    | 339      |\n",
      "|    total_timesteps | 59648    |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 57.4      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 175       |\n",
      "|    iterations           | 234       |\n",
      "|    time_elapsed         | 341       |\n",
      "|    total_timesteps      | 59904     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.0917923 |\n",
      "|    clip_fraction        | 0.29      |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.163    |\n",
      "|    explained_variance   | 0.87      |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.101    |\n",
      "|    n_updates            | 4427      |\n",
      "|    policy_gradient_loss | -0.081    |\n",
      "|    value_loss           | 0.101     |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=60000, episode_reward=50.26 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 50.3      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 60000     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.5721666 |\n",
      "|    clip_fraction        | 0.155     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0829   |\n",
      "|    explained_variance   | 0.324     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0899   |\n",
      "|    n_updates            | 4446      |\n",
      "|    policy_gradient_loss | -0.062    |\n",
      "|    value_loss           | 0.0667    |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 57.4     |\n",
      "| time/              |          |\n",
      "|    fps             | 175      |\n",
      "|    iterations      | 235      |\n",
      "|    time_elapsed    | 342      |\n",
      "|    total_timesteps | 60160    |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 57.4      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 175       |\n",
      "|    iterations           | 236       |\n",
      "|    time_elapsed         | 344       |\n",
      "|    total_timesteps      | 60416     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.6262362 |\n",
      "|    clip_fraction        | 0.203     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0709   |\n",
      "|    explained_variance   | 0.617     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.114    |\n",
      "|    n_updates            | 4465      |\n",
      "|    policy_gradient_loss | -0.0833   |\n",
      "|    value_loss           | 0.215     |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=60500, episode_reward=30.17 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 30.2      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 60500     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.7000179 |\n",
      "|    clip_fraction        | 0.191     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.122    |\n",
      "|    explained_variance   | 0.154     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0865   |\n",
      "|    n_updates            | 4484      |\n",
      "|    policy_gradient_loss | -0.063    |\n",
      "|    value_loss           | 0.385     |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 57.3     |\n",
      "| time/              |          |\n",
      "|    fps             | 175      |\n",
      "|    iterations      | 237      |\n",
      "|    time_elapsed    | 345      |\n",
      "|    total_timesteps | 60672    |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 57.3       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 175        |\n",
      "|    iterations           | 238        |\n",
      "|    time_elapsed         | 347        |\n",
      "|    total_timesteps      | 60928      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.98409724 |\n",
      "|    clip_fraction        | 0.27       |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.135     |\n",
      "|    explained_variance   | 0.294      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0986    |\n",
      "|    n_updates            | 4503       |\n",
      "|    policy_gradient_loss | -0.0813    |\n",
      "|    value_loss           | 0.104      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=61000, episode_reward=20.51 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 20.5      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 61000     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.1573615 |\n",
      "|    clip_fraction        | 0.204     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0738   |\n",
      "|    explained_variance   | 0.819     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0465   |\n",
      "|    n_updates            | 4522      |\n",
      "|    policy_gradient_loss | -0.072    |\n",
      "|    value_loss           | 0.0613    |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 57.3     |\n",
      "| time/              |          |\n",
      "|    fps             | 175      |\n",
      "|    iterations      | 239      |\n",
      "|    time_elapsed    | 349      |\n",
      "|    total_timesteps | 61184    |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 57.3      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 175       |\n",
      "|    iterations           | 240       |\n",
      "|    time_elapsed         | 350       |\n",
      "|    total_timesteps      | 61440     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.7849928 |\n",
      "|    clip_fraction        | 0.147     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0771   |\n",
      "|    explained_variance   | 0.254     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.148    |\n",
      "|    n_updates            | 4541      |\n",
      "|    policy_gradient_loss | -0.0449   |\n",
      "|    value_loss           | 0.19      |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=61500, episode_reward=37.68 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 37.7      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 61500     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.1461611 |\n",
      "|    clip_fraction        | 0.267     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.148    |\n",
      "|    explained_variance   | 0.542     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0958   |\n",
      "|    n_updates            | 4560      |\n",
      "|    policy_gradient_loss | -0.0674   |\n",
      "|    value_loss           | 0.214     |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 57.3     |\n",
      "| time/              |          |\n",
      "|    fps             | 175      |\n",
      "|    iterations      | 241      |\n",
      "|    time_elapsed    | 352      |\n",
      "|    total_timesteps | 61696    |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 57.3      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 175       |\n",
      "|    iterations           | 242       |\n",
      "|    time_elapsed         | 353       |\n",
      "|    total_timesteps      | 61952     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.8903501 |\n",
      "|    clip_fraction        | 0.179     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.087    |\n",
      "|    explained_variance   | 0.508     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.127    |\n",
      "|    n_updates            | 4579      |\n",
      "|    policy_gradient_loss | -0.0778   |\n",
      "|    value_loss           | 0.0501    |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=62000, episode_reward=62.85 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 62.9      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 62000     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.6876683 |\n",
      "|    clip_fraction        | 0.228     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0765   |\n",
      "|    explained_variance   | 0.718     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0415   |\n",
      "|    n_updates            | 4598      |\n",
      "|    policy_gradient_loss | -0.0735   |\n",
      "|    value_loss           | 0.139     |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 57.3     |\n",
      "| time/              |          |\n",
      "|    fps             | 175      |\n",
      "|    iterations      | 243      |\n",
      "|    time_elapsed    | 355      |\n",
      "|    total_timesteps | 62208    |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 57.6      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 175       |\n",
      "|    iterations           | 244       |\n",
      "|    time_elapsed         | 356       |\n",
      "|    total_timesteps      | 62464     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.7727988 |\n",
      "|    clip_fraction        | 0.21      |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.124    |\n",
      "|    explained_variance   | 0.611     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0667   |\n",
      "|    n_updates            | 4617      |\n",
      "|    policy_gradient_loss | -0.0426   |\n",
      "|    value_loss           | 0.399     |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=62500, episode_reward=36.66 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 36.7      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 62500     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.6237158 |\n",
      "|    clip_fraction        | 0.193     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0972   |\n",
      "|    explained_variance   | 0.645     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0584   |\n",
      "|    n_updates            | 4636      |\n",
      "|    policy_gradient_loss | -0.0705   |\n",
      "|    value_loss           | 0.0725    |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 57.6     |\n",
      "| time/              |          |\n",
      "|    fps             | 174      |\n",
      "|    iterations      | 245      |\n",
      "|    time_elapsed    | 358      |\n",
      "|    total_timesteps | 62720    |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 57.6      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 174       |\n",
      "|    iterations           | 246       |\n",
      "|    time_elapsed         | 359       |\n",
      "|    total_timesteps      | 62976     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.1501126 |\n",
      "|    clip_fraction        | 0.179     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0634   |\n",
      "|    explained_variance   | 0.305     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0576   |\n",
      "|    n_updates            | 4655      |\n",
      "|    policy_gradient_loss | -0.0674   |\n",
      "|    value_loss           | 0.252     |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=63000, episode_reward=66.80 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 66.8      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 63000     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.9690529 |\n",
      "|    clip_fraction        | 0.202     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0901   |\n",
      "|    explained_variance   | 0.585     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.036    |\n",
      "|    n_updates            | 4674      |\n",
      "|    policy_gradient_loss | -0.0777   |\n",
      "|    value_loss           | 0.143     |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 57.6     |\n",
      "| time/              |          |\n",
      "|    fps             | 174      |\n",
      "|    iterations      | 247      |\n",
      "|    time_elapsed    | 361      |\n",
      "|    total_timesteps | 63232    |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 57.6       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 174        |\n",
      "|    iterations           | 248        |\n",
      "|    time_elapsed         | 362        |\n",
      "|    total_timesteps      | 63488      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.62447536 |\n",
      "|    clip_fraction        | 0.216      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.154     |\n",
      "|    explained_variance   | 0.752      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | 0.448      |\n",
      "|    n_updates            | 4693       |\n",
      "|    policy_gradient_loss | -0.059     |\n",
      "|    value_loss           | 0.496      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=63500, episode_reward=28.91 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 28.9      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 63500     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.8560127 |\n",
      "|    clip_fraction        | 0.182     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.093    |\n",
      "|    explained_variance   | 0.289     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0807   |\n",
      "|    n_updates            | 4712      |\n",
      "|    policy_gradient_loss | -0.0572   |\n",
      "|    value_loss           | 0.0611    |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 57.6     |\n",
      "| time/              |          |\n",
      "|    fps             | 174      |\n",
      "|    iterations      | 249      |\n",
      "|    time_elapsed    | 364      |\n",
      "|    total_timesteps | 63744    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=64000, episode_reward=54.62 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 54.6      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 64000     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.9527048 |\n",
      "|    clip_fraction        | 0.17      |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0651   |\n",
      "|    explained_variance   | 0.737     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0827   |\n",
      "|    n_updates            | 4731      |\n",
      "|    policy_gradient_loss | -0.0646   |\n",
      "|    value_loss           | 0.162     |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 57.6     |\n",
      "| time/              |          |\n",
      "|    fps             | 174      |\n",
      "|    iterations      | 250      |\n",
      "|    time_elapsed    | 365      |\n",
      "|    total_timesteps | 64000    |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 57.7       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 175        |\n",
      "|    iterations           | 251        |\n",
      "|    time_elapsed         | 366        |\n",
      "|    total_timesteps      | 64256      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.85031855 |\n",
      "|    clip_fraction        | 0.246      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.155     |\n",
      "|    explained_variance   | 0.753      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | 0.178      |\n",
      "|    n_updates            | 4750       |\n",
      "|    policy_gradient_loss | -0.0515    |\n",
      "|    value_loss           | 0.359      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=64500, episode_reward=45.91 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "--------------------------------------\n",
      "| eval/                   |          |\n",
      "|    mean_ep_length       | 230      |\n",
      "|    mean_reward          | 45.9     |\n",
      "| time/                   |          |\n",
      "|    total_timesteps      | 64500    |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 1.661598 |\n",
      "|    clip_fraction        | 0.225    |\n",
      "|    clip_range           | 0.4      |\n",
      "|    entropy_loss         | -0.109   |\n",
      "|    explained_variance   | 0.515    |\n",
      "|    learning_rate        | 0.000389 |\n",
      "|    loss                 | -0.123   |\n",
      "|    n_updates            | 4769     |\n",
      "|    policy_gradient_loss | -0.0741  |\n",
      "|    value_loss           | 0.097    |\n",
      "--------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 57.7     |\n",
      "| time/              |          |\n",
      "|    fps             | 175      |\n",
      "|    iterations      | 252      |\n",
      "|    time_elapsed    | 368      |\n",
      "|    total_timesteps | 64512    |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 57.7       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 175        |\n",
      "|    iterations           | 253        |\n",
      "|    time_elapsed         | 369        |\n",
      "|    total_timesteps      | 64768      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.92081857 |\n",
      "|    clip_fraction        | 0.169      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0665    |\n",
      "|    explained_variance   | 0.311      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.00739   |\n",
      "|    n_updates            | 4788       |\n",
      "|    policy_gradient_loss | -0.0525    |\n",
      "|    value_loss           | 0.224      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=65000, episode_reward=14.67 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 14.7      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 65000     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.8251245 |\n",
      "|    clip_fraction        | 0.133     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0752   |\n",
      "|    explained_variance   | 0.444     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0898   |\n",
      "|    n_updates            | 4807      |\n",
      "|    policy_gradient_loss | -0.0514   |\n",
      "|    value_loss           | 0.128     |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 57.8     |\n",
      "| time/              |          |\n",
      "|    fps             | 175      |\n",
      "|    iterations      | 254      |\n",
      "|    time_elapsed    | 371      |\n",
      "|    total_timesteps | 65024    |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 57.8      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 174       |\n",
      "|    iterations           | 255       |\n",
      "|    time_elapsed         | 373       |\n",
      "|    total_timesteps      | 65280     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.5646626 |\n",
      "|    clip_fraction        | 0.233     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.136    |\n",
      "|    explained_variance   | 0.686     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.08     |\n",
      "|    n_updates            | 4826      |\n",
      "|    policy_gradient_loss | -0.0667   |\n",
      "|    value_loss           | 0.385     |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=65500, episode_reward=-0.91 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | -0.906    |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 65500     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.4120219 |\n",
      "|    clip_fraction        | 0.266     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0787   |\n",
      "|    explained_variance   | 0.351     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0716   |\n",
      "|    n_updates            | 4845      |\n",
      "|    policy_gradient_loss | -0.0891   |\n",
      "|    value_loss           | 0.0422    |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 57.8     |\n",
      "| time/              |          |\n",
      "|    fps             | 174      |\n",
      "|    iterations      | 256      |\n",
      "|    time_elapsed    | 374      |\n",
      "|    total_timesteps | 65536    |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 58        |\n",
      "| time/                   |           |\n",
      "|    fps                  | 174       |\n",
      "|    iterations           | 257       |\n",
      "|    time_elapsed         | 376       |\n",
      "|    total_timesteps      | 65792     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.0908018 |\n",
      "|    clip_fraction        | 0.175     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0805   |\n",
      "|    explained_variance   | 0.536     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0792   |\n",
      "|    n_updates            | 4864      |\n",
      "|    policy_gradient_loss | -0.0738   |\n",
      "|    value_loss           | 0.112     |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=66000, episode_reward=16.64 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 16.6      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 66000     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.0983942 |\n",
      "|    clip_fraction        | 0.256     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.13     |\n",
      "|    explained_variance   | 0.766     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0864   |\n",
      "|    n_updates            | 4883      |\n",
      "|    policy_gradient_loss | -0.0729   |\n",
      "|    value_loss           | 0.217     |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 58       |\n",
      "| time/              |          |\n",
      "|    fps             | 174      |\n",
      "|    iterations      | 258      |\n",
      "|    time_elapsed    | 378      |\n",
      "|    total_timesteps | 66048    |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 58        |\n",
      "| time/                   |           |\n",
      "|    fps                  | 174       |\n",
      "|    iterations           | 259       |\n",
      "|    time_elapsed         | 379       |\n",
      "|    total_timesteps      | 66304     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.8848078 |\n",
      "|    clip_fraction        | 0.258     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.118    |\n",
      "|    explained_variance   | 0.258     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.111    |\n",
      "|    n_updates            | 4902      |\n",
      "|    policy_gradient_loss | -0.0772   |\n",
      "|    value_loss           | 0.0447    |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=66500, episode_reward=22.94 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 22.9      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 66500     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.6005636 |\n",
      "|    clip_fraction        | 0.181     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0793   |\n",
      "|    explained_variance   | 0.686     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.134    |\n",
      "|    n_updates            | 4921      |\n",
      "|    policy_gradient_loss | -0.065    |\n",
      "|    value_loss           | 0.157     |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 58       |\n",
      "| time/              |          |\n",
      "|    fps             | 174      |\n",
      "|    iterations      | 260      |\n",
      "|    time_elapsed    | 380      |\n",
      "|    total_timesteps | 66560    |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 58.3       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 175        |\n",
      "|    iterations           | 261        |\n",
      "|    time_elapsed         | 381        |\n",
      "|    total_timesteps      | 66816      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.44766128 |\n",
      "|    clip_fraction        | 0.177      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.094     |\n",
      "|    explained_variance   | 0.651      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.056     |\n",
      "|    n_updates            | 4940       |\n",
      "|    policy_gradient_loss | -0.0622    |\n",
      "|    value_loss           | 0.107      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=67000, episode_reward=60.56 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 230        |\n",
      "|    mean_reward          | 60.6       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 67000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.79229367 |\n",
      "|    clip_fraction        | 0.212      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.139     |\n",
      "|    explained_variance   | 0.745      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0952    |\n",
      "|    n_updates            | 4959       |\n",
      "|    policy_gradient_loss | -0.0473    |\n",
      "|    value_loss           | 0.274      |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 58.3     |\n",
      "| time/              |          |\n",
      "|    fps             | 174      |\n",
      "|    iterations      | 262      |\n",
      "|    time_elapsed    | 383      |\n",
      "|    total_timesteps | 67072    |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 58.3       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 175        |\n",
      "|    iterations           | 263        |\n",
      "|    time_elapsed         | 384        |\n",
      "|    total_timesteps      | 67328      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.42495278 |\n",
      "|    clip_fraction        | 0.149      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0741    |\n",
      "|    explained_variance   | 0.745      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.089     |\n",
      "|    n_updates            | 4978       |\n",
      "|    policy_gradient_loss | -0.0435    |\n",
      "|    value_loss           | 0.06       |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=67500, episode_reward=83.65 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 83.6      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 67500     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.7837204 |\n",
      "|    clip_fraction        | 0.173     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0796   |\n",
      "|    explained_variance   | 0.649     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.139    |\n",
      "|    n_updates            | 4997      |\n",
      "|    policy_gradient_loss | -0.0609   |\n",
      "|    value_loss           | 0.121     |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 58.6     |\n",
      "| time/              |          |\n",
      "|    fps             | 175      |\n",
      "|    iterations      | 264      |\n",
      "|    time_elapsed    | 385      |\n",
      "|    total_timesteps | 67584    |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 58.6      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 175       |\n",
      "|    iterations           | 265       |\n",
      "|    time_elapsed         | 387       |\n",
      "|    total_timesteps      | 67840     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.4447834 |\n",
      "|    clip_fraction        | 0.151     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.101    |\n",
      "|    explained_variance   | 0.606     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0438   |\n",
      "|    n_updates            | 5016      |\n",
      "|    policy_gradient_loss | -0.0518   |\n",
      "|    value_loss           | 0.522     |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=68000, episode_reward=99.22 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 230        |\n",
      "|    mean_reward          | 99.2       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 68000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.69032705 |\n",
      "|    clip_fraction        | 0.222      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.14      |\n",
      "|    explained_variance   | 0.385      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0486    |\n",
      "|    n_updates            | 5035       |\n",
      "|    policy_gradient_loss | -0.0866    |\n",
      "|    value_loss           | 0.0766     |\n",
      "----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 58.6     |\n",
      "| time/              |          |\n",
      "|    fps             | 175      |\n",
      "|    iterations      | 266      |\n",
      "|    time_elapsed    | 388      |\n",
      "|    total_timesteps | 68096    |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 58.6      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 175       |\n",
      "|    iterations           | 267       |\n",
      "|    time_elapsed         | 390       |\n",
      "|    total_timesteps      | 68352     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.7297627 |\n",
      "|    clip_fraction        | 0.169     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0708   |\n",
      "|    explained_variance   | 0.61      |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0745   |\n",
      "|    n_updates            | 5054      |\n",
      "|    policy_gradient_loss | -0.0644   |\n",
      "|    value_loss           | 0.18      |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=68500, episode_reward=58.80 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 58.8      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 68500     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.8040324 |\n",
      "|    clip_fraction        | 0.177     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.089    |\n",
      "|    explained_variance   | 0.561     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.153    |\n",
      "|    n_updates            | 5073      |\n",
      "|    policy_gradient_loss | -0.0643   |\n",
      "|    value_loss           | 0.111     |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 58.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 175      |\n",
      "|    iterations      | 268      |\n",
      "|    time_elapsed    | 391      |\n",
      "|    total_timesteps | 68608    |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 58.9       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 175        |\n",
      "|    iterations           | 269        |\n",
      "|    time_elapsed         | 392        |\n",
      "|    total_timesteps      | 68864      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.75519127 |\n",
      "|    clip_fraction        | 0.26       |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.166     |\n",
      "|    explained_variance   | 0.755      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | 0.0688     |\n",
      "|    n_updates            | 5092       |\n",
      "|    policy_gradient_loss | -0.0776    |\n",
      "|    value_loss           | 0.283      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=69000, episode_reward=17.00 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 17        |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 69000     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.0531937 |\n",
      "|    clip_fraction        | 0.195     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0692   |\n",
      "|    explained_variance   | 0.778     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0947   |\n",
      "|    n_updates            | 5111      |\n",
      "|    policy_gradient_loss | -0.0687   |\n",
      "|    value_loss           | 0.058     |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 58.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 175      |\n",
      "|    iterations      | 270      |\n",
      "|    time_elapsed    | 394      |\n",
      "|    total_timesteps | 69120    |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 59.1      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 175       |\n",
      "|    iterations           | 271       |\n",
      "|    time_elapsed         | 395       |\n",
      "|    total_timesteps      | 69376     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.8872382 |\n",
      "|    clip_fraction        | 0.241     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0969   |\n",
      "|    explained_variance   | 0.656     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0487   |\n",
      "|    n_updates            | 5130      |\n",
      "|    policy_gradient_loss | -0.0862   |\n",
      "|    value_loss           | 0.104     |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=69500, episode_reward=30.81 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 30.8      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 69500     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.7008506 |\n",
      "|    clip_fraction        | 0.204     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.11     |\n",
      "|    explained_variance   | 0.316     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0441   |\n",
      "|    n_updates            | 5149      |\n",
      "|    policy_gradient_loss | -0.0561   |\n",
      "|    value_loss           | 0.496     |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 59.1     |\n",
      "| time/              |          |\n",
      "|    fps             | 175      |\n",
      "|    iterations      | 272      |\n",
      "|    time_elapsed    | 397      |\n",
      "|    total_timesteps | 69632    |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 59.1      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 175       |\n",
      "|    iterations           | 273       |\n",
      "|    time_elapsed         | 398       |\n",
      "|    total_timesteps      | 69888     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.2304922 |\n",
      "|    clip_fraction        | 0.22      |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.113    |\n",
      "|    explained_variance   | 0.254     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0765   |\n",
      "|    n_updates            | 5168      |\n",
      "|    policy_gradient_loss | -0.0763   |\n",
      "|    value_loss           | 0.0433    |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=70000, episode_reward=52.24 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 52.2      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 70000     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.0688932 |\n",
      "|    clip_fraction        | 0.214     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0783   |\n",
      "|    explained_variance   | 0.638     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0854   |\n",
      "|    n_updates            | 5187      |\n",
      "|    policy_gradient_loss | -0.0791   |\n",
      "|    value_loss           | 0.0874    |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 59.1     |\n",
      "| time/              |          |\n",
      "|    fps             | 175      |\n",
      "|    iterations      | 274      |\n",
      "|    time_elapsed    | 399      |\n",
      "|    total_timesteps | 70144    |\n",
      "---------------------------------\n",
      "--------------------------------------\n",
      "| rollout/                |          |\n",
      "|    ep_len_mean          | 889      |\n",
      "|    ep_rew_mean          | 59.3     |\n",
      "| time/                   |          |\n",
      "|    fps                  | 175      |\n",
      "|    iterations           | 275      |\n",
      "|    time_elapsed         | 401      |\n",
      "|    total_timesteps      | 70400    |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 0.810743 |\n",
      "|    clip_fraction        | 0.144    |\n",
      "|    clip_range           | 0.4      |\n",
      "|    entropy_loss         | -0.0885  |\n",
      "|    explained_variance   | 0.436    |\n",
      "|    learning_rate        | 0.000389 |\n",
      "|    loss                 | -0.0691  |\n",
      "|    n_updates            | 5206     |\n",
      "|    policy_gradient_loss | -0.0537  |\n",
      "|    value_loss           | 0.174    |\n",
      "--------------------------------------\n",
      "Eval num_timesteps=70500, episode_reward=38.44 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 38.4      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 70500     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.1007764 |\n",
      "|    clip_fraction        | 0.247     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.143    |\n",
      "|    explained_variance   | -0.104    |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.13     |\n",
      "|    n_updates            | 5225      |\n",
      "|    policy_gradient_loss | -0.0819   |\n",
      "|    value_loss           | 0.487     |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 59.3     |\n",
      "| time/              |          |\n",
      "|    fps             | 175      |\n",
      "|    iterations      | 276      |\n",
      "|    time_elapsed    | 402      |\n",
      "|    total_timesteps | 70656    |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 59.3      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 175       |\n",
      "|    iterations           | 277       |\n",
      "|    time_elapsed         | 404       |\n",
      "|    total_timesteps      | 70912     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.7691061 |\n",
      "|    clip_fraction        | 0.156     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0735   |\n",
      "|    explained_variance   | 0.582     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0735   |\n",
      "|    n_updates            | 5244      |\n",
      "|    policy_gradient_loss | -0.0516   |\n",
      "|    value_loss           | 0.049     |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=71000, episode_reward=37.52 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 37.5      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 71000     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.1011559 |\n",
      "|    clip_fraction        | 0.23      |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.1      |\n",
      "|    explained_variance   | 0.502     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0712   |\n",
      "|    n_updates            | 5263      |\n",
      "|    policy_gradient_loss | -0.0769   |\n",
      "|    value_loss           | 0.181     |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 59.4     |\n",
      "| time/              |          |\n",
      "|    fps             | 175      |\n",
      "|    iterations      | 278      |\n",
      "|    time_elapsed    | 406      |\n",
      "|    total_timesteps | 71168    |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 59.4      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 175       |\n",
      "|    iterations           | 279       |\n",
      "|    time_elapsed         | 407       |\n",
      "|    total_timesteps      | 71424     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.7167101 |\n",
      "|    clip_fraction        | 0.201     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.133    |\n",
      "|    explained_variance   | 0.438     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | 0.184     |\n",
      "|    n_updates            | 5282      |\n",
      "|    policy_gradient_loss | -0.0524   |\n",
      "|    value_loss           | 0.42      |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=71500, episode_reward=21.11 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 230        |\n",
      "|    mean_reward          | 21.1       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 71500      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.85076517 |\n",
      "|    clip_fraction        | 0.221      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.109     |\n",
      "|    explained_variance   | 0.337      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0713    |\n",
      "|    n_updates            | 5301       |\n",
      "|    policy_gradient_loss | -0.0597    |\n",
      "|    value_loss           | 0.0477     |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 59.4     |\n",
      "| time/              |          |\n",
      "|    fps             | 175      |\n",
      "|    iterations      | 280      |\n",
      "|    time_elapsed    | 409      |\n",
      "|    total_timesteps | 71680    |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 59.4      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 175       |\n",
      "|    iterations           | 281       |\n",
      "|    time_elapsed         | 410       |\n",
      "|    total_timesteps      | 71936     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.9384446 |\n",
      "|    clip_fraction        | 0.17      |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0749   |\n",
      "|    explained_variance   | 0.331     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | 0.018     |\n",
      "|    n_updates            | 5320      |\n",
      "|    policy_gradient_loss | -0.0532   |\n",
      "|    value_loss           | 0.306     |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=72000, episode_reward=9.04 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 9.04      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 72000     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.6757501 |\n",
      "|    clip_fraction        | 0.207     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.114    |\n",
      "|    explained_variance   | 0.839     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0617   |\n",
      "|    n_updates            | 5339      |\n",
      "|    policy_gradient_loss | -0.0602   |\n",
      "|    value_loss           | 0.221     |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 59.7     |\n",
      "| time/              |          |\n",
      "|    fps             | 175      |\n",
      "|    iterations      | 282      |\n",
      "|    time_elapsed    | 412      |\n",
      "|    total_timesteps | 72192    |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 59.7      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 174       |\n",
      "|    iterations           | 283       |\n",
      "|    time_elapsed         | 414       |\n",
      "|    total_timesteps      | 72448     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.0186932 |\n",
      "|    clip_fraction        | 0.226     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.12     |\n",
      "|    explained_variance   | 0.191     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.101    |\n",
      "|    n_updates            | 5358      |\n",
      "|    policy_gradient_loss | -0.0756   |\n",
      "|    value_loss           | 0.211     |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=72500, episode_reward=6.24 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 6.24      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 72500     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.0271895 |\n",
      "|    clip_fraction        | 0.165     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0676   |\n",
      "|    explained_variance   | 0.18      |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.123    |\n",
      "|    n_updates            | 5377      |\n",
      "|    policy_gradient_loss | -0.0668   |\n",
      "|    value_loss           | 0.192     |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 59.7     |\n",
      "| time/              |          |\n",
      "|    fps             | 174      |\n",
      "|    iterations      | 284      |\n",
      "|    time_elapsed    | 415      |\n",
      "|    total_timesteps | 72704    |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 59.8       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 174        |\n",
      "|    iterations           | 285        |\n",
      "|    time_elapsed         | 417        |\n",
      "|    total_timesteps      | 72960      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.90572107 |\n",
      "|    clip_fraction        | 0.183      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0856    |\n",
      "|    explained_variance   | 0.516      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.00337   |\n",
      "|    n_updates            | 5396       |\n",
      "|    policy_gradient_loss | -0.0632    |\n",
      "|    value_loss           | 0.286      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=73000, episode_reward=7.57 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 7.57      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 73000     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.7483847 |\n",
      "|    clip_fraction        | 0.232     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.15     |\n",
      "|    explained_variance   | 0.743     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.167    |\n",
      "|    n_updates            | 5415      |\n",
      "|    policy_gradient_loss | -0.0718   |\n",
      "|    value_loss           | 0.378     |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 59.8     |\n",
      "| time/              |          |\n",
      "|    fps             | 174      |\n",
      "|    iterations      | 286      |\n",
      "|    time_elapsed    | 419      |\n",
      "|    total_timesteps | 73216    |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 59.8       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 174        |\n",
      "|    iterations           | 287        |\n",
      "|    time_elapsed         | 420        |\n",
      "|    total_timesteps      | 73472      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.91528285 |\n",
      "|    clip_fraction        | 0.196      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.101     |\n",
      "|    explained_variance   | 0.386      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.11      |\n",
      "|    n_updates            | 5434       |\n",
      "|    policy_gradient_loss | -0.0775    |\n",
      "|    value_loss           | 0.0442     |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=73500, episode_reward=-8.28 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | -8.28     |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 73500     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.9178437 |\n",
      "|    clip_fraction        | 0.17      |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0792   |\n",
      "|    explained_variance   | 0.652     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.108    |\n",
      "|    n_updates            | 5453      |\n",
      "|    policy_gradient_loss | -0.0749   |\n",
      "|    value_loss           | 0.257     |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 59.8     |\n",
      "| time/              |          |\n",
      "|    fps             | 174      |\n",
      "|    iterations      | 288      |\n",
      "|    time_elapsed    | 422      |\n",
      "|    total_timesteps | 73728    |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 60         |\n",
      "| time/                   |            |\n",
      "|    fps                  | 174        |\n",
      "|    iterations           | 289        |\n",
      "|    time_elapsed         | 423        |\n",
      "|    total_timesteps      | 73984      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.65711653 |\n",
      "|    clip_fraction        | 0.153      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0891    |\n",
      "|    explained_variance   | 0.915      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0654    |\n",
      "|    n_updates            | 5472       |\n",
      "|    policy_gradient_loss | -0.0622    |\n",
      "|    value_loss           | 0.131      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=74000, episode_reward=3.99 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 3.99      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 74000     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.8206906 |\n",
      "|    clip_fraction        | 0.234     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.114    |\n",
      "|    explained_variance   | 0.221     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.111    |\n",
      "|    n_updates            | 5491      |\n",
      "|    policy_gradient_loss | -0.0605   |\n",
      "|    value_loss           | 0.091     |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 60       |\n",
      "| time/              |          |\n",
      "|    fps             | 174      |\n",
      "|    iterations      | 290      |\n",
      "|    time_elapsed    | 425      |\n",
      "|    total_timesteps | 74240    |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 60         |\n",
      "| time/                   |            |\n",
      "|    fps                  | 174        |\n",
      "|    iterations           | 291        |\n",
      "|    time_elapsed         | 426        |\n",
      "|    total_timesteps      | 74496      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.85838443 |\n",
      "|    clip_fraction        | 0.174      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0685    |\n",
      "|    explained_variance   | 0.442      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.111     |\n",
      "|    n_updates            | 5510       |\n",
      "|    policy_gradient_loss | -0.0737    |\n",
      "|    value_loss           | 0.122      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=74500, episode_reward=15.94 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "--------------------------------------\n",
      "| eval/                   |          |\n",
      "|    mean_ep_length       | 230      |\n",
      "|    mean_reward          | 15.9     |\n",
      "| time/                   |          |\n",
      "|    total_timesteps      | 74500    |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 1.108474 |\n",
      "|    clip_fraction        | 0.149    |\n",
      "|    clip_range           | 0.4      |\n",
      "|    entropy_loss         | -0.0718  |\n",
      "|    explained_variance   | 0.575    |\n",
      "|    learning_rate        | 0.000389 |\n",
      "|    loss                 | 0.00261  |\n",
      "|    n_updates            | 5529     |\n",
      "|    policy_gradient_loss | -0.0565  |\n",
      "|    value_loss           | 0.281    |\n",
      "--------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 60.2     |\n",
      "| time/              |          |\n",
      "|    fps             | 174      |\n",
      "|    iterations      | 292      |\n",
      "|    time_elapsed    | 427      |\n",
      "|    total_timesteps | 74752    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=75000, episode_reward=-11.81 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "--------------------------------------\n",
      "| eval/                   |          |\n",
      "|    mean_ep_length       | 230      |\n",
      "|    mean_reward          | -11.8    |\n",
      "| time/                   |          |\n",
      "|    total_timesteps      | 75000    |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 2.677266 |\n",
      "|    clip_fraction        | 0.259    |\n",
      "|    clip_range           | 0.4      |\n",
      "|    entropy_loss         | -0.129   |\n",
      "|    explained_variance   | 0.916    |\n",
      "|    learning_rate        | 0.000389 |\n",
      "|    loss                 | -0.0313  |\n",
      "|    n_updates            | 5548     |\n",
      "|    policy_gradient_loss | -0.0706  |\n",
      "|    value_loss           | 0.109    |\n",
      "--------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 60.2     |\n",
      "| time/              |          |\n",
      "|    fps             | 174      |\n",
      "|    iterations      | 293      |\n",
      "|    time_elapsed    | 429      |\n",
      "|    total_timesteps | 75008    |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 60.2      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 174       |\n",
      "|    iterations           | 294       |\n",
      "|    time_elapsed         | 430       |\n",
      "|    total_timesteps      | 75264     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.0047292 |\n",
      "|    clip_fraction        | 0.252     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.112    |\n",
      "|    explained_variance   | 0.514     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.03     |\n",
      "|    n_updates            | 5567      |\n",
      "|    policy_gradient_loss | -0.063    |\n",
      "|    value_loss           | 0.0552    |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=75500, episode_reward=-0.28 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 230        |\n",
      "|    mean_reward          | -0.275     |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 75500      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.55574816 |\n",
      "|    clip_fraction        | 0.154      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0703    |\n",
      "|    explained_variance   | 0.77       |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0709    |\n",
      "|    n_updates            | 5586       |\n",
      "|    policy_gradient_loss | -0.0508    |\n",
      "|    value_loss           | 0.123      |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 60.2     |\n",
      "| time/              |          |\n",
      "|    fps             | 174      |\n",
      "|    iterations      | 295      |\n",
      "|    time_elapsed    | 431      |\n",
      "|    total_timesteps | 75520    |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 60.4      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 175       |\n",
      "|    iterations           | 296       |\n",
      "|    time_elapsed         | 432       |\n",
      "|    total_timesteps      | 75776     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.1711569 |\n",
      "|    clip_fraction        | 0.233     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0998   |\n",
      "|    explained_variance   | 0.902     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0716   |\n",
      "|    n_updates            | 5605      |\n",
      "|    policy_gradient_loss | -0.0902   |\n",
      "|    value_loss           | 0.102     |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=76000, episode_reward=-9.19 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "--------------------------------------\n",
      "| eval/                   |          |\n",
      "|    mean_ep_length       | 230      |\n",
      "|    mean_reward          | -9.19    |\n",
      "| time/                   |          |\n",
      "|    total_timesteps      | 76000    |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 1.010642 |\n",
      "|    clip_fraction        | 0.253    |\n",
      "|    clip_range           | 0.4      |\n",
      "|    entropy_loss         | -0.121   |\n",
      "|    explained_variance   | 0.577    |\n",
      "|    learning_rate        | 0.000389 |\n",
      "|    loss                 | -0.108   |\n",
      "|    n_updates            | 5624     |\n",
      "|    policy_gradient_loss | -0.0841  |\n",
      "|    value_loss           | 0.0615   |\n",
      "--------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 60.4     |\n",
      "| time/              |          |\n",
      "|    fps             | 174      |\n",
      "|    iterations      | 297      |\n",
      "|    time_elapsed    | 434      |\n",
      "|    total_timesteps | 76032    |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 60.4      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 174       |\n",
      "|    iterations           | 298       |\n",
      "|    time_elapsed         | 435       |\n",
      "|    total_timesteps      | 76288     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.1536504 |\n",
      "|    clip_fraction        | 0.18      |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.097    |\n",
      "|    explained_variance   | 0.582     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0682   |\n",
      "|    n_updates            | 5643      |\n",
      "|    policy_gradient_loss | -0.0583   |\n",
      "|    value_loss           | 0.117     |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=76500, episode_reward=-5.15 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 230        |\n",
      "|    mean_reward          | -5.15      |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 76500      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.58163583 |\n",
      "|    clip_fraction        | 0.151      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0727    |\n",
      "|    explained_variance   | 0.792      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.126     |\n",
      "|    n_updates            | 5662       |\n",
      "|    policy_gradient_loss | -0.0534    |\n",
      "|    value_loss           | 0.118      |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 60.6     |\n",
      "| time/              |          |\n",
      "|    fps             | 174      |\n",
      "|    iterations      | 299      |\n",
      "|    time_elapsed    | 437      |\n",
      "|    total_timesteps | 76544    |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 60.6       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 174        |\n",
      "|    iterations           | 300        |\n",
      "|    time_elapsed         | 438        |\n",
      "|    total_timesteps      | 76800      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.44259197 |\n",
      "|    clip_fraction        | 0.18       |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.118     |\n",
      "|    explained_variance   | 0.77       |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | 0.0721     |\n",
      "|    n_updates            | 5681       |\n",
      "|    policy_gradient_loss | -0.0521    |\n",
      "|    value_loss           | 0.2        |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=77000, episode_reward=-30.82 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "--------------------------------------\n",
      "| eval/                   |          |\n",
      "|    mean_ep_length       | 230      |\n",
      "|    mean_reward          | -30.8    |\n",
      "| time/                   |          |\n",
      "|    total_timesteps      | 77000    |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 1.140032 |\n",
      "|    clip_fraction        | 0.232    |\n",
      "|    clip_range           | 0.4      |\n",
      "|    entropy_loss         | -0.0964  |\n",
      "|    explained_variance   | 0.715    |\n",
      "|    learning_rate        | 0.000389 |\n",
      "|    loss                 | -0.128   |\n",
      "|    n_updates            | 5700     |\n",
      "|    policy_gradient_loss | -0.0838  |\n",
      "|    value_loss           | 0.0278   |\n",
      "--------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 60.6     |\n",
      "| time/              |          |\n",
      "|    fps             | 174      |\n",
      "|    iterations      | 301      |\n",
      "|    time_elapsed    | 440      |\n",
      "|    total_timesteps | 77056    |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 60.6       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 175        |\n",
      "|    iterations           | 302        |\n",
      "|    time_elapsed         | 441        |\n",
      "|    total_timesteps      | 77312      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.69298375 |\n",
      "|    clip_fraction        | 0.171      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0838    |\n",
      "|    explained_variance   | 0.785      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0364    |\n",
      "|    n_updates            | 5719       |\n",
      "|    policy_gradient_loss | -0.0707    |\n",
      "|    value_loss           | 0.154      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=77500, episode_reward=-16.39 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | -16.4     |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 77500     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.1126782 |\n",
      "|    clip_fraction        | 0.168     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0649   |\n",
      "|    explained_variance   | 0.789     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0853   |\n",
      "|    n_updates            | 5738      |\n",
      "|    policy_gradient_loss | -0.0512   |\n",
      "|    value_loss           | 0.181     |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 60.7     |\n",
      "| time/              |          |\n",
      "|    fps             | 175      |\n",
      "|    iterations      | 303      |\n",
      "|    time_elapsed    | 443      |\n",
      "|    total_timesteps | 77568    |\n",
      "---------------------------------\n",
      "--------------------------------------\n",
      "| rollout/                |          |\n",
      "|    ep_len_mean          | 889      |\n",
      "|    ep_rew_mean          | 60.7     |\n",
      "| time/                   |          |\n",
      "|    fps                  | 175      |\n",
      "|    iterations           | 304      |\n",
      "|    time_elapsed         | 444      |\n",
      "|    total_timesteps      | 77824    |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 0.833333 |\n",
      "|    clip_fraction        | 0.261    |\n",
      "|    clip_range           | 0.4      |\n",
      "|    entropy_loss         | -0.123   |\n",
      "|    explained_variance   | 0.729    |\n",
      "|    learning_rate        | 0.000389 |\n",
      "|    loss                 | -0.128   |\n",
      "|    n_updates            | 5757     |\n",
      "|    policy_gradient_loss | -0.0758  |\n",
      "|    value_loss           | 0.0518   |\n",
      "--------------------------------------\n",
      "Eval num_timesteps=78000, episode_reward=0.92 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 0.923     |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 78000     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.0488791 |\n",
      "|    clip_fraction        | 0.208     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0849   |\n",
      "|    explained_variance   | 0.786     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0802   |\n",
      "|    n_updates            | 5776      |\n",
      "|    policy_gradient_loss | -0.0819   |\n",
      "|    value_loss           | 0.0928    |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 60.7     |\n",
      "| time/              |          |\n",
      "|    fps             | 175      |\n",
      "|    iterations      | 305      |\n",
      "|    time_elapsed    | 445      |\n",
      "|    total_timesteps | 78080    |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 60.7      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 175       |\n",
      "|    iterations           | 306       |\n",
      "|    time_elapsed         | 446       |\n",
      "|    total_timesteps      | 78336     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.0584798 |\n",
      "|    clip_fraction        | 0.15      |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0664   |\n",
      "|    explained_variance   | 0.729     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.165    |\n",
      "|    n_updates            | 5795      |\n",
      "|    policy_gradient_loss | -0.0667   |\n",
      "|    value_loss           | 0.153     |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=78500, episode_reward=15.61 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "--------------------------------------\n",
      "| eval/                   |          |\n",
      "|    mean_ep_length       | 230      |\n",
      "|    mean_reward          | 15.6     |\n",
      "| time/                   |          |\n",
      "|    total_timesteps      | 78500    |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 1.258327 |\n",
      "|    clip_fraction        | 0.25     |\n",
      "|    clip_range           | 0.4      |\n",
      "|    entropy_loss         | -0.109   |\n",
      "|    explained_variance   | 0.755    |\n",
      "|    learning_rate        | 0.000389 |\n",
      "|    loss                 | -0.0765  |\n",
      "|    n_updates            | 5814     |\n",
      "|    policy_gradient_loss | -0.0782  |\n",
      "|    value_loss           | 0.17     |\n",
      "--------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 60.7     |\n",
      "| time/              |          |\n",
      "|    fps             | 175      |\n",
      "|    iterations      | 307      |\n",
      "|    time_elapsed    | 448      |\n",
      "|    total_timesteps | 78592    |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 60.7      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 175       |\n",
      "|    iterations           | 308       |\n",
      "|    time_elapsed         | 449       |\n",
      "|    total_timesteps      | 78848     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.2040379 |\n",
      "|    clip_fraction        | 0.221     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0868   |\n",
      "|    explained_variance   | 0.433     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.165    |\n",
      "|    n_updates            | 5833      |\n",
      "|    policy_gradient_loss | -0.0842   |\n",
      "|    value_loss           | 0.0426    |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=79000, episode_reward=-3.56 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | -3.56     |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 79000     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.1447072 |\n",
      "|    clip_fraction        | 0.157     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0726   |\n",
      "|    explained_variance   | 0.661     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | 0.000163  |\n",
      "|    n_updates            | 5852      |\n",
      "|    policy_gradient_loss | -0.0581   |\n",
      "|    value_loss           | 0.343     |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 60.7     |\n",
      "| time/              |          |\n",
      "|    fps             | 175      |\n",
      "|    iterations      | 309      |\n",
      "|    time_elapsed    | 451      |\n",
      "|    total_timesteps | 79104    |\n",
      "---------------------------------\n",
      "--------------------------------------\n",
      "| rollout/                |          |\n",
      "|    ep_len_mean          | 889      |\n",
      "|    ep_rew_mean          | 60.8     |\n",
      "| time/                   |          |\n",
      "|    fps                  | 175      |\n",
      "|    iterations           | 310      |\n",
      "|    time_elapsed         | 452      |\n",
      "|    total_timesteps      | 79360    |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 0.999928 |\n",
      "|    clip_fraction        | 0.174    |\n",
      "|    clip_range           | 0.4      |\n",
      "|    entropy_loss         | -0.0834  |\n",
      "|    explained_variance   | 0.805    |\n",
      "|    learning_rate        | 0.000389 |\n",
      "|    loss                 | 0.0669   |\n",
      "|    n_updates            | 5871     |\n",
      "|    policy_gradient_loss | -0.0495  |\n",
      "|    value_loss           | 0.2      |\n",
      "--------------------------------------\n",
      "Eval num_timesteps=79500, episode_reward=53.53 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 230        |\n",
      "|    mean_reward          | 53.5       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 79500      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.99834025 |\n",
      "|    clip_fraction        | 0.207      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.105     |\n",
      "|    explained_variance   | 0.701      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.125     |\n",
      "|    n_updates            | 5890       |\n",
      "|    policy_gradient_loss | -0.0695    |\n",
      "|    value_loss           | 0.039      |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 60.8     |\n",
      "| time/              |          |\n",
      "|    fps             | 175      |\n",
      "|    iterations      | 311      |\n",
      "|    time_elapsed    | 454      |\n",
      "|    total_timesteps | 79616    |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 60.8      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 175       |\n",
      "|    iterations           | 312       |\n",
      "|    time_elapsed         | 455       |\n",
      "|    total_timesteps      | 79872     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.1441402 |\n",
      "|    clip_fraction        | 0.189     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0672   |\n",
      "|    explained_variance   | 0.658     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0918   |\n",
      "|    n_updates            | 5909      |\n",
      "|    policy_gradient_loss | -0.0695   |\n",
      "|    value_loss           | 0.109     |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=80000, episode_reward=56.81 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "--------------------------------------\n",
      "| eval/                   |          |\n",
      "|    mean_ep_length       | 230      |\n",
      "|    mean_reward          | 56.8     |\n",
      "| time/                   |          |\n",
      "|    total_timesteps      | 80000    |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 7.463478 |\n",
      "|    clip_fraction        | 0.156    |\n",
      "|    clip_range           | 0.4      |\n",
      "|    entropy_loss         | -0.0633  |\n",
      "|    explained_variance   | 0.762    |\n",
      "|    learning_rate        | 0.000389 |\n",
      "|    loss                 | 0.0106   |\n",
      "|    n_updates            | 5928     |\n",
      "|    policy_gradient_loss | -0.0555  |\n",
      "|    value_loss           | 0.239    |\n",
      "--------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 60.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 175      |\n",
      "|    iterations      | 313      |\n",
      "|    time_elapsed    | 456      |\n",
      "|    total_timesteps | 80128    |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 60.9      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 175       |\n",
      "|    iterations           | 314       |\n",
      "|    time_elapsed         | 458       |\n",
      "|    total_timesteps      | 80384     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.6640682 |\n",
      "|    clip_fraction        | 0.214     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.136    |\n",
      "|    explained_variance   | 0.815     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.116    |\n",
      "|    n_updates            | 5947      |\n",
      "|    policy_gradient_loss | -0.0665   |\n",
      "|    value_loss           | 0.208     |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=80500, episode_reward=39.86 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "--------------------------------------\n",
      "| eval/                   |          |\n",
      "|    mean_ep_length       | 230      |\n",
      "|    mean_reward          | 39.9     |\n",
      "| time/                   |          |\n",
      "|    total_timesteps      | 80500    |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 1.750778 |\n",
      "|    clip_fraction        | 0.215    |\n",
      "|    clip_range           | 0.4      |\n",
      "|    entropy_loss         | -0.0769  |\n",
      "|    explained_variance   | 0.411    |\n",
      "|    learning_rate        | 0.000389 |\n",
      "|    loss                 | -0.157   |\n",
      "|    n_updates            | 5966     |\n",
      "|    policy_gradient_loss | -0.0863  |\n",
      "|    value_loss           | 0.0556   |\n",
      "--------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 60.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 175      |\n",
      "|    iterations      | 315      |\n",
      "|    time_elapsed    | 459      |\n",
      "|    total_timesteps | 80640    |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 60.9      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 175       |\n",
      "|    iterations           | 316       |\n",
      "|    time_elapsed         | 460       |\n",
      "|    total_timesteps      | 80896     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.4616751 |\n",
      "|    clip_fraction        | 0.0999    |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0443   |\n",
      "|    explained_variance   | 0.587     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | 0.0168    |\n",
      "|    n_updates            | 5985      |\n",
      "|    policy_gradient_loss | -0.0479   |\n",
      "|    value_loss           | 0.416     |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=81000, episode_reward=49.17 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 230        |\n",
      "|    mean_reward          | 49.2       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 81000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.84643704 |\n",
      "|    clip_fraction        | 0.213      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.104     |\n",
      "|    explained_variance   | 0.777      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0184    |\n",
      "|    n_updates            | 6004       |\n",
      "|    policy_gradient_loss | -0.0655    |\n",
      "|    value_loss           | 0.228      |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 60.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 175      |\n",
      "|    iterations      | 317      |\n",
      "|    time_elapsed    | 462      |\n",
      "|    total_timesteps | 81152    |\n",
      "---------------------------------\n",
      "--------------------------------------\n",
      "| rollout/                |          |\n",
      "|    ep_len_mean          | 889      |\n",
      "|    ep_rew_mean          | 60.9     |\n",
      "| time/                   |          |\n",
      "|    fps                  | 175      |\n",
      "|    iterations           | 318      |\n",
      "|    time_elapsed         | 463      |\n",
      "|    total_timesteps      | 81408    |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 0.740708 |\n",
      "|    clip_fraction        | 0.179    |\n",
      "|    clip_range           | 0.4      |\n",
      "|    entropy_loss         | -0.0813  |\n",
      "|    explained_variance   | 0.503    |\n",
      "|    learning_rate        | 0.000389 |\n",
      "|    loss                 | -0.0503  |\n",
      "|    n_updates            | 6023     |\n",
      "|    policy_gradient_loss | -0.0431  |\n",
      "|    value_loss           | 0.0534   |\n",
      "--------------------------------------\n",
      "Eval num_timesteps=81500, episode_reward=40.81 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 40.8      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 81500     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.8616078 |\n",
      "|    clip_fraction        | 0.124     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0453   |\n",
      "|    explained_variance   | 0.63      |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | 0.023     |\n",
      "|    n_updates            | 6042      |\n",
      "|    policy_gradient_loss | -0.0585   |\n",
      "|    value_loss           | 0.126     |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 60.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 175      |\n",
      "|    iterations      | 319      |\n",
      "|    time_elapsed    | 465      |\n",
      "|    total_timesteps | 81664    |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 60.9       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 175        |\n",
      "|    iterations           | 320        |\n",
      "|    time_elapsed         | 466        |\n",
      "|    total_timesteps      | 81920      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.89181024 |\n",
      "|    clip_fraction        | 0.171      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0627    |\n",
      "|    explained_variance   | 0.834      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0589    |\n",
      "|    n_updates            | 6061       |\n",
      "|    policy_gradient_loss | -0.0522    |\n",
      "|    value_loss           | 0.16       |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=82000, episode_reward=28.70 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 28.7      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 82000     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.7867814 |\n",
      "|    clip_fraction        | 0.247     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.134    |\n",
      "|    explained_variance   | 0.922     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0651   |\n",
      "|    n_updates            | 6080      |\n",
      "|    policy_gradient_loss | -0.0743   |\n",
      "|    value_loss           | 0.163     |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 60.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 175      |\n",
      "|    iterations      | 321      |\n",
      "|    time_elapsed    | 467      |\n",
      "|    total_timesteps | 82176    |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 60.9      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 175       |\n",
      "|    iterations           | 322       |\n",
      "|    time_elapsed         | 469       |\n",
      "|    total_timesteps      | 82432     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.1388345 |\n",
      "|    clip_fraction        | 0.155     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0621   |\n",
      "|    explained_variance   | 0.586     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0734   |\n",
      "|    n_updates            | 6099      |\n",
      "|    policy_gradient_loss | -0.0565   |\n",
      "|    value_loss           | 0.0328    |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=82500, episode_reward=30.66 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 30.7      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 82500     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.6024652 |\n",
      "|    clip_fraction        | 0.109     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0572   |\n",
      "|    explained_variance   | 0.76      |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0377   |\n",
      "|    n_updates            | 6118      |\n",
      "|    policy_gradient_loss | -0.0387   |\n",
      "|    value_loss           | 0.179     |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 61.1     |\n",
      "| time/              |          |\n",
      "|    fps             | 175      |\n",
      "|    iterations      | 323      |\n",
      "|    time_elapsed    | 470      |\n",
      "|    total_timesteps | 82688    |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 61.1       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 175        |\n",
      "|    iterations           | 324        |\n",
      "|    time_elapsed         | 472        |\n",
      "|    total_timesteps      | 82944      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.54245245 |\n",
      "|    clip_fraction        | 0.209      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.123     |\n",
      "|    explained_variance   | 0.816      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0468    |\n",
      "|    n_updates            | 6137       |\n",
      "|    policy_gradient_loss | -0.0156    |\n",
      "|    value_loss           | 0.271      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=83000, episode_reward=54.13 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 230        |\n",
      "|    mean_reward          | 54.1       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 83000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.64887345 |\n",
      "|    clip_fraction        | 0.165      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0911    |\n",
      "|    explained_variance   | -0.133     |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0561    |\n",
      "|    n_updates            | 6156       |\n",
      "|    policy_gradient_loss | -0.0513    |\n",
      "|    value_loss           | 0.142      |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 61.1     |\n",
      "| time/              |          |\n",
      "|    fps             | 175      |\n",
      "|    iterations      | 325      |\n",
      "|    time_elapsed    | 473      |\n",
      "|    total_timesteps | 83200    |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 61.1      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 175       |\n",
      "|    iterations           | 326       |\n",
      "|    time_elapsed         | 474       |\n",
      "|    total_timesteps      | 83456     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.4612181 |\n",
      "|    clip_fraction        | 0.171     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0547   |\n",
      "|    explained_variance   | 0.477     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0979   |\n",
      "|    n_updates            | 6175      |\n",
      "|    policy_gradient_loss | -0.0526   |\n",
      "|    value_loss           | 0.216     |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=83500, episode_reward=-9.54 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 230        |\n",
      "|    mean_reward          | -9.54      |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 83500      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.95147806 |\n",
      "|    clip_fraction        | 0.211      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0868    |\n",
      "|    explained_variance   | 0.824      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0533    |\n",
      "|    n_updates            | 6194       |\n",
      "|    policy_gradient_loss | -0.0751    |\n",
      "|    value_loss           | 0.0723     |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 61       |\n",
      "| time/              |          |\n",
      "|    fps             | 175      |\n",
      "|    iterations      | 327      |\n",
      "|    time_elapsed    | 476      |\n",
      "|    total_timesteps | 83712    |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 61        |\n",
      "| time/                   |           |\n",
      "|    fps                  | 175       |\n",
      "|    iterations           | 328       |\n",
      "|    time_elapsed         | 477       |\n",
      "|    total_timesteps      | 83968     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.0564163 |\n",
      "|    clip_fraction        | 0.213     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0946   |\n",
      "|    explained_variance   | 0.22      |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0525   |\n",
      "|    n_updates            | 6213      |\n",
      "|    policy_gradient_loss | -0.0576   |\n",
      "|    value_loss           | 0.495     |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=84000, episode_reward=35.96 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 36        |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 84000     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.6998348 |\n",
      "|    clip_fraction        | 0.117     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0483   |\n",
      "|    explained_variance   | 0.423     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.116    |\n",
      "|    n_updates            | 6232      |\n",
      "|    policy_gradient_loss | -0.0375   |\n",
      "|    value_loss           | 0.0429    |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 61       |\n",
      "| time/              |          |\n",
      "|    fps             | 175      |\n",
      "|    iterations      | 329      |\n",
      "|    time_elapsed    | 478      |\n",
      "|    total_timesteps | 84224    |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 61         |\n",
      "| time/                   |            |\n",
      "|    fps                  | 175        |\n",
      "|    iterations           | 330        |\n",
      "|    time_elapsed         | 480        |\n",
      "|    total_timesteps      | 84480      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.78901523 |\n",
      "|    clip_fraction        | 0.181      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0682    |\n",
      "|    explained_variance   | 0.761      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.109     |\n",
      "|    n_updates            | 6251       |\n",
      "|    policy_gradient_loss | -0.0692    |\n",
      "|    value_loss           | 0.165      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=84500, episode_reward=5.38 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 5.38      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 84500     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.3277751 |\n",
      "|    clip_fraction        | 0.2       |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0747   |\n",
      "|    explained_variance   | 0.685     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | 0.00255   |\n",
      "|    n_updates            | 6270      |\n",
      "|    policy_gradient_loss | -0.0581   |\n",
      "|    value_loss           | 0.262     |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 61       |\n",
      "| time/              |          |\n",
      "|    fps             | 175      |\n",
      "|    iterations      | 331      |\n",
      "|    time_elapsed    | 481      |\n",
      "|    total_timesteps | 84736    |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 61        |\n",
      "| time/                   |           |\n",
      "|    fps                  | 175       |\n",
      "|    iterations           | 332       |\n",
      "|    time_elapsed         | 483       |\n",
      "|    total_timesteps      | 84992     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.9533705 |\n",
      "|    clip_fraction        | 0.194     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.112    |\n",
      "|    explained_variance   | 0.454     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.047    |\n",
      "|    n_updates            | 6289      |\n",
      "|    policy_gradient_loss | -0.0563   |\n",
      "|    value_loss           | 0.0467    |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=85000, episode_reward=7.75 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 7.75      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 85000     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.3491974 |\n",
      "|    clip_fraction        | 0.113     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0654   |\n",
      "|    explained_variance   | 0.778     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.104    |\n",
      "|    n_updates            | 6308      |\n",
      "|    policy_gradient_loss | -0.0441   |\n",
      "|    value_loss           | 0.108     |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 61       |\n",
      "| time/              |          |\n",
      "|    fps             | 175      |\n",
      "|    iterations      | 333      |\n",
      "|    time_elapsed    | 484      |\n",
      "|    total_timesteps | 85248    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=85500, episode_reward=9.23 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 9.23      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 85500     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.2342722 |\n",
      "|    clip_fraction        | 0.22      |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0788   |\n",
      "|    explained_variance   | 0.85      |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.165    |\n",
      "|    n_updates            | 6327      |\n",
      "|    policy_gradient_loss | -0.0817   |\n",
      "|    value_loss           | 0.0808    |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 61       |\n",
      "| time/              |          |\n",
      "|    fps             | 175      |\n",
      "|    iterations      | 334      |\n",
      "|    time_elapsed    | 485      |\n",
      "|    total_timesteps | 85504    |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 61        |\n",
      "| time/                   |           |\n",
      "|    fps                  | 175       |\n",
      "|    iterations           | 335       |\n",
      "|    time_elapsed         | 487       |\n",
      "|    total_timesteps      | 85760     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.2124072 |\n",
      "|    clip_fraction        | 0.221     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.126    |\n",
      "|    explained_variance   | 0.804     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.111    |\n",
      "|    n_updates            | 6346      |\n",
      "|    policy_gradient_loss | -0.0757   |\n",
      "|    value_loss           | 0.159     |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=86000, episode_reward=26.09 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 26.1      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 86000     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.6704784 |\n",
      "|    clip_fraction        | 0.203     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0817   |\n",
      "|    explained_variance   | 0.724     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.148    |\n",
      "|    n_updates            | 6365      |\n",
      "|    policy_gradient_loss | -0.0861   |\n",
      "|    value_loss           | 0.0409    |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 61       |\n",
      "| time/              |          |\n",
      "|    fps             | 175      |\n",
      "|    iterations      | 336      |\n",
      "|    time_elapsed    | 488      |\n",
      "|    total_timesteps | 86016    |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 61.1      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 176       |\n",
      "|    iterations           | 337       |\n",
      "|    time_elapsed         | 490       |\n",
      "|    total_timesteps      | 86272     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.0312319 |\n",
      "|    clip_fraction        | 0.176     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0694   |\n",
      "|    explained_variance   | 0.762     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.143    |\n",
      "|    n_updates            | 6384      |\n",
      "|    policy_gradient_loss | -0.0627   |\n",
      "|    value_loss           | 0.0787    |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=86500, episode_reward=4.39 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "--------------------------------------\n",
      "| eval/                   |          |\n",
      "|    mean_ep_length       | 230      |\n",
      "|    mean_reward          | 4.39     |\n",
      "| time/                   |          |\n",
      "|    total_timesteps      | 86500    |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 1.013899 |\n",
      "|    clip_fraction        | 0.176    |\n",
      "|    clip_range           | 0.4      |\n",
      "|    entropy_loss         | -0.0897  |\n",
      "|    explained_variance   | 0.826    |\n",
      "|    learning_rate        | 0.000389 |\n",
      "|    loss                 | -0.13    |\n",
      "|    n_updates            | 6403     |\n",
      "|    policy_gradient_loss | -0.0611  |\n",
      "|    value_loss           | 0.153    |\n",
      "--------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 61.1     |\n",
      "| time/              |          |\n",
      "|    fps             | 175      |\n",
      "|    iterations      | 338      |\n",
      "|    time_elapsed    | 491      |\n",
      "|    total_timesteps | 86528    |\n",
      "---------------------------------\n",
      "--------------------------------------\n",
      "| rollout/                |          |\n",
      "|    ep_len_mean          | 889      |\n",
      "|    ep_rew_mean          | 61.1     |\n",
      "| time/                   |          |\n",
      "|    fps                  | 176      |\n",
      "|    iterations           | 339      |\n",
      "|    time_elapsed         | 492      |\n",
      "|    total_timesteps      | 86784    |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 1.245925 |\n",
      "|    clip_fraction        | 0.262    |\n",
      "|    clip_range           | 0.4      |\n",
      "|    entropy_loss         | -0.104   |\n",
      "|    explained_variance   | 0.19     |\n",
      "|    learning_rate        | 0.000389 |\n",
      "|    loss                 | -0.128   |\n",
      "|    n_updates            | 6422     |\n",
      "|    policy_gradient_loss | -0.101   |\n",
      "|    value_loss           | 0.0407   |\n",
      "--------------------------------------\n",
      "Eval num_timesteps=87000, episode_reward=27.86 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 27.9      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 87000     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.0990818 |\n",
      "|    clip_fraction        | 0.186     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0773   |\n",
      "|    explained_variance   | 0.767     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0994   |\n",
      "|    n_updates            | 6441      |\n",
      "|    policy_gradient_loss | -0.0675   |\n",
      "|    value_loss           | 0.0937    |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 61.1     |\n",
      "| time/              |          |\n",
      "|    fps             | 176      |\n",
      "|    iterations      | 340      |\n",
      "|    time_elapsed    | 494      |\n",
      "|    total_timesteps | 87040    |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 61.1       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 176        |\n",
      "|    iterations           | 341        |\n",
      "|    time_elapsed         | 495        |\n",
      "|    total_timesteps      | 87296      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.63212657 |\n",
      "|    clip_fraction        | 0.134      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0594    |\n",
      "|    explained_variance   | 0.559      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.00921   |\n",
      "|    n_updates            | 6460       |\n",
      "|    policy_gradient_loss | -0.0464    |\n",
      "|    value_loss           | 0.253      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=87500, episode_reward=33.55 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 33.6      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 87500     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.6032082 |\n",
      "|    clip_fraction        | 0.188     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.122    |\n",
      "|    explained_variance   | 0.414     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.095    |\n",
      "|    n_updates            | 6479      |\n",
      "|    policy_gradient_loss | -0.0433   |\n",
      "|    value_loss           | 0.389     |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 61.1     |\n",
      "| time/              |          |\n",
      "|    fps             | 176      |\n",
      "|    iterations      | 342      |\n",
      "|    time_elapsed    | 497      |\n",
      "|    total_timesteps | 87552    |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 61.1      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 176       |\n",
      "|    iterations           | 343       |\n",
      "|    time_elapsed         | 498       |\n",
      "|    total_timesteps      | 87808     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.9917754 |\n",
      "|    clip_fraction        | 0.133     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.075    |\n",
      "|    explained_variance   | 0.458     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0451   |\n",
      "|    n_updates            | 6498      |\n",
      "|    policy_gradient_loss | -0.0553   |\n",
      "|    value_loss           | 0.147     |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=88000, episode_reward=29.62 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 230        |\n",
      "|    mean_reward          | 29.6       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 88000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.74576735 |\n",
      "|    clip_fraction        | 0.165      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0842    |\n",
      "|    explained_variance   | 0.858      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.00654   |\n",
      "|    n_updates            | 6517       |\n",
      "|    policy_gradient_loss | -0.0577    |\n",
      "|    value_loss           | 0.103      |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 61.2     |\n",
      "| time/              |          |\n",
      "|    fps             | 176      |\n",
      "|    iterations      | 344      |\n",
      "|    time_elapsed    | 500      |\n",
      "|    total_timesteps | 88064    |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 61.2      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 175       |\n",
      "|    iterations           | 345       |\n",
      "|    time_elapsed         | 502       |\n",
      "|    total_timesteps      | 88320     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.6976318 |\n",
      "|    clip_fraction        | 0.152     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0848   |\n",
      "|    explained_variance   | 0.627     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | 0.0138    |\n",
      "|    n_updates            | 6536      |\n",
      "|    policy_gradient_loss | -0.0413   |\n",
      "|    value_loss           | 0.522     |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=88500, episode_reward=-0.98 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | -0.976    |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 88500     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.8803562 |\n",
      "|    clip_fraction        | 0.202     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0857   |\n",
      "|    explained_variance   | 0.504     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0663   |\n",
      "|    n_updates            | 6555      |\n",
      "|    policy_gradient_loss | -0.0701   |\n",
      "|    value_loss           | 0.0397    |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 61.2     |\n",
      "| time/              |          |\n",
      "|    fps             | 175      |\n",
      "|    iterations      | 346      |\n",
      "|    time_elapsed    | 503      |\n",
      "|    total_timesteps | 88576    |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 61.2      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 175       |\n",
      "|    iterations           | 347       |\n",
      "|    time_elapsed         | 505       |\n",
      "|    total_timesteps      | 88832     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.0142908 |\n",
      "|    clip_fraction        | 0.158     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0656   |\n",
      "|    explained_variance   | 0.755     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | 0.141     |\n",
      "|    n_updates            | 6574      |\n",
      "|    policy_gradient_loss | -0.0546   |\n",
      "|    value_loss           | 0.192     |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=89000, episode_reward=2.44 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 2.44      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 89000     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.6731547 |\n",
      "|    clip_fraction        | 0.136     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0631   |\n",
      "|    explained_variance   | 0.83      |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.083    |\n",
      "|    n_updates            | 6593      |\n",
      "|    policy_gradient_loss | -0.0605   |\n",
      "|    value_loss           | 0.175     |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 61.1     |\n",
      "| time/              |          |\n",
      "|    fps             | 175      |\n",
      "|    iterations      | 348      |\n",
      "|    time_elapsed    | 507      |\n",
      "|    total_timesteps | 89088    |\n",
      "---------------------------------\n",
      "--------------------------------------\n",
      "| rollout/                |          |\n",
      "|    ep_len_mean          | 889      |\n",
      "|    ep_rew_mean          | 61.1     |\n",
      "| time/                   |          |\n",
      "|    fps                  | 175      |\n",
      "|    iterations           | 349      |\n",
      "|    time_elapsed         | 508      |\n",
      "|    total_timesteps      | 89344    |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 1.479846 |\n",
      "|    clip_fraction        | 0.193    |\n",
      "|    clip_range           | 0.4      |\n",
      "|    entropy_loss         | -0.0811  |\n",
      "|    explained_variance   | 0.28     |\n",
      "|    learning_rate        | 0.000389 |\n",
      "|    loss                 | -0.0404  |\n",
      "|    n_updates            | 6612     |\n",
      "|    policy_gradient_loss | -0.0564  |\n",
      "|    value_loss           | 0.0863   |\n",
      "--------------------------------------\n",
      "Eval num_timesteps=89500, episode_reward=24.64 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 24.6      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 89500     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.0298076 |\n",
      "|    clip_fraction        | 0.184     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0676   |\n",
      "|    explained_variance   | 0.673     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.108    |\n",
      "|    n_updates            | 6631      |\n",
      "|    policy_gradient_loss | -0.0622   |\n",
      "|    value_loss           | 0.127     |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 61.1     |\n",
      "| time/              |          |\n",
      "|    fps             | 175      |\n",
      "|    iterations      | 350      |\n",
      "|    time_elapsed    | 510      |\n",
      "|    total_timesteps | 89600    |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 61.3      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 175       |\n",
      "|    iterations           | 351       |\n",
      "|    time_elapsed         | 511       |\n",
      "|    total_timesteps      | 89856     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.2476829 |\n",
      "|    clip_fraction        | 0.181     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0799   |\n",
      "|    explained_variance   | 0.405     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0795   |\n",
      "|    n_updates            | 6650      |\n",
      "|    policy_gradient_loss | -0.0535   |\n",
      "|    value_loss           | 0.265     |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=90000, episode_reward=61.13 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 230        |\n",
      "|    mean_reward          | 61.1       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 90000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.75378436 |\n",
      "|    clip_fraction        | 0.192      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0958    |\n",
      "|    explained_variance   | 0.824      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0389    |\n",
      "|    n_updates            | 6669       |\n",
      "|    policy_gradient_loss | -0.0387    |\n",
      "|    value_loss           | 0.145      |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 61.3     |\n",
      "| time/              |          |\n",
      "|    fps             | 175      |\n",
      "|    iterations      | 352      |\n",
      "|    time_elapsed    | 513      |\n",
      "|    total_timesteps | 90112    |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 61.3      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 175       |\n",
      "|    iterations           | 353       |\n",
      "|    time_elapsed         | 514       |\n",
      "|    total_timesteps      | 90368     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.8217348 |\n",
      "|    clip_fraction        | 0.225     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0985   |\n",
      "|    explained_variance   | 0.506     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.105    |\n",
      "|    n_updates            | 6688      |\n",
      "|    policy_gradient_loss | -0.0699   |\n",
      "|    value_loss           | 0.0616    |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=90500, episode_reward=91.32 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 91.3      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 90500     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.2342964 |\n",
      "|    clip_fraction        | 0.176     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0706   |\n",
      "|    explained_variance   | 0.74      |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0508   |\n",
      "|    n_updates            | 6707      |\n",
      "|    policy_gradient_loss | -0.0651   |\n",
      "|    value_loss           | 0.205     |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 61.3     |\n",
      "| time/              |          |\n",
      "|    fps             | 175      |\n",
      "|    iterations      | 354      |\n",
      "|    time_elapsed    | 516      |\n",
      "|    total_timesteps | 90624    |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 61.6      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 175       |\n",
      "|    iterations           | 355       |\n",
      "|    time_elapsed         | 518       |\n",
      "|    total_timesteps      | 90880     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.5527483 |\n",
      "|    clip_fraction        | 0.14      |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0723   |\n",
      "|    explained_variance   | 0.915     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0227   |\n",
      "|    n_updates            | 6726      |\n",
      "|    policy_gradient_loss | -0.0516   |\n",
      "|    value_loss           | 0.0793    |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=91000, episode_reward=66.62 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 66.6      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 91000     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.8449069 |\n",
      "|    clip_fraction        | 0.215     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.116    |\n",
      "|    explained_variance   | 0.364     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0437   |\n",
      "|    n_updates            | 6745      |\n",
      "|    policy_gradient_loss | -0.0666   |\n",
      "|    value_loss           | 0.187     |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 61.6     |\n",
      "| time/              |          |\n",
      "|    fps             | 175      |\n",
      "|    iterations      | 356      |\n",
      "|    time_elapsed    | 520      |\n",
      "|    total_timesteps | 91136    |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 61.6      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 175       |\n",
      "|    iterations           | 357       |\n",
      "|    time_elapsed         | 521       |\n",
      "|    total_timesteps      | 91392     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.2944653 |\n",
      "|    clip_fraction        | 0.177     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0614   |\n",
      "|    explained_variance   | 0.596     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0977   |\n",
      "|    n_updates            | 6764      |\n",
      "|    policy_gradient_loss | -0.0511   |\n",
      "|    value_loss           | 0.0854    |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=91500, episode_reward=73.06 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 73.1      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 91500     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.1385801 |\n",
      "|    clip_fraction        | 0.156     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0574   |\n",
      "|    explained_variance   | 0.681     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0447   |\n",
      "|    n_updates            | 6783      |\n",
      "|    policy_gradient_loss | -0.0506   |\n",
      "|    value_loss           | 0.134     |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 61.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 175      |\n",
      "|    iterations      | 358      |\n",
      "|    time_elapsed    | 523      |\n",
      "|    total_timesteps | 91648    |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 61.9       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 175        |\n",
      "|    iterations           | 359        |\n",
      "|    time_elapsed         | 525        |\n",
      "|    total_timesteps      | 91904      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.91278815 |\n",
      "|    clip_fraction        | 0.212      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0996    |\n",
      "|    explained_variance   | 0.811      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | 0.108      |\n",
      "|    n_updates            | 6802       |\n",
      "|    policy_gradient_loss | -0.0585    |\n",
      "|    value_loss           | 0.281      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=92000, episode_reward=72.57 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 72.6      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 92000     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.6397974 |\n",
      "|    clip_fraction        | 0.188     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0929   |\n",
      "|    explained_variance   | 0.643     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0667   |\n",
      "|    n_updates            | 6821      |\n",
      "|    policy_gradient_loss | -0.0681   |\n",
      "|    value_loss           | 0.0393    |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 61.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 174      |\n",
      "|    iterations      | 360      |\n",
      "|    time_elapsed    | 526      |\n",
      "|    total_timesteps | 92160    |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 61.9      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 174       |\n",
      "|    iterations           | 361       |\n",
      "|    time_elapsed         | 528       |\n",
      "|    total_timesteps      | 92416     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.4965451 |\n",
      "|    clip_fraction        | 0.182     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0776   |\n",
      "|    explained_variance   | 0.183     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.131    |\n",
      "|    n_updates            | 6840      |\n",
      "|    policy_gradient_loss | -0.0693   |\n",
      "|    value_loss           | 0.234     |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=92500, episode_reward=67.13 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 67.1      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 92500     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.7272798 |\n",
      "|    clip_fraction        | 0.167     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0823   |\n",
      "|    explained_variance   | 0.164     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0317   |\n",
      "|    n_updates            | 6859      |\n",
      "|    policy_gradient_loss | -0.058    |\n",
      "|    value_loss           | 0.278     |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 62.1     |\n",
      "| time/              |          |\n",
      "|    fps             | 174      |\n",
      "|    iterations      | 362      |\n",
      "|    time_elapsed    | 530      |\n",
      "|    total_timesteps | 92672    |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 62.1      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 174       |\n",
      "|    iterations           | 363       |\n",
      "|    time_elapsed         | 532       |\n",
      "|    total_timesteps      | 92928     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.5036869 |\n",
      "|    clip_fraction        | 0.224     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.118    |\n",
      "|    explained_variance   | 0.73      |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.102    |\n",
      "|    n_updates            | 6878      |\n",
      "|    policy_gradient_loss | -0.0665   |\n",
      "|    value_loss           | 0.075     |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=93000, episode_reward=51.88 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 230        |\n",
      "|    mean_reward          | 51.9       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 93000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.85732484 |\n",
      "|    clip_fraction        | 0.157      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0554    |\n",
      "|    explained_variance   | 0.406      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0461    |\n",
      "|    n_updates            | 6897       |\n",
      "|    policy_gradient_loss | -0.0443    |\n",
      "|    value_loss           | 0.124      |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 62.1     |\n",
      "| time/              |          |\n",
      "|    fps             | 174      |\n",
      "|    iterations      | 364      |\n",
      "|    time_elapsed    | 533      |\n",
      "|    total_timesteps | 93184    |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 62.2      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 174       |\n",
      "|    iterations           | 365       |\n",
      "|    time_elapsed         | 535       |\n",
      "|    total_timesteps      | 93440     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.9009043 |\n",
      "|    clip_fraction        | 0.158     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0786   |\n",
      "|    explained_variance   | 0.502     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0678   |\n",
      "|    n_updates            | 6916      |\n",
      "|    policy_gradient_loss | -0.0613   |\n",
      "|    value_loss           | 0.225     |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=93500, episode_reward=29.68 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 29.7      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 93500     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.9512762 |\n",
      "|    clip_fraction        | 0.255     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.128    |\n",
      "|    explained_variance   | 0.564     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.115    |\n",
      "|    n_updates            | 6935      |\n",
      "|    policy_gradient_loss | -0.0686   |\n",
      "|    value_loss           | 0.251     |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 62.2     |\n",
      "| time/              |          |\n",
      "|    fps             | 174      |\n",
      "|    iterations      | 366      |\n",
      "|    time_elapsed    | 537      |\n",
      "|    total_timesteps | 93696    |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 62.2       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 174        |\n",
      "|    iterations           | 367        |\n",
      "|    time_elapsed         | 538        |\n",
      "|    total_timesteps      | 93952      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.73907316 |\n",
      "|    clip_fraction        | 0.206      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.1       |\n",
      "|    explained_variance   | 0.183      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.128     |\n",
      "|    n_updates            | 6954       |\n",
      "|    policy_gradient_loss | -0.0575    |\n",
      "|    value_loss           | 0.0805     |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=94000, episode_reward=67.81 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 67.8      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 94000     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.5213601 |\n",
      "|    clip_fraction        | 0.119     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0659   |\n",
      "|    explained_variance   | 0.566     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.121    |\n",
      "|    n_updates            | 6973      |\n",
      "|    policy_gradient_loss | -0.0538   |\n",
      "|    value_loss           | 0.216     |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 62.2     |\n",
      "| time/              |          |\n",
      "|    fps             | 174      |\n",
      "|    iterations      | 368      |\n",
      "|    time_elapsed    | 540      |\n",
      "|    total_timesteps | 94208    |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 62.2      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 174       |\n",
      "|    iterations           | 369       |\n",
      "|    time_elapsed         | 541       |\n",
      "|    total_timesteps      | 94464     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.6734297 |\n",
      "|    clip_fraction        | 0.242     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0793   |\n",
      "|    explained_variance   | 0.592     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0612   |\n",
      "|    n_updates            | 6992      |\n",
      "|    policy_gradient_loss | -0.0429   |\n",
      "|    value_loss           | 0.238     |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=94500, episode_reward=58.48 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 58.5      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 94500     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.7304881 |\n",
      "|    clip_fraction        | 0.243     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.137    |\n",
      "|    explained_variance   | -0.143    |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0583   |\n",
      "|    n_updates            | 7011      |\n",
      "|    policy_gradient_loss | -0.0849   |\n",
      "|    value_loss           | 0.0987    |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 62.2     |\n",
      "| time/              |          |\n",
      "|    fps             | 174      |\n",
      "|    iterations      | 370      |\n",
      "|    time_elapsed    | 543      |\n",
      "|    total_timesteps | 94720    |\n",
      "---------------------------------\n",
      "--------------------------------------\n",
      "| rollout/                |          |\n",
      "|    ep_len_mean          | 889      |\n",
      "|    ep_rew_mean          | 62.2     |\n",
      "| time/                   |          |\n",
      "|    fps                  | 174      |\n",
      "|    iterations           | 371      |\n",
      "|    time_elapsed         | 544      |\n",
      "|    total_timesteps      | 94976    |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 0.827343 |\n",
      "|    clip_fraction        | 0.173    |\n",
      "|    clip_range           | 0.4      |\n",
      "|    entropy_loss         | -0.0766  |\n",
      "|    explained_variance   | 0.507    |\n",
      "|    learning_rate        | 0.000389 |\n",
      "|    loss                 | -0.132   |\n",
      "|    n_updates            | 7030     |\n",
      "|    policy_gradient_loss | -0.0641  |\n",
      "|    value_loss           | 0.0938   |\n",
      "--------------------------------------\n",
      "Eval num_timesteps=95000, episode_reward=45.67 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 45.7      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 95000     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.0802587 |\n",
      "|    clip_fraction        | 0.193     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0676   |\n",
      "|    explained_variance   | 0.614     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.144    |\n",
      "|    n_updates            | 7049      |\n",
      "|    policy_gradient_loss | -0.0682   |\n",
      "|    value_loss           | 0.121     |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 62.3     |\n",
      "| time/              |          |\n",
      "|    fps             | 174      |\n",
      "|    iterations      | 372      |\n",
      "|    time_elapsed    | 546      |\n",
      "|    total_timesteps | 95232    |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 62.3       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 174        |\n",
      "|    iterations           | 373        |\n",
      "|    time_elapsed         | 547        |\n",
      "|    total_timesteps      | 95488      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.77692556 |\n",
      "|    clip_fraction        | 0.178      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.124     |\n",
      "|    explained_variance   | -0.175     |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0732    |\n",
      "|    n_updates            | 7068       |\n",
      "|    policy_gradient_loss | -0.0605    |\n",
      "|    value_loss           | 2.17       |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=95500, episode_reward=29.66 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "--------------------------------------\n",
      "| eval/                   |          |\n",
      "|    mean_ep_length       | 230      |\n",
      "|    mean_reward          | 29.7     |\n",
      "| time/                   |          |\n",
      "|    total_timesteps      | 95500    |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 0.668236 |\n",
      "|    clip_fraction        | 0.178    |\n",
      "|    clip_range           | 0.4      |\n",
      "|    entropy_loss         | -0.0763  |\n",
      "|    explained_variance   | -0.0314  |\n",
      "|    learning_rate        | 0.000389 |\n",
      "|    loss                 | -0.0735  |\n",
      "|    n_updates            | 7087     |\n",
      "|    policy_gradient_loss | -0.0576  |\n",
      "|    value_loss           | 0.0674   |\n",
      "--------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 62.3     |\n",
      "| time/              |          |\n",
      "|    fps             | 174      |\n",
      "|    iterations      | 374      |\n",
      "|    time_elapsed    | 549      |\n",
      "|    total_timesteps | 95744    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=96000, episode_reward=69.84 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 230        |\n",
      "|    mean_reward          | 69.8       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 96000      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.40371987 |\n",
      "|    clip_fraction        | 0.118      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0572    |\n",
      "|    explained_variance   | 0.446      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.00205   |\n",
      "|    n_updates            | 7106       |\n",
      "|    policy_gradient_loss | -0.0367    |\n",
      "|    value_loss           | 0.129      |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 62.3     |\n",
      "| time/              |          |\n",
      "|    fps             | 174      |\n",
      "|    iterations      | 375      |\n",
      "|    time_elapsed    | 550      |\n",
      "|    total_timesteps | 96000    |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 62.4      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 174       |\n",
      "|    iterations           | 376       |\n",
      "|    time_elapsed         | 551       |\n",
      "|    total_timesteps      | 96256     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.2370358 |\n",
      "|    clip_fraction        | 0.175     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0882   |\n",
      "|    explained_variance   | 0.35      |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.101    |\n",
      "|    n_updates            | 7125      |\n",
      "|    policy_gradient_loss | -0.0615   |\n",
      "|    value_loss           | 2.01      |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=96500, episode_reward=74.88 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 230        |\n",
      "|    mean_reward          | 74.9       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 96500      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.89651334 |\n",
      "|    clip_fraction        | 0.256      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.114     |\n",
      "|    explained_variance   | 0.47       |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.138     |\n",
      "|    n_updates            | 7144       |\n",
      "|    policy_gradient_loss | -0.0643    |\n",
      "|    value_loss           | 0.0628     |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 62.4     |\n",
      "| time/              |          |\n",
      "|    fps             | 174      |\n",
      "|    iterations      | 377      |\n",
      "|    time_elapsed    | 553      |\n",
      "|    total_timesteps | 96512    |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 62.4      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 174       |\n",
      "|    iterations           | 378       |\n",
      "|    time_elapsed         | 554       |\n",
      "|    total_timesteps      | 96768     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.9984306 |\n",
      "|    clip_fraction        | 0.146     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0469   |\n",
      "|    explained_variance   | 0.559     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0651   |\n",
      "|    n_updates            | 7163      |\n",
      "|    policy_gradient_loss | -0.0419   |\n",
      "|    value_loss           | 0.093     |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=97000, episode_reward=67.75 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 67.7      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 97000     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.8622317 |\n",
      "|    clip_fraction        | 0.125     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0423   |\n",
      "|    explained_variance   | 0.675     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0412   |\n",
      "|    n_updates            | 7182      |\n",
      "|    policy_gradient_loss | -0.0489   |\n",
      "|    value_loss           | 0.1       |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 62.7     |\n",
      "| time/              |          |\n",
      "|    fps             | 174      |\n",
      "|    iterations      | 379      |\n",
      "|    time_elapsed    | 556      |\n",
      "|    total_timesteps | 97024    |\n",
      "---------------------------------\n",
      "--------------------------------------\n",
      "| rollout/                |          |\n",
      "|    ep_len_mean          | 889      |\n",
      "|    ep_rew_mean          | 62.7     |\n",
      "| time/                   |          |\n",
      "|    fps                  | 174      |\n",
      "|    iterations           | 380      |\n",
      "|    time_elapsed         | 557      |\n",
      "|    total_timesteps      | 97280    |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 1.043956 |\n",
      "|    clip_fraction        | 0.237    |\n",
      "|    clip_range           | 0.4      |\n",
      "|    entropy_loss         | -0.119   |\n",
      "|    explained_variance   | 0.646    |\n",
      "|    learning_rate        | 0.000389 |\n",
      "|    loss                 | -0.00266 |\n",
      "|    n_updates            | 7201     |\n",
      "|    policy_gradient_loss | -0.055   |\n",
      "|    value_loss           | 1.03     |\n",
      "--------------------------------------\n",
      "Eval num_timesteps=97500, episode_reward=30.93 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 30.9      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 97500     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.2395521 |\n",
      "|    clip_fraction        | 0.182     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0623   |\n",
      "|    explained_variance   | 0.439     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0833   |\n",
      "|    n_updates            | 7220      |\n",
      "|    policy_gradient_loss | -0.0813   |\n",
      "|    value_loss           | 0.0474    |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 62.7     |\n",
      "| time/              |          |\n",
      "|    fps             | 174      |\n",
      "|    iterations      | 381      |\n",
      "|    time_elapsed    | 559      |\n",
      "|    total_timesteps | 97536    |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 63        |\n",
      "| time/                   |           |\n",
      "|    fps                  | 174       |\n",
      "|    iterations           | 382       |\n",
      "|    time_elapsed         | 560       |\n",
      "|    total_timesteps      | 97792     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.8606298 |\n",
      "|    clip_fraction        | 0.141     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0535   |\n",
      "|    explained_variance   | 0.705     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.11     |\n",
      "|    n_updates            | 7239      |\n",
      "|    policy_gradient_loss | -0.0544   |\n",
      "|    value_loss           | 0.128     |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=98000, episode_reward=16.08 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 16.1      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 98000     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.8574955 |\n",
      "|    clip_fraction        | 0.18      |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0752   |\n",
      "|    explained_variance   | 0.754     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0517   |\n",
      "|    n_updates            | 7258      |\n",
      "|    policy_gradient_loss | -0.0117   |\n",
      "|    value_loss           | 0.671     |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 63       |\n",
      "| time/              |          |\n",
      "|    fps             | 174      |\n",
      "|    iterations      | 383      |\n",
      "|    time_elapsed    | 562      |\n",
      "|    total_timesteps | 98048    |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 63        |\n",
      "| time/                   |           |\n",
      "|    fps                  | 174       |\n",
      "|    iterations           | 384       |\n",
      "|    time_elapsed         | 563       |\n",
      "|    total_timesteps      | 98304     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.2487099 |\n",
      "|    clip_fraction        | 0.267     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.115    |\n",
      "|    explained_variance   | 0.129     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.132    |\n",
      "|    n_updates            | 7277      |\n",
      "|    policy_gradient_loss | -0.0834   |\n",
      "|    value_loss           | 0.0408    |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=98500, episode_reward=30.85 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 30.9      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 98500     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.2369955 |\n",
      "|    clip_fraction        | 0.182     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0627   |\n",
      "|    explained_variance   | 0.662     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0924   |\n",
      "|    n_updates            | 7296      |\n",
      "|    policy_gradient_loss | -0.0589   |\n",
      "|    value_loss           | 0.0943    |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 63       |\n",
      "| time/              |          |\n",
      "|    fps             | 174      |\n",
      "|    iterations      | 385      |\n",
      "|    time_elapsed    | 564      |\n",
      "|    total_timesteps | 98560    |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 63.2      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 174       |\n",
      "|    iterations           | 386       |\n",
      "|    time_elapsed         | 566       |\n",
      "|    total_timesteps      | 98816     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.2534974 |\n",
      "|    clip_fraction        | 0.198     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0674   |\n",
      "|    explained_variance   | 0.757     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0799   |\n",
      "|    n_updates            | 7315      |\n",
      "|    policy_gradient_loss | -0.0812   |\n",
      "|    value_loss           | 0.0835    |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=99000, episode_reward=18.32 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 18.3      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 99000     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.2589505 |\n",
      "|    clip_fraction        | 0.211     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.093    |\n",
      "|    explained_variance   | 0.83      |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.022    |\n",
      "|    n_updates            | 7334      |\n",
      "|    policy_gradient_loss | -0.065    |\n",
      "|    value_loss           | 0.385     |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 63.2     |\n",
      "| time/              |          |\n",
      "|    fps             | 174      |\n",
      "|    iterations      | 387      |\n",
      "|    time_elapsed    | 567      |\n",
      "|    total_timesteps | 99072    |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 63.2      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 174       |\n",
      "|    iterations           | 388       |\n",
      "|    time_elapsed         | 568       |\n",
      "|    total_timesteps      | 99328     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.2525744 |\n",
      "|    clip_fraction        | 0.169     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0543   |\n",
      "|    explained_variance   | 0.404     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0401   |\n",
      "|    n_updates            | 7353      |\n",
      "|    policy_gradient_loss | -0.0559   |\n",
      "|    value_loss           | 0.0772    |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=99500, episode_reward=23.81 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "--------------------------------------\n",
      "| eval/                   |          |\n",
      "|    mean_ep_length       | 230      |\n",
      "|    mean_reward          | 23.8     |\n",
      "| time/                   |          |\n",
      "|    total_timesteps      | 99500    |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 1.152108 |\n",
      "|    clip_fraction        | 0.132    |\n",
      "|    clip_range           | 0.4      |\n",
      "|    entropy_loss         | -0.0572  |\n",
      "|    explained_variance   | 0.756    |\n",
      "|    learning_rate        | 0.000389 |\n",
      "|    loss                 | -0.0677  |\n",
      "|    n_updates            | 7372     |\n",
      "|    policy_gradient_loss | -0.0539  |\n",
      "|    value_loss           | 0.0888   |\n",
      "--------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 63.3     |\n",
      "| time/              |          |\n",
      "|    fps             | 174      |\n",
      "|    iterations      | 389      |\n",
      "|    time_elapsed    | 570      |\n",
      "|    total_timesteps | 99584    |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 63.3       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 174        |\n",
      "|    iterations           | 390        |\n",
      "|    time_elapsed         | 571        |\n",
      "|    total_timesteps      | 99840      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.44377178 |\n",
      "|    clip_fraction        | 0.125      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0695    |\n",
      "|    explained_variance   | -0.0749    |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.11      |\n",
      "|    n_updates            | 7391       |\n",
      "|    policy_gradient_loss | -0.0602    |\n",
      "|    value_loss           | 1.2        |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=100000, episode_reward=18.72 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 230        |\n",
      "|    mean_reward          | 18.7       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 100000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.69198006 |\n",
      "|    clip_fraction        | 0.206      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0905    |\n",
      "|    explained_variance   | -0.163     |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.135     |\n",
      "|    n_updates            | 7410       |\n",
      "|    policy_gradient_loss | -0.0739    |\n",
      "|    value_loss           | 0.0728     |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 63.3     |\n",
      "| time/              |          |\n",
      "|    fps             | 174      |\n",
      "|    iterations      | 391      |\n",
      "|    time_elapsed    | 573      |\n",
      "|    total_timesteps | 100096   |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 63.3      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 174       |\n",
      "|    iterations           | 392       |\n",
      "|    time_elapsed         | 574       |\n",
      "|    total_timesteps      | 100352    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.1828882 |\n",
      "|    clip_fraction        | 0.188     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0528   |\n",
      "|    explained_variance   | 0.56      |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.00474  |\n",
      "|    n_updates            | 7429      |\n",
      "|    policy_gradient_loss | -0.0596   |\n",
      "|    value_loss           | 0.189     |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=100500, episode_reward=11.98 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 12        |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 100500    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.6937324 |\n",
      "|    clip_fraction        | 0.123     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0564   |\n",
      "|    explained_variance   | 0.592     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.155    |\n",
      "|    n_updates            | 7448      |\n",
      "|    policy_gradient_loss | -0.0471   |\n",
      "|    value_loss           | 0.133     |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 63.3     |\n",
      "| time/              |          |\n",
      "|    fps             | 174      |\n",
      "|    iterations      | 393      |\n",
      "|    time_elapsed    | 575      |\n",
      "|    total_timesteps | 100608   |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 63.3      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 174       |\n",
      "|    iterations           | 394       |\n",
      "|    time_elapsed         | 577       |\n",
      "|    total_timesteps      | 100864    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.5624058 |\n",
      "|    clip_fraction        | 0.206     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.116    |\n",
      "|    explained_variance   | 0.607     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | 0.283     |\n",
      "|    n_updates            | 7467      |\n",
      "|    policy_gradient_loss | -0.0643   |\n",
      "|    value_loss           | 0.433     |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=101000, episode_reward=23.21 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 23.2      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 101000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.9807875 |\n",
      "|    clip_fraction        | 0.143     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0398   |\n",
      "|    explained_variance   | 0.649     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.104    |\n",
      "|    n_updates            | 7486      |\n",
      "|    policy_gradient_loss | -0.0509   |\n",
      "|    value_loss           | 0.0885    |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 63.3     |\n",
      "| time/              |          |\n",
      "|    fps             | 174      |\n",
      "|    iterations      | 395      |\n",
      "|    time_elapsed    | 578      |\n",
      "|    total_timesteps | 101120   |\n",
      "---------------------------------\n",
      "--------------------------------------\n",
      "| rollout/                |          |\n",
      "|    ep_len_mean          | 889      |\n",
      "|    ep_rew_mean          | 63.6     |\n",
      "| time/                   |          |\n",
      "|    fps                  | 174      |\n",
      "|    iterations           | 396      |\n",
      "|    time_elapsed         | 579      |\n",
      "|    total_timesteps      | 101376   |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 1.173624 |\n",
      "|    clip_fraction        | 0.226    |\n",
      "|    clip_range           | 0.4      |\n",
      "|    entropy_loss         | -0.0684  |\n",
      "|    explained_variance   | 0.498    |\n",
      "|    learning_rate        | 0.000389 |\n",
      "|    loss                 | -0.077   |\n",
      "|    n_updates            | 7505     |\n",
      "|    policy_gradient_loss | -0.0554  |\n",
      "|    value_loss           | 0.4      |\n",
      "--------------------------------------\n",
      "Eval num_timesteps=101500, episode_reward=35.45 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 35.4      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 101500    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.7006047 |\n",
      "|    clip_fraction        | 0.149     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0827   |\n",
      "|    explained_variance   | 0.751     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0625   |\n",
      "|    n_updates            | 7524      |\n",
      "|    policy_gradient_loss | -0.0588   |\n",
      "|    value_loss           | 0.273     |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 63.6     |\n",
      "| time/              |          |\n",
      "|    fps             | 174      |\n",
      "|    iterations      | 397      |\n",
      "|    time_elapsed    | 581      |\n",
      "|    total_timesteps | 101632   |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 63.6      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 174       |\n",
      "|    iterations           | 398       |\n",
      "|    time_elapsed         | 582       |\n",
      "|    total_timesteps      | 101888    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.6310574 |\n",
      "|    clip_fraction        | 0.201     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0917   |\n",
      "|    explained_variance   | 0.156     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.156    |\n",
      "|    n_updates            | 7543      |\n",
      "|    policy_gradient_loss | -0.0721   |\n",
      "|    value_loss           | 0.0398    |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=102000, episode_reward=26.94 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 26.9      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 102000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.9775245 |\n",
      "|    clip_fraction        | 0.168     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0671   |\n",
      "|    explained_variance   | 0.801     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0496   |\n",
      "|    n_updates            | 7562      |\n",
      "|    policy_gradient_loss | -0.055    |\n",
      "|    value_loss           | 0.202     |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 63.6     |\n",
      "| time/              |          |\n",
      "|    fps             | 174      |\n",
      "|    iterations      | 399      |\n",
      "|    time_elapsed    | 583      |\n",
      "|    total_timesteps | 102144   |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 63.7       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 175        |\n",
      "|    iterations           | 400        |\n",
      "|    time_elapsed         | 585        |\n",
      "|    total_timesteps      | 102400     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.88462126 |\n",
      "|    clip_fraction        | 0.18       |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0824    |\n",
      "|    explained_variance   | 0.68       |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | 0.0328     |\n",
      "|    n_updates            | 7581       |\n",
      "|    policy_gradient_loss | -0.0651    |\n",
      "|    value_loss           | 0.134      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=102500, episode_reward=26.70 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 26.7      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 102500    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.6493073 |\n",
      "|    clip_fraction        | 0.209     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.127    |\n",
      "|    explained_variance   | 0.167     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0314   |\n",
      "|    n_updates            | 7600      |\n",
      "|    policy_gradient_loss | -0.0566   |\n",
      "|    value_loss           | 1.24      |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 63.7     |\n",
      "| time/              |          |\n",
      "|    fps             | 174      |\n",
      "|    iterations      | 401      |\n",
      "|    time_elapsed    | 586      |\n",
      "|    total_timesteps | 102656   |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 63.7      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 175       |\n",
      "|    iterations           | 402       |\n",
      "|    time_elapsed         | 587       |\n",
      "|    total_timesteps      | 102912    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.8288665 |\n",
      "|    clip_fraction        | 0.156     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0618   |\n",
      "|    explained_variance   | 0.906     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.117    |\n",
      "|    n_updates            | 7619      |\n",
      "|    policy_gradient_loss | -0.0551   |\n",
      "|    value_loss           | 0.0824    |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=103000, episode_reward=46.30 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 46.3      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 103000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.2088983 |\n",
      "|    clip_fraction        | 0.199     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0842   |\n",
      "|    explained_variance   | 0.64      |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0529   |\n",
      "|    n_updates            | 7638      |\n",
      "|    policy_gradient_loss | -0.0647   |\n",
      "|    value_loss           | 0.147     |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 63.7     |\n",
      "| time/              |          |\n",
      "|    fps             | 175      |\n",
      "|    iterations      | 403      |\n",
      "|    time_elapsed    | 589      |\n",
      "|    total_timesteps | 103168   |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 63.7       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 175        |\n",
      "|    iterations           | 404        |\n",
      "|    time_elapsed         | 590        |\n",
      "|    total_timesteps      | 103424     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.95506954 |\n",
      "|    clip_fraction        | 0.228      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.111     |\n",
      "|    explained_variance   | 0.494      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0769    |\n",
      "|    n_updates            | 7657       |\n",
      "|    policy_gradient_loss | -0.0787    |\n",
      "|    value_loss           | 0.839      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=103500, episode_reward=45.87 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 45.9      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 103500    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.6320146 |\n",
      "|    clip_fraction        | 0.158     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0689   |\n",
      "|    explained_variance   | 0.616     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0731   |\n",
      "|    n_updates            | 7676      |\n",
      "|    policy_gradient_loss | -0.0619   |\n",
      "|    value_loss           | 0.0438    |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 63.7     |\n",
      "| time/              |          |\n",
      "|    fps             | 175      |\n",
      "|    iterations      | 405      |\n",
      "|    time_elapsed    | 592      |\n",
      "|    total_timesteps | 103680   |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 63.7      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 175       |\n",
      "|    iterations           | 406       |\n",
      "|    time_elapsed         | 593       |\n",
      "|    total_timesteps      | 103936    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.7487899 |\n",
      "|    clip_fraction        | 0.123     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0524   |\n",
      "|    explained_variance   | 0.787     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0534   |\n",
      "|    n_updates            | 7695      |\n",
      "|    policy_gradient_loss | -0.0514   |\n",
      "|    value_loss           | 0.195     |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=104000, episode_reward=-3.90 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | -3.9      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 104000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.9624499 |\n",
      "|    clip_fraction        | 0.203     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.085    |\n",
      "|    explained_variance   | 0.9       |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.132    |\n",
      "|    n_updates            | 7714      |\n",
      "|    policy_gradient_loss | -0.0729   |\n",
      "|    value_loss           | 0.0879    |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 63.7     |\n",
      "| time/              |          |\n",
      "|    fps             | 175      |\n",
      "|    iterations      | 407      |\n",
      "|    time_elapsed    | 594      |\n",
      "|    total_timesteps | 104192   |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 63.7      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 175       |\n",
      "|    iterations           | 408       |\n",
      "|    time_elapsed         | 596       |\n",
      "|    total_timesteps      | 104448    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.6595392 |\n",
      "|    clip_fraction        | 0.191     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.111    |\n",
      "|    explained_variance   | 0.421     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | 0.00517   |\n",
      "|    n_updates            | 7733      |\n",
      "|    policy_gradient_loss | -0.0505   |\n",
      "|    value_loss           | 0.215     |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=104500, episode_reward=32.56 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 32.6      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 104500    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.0572294 |\n",
      "|    clip_fraction        | 0.17      |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0674   |\n",
      "|    explained_variance   | 0.777     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.109    |\n",
      "|    n_updates            | 7752      |\n",
      "|    policy_gradient_loss | -0.0636   |\n",
      "|    value_loss           | 0.0941    |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 63.7     |\n",
      "| time/              |          |\n",
      "|    fps             | 175      |\n",
      "|    iterations      | 409      |\n",
      "|    time_elapsed    | 597      |\n",
      "|    total_timesteps | 104704   |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 63.6      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 175       |\n",
      "|    iterations           | 410       |\n",
      "|    time_elapsed         | 598       |\n",
      "|    total_timesteps      | 104960    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.8232045 |\n",
      "|    clip_fraction        | 0.188     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.083    |\n",
      "|    explained_variance   | 0.668     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0606   |\n",
      "|    n_updates            | 7771      |\n",
      "|    policy_gradient_loss | -0.0696   |\n",
      "|    value_loss           | 0.195     |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=105000, episode_reward=16.43 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 16.4      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 105000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.7998105 |\n",
      "|    clip_fraction        | 0.171     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0906   |\n",
      "|    explained_variance   | 0.39      |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0455   |\n",
      "|    n_updates            | 7790      |\n",
      "|    policy_gradient_loss | -0.0551   |\n",
      "|    value_loss           | 0.684     |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 63.6     |\n",
      "| time/              |          |\n",
      "|    fps             | 175      |\n",
      "|    iterations      | 411      |\n",
      "|    time_elapsed    | 600      |\n",
      "|    total_timesteps | 105216   |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 63.6       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 175        |\n",
      "|    iterations           | 412        |\n",
      "|    time_elapsed         | 601        |\n",
      "|    total_timesteps      | 105472     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.87942344 |\n",
      "|    clip_fraction        | 0.214      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0967    |\n",
      "|    explained_variance   | 0.559      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0874    |\n",
      "|    n_updates            | 7809       |\n",
      "|    policy_gradient_loss | -0.0722    |\n",
      "|    value_loss           | 0.052      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=105500, episode_reward=9.83 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 9.83      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 105500    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.1805027 |\n",
      "|    clip_fraction        | 0.169     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0543   |\n",
      "|    explained_variance   | 0.637     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.143    |\n",
      "|    n_updates            | 7828      |\n",
      "|    policy_gradient_loss | -0.0539   |\n",
      "|    value_loss           | 0.222     |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 63.6     |\n",
      "| time/              |          |\n",
      "|    fps             | 175      |\n",
      "|    iterations      | 413      |\n",
      "|    time_elapsed    | 603      |\n",
      "|    total_timesteps | 105728   |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 63.8      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 175       |\n",
      "|    iterations           | 414       |\n",
      "|    time_elapsed         | 604       |\n",
      "|    total_timesteps      | 105984    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.8794014 |\n",
      "|    clip_fraction        | 0.203     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0956   |\n",
      "|    explained_variance   | 0.84      |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0885   |\n",
      "|    n_updates            | 7847      |\n",
      "|    policy_gradient_loss | -0.0847   |\n",
      "|    value_loss           | 0.145     |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=106000, episode_reward=5.84 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 5.84      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 106000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.9272164 |\n",
      "|    clip_fraction        | 0.247     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.123    |\n",
      "|    explained_variance   | 0.467     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0738   |\n",
      "|    n_updates            | 7866      |\n",
      "|    policy_gradient_loss | -0.0775   |\n",
      "|    value_loss           | 0.134     |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 63.8     |\n",
      "| time/              |          |\n",
      "|    fps             | 175      |\n",
      "|    iterations      | 415      |\n",
      "|    time_elapsed    | 605      |\n",
      "|    total_timesteps | 106240   |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 63.8       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 175        |\n",
      "|    iterations           | 416        |\n",
      "|    time_elapsed         | 607        |\n",
      "|    total_timesteps      | 106496     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.44937792 |\n",
      "|    clip_fraction        | 0.12       |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0598    |\n",
      "|    explained_variance   | 0.432      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0916    |\n",
      "|    n_updates            | 7885       |\n",
      "|    policy_gradient_loss | -0.0376    |\n",
      "|    value_loss           | 0.274      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=106500, episode_reward=18.82 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 230        |\n",
      "|    mean_reward          | 18.8       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 106500     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.95947146 |\n",
      "|    clip_fraction        | 0.173      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0842    |\n",
      "|    explained_variance   | 0.647      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0696    |\n",
      "|    n_updates            | 7904       |\n",
      "|    policy_gradient_loss | -0.0661    |\n",
      "|    value_loss           | 0.244      |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 63.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 175      |\n",
      "|    iterations      | 417      |\n",
      "|    time_elapsed    | 608      |\n",
      "|    total_timesteps | 106752   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=107000, episode_reward=7.40 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "--------------------------------------\n",
      "| eval/                   |          |\n",
      "|    mean_ep_length       | 230      |\n",
      "|    mean_reward          | 7.4      |\n",
      "| time/                   |          |\n",
      "|    total_timesteps      | 107000   |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 0.866322 |\n",
      "|    clip_fraction        | 0.214    |\n",
      "|    clip_range           | 0.4      |\n",
      "|    entropy_loss         | -0.112   |\n",
      "|    explained_variance   | 0.677    |\n",
      "|    learning_rate        | 0.000389 |\n",
      "|    loss                 | -0.0898  |\n",
      "|    n_updates            | 7923     |\n",
      "|    policy_gradient_loss | -0.0813  |\n",
      "|    value_loss           | 0.465    |\n",
      "--------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 63.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 175      |\n",
      "|    iterations      | 418      |\n",
      "|    time_elapsed    | 610      |\n",
      "|    total_timesteps | 107008   |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 63.9       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 175        |\n",
      "|    iterations           | 419        |\n",
      "|    time_elapsed         | 611        |\n",
      "|    total_timesteps      | 107264     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.91045296 |\n",
      "|    clip_fraction        | 0.204      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0752    |\n",
      "|    explained_variance   | 0.451      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.113     |\n",
      "|    n_updates            | 7942       |\n",
      "|    policy_gradient_loss | -0.0819    |\n",
      "|    value_loss           | 0.0408     |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=107500, episode_reward=5.07 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 5.07      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 107500    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.1160641 |\n",
      "|    clip_fraction        | 0.151     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0478   |\n",
      "|    explained_variance   | 0.827     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.117    |\n",
      "|    n_updates            | 7961      |\n",
      "|    policy_gradient_loss | -0.0549   |\n",
      "|    value_loss           | 0.117     |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 63.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 175      |\n",
      "|    iterations      | 420      |\n",
      "|    time_elapsed    | 612      |\n",
      "|    total_timesteps | 107520   |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 64.1      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 175       |\n",
      "|    iterations           | 421       |\n",
      "|    time_elapsed         | 613       |\n",
      "|    total_timesteps      | 107776    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.8225266 |\n",
      "|    clip_fraction        | 0.175     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0823   |\n",
      "|    explained_variance   | 0.538     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0402   |\n",
      "|    n_updates            | 7980      |\n",
      "|    policy_gradient_loss | -0.0626   |\n",
      "|    value_loss           | 0.488     |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=108000, episode_reward=-8.31 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "--------------------------------------\n",
      "| eval/                   |          |\n",
      "|    mean_ep_length       | 230      |\n",
      "|    mean_reward          | -8.31    |\n",
      "| time/                   |          |\n",
      "|    total_timesteps      | 108000   |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 1.096994 |\n",
      "|    clip_fraction        | 0.226    |\n",
      "|    clip_range           | 0.4      |\n",
      "|    entropy_loss         | -0.0848  |\n",
      "|    explained_variance   | 0.516    |\n",
      "|    learning_rate        | 0.000389 |\n",
      "|    loss                 | -0.0542  |\n",
      "|    n_updates            | 7999     |\n",
      "|    policy_gradient_loss | -0.0722  |\n",
      "|    value_loss           | 0.105    |\n",
      "--------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 64.1     |\n",
      "| time/              |          |\n",
      "|    fps             | 175      |\n",
      "|    iterations      | 422      |\n",
      "|    time_elapsed    | 615      |\n",
      "|    total_timesteps | 108032   |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 64.1      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 175       |\n",
      "|    iterations           | 423       |\n",
      "|    time_elapsed         | 616       |\n",
      "|    total_timesteps      | 108288    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.1797378 |\n",
      "|    clip_fraction        | 0.122     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0415   |\n",
      "|    explained_variance   | 0.75      |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0323   |\n",
      "|    n_updates            | 8018      |\n",
      "|    policy_gradient_loss | -0.0419   |\n",
      "|    value_loss           | 0.084     |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=108500, episode_reward=7.74 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 7.74      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 108500    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.8885014 |\n",
      "|    clip_fraction        | 0.14      |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0546   |\n",
      "|    explained_variance   | 0.576     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.124    |\n",
      "|    n_updates            | 8037      |\n",
      "|    policy_gradient_loss | -0.0622   |\n",
      "|    value_loss           | 0.172     |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 64       |\n",
      "| time/              |          |\n",
      "|    fps             | 175      |\n",
      "|    iterations      | 424      |\n",
      "|    time_elapsed    | 618      |\n",
      "|    total_timesteps | 108544   |\n",
      "---------------------------------\n",
      "--------------------------------------\n",
      "| rollout/                |          |\n",
      "|    ep_len_mean          | 889      |\n",
      "|    ep_rew_mean          | 64       |\n",
      "| time/                   |          |\n",
      "|    fps                  | 175      |\n",
      "|    iterations           | 425      |\n",
      "|    time_elapsed         | 619      |\n",
      "|    total_timesteps      | 108800   |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 1.223774 |\n",
      "|    clip_fraction        | 0.238    |\n",
      "|    clip_range           | 0.4      |\n",
      "|    entropy_loss         | -0.0955  |\n",
      "|    explained_variance   | 0.581    |\n",
      "|    learning_rate        | 0.000389 |\n",
      "|    loss                 | -0.0123  |\n",
      "|    n_updates            | 8056     |\n",
      "|    policy_gradient_loss | -0.0709  |\n",
      "|    value_loss           | 0.317    |\n",
      "--------------------------------------\n",
      "Eval num_timesteps=109000, episode_reward=-16.68 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 230        |\n",
      "|    mean_reward          | -16.7      |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 109000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.68689275 |\n",
      "|    clip_fraction        | 0.181      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0751    |\n",
      "|    explained_variance   | 0.102      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0261    |\n",
      "|    n_updates            | 8075       |\n",
      "|    policy_gradient_loss | -0.0605    |\n",
      "|    value_loss           | 0.0499     |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 64       |\n",
      "| time/              |          |\n",
      "|    fps             | 175      |\n",
      "|    iterations      | 426      |\n",
      "|    time_elapsed    | 621      |\n",
      "|    total_timesteps | 109056   |\n",
      "---------------------------------\n",
      "--------------------------------------\n",
      "| rollout/                |          |\n",
      "|    ep_len_mean          | 889      |\n",
      "|    ep_rew_mean          | 64       |\n",
      "| time/                   |          |\n",
      "|    fps                  | 175      |\n",
      "|    iterations           | 427      |\n",
      "|    time_elapsed         | 622      |\n",
      "|    total_timesteps      | 109312   |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 1.314589 |\n",
      "|    clip_fraction        | 0.143    |\n",
      "|    clip_range           | 0.4      |\n",
      "|    entropy_loss         | -0.0411  |\n",
      "|    explained_variance   | 0.443    |\n",
      "|    learning_rate        | 0.000389 |\n",
      "|    loss                 | 0.00163  |\n",
      "|    n_updates            | 8094     |\n",
      "|    policy_gradient_loss | -0.031   |\n",
      "|    value_loss           | 0.318    |\n",
      "--------------------------------------\n",
      "Eval num_timesteps=109500, episode_reward=4.10 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 4.1       |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 109500    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.4412487 |\n",
      "|    clip_fraction        | 0.184     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0759   |\n",
      "|    explained_variance   | 0.741     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0229   |\n",
      "|    n_updates            | 8113      |\n",
      "|    policy_gradient_loss | -0.07     |\n",
      "|    value_loss           | 0.19      |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 64.2     |\n",
      "| time/              |          |\n",
      "|    fps             | 175      |\n",
      "|    iterations      | 428      |\n",
      "|    time_elapsed    | 623      |\n",
      "|    total_timesteps | 109568   |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 64.2      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 175       |\n",
      "|    iterations           | 429       |\n",
      "|    time_elapsed         | 625       |\n",
      "|    total_timesteps      | 109824    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.9091437 |\n",
      "|    clip_fraction        | 0.196     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0924   |\n",
      "|    explained_variance   | -0.047    |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.176    |\n",
      "|    n_updates            | 8132      |\n",
      "|    policy_gradient_loss | -0.0646   |\n",
      "|    value_loss           | 0.132     |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=110000, episode_reward=6.07 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 6.07      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 110000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.3801008 |\n",
      "|    clip_fraction        | 0.0831    |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0378   |\n",
      "|    explained_variance   | 0.536     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | 0.00129   |\n",
      "|    n_updates            | 8151      |\n",
      "|    policy_gradient_loss | -0.0364   |\n",
      "|    value_loss           | 0.141     |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 64.2     |\n",
      "| time/              |          |\n",
      "|    fps             | 175      |\n",
      "|    iterations      | 430      |\n",
      "|    time_elapsed    | 626      |\n",
      "|    total_timesteps | 110080   |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 64.2       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 175        |\n",
      "|    iterations           | 431        |\n",
      "|    time_elapsed         | 628        |\n",
      "|    total_timesteps      | 110336     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.97334826 |\n",
      "|    clip_fraction        | 0.119      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0513    |\n",
      "|    explained_variance   | 0.623      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0499    |\n",
      "|    n_updates            | 8170       |\n",
      "|    policy_gradient_loss | -0.0549    |\n",
      "|    value_loss           | 0.173      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=110500, episode_reward=0.98 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 0.98      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 110500    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.7685495 |\n",
      "|    clip_fraction        | 0.189     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0937   |\n",
      "|    explained_variance   | 0.649     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | 0.0257    |\n",
      "|    n_updates            | 8189      |\n",
      "|    policy_gradient_loss | -0.0639   |\n",
      "|    value_loss           | 0.342     |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 64.2     |\n",
      "| time/              |          |\n",
      "|    fps             | 175      |\n",
      "|    iterations      | 432      |\n",
      "|    time_elapsed    | 630      |\n",
      "|    total_timesteps | 110592   |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 64.2       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 175        |\n",
      "|    iterations           | 433        |\n",
      "|    time_elapsed         | 631        |\n",
      "|    total_timesteps      | 110848     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.73936033 |\n",
      "|    clip_fraction        | 0.16       |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0715    |\n",
      "|    explained_variance   | 0.475      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.108     |\n",
      "|    n_updates            | 8208       |\n",
      "|    policy_gradient_loss | -0.0671    |\n",
      "|    value_loss           | 0.0348     |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=111000, episode_reward=16.66 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 16.7      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 111000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.0095534 |\n",
      "|    clip_fraction        | 0.111     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.041    |\n",
      "|    explained_variance   | 0.629     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.00659  |\n",
      "|    n_updates            | 8227      |\n",
      "|    policy_gradient_loss | -0.0355   |\n",
      "|    value_loss           | 0.309     |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 64.2     |\n",
      "| time/              |          |\n",
      "|    fps             | 175      |\n",
      "|    iterations      | 434      |\n",
      "|    time_elapsed    | 632      |\n",
      "|    total_timesteps | 111104   |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 64.4      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 175       |\n",
      "|    iterations           | 435       |\n",
      "|    time_elapsed         | 633       |\n",
      "|    total_timesteps      | 111360    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.8209222 |\n",
      "|    clip_fraction        | 0.156     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0838   |\n",
      "|    explained_variance   | 0.589     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | 0.0228    |\n",
      "|    n_updates            | 8246      |\n",
      "|    policy_gradient_loss | -0.0488   |\n",
      "|    value_loss           | 0.236     |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=111500, episode_reward=-7.04 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | -7.04     |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 111500    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.1886456 |\n",
      "|    clip_fraction        | 0.222     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0899   |\n",
      "|    explained_variance   | 0.592     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.137    |\n",
      "|    n_updates            | 8265      |\n",
      "|    policy_gradient_loss | -0.0803   |\n",
      "|    value_loss           | 0.0645    |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 64.4     |\n",
      "| time/              |          |\n",
      "|    fps             | 175      |\n",
      "|    iterations      | 436      |\n",
      "|    time_elapsed    | 635      |\n",
      "|    total_timesteps | 111616   |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 64.4      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 175       |\n",
      "|    iterations           | 437       |\n",
      "|    time_elapsed         | 637       |\n",
      "|    total_timesteps      | 111872    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.9165752 |\n",
      "|    clip_fraction        | 0.121     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0444   |\n",
      "|    explained_variance   | 0.81      |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0143   |\n",
      "|    n_updates            | 8284      |\n",
      "|    policy_gradient_loss | -0.0483   |\n",
      "|    value_loss           | 0.0605    |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=112000, episode_reward=20.59 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 230        |\n",
      "|    mean_reward          | 20.6       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 112000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.98526883 |\n",
      "|    clip_fraction        | 0.101      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0527    |\n",
      "|    explained_variance   | 0.606      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0547    |\n",
      "|    n_updates            | 8303       |\n",
      "|    policy_gradient_loss | -0.048     |\n",
      "|    value_loss           | 0.229      |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 64.3     |\n",
      "| time/              |          |\n",
      "|    fps             | 175      |\n",
      "|    iterations      | 438      |\n",
      "|    time_elapsed    | 638      |\n",
      "|    total_timesteps | 112128   |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 64.3      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 175       |\n",
      "|    iterations           | 439       |\n",
      "|    time_elapsed         | 639       |\n",
      "|    total_timesteps      | 112384    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.5314996 |\n",
      "|    clip_fraction        | 0.162     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0942   |\n",
      "|    explained_variance   | 0.43      |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.00988  |\n",
      "|    n_updates            | 8322      |\n",
      "|    policy_gradient_loss | -0.034    |\n",
      "|    value_loss           | 0.21      |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=112500, episode_reward=1.08 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 1.08      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 112500    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.7609372 |\n",
      "|    clip_fraction        | 0.159     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0731   |\n",
      "|    explained_variance   | 0.348     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0227   |\n",
      "|    n_updates            | 8341      |\n",
      "|    policy_gradient_loss | -0.0438   |\n",
      "|    value_loss           | 0.0908    |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 64.3     |\n",
      "| time/              |          |\n",
      "|    fps             | 175      |\n",
      "|    iterations      | 440      |\n",
      "|    time_elapsed    | 641      |\n",
      "|    total_timesteps | 112640   |\n",
      "---------------------------------\n",
      "--------------------------------------\n",
      "| rollout/                |          |\n",
      "|    ep_len_mean          | 889      |\n",
      "|    ep_rew_mean          | 64.3     |\n",
      "| time/                   |          |\n",
      "|    fps                  | 175      |\n",
      "|    iterations           | 441      |\n",
      "|    time_elapsed         | 642      |\n",
      "|    total_timesteps      | 112896   |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 1.300551 |\n",
      "|    clip_fraction        | 0.127    |\n",
      "|    clip_range           | 0.4      |\n",
      "|    entropy_loss         | -0.0396  |\n",
      "|    explained_variance   | 0.613    |\n",
      "|    learning_rate        | 0.000389 |\n",
      "|    loss                 | -0.0854  |\n",
      "|    n_updates            | 8360     |\n",
      "|    policy_gradient_loss | -0.0471  |\n",
      "|    value_loss           | 0.22     |\n",
      "--------------------------------------\n",
      "Eval num_timesteps=113000, episode_reward=7.83 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 7.83      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 113000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.9467337 |\n",
      "|    clip_fraction        | 0.148     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.074    |\n",
      "|    explained_variance   | 0.498     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | 0.0356    |\n",
      "|    n_updates            | 8379      |\n",
      "|    policy_gradient_loss | -0.038    |\n",
      "|    value_loss           | 0.255     |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 64.2     |\n",
      "| time/              |          |\n",
      "|    fps             | 175      |\n",
      "|    iterations      | 442      |\n",
      "|    time_elapsed    | 644      |\n",
      "|    total_timesteps | 113152   |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 64.2      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 175       |\n",
      "|    iterations           | 443       |\n",
      "|    time_elapsed         | 645       |\n",
      "|    total_timesteps      | 113408    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.5087847 |\n",
      "|    clip_fraction        | 0.184     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0758   |\n",
      "|    explained_variance   | 0.674     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0881   |\n",
      "|    n_updates            | 8398      |\n",
      "|    policy_gradient_loss | -0.0557   |\n",
      "|    value_loss           | 0.0466    |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=113500, episode_reward=27.93 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 27.9      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 113500    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.3985636 |\n",
      "|    clip_fraction        | 0.215     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0728   |\n",
      "|    explained_variance   | 0.556     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0158   |\n",
      "|    n_updates            | 8417      |\n",
      "|    policy_gradient_loss | -0.0561   |\n",
      "|    value_loss           | 0.193     |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 64.2     |\n",
      "| time/              |          |\n",
      "|    fps             | 175      |\n",
      "|    iterations      | 444      |\n",
      "|    time_elapsed    | 646      |\n",
      "|    total_timesteps | 113664   |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 64.1      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 175       |\n",
      "|    iterations           | 445       |\n",
      "|    time_elapsed         | 647       |\n",
      "|    total_timesteps      | 113920    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.1906489 |\n",
      "|    clip_fraction        | 0.141     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0441   |\n",
      "|    explained_variance   | 0.72      |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0866   |\n",
      "|    n_updates            | 8436      |\n",
      "|    policy_gradient_loss | -0.0353   |\n",
      "|    value_loss           | 0.145     |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=114000, episode_reward=3.52 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 3.52      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 114000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.1047885 |\n",
      "|    clip_fraction        | 0.256     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.121    |\n",
      "|    explained_variance   | 0.697     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.106    |\n",
      "|    n_updates            | 8455      |\n",
      "|    policy_gradient_loss | -0.0947   |\n",
      "|    value_loss           | 0.183     |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 64.1     |\n",
      "| time/              |          |\n",
      "|    fps             | 175      |\n",
      "|    iterations      | 446      |\n",
      "|    time_elapsed    | 649      |\n",
      "|    total_timesteps | 114176   |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 64.1      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 175       |\n",
      "|    iterations           | 447       |\n",
      "|    time_elapsed         | 650       |\n",
      "|    total_timesteps      | 114432    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.0064788 |\n",
      "|    clip_fraction        | 0.133     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0489   |\n",
      "|    explained_variance   | 0.515     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0138   |\n",
      "|    n_updates            | 8474      |\n",
      "|    policy_gradient_loss | -0.0503   |\n",
      "|    value_loss           | 0.0848    |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=114500, episode_reward=43.82 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 230        |\n",
      "|    mean_reward          | 43.8       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 114500     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.66679144 |\n",
      "|    clip_fraction        | 0.168      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0763    |\n",
      "|    explained_variance   | 0.512      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0713    |\n",
      "|    n_updates            | 8493       |\n",
      "|    policy_gradient_loss | -0.0584    |\n",
      "|    value_loss           | 0.209      |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 64       |\n",
      "| time/              |          |\n",
      "|    fps             | 175      |\n",
      "|    iterations      | 448      |\n",
      "|    time_elapsed    | 652      |\n",
      "|    total_timesteps | 114688   |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 64        |\n",
      "| time/                   |           |\n",
      "|    fps                  | 175       |\n",
      "|    iterations           | 449       |\n",
      "|    time_elapsed         | 653       |\n",
      "|    total_timesteps      | 114944    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.9904432 |\n",
      "|    clip_fraction        | 0.196     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0918   |\n",
      "|    explained_variance   | 0.13      |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | 0.00613   |\n",
      "|    n_updates            | 8512      |\n",
      "|    policy_gradient_loss | -0.0585   |\n",
      "|    value_loss           | 0.292     |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=115000, episode_reward=62.43 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 62.4      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 115000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.3258299 |\n",
      "|    clip_fraction        | 0.247     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.1      |\n",
      "|    explained_variance   | 0.314     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0978   |\n",
      "|    n_updates            | 8531      |\n",
      "|    policy_gradient_loss | -0.079    |\n",
      "|    value_loss           | 0.0727    |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 64       |\n",
      "| time/              |          |\n",
      "|    fps             | 175      |\n",
      "|    iterations      | 450      |\n",
      "|    time_elapsed    | 655      |\n",
      "|    total_timesteps | 115200   |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 64        |\n",
      "| time/                   |           |\n",
      "|    fps                  | 175       |\n",
      "|    iterations           | 451       |\n",
      "|    time_elapsed         | 656       |\n",
      "|    total_timesteps      | 115456    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.0615609 |\n",
      "|    clip_fraction        | 0.15      |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0624   |\n",
      "|    explained_variance   | 0.643     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.00239  |\n",
      "|    n_updates            | 8550      |\n",
      "|    policy_gradient_loss | -0.0543   |\n",
      "|    value_loss           | 0.114     |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=115500, episode_reward=77.58 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 77.6      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 115500    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.3091183 |\n",
      "|    clip_fraction        | 0.163     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0725   |\n",
      "|    explained_variance   | 0.777     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0816   |\n",
      "|    n_updates            | 8569      |\n",
      "|    policy_gradient_loss | -0.0559   |\n",
      "|    value_loss           | 0.122     |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 64       |\n",
      "| time/              |          |\n",
      "|    fps             | 175      |\n",
      "|    iterations      | 452      |\n",
      "|    time_elapsed    | 657      |\n",
      "|    total_timesteps | 115712   |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 64        |\n",
      "| time/                   |           |\n",
      "|    fps                  | 175       |\n",
      "|    iterations           | 453       |\n",
      "|    time_elapsed         | 659       |\n",
      "|    total_timesteps      | 115968    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.0583993 |\n",
      "|    clip_fraction        | 0.217     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0828   |\n",
      "|    explained_variance   | 0.446     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0635   |\n",
      "|    n_updates            | 8588      |\n",
      "|    policy_gradient_loss | -0.0702   |\n",
      "|    value_loss           | 0.296     |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=116000, episode_reward=8.46 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 230        |\n",
      "|    mean_reward          | 8.46       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 116000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.70681614 |\n",
      "|    clip_fraction        | 0.13       |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0643    |\n",
      "|    explained_variance   | 0.49       |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0149    |\n",
      "|    n_updates            | 8607       |\n",
      "|    policy_gradient_loss | -0.0516    |\n",
      "|    value_loss           | 0.0611     |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 64       |\n",
      "| time/              |          |\n",
      "|    fps             | 175      |\n",
      "|    iterations      | 454      |\n",
      "|    time_elapsed    | 660      |\n",
      "|    total_timesteps | 116224   |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 64         |\n",
      "| time/                   |            |\n",
      "|    fps                  | 176        |\n",
      "|    iterations           | 455        |\n",
      "|    time_elapsed         | 661        |\n",
      "|    total_timesteps      | 116480     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.67838204 |\n",
      "|    clip_fraction        | 0.17       |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0769    |\n",
      "|    explained_variance   | 0.637      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0788    |\n",
      "|    n_updates            | 8626       |\n",
      "|    policy_gradient_loss | -0.0571    |\n",
      "|    value_loss           | 0.104      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=116500, episode_reward=2.92 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 2.92      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 116500    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.3061335 |\n",
      "|    clip_fraction        | 0.191     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0807   |\n",
      "|    explained_variance   | 0.565     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0911   |\n",
      "|    n_updates            | 8645      |\n",
      "|    policy_gradient_loss | -0.06     |\n",
      "|    value_loss           | 0.256     |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 64       |\n",
      "| time/              |          |\n",
      "|    fps             | 175      |\n",
      "|    iterations      | 456      |\n",
      "|    time_elapsed    | 663      |\n",
      "|    total_timesteps | 116736   |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 64         |\n",
      "| time/                   |            |\n",
      "|    fps                  | 176        |\n",
      "|    iterations           | 457        |\n",
      "|    time_elapsed         | 664        |\n",
      "|    total_timesteps      | 116992     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.54667234 |\n",
      "|    clip_fraction        | 0.142      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0702    |\n",
      "|    explained_variance   | 0.0112     |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0815    |\n",
      "|    n_updates            | 8664       |\n",
      "|    policy_gradient_loss | -0.0411    |\n",
      "|    value_loss           | 0.0444     |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=117000, episode_reward=-25.29 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 230        |\n",
      "|    mean_reward          | -25.3      |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 117000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.46949828 |\n",
      "|    clip_fraction        | 0.149      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0644    |\n",
      "|    explained_variance   | 0.71       |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.127     |\n",
      "|    n_updates            | 8683       |\n",
      "|    policy_gradient_loss | -0.0605    |\n",
      "|    value_loss           | 0.0835     |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 64       |\n",
      "| time/              |          |\n",
      "|    fps             | 176      |\n",
      "|    iterations      | 458      |\n",
      "|    time_elapsed    | 666      |\n",
      "|    total_timesteps | 117248   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=117500, episode_reward=1.27 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 1.27      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 117500    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.7742593 |\n",
      "|    clip_fraction        | 0.142     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0683   |\n",
      "|    explained_variance   | 0.836     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.104    |\n",
      "|    n_updates            | 8702      |\n",
      "|    policy_gradient_loss | -0.0481   |\n",
      "|    value_loss           | 0.109     |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 64.1     |\n",
      "| time/              |          |\n",
      "|    fps             | 175      |\n",
      "|    iterations      | 459      |\n",
      "|    time_elapsed    | 667      |\n",
      "|    total_timesteps | 117504   |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 64.1      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 175       |\n",
      "|    iterations           | 460       |\n",
      "|    time_elapsed         | 669       |\n",
      "|    total_timesteps      | 117760    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.9745123 |\n",
      "|    clip_fraction        | 0.175     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0718   |\n",
      "|    explained_variance   | 0.536     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0559   |\n",
      "|    n_updates            | 8721      |\n",
      "|    policy_gradient_loss | -0.0395   |\n",
      "|    value_loss           | 0.483     |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=118000, episode_reward=20.60 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 20.6      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 118000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.5183887 |\n",
      "|    clip_fraction        | 0.148     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0619   |\n",
      "|    explained_variance   | 0.778     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0228   |\n",
      "|    n_updates            | 8740      |\n",
      "|    policy_gradient_loss | -0.0495   |\n",
      "|    value_loss           | 0.0397    |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 64.1     |\n",
      "| time/              |          |\n",
      "|    fps             | 175      |\n",
      "|    iterations      | 461      |\n",
      "|    time_elapsed    | 670      |\n",
      "|    total_timesteps | 118016   |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 64.2       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 176        |\n",
      "|    iterations           | 462        |\n",
      "|    time_elapsed         | 671        |\n",
      "|    total_timesteps      | 118272     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.39884073 |\n",
      "|    clip_fraction        | 0.112      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.067     |\n",
      "|    explained_variance   | 0.529      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0268    |\n",
      "|    n_updates            | 8759       |\n",
      "|    policy_gradient_loss | -0.0448    |\n",
      "|    value_loss           | 0.172      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=118500, episode_reward=21.96 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 22        |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 118500    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.3480031 |\n",
      "|    clip_fraction        | 0.187     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0671   |\n",
      "|    explained_variance   | 0.414     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0807   |\n",
      "|    n_updates            | 8778      |\n",
      "|    policy_gradient_loss | -0.0566   |\n",
      "|    value_loss           | 0.214     |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 64.2     |\n",
      "| time/              |          |\n",
      "|    fps             | 176      |\n",
      "|    iterations      | 463      |\n",
      "|    time_elapsed    | 673      |\n",
      "|    total_timesteps | 118528   |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 64.2      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 176       |\n",
      "|    iterations           | 464       |\n",
      "|    time_elapsed         | 674       |\n",
      "|    total_timesteps      | 118784    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.8391813 |\n",
      "|    clip_fraction        | 0.175     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0633   |\n",
      "|    explained_variance   | 0.601     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.075    |\n",
      "|    n_updates            | 8797      |\n",
      "|    policy_gradient_loss | -0.0673   |\n",
      "|    value_loss           | 0.0531    |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=119000, episode_reward=-2.60 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 230        |\n",
      "|    mean_reward          | -2.6       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 119000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.80835235 |\n",
      "|    clip_fraction        | 0.121      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0453    |\n",
      "|    explained_variance   | 0.567      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0821    |\n",
      "|    n_updates            | 8816       |\n",
      "|    policy_gradient_loss | -0.0435    |\n",
      "|    value_loss           | 0.105      |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 64.2     |\n",
      "| time/              |          |\n",
      "|    fps             | 176      |\n",
      "|    iterations      | 465      |\n",
      "|    time_elapsed    | 675      |\n",
      "|    total_timesteps | 119040   |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 64         |\n",
      "| time/                   |            |\n",
      "|    fps                  | 176        |\n",
      "|    iterations           | 466        |\n",
      "|    time_elapsed         | 677        |\n",
      "|    total_timesteps      | 119296     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.88399446 |\n",
      "|    clip_fraction        | 0.155      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.067     |\n",
      "|    explained_variance   | 0.726      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.133     |\n",
      "|    n_updates            | 8835       |\n",
      "|    policy_gradient_loss | -0.0574    |\n",
      "|    value_loss           | 0.0932     |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=119500, episode_reward=28.10 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 28.1      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 119500    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.0383449 |\n",
      "|    clip_fraction        | 0.166     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.074    |\n",
      "|    explained_variance   | 0.463     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0212   |\n",
      "|    n_updates            | 8854      |\n",
      "|    policy_gradient_loss | -0.042    |\n",
      "|    value_loss           | 0.238     |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 64       |\n",
      "| time/              |          |\n",
      "|    fps             | 176      |\n",
      "|    iterations      | 467      |\n",
      "|    time_elapsed    | 678      |\n",
      "|    total_timesteps | 119552   |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 64        |\n",
      "| time/                   |           |\n",
      "|    fps                  | 176       |\n",
      "|    iterations           | 468       |\n",
      "|    time_elapsed         | 679       |\n",
      "|    total_timesteps      | 119808    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.3678635 |\n",
      "|    clip_fraction        | 0.163     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0566   |\n",
      "|    explained_variance   | 0.423     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0734   |\n",
      "|    n_updates            | 8873      |\n",
      "|    policy_gradient_loss | -0.0522   |\n",
      "|    value_loss           | 0.058     |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=120000, episode_reward=12.67 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 12.7      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 120000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.1941378 |\n",
      "|    clip_fraction        | 0.162     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0559   |\n",
      "|    explained_variance   | 0.669     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0315   |\n",
      "|    n_updates            | 8892      |\n",
      "|    policy_gradient_loss | -0.0703   |\n",
      "|    value_loss           | 0.117     |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 64.1     |\n",
      "| time/              |          |\n",
      "|    fps             | 176      |\n",
      "|    iterations      | 469      |\n",
      "|    time_elapsed    | 681      |\n",
      "|    total_timesteps | 120064   |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 64.1      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 176       |\n",
      "|    iterations           | 470       |\n",
      "|    time_elapsed         | 682       |\n",
      "|    total_timesteps      | 120320    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.4667859 |\n",
      "|    clip_fraction        | 0.243     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0976   |\n",
      "|    explained_variance   | 0.243     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.18     |\n",
      "|    n_updates            | 8911      |\n",
      "|    policy_gradient_loss | -0.0794   |\n",
      "|    value_loss           | 0.673     |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=120500, episode_reward=7.72 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 7.72      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 120500    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.4755342 |\n",
      "|    clip_fraction        | 0.192     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0801   |\n",
      "|    explained_variance   | 0.537     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0432   |\n",
      "|    n_updates            | 8930      |\n",
      "|    policy_gradient_loss | -0.0562   |\n",
      "|    value_loss           | 0.0449    |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 64.1     |\n",
      "| time/              |          |\n",
      "|    fps             | 176      |\n",
      "|    iterations      | 471      |\n",
      "|    time_elapsed    | 684      |\n",
      "|    total_timesteps | 120576   |\n",
      "---------------------------------\n",
      "--------------------------------------\n",
      "| rollout/                |          |\n",
      "|    ep_len_mean          | 889      |\n",
      "|    ep_rew_mean          | 64.1     |\n",
      "| time/                   |          |\n",
      "|    fps                  | 176      |\n",
      "|    iterations           | 472      |\n",
      "|    time_elapsed         | 685      |\n",
      "|    total_timesteps      | 120832   |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 0.631665 |\n",
      "|    clip_fraction        | 0.116    |\n",
      "|    clip_range           | 0.4      |\n",
      "|    entropy_loss         | -0.0502  |\n",
      "|    explained_variance   | 0.65     |\n",
      "|    learning_rate        | 0.000389 |\n",
      "|    loss                 | -0.0887  |\n",
      "|    n_updates            | 8949     |\n",
      "|    policy_gradient_loss | -0.0448  |\n",
      "|    value_loss           | 0.119    |\n",
      "--------------------------------------\n",
      "Eval num_timesteps=121000, episode_reward=17.84 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 230        |\n",
      "|    mean_reward          | 17.8       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 121000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.93378127 |\n",
      "|    clip_fraction        | 0.188      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0803    |\n",
      "|    explained_variance   | 0.676      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | 0.045      |\n",
      "|    n_updates            | 8968       |\n",
      "|    policy_gradient_loss | -0.0547    |\n",
      "|    value_loss           | 0.535      |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 64.2     |\n",
      "| time/              |          |\n",
      "|    fps             | 176      |\n",
      "|    iterations      | 473      |\n",
      "|    time_elapsed    | 686      |\n",
      "|    total_timesteps | 121088   |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 64.2      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 176       |\n",
      "|    iterations           | 474       |\n",
      "|    time_elapsed         | 688       |\n",
      "|    total_timesteps      | 121344    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.7848258 |\n",
      "|    clip_fraction        | 0.193     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0883   |\n",
      "|    explained_variance   | 0.703     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.119    |\n",
      "|    n_updates            | 8987      |\n",
      "|    policy_gradient_loss | -0.0638   |\n",
      "|    value_loss           | 0.0901    |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=121500, episode_reward=23.97 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 24        |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 121500    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.6051512 |\n",
      "|    clip_fraction        | 0.124     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0438   |\n",
      "|    explained_variance   | 0.596     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0768   |\n",
      "|    n_updates            | 9006      |\n",
      "|    policy_gradient_loss | -0.036    |\n",
      "|    value_loss           | 0.0625    |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 64.2     |\n",
      "| time/              |          |\n",
      "|    fps             | 176      |\n",
      "|    iterations      | 475      |\n",
      "|    time_elapsed    | 689      |\n",
      "|    total_timesteps | 121600   |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 64.2      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 176       |\n",
      "|    iterations           | 476       |\n",
      "|    time_elapsed         | 691       |\n",
      "|    total_timesteps      | 121856    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.7749217 |\n",
      "|    clip_fraction        | 0.148     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0603   |\n",
      "|    explained_variance   | 0.665     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0385   |\n",
      "|    n_updates            | 9025      |\n",
      "|    policy_gradient_loss | -0.049    |\n",
      "|    value_loss           | 0.121     |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=122000, episode_reward=15.92 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 15.9      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 122000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.9994633 |\n",
      "|    clip_fraction        | 0.201     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0932   |\n",
      "|    explained_variance   | 0.802     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.133    |\n",
      "|    n_updates            | 9044      |\n",
      "|    policy_gradient_loss | -0.0828   |\n",
      "|    value_loss           | 0.209     |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 64.2     |\n",
      "| time/              |          |\n",
      "|    fps             | 176      |\n",
      "|    iterations      | 477      |\n",
      "|    time_elapsed    | 692      |\n",
      "|    total_timesteps | 122112   |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 64.2      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 176       |\n",
      "|    iterations           | 478       |\n",
      "|    time_elapsed         | 693       |\n",
      "|    total_timesteps      | 122368    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.1006054 |\n",
      "|    clip_fraction        | 0.204     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0849   |\n",
      "|    explained_variance   | 0.31      |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.104    |\n",
      "|    n_updates            | 9063      |\n",
      "|    policy_gradient_loss | -0.0762   |\n",
      "|    value_loss           | 0.0573    |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=122500, episode_reward=26.25 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 230        |\n",
      "|    mean_reward          | 26.3       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 122500     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.28639746 |\n",
      "|    clip_fraction        | 0.0508     |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0272    |\n",
      "|    explained_variance   | 0.672      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0233    |\n",
      "|    n_updates            | 9082       |\n",
      "|    policy_gradient_loss | -0.0221    |\n",
      "|    value_loss           | 0.0768     |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 64.2     |\n",
      "| time/              |          |\n",
      "|    fps             | 176      |\n",
      "|    iterations      | 479      |\n",
      "|    time_elapsed    | 695      |\n",
      "|    total_timesteps | 122624   |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 64.1      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 176       |\n",
      "|    iterations           | 480       |\n",
      "|    time_elapsed         | 696       |\n",
      "|    total_timesteps      | 122880    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.9926605 |\n",
      "|    clip_fraction        | 0.163     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.068    |\n",
      "|    explained_variance   | 0.935     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0677   |\n",
      "|    n_updates            | 9101      |\n",
      "|    policy_gradient_loss | -0.0611   |\n",
      "|    value_loss           | 0.0838    |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=123000, episode_reward=71.16 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 71.2      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 123000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.7772634 |\n",
      "|    clip_fraction        | 0.18      |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0909   |\n",
      "|    explained_variance   | 0.636     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.119    |\n",
      "|    n_updates            | 9120      |\n",
      "|    policy_gradient_loss | -0.0597   |\n",
      "|    value_loss           | 0.254     |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 64.1     |\n",
      "| time/              |          |\n",
      "|    fps             | 176      |\n",
      "|    iterations      | 481      |\n",
      "|    time_elapsed    | 698      |\n",
      "|    total_timesteps | 123136   |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 64.1       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 176        |\n",
      "|    iterations           | 482        |\n",
      "|    time_elapsed         | 699        |\n",
      "|    total_timesteps      | 123392     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.86901534 |\n",
      "|    clip_fraction        | 0.123      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0474    |\n",
      "|    explained_variance   | 0.722      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0847    |\n",
      "|    n_updates            | 9139       |\n",
      "|    policy_gradient_loss | -0.0496    |\n",
      "|    value_loss           | 0.054      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=123500, episode_reward=36.65 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "--------------------------------------\n",
      "| eval/                   |          |\n",
      "|    mean_ep_length       | 230      |\n",
      "|    mean_reward          | 36.7     |\n",
      "| time/                   |          |\n",
      "|    total_timesteps      | 123500   |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 0.907044 |\n",
      "|    clip_fraction        | 0.148    |\n",
      "|    clip_range           | 0.4      |\n",
      "|    entropy_loss         | -0.0603  |\n",
      "|    explained_variance   | 0.631    |\n",
      "|    learning_rate        | 0.000389 |\n",
      "|    loss                 | -0.0669  |\n",
      "|    n_updates            | 9158     |\n",
      "|    policy_gradient_loss | -0.0514  |\n",
      "|    value_loss           | 0.108    |\n",
      "--------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 64       |\n",
      "| time/              |          |\n",
      "|    fps             | 176      |\n",
      "|    iterations      | 483      |\n",
      "|    time_elapsed    | 700      |\n",
      "|    total_timesteps | 123648   |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 64        |\n",
      "| time/                   |           |\n",
      "|    fps                  | 176       |\n",
      "|    iterations           | 484       |\n",
      "|    time_elapsed         | 702       |\n",
      "|    total_timesteps      | 123904    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.9727936 |\n",
      "|    clip_fraction        | 0.209     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.107    |\n",
      "|    explained_variance   | 0.867     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0582   |\n",
      "|    n_updates            | 9177      |\n",
      "|    policy_gradient_loss | -0.0594   |\n",
      "|    value_loss           | 0.27      |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=124000, episode_reward=32.62 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 32.6      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 124000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.1614766 |\n",
      "|    clip_fraction        | 0.188     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0758   |\n",
      "|    explained_variance   | 0.588     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.174    |\n",
      "|    n_updates            | 9196      |\n",
      "|    policy_gradient_loss | -0.0691   |\n",
      "|    value_loss           | 0.0436    |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 64       |\n",
      "| time/              |          |\n",
      "|    fps             | 176      |\n",
      "|    iterations      | 485      |\n",
      "|    time_elapsed    | 703      |\n",
      "|    total_timesteps | 124160   |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 64        |\n",
      "| time/                   |           |\n",
      "|    fps                  | 176       |\n",
      "|    iterations           | 486       |\n",
      "|    time_elapsed         | 705       |\n",
      "|    total_timesteps      | 124416    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.8471935 |\n",
      "|    clip_fraction        | 0.113     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0376   |\n",
      "|    explained_variance   | 0.638     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0863   |\n",
      "|    n_updates            | 9215      |\n",
      "|    policy_gradient_loss | -0.038    |\n",
      "|    value_loss           | 0.103     |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=124500, episode_reward=28.32 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 230        |\n",
      "|    mean_reward          | 28.3       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 124500     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.56161135 |\n",
      "|    clip_fraction        | 0.172      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0824    |\n",
      "|    explained_variance   | 0.853      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | 0.0517     |\n",
      "|    n_updates            | 9234       |\n",
      "|    policy_gradient_loss | -0.0455    |\n",
      "|    value_loss           | 0.333      |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 63.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 176      |\n",
      "|    iterations      | 487      |\n",
      "|    time_elapsed    | 706      |\n",
      "|    total_timesteps | 124672   |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 63.9      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 176       |\n",
      "|    iterations           | 488       |\n",
      "|    time_elapsed         | 707       |\n",
      "|    total_timesteps      | 124928    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.7461654 |\n",
      "|    clip_fraction        | 0.204     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0881   |\n",
      "|    explained_variance   | 0.736     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | 0.0166    |\n",
      "|    n_updates            | 9253      |\n",
      "|    policy_gradient_loss | -0.0636   |\n",
      "|    value_loss           | 0.053     |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=125000, episode_reward=27.82 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 27.8      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 125000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.6256496 |\n",
      "|    clip_fraction        | 0.151     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0558   |\n",
      "|    explained_variance   | 0.75      |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.127    |\n",
      "|    n_updates            | 9272      |\n",
      "|    policy_gradient_loss | -0.0559   |\n",
      "|    value_loss           | 0.0506    |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 63.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 176      |\n",
      "|    iterations      | 489      |\n",
      "|    time_elapsed    | 709      |\n",
      "|    total_timesteps | 125184   |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 63.9      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 176       |\n",
      "|    iterations           | 490       |\n",
      "|    time_elapsed         | 710       |\n",
      "|    total_timesteps      | 125440    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.5417018 |\n",
      "|    clip_fraction        | 0.135     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0628   |\n",
      "|    explained_variance   | 0.706     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0505   |\n",
      "|    n_updates            | 9291      |\n",
      "|    policy_gradient_loss | -0.0604   |\n",
      "|    value_loss           | 0.128     |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=125500, episode_reward=35.92 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 230        |\n",
      "|    mean_reward          | 35.9       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 125500     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.53392327 |\n",
      "|    clip_fraction        | 0.152      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0808    |\n",
      "|    explained_variance   | 0.905      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | 0.0402     |\n",
      "|    n_updates            | 9310       |\n",
      "|    policy_gradient_loss | -0.0414    |\n",
      "|    value_loss           | 0.207      |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 63.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 176      |\n",
      "|    iterations      | 491      |\n",
      "|    time_elapsed    | 712      |\n",
      "|    total_timesteps | 125696   |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 63.9      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 176       |\n",
      "|    iterations           | 492       |\n",
      "|    time_elapsed         | 713       |\n",
      "|    total_timesteps      | 125952    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.9906292 |\n",
      "|    clip_fraction        | 0.207     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0695   |\n",
      "|    explained_variance   | 0.427     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0562   |\n",
      "|    n_updates            | 9329      |\n",
      "|    policy_gradient_loss | -0.0785   |\n",
      "|    value_loss           | 0.0456    |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=126000, episode_reward=29.78 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 230        |\n",
      "|    mean_reward          | 29.8       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 126000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.70902544 |\n",
      "|    clip_fraction        | 0.0925     |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0329    |\n",
      "|    explained_variance   | 0.723      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.118     |\n",
      "|    n_updates            | 9348       |\n",
      "|    policy_gradient_loss | -0.0403    |\n",
      "|    value_loss           | 0.126      |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 63.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 176      |\n",
      "|    iterations      | 493      |\n",
      "|    time_elapsed    | 714      |\n",
      "|    total_timesteps | 126208   |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 63.8      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 176       |\n",
      "|    iterations           | 494       |\n",
      "|    time_elapsed         | 716       |\n",
      "|    total_timesteps      | 126464    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.2997292 |\n",
      "|    clip_fraction        | 0.154     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0676   |\n",
      "|    explained_variance   | 0.696     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0482   |\n",
      "|    n_updates            | 9367      |\n",
      "|    policy_gradient_loss | -0.0426   |\n",
      "|    value_loss           | 0.441     |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=126500, episode_reward=19.45 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 19.5      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 126500    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.0666933 |\n",
      "|    clip_fraction        | 0.215     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0881   |\n",
      "|    explained_variance   | 0.239     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | 0.0249    |\n",
      "|    n_updates            | 9386      |\n",
      "|    policy_gradient_loss | -0.0582   |\n",
      "|    value_loss           | 0.129     |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 63.8     |\n",
      "| time/              |          |\n",
      "|    fps             | 176      |\n",
      "|    iterations      | 495      |\n",
      "|    time_elapsed    | 718      |\n",
      "|    total_timesteps | 126720   |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 63.8      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 176       |\n",
      "|    iterations           | 496       |\n",
      "|    time_elapsed         | 719       |\n",
      "|    total_timesteps      | 126976    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.0296202 |\n",
      "|    clip_fraction        | 0.0964    |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0505   |\n",
      "|    explained_variance   | 0.304     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.101    |\n",
      "|    n_updates            | 9405      |\n",
      "|    policy_gradient_loss | -0.0359   |\n",
      "|    value_loss           | 0.169     |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=127000, episode_reward=32.68 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 230        |\n",
      "|    mean_reward          | 32.7       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 127000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.65520024 |\n",
      "|    clip_fraction        | 0.106      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0292    |\n",
      "|    explained_variance   | 0.755      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0173    |\n",
      "|    n_updates            | 9424       |\n",
      "|    policy_gradient_loss | -0.0561    |\n",
      "|    value_loss           | 0.124      |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 63.4     |\n",
      "| time/              |          |\n",
      "|    fps             | 176      |\n",
      "|    iterations      | 497      |\n",
      "|    time_elapsed    | 721      |\n",
      "|    total_timesteps | 127232   |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 63.4      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 176       |\n",
      "|    iterations           | 498       |\n",
      "|    time_elapsed         | 722       |\n",
      "|    total_timesteps      | 127488    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.5182226 |\n",
      "|    clip_fraction        | 0.171     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.109    |\n",
      "|    explained_variance   | 0.849     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | 0.0143    |\n",
      "|    n_updates            | 9443      |\n",
      "|    policy_gradient_loss | -0.0528   |\n",
      "|    value_loss           | 0.227     |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=127500, episode_reward=45.02 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 45        |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 127500    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.6830723 |\n",
      "|    clip_fraction        | 0.141     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0535   |\n",
      "|    explained_variance   | 0.497     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.167    |\n",
      "|    n_updates            | 9462      |\n",
      "|    policy_gradient_loss | -0.0616   |\n",
      "|    value_loss           | 0.0501    |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 63.4     |\n",
      "| time/              |          |\n",
      "|    fps             | 176      |\n",
      "|    iterations      | 499      |\n",
      "|    time_elapsed    | 723      |\n",
      "|    total_timesteps | 127744   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=128000, episode_reward=57.85 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 57.8      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 128000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.5555361 |\n",
      "|    clip_fraction        | 0.0746    |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0234   |\n",
      "|    explained_variance   | 0.593     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0123   |\n",
      "|    n_updates            | 9481      |\n",
      "|    policy_gradient_loss | -0.038    |\n",
      "|    value_loss           | 0.18      |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 63.4     |\n",
      "| time/              |          |\n",
      "|    fps             | 176      |\n",
      "|    iterations      | 500      |\n",
      "|    time_elapsed    | 725      |\n",
      "|    total_timesteps | 128000   |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 63.1      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 176       |\n",
      "|    iterations           | 501       |\n",
      "|    time_elapsed         | 726       |\n",
      "|    total_timesteps      | 128256    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.8614106 |\n",
      "|    clip_fraction        | 0.144     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0754   |\n",
      "|    explained_variance   | 0.884     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.114    |\n",
      "|    n_updates            | 9500      |\n",
      "|    policy_gradient_loss | -0.0331   |\n",
      "|    value_loss           | 0.117     |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=128500, episode_reward=59.14 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 59.1      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 128500    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.6291661 |\n",
      "|    clip_fraction        | 0.193     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0865   |\n",
      "|    explained_variance   | 0.621     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0686   |\n",
      "|    n_updates            | 9519      |\n",
      "|    policy_gradient_loss | -0.0705   |\n",
      "|    value_loss           | 0.0321    |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 63.1     |\n",
      "| time/              |          |\n",
      "|    fps             | 176      |\n",
      "|    iterations      | 502      |\n",
      "|    time_elapsed    | 727      |\n",
      "|    total_timesteps | 128512   |\n",
      "---------------------------------\n",
      "--------------------------------------\n",
      "| rollout/                |          |\n",
      "|    ep_len_mean          | 889      |\n",
      "|    ep_rew_mean          | 63.1     |\n",
      "| time/                   |          |\n",
      "|    fps                  | 176      |\n",
      "|    iterations           | 503      |\n",
      "|    time_elapsed         | 728      |\n",
      "|    total_timesteps      | 128768   |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 1.097777 |\n",
      "|    clip_fraction        | 0.135    |\n",
      "|    clip_range           | 0.4      |\n",
      "|    entropy_loss         | -0.0402  |\n",
      "|    explained_variance   | 0.71     |\n",
      "|    learning_rate        | 0.000389 |\n",
      "|    loss                 | 0.0114   |\n",
      "|    n_updates            | 9538     |\n",
      "|    policy_gradient_loss | -0.0493  |\n",
      "|    value_loss           | 0.119    |\n",
      "--------------------------------------\n",
      "Eval num_timesteps=129000, episode_reward=39.61 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 39.6      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 129000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.7280863 |\n",
      "|    clip_fraction        | 0.144     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0489   |\n",
      "|    explained_variance   | 0.876     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.122    |\n",
      "|    n_updates            | 9557      |\n",
      "|    policy_gradient_loss | -0.0546   |\n",
      "|    value_loss           | 0.0684    |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 62.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 176      |\n",
      "|    iterations      | 504      |\n",
      "|    time_elapsed    | 730      |\n",
      "|    total_timesteps | 129024   |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 62.9      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 176       |\n",
      "|    iterations           | 505       |\n",
      "|    time_elapsed         | 731       |\n",
      "|    total_timesteps      | 129280    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.7559021 |\n",
      "|    clip_fraction        | 0.17      |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0944   |\n",
      "|    explained_variance   | 0.732     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.132    |\n",
      "|    n_updates            | 9576      |\n",
      "|    policy_gradient_loss | -0.052    |\n",
      "|    value_loss           | 0.195     |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=129500, episode_reward=59.87 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 59.9      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 129500    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.9530872 |\n",
      "|    clip_fraction        | 0.173     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0678   |\n",
      "|    explained_variance   | 0.294     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0492   |\n",
      "|    n_updates            | 9595      |\n",
      "|    policy_gradient_loss | -0.0579   |\n",
      "|    value_loss           | 0.0846    |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 62.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 176      |\n",
      "|    iterations      | 506      |\n",
      "|    time_elapsed    | 733      |\n",
      "|    total_timesteps | 129536   |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 62.9       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 176        |\n",
      "|    iterations           | 507        |\n",
      "|    time_elapsed         | 734        |\n",
      "|    total_timesteps      | 129792     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.30772406 |\n",
      "|    clip_fraction        | 0.0866     |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0444    |\n",
      "|    explained_variance   | 0.733      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0562    |\n",
      "|    n_updates            | 9614       |\n",
      "|    policy_gradient_loss | -0.0393    |\n",
      "|    value_loss           | 0.2        |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=130000, episode_reward=33.58 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 33.6      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 130000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.3063916 |\n",
      "|    clip_fraction        | 0.128     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0725   |\n",
      "|    explained_variance   | 0.512     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0252   |\n",
      "|    n_updates            | 9633      |\n",
      "|    policy_gradient_loss | -0.0415   |\n",
      "|    value_loss           | 0.681     |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 62.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 176      |\n",
      "|    iterations      | 508      |\n",
      "|    time_elapsed    | 735      |\n",
      "|    total_timesteps | 130048   |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 62.9      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 176       |\n",
      "|    iterations           | 509       |\n",
      "|    time_elapsed         | 737       |\n",
      "|    total_timesteps      | 130304    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.1725185 |\n",
      "|    clip_fraction        | 0.174     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0817   |\n",
      "|    explained_variance   | 0.391     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0813   |\n",
      "|    n_updates            | 9652      |\n",
      "|    policy_gradient_loss | -0.0583   |\n",
      "|    value_loss           | 0.0477    |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=130500, episode_reward=43.87 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 43.9      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 130500    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.2467864 |\n",
      "|    clip_fraction        | 0.131     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0419   |\n",
      "|    explained_variance   | 0.77      |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0963   |\n",
      "|    n_updates            | 9671      |\n",
      "|    policy_gradient_loss | -0.053    |\n",
      "|    value_loss           | 0.071     |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 62.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 176      |\n",
      "|    iterations      | 510      |\n",
      "|    time_elapsed    | 738      |\n",
      "|    total_timesteps | 130560   |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 62.9       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 176        |\n",
      "|    iterations           | 511        |\n",
      "|    time_elapsed         | 739        |\n",
      "|    total_timesteps      | 130816     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.43368167 |\n",
      "|    clip_fraction        | 0.11       |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0497    |\n",
      "|    explained_variance   | 0.908      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0278    |\n",
      "|    n_updates            | 9690       |\n",
      "|    policy_gradient_loss | -0.041     |\n",
      "|    value_loss           | 0.0673     |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=131000, episode_reward=39.95 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 40        |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 131000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.5851824 |\n",
      "|    clip_fraction        | 0.154     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0956   |\n",
      "|    explained_variance   | 0.688     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0216   |\n",
      "|    n_updates            | 9709      |\n",
      "|    policy_gradient_loss | -0.0388   |\n",
      "|    value_loss           | 0.357     |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 62.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 176      |\n",
      "|    iterations      | 512      |\n",
      "|    time_elapsed    | 741      |\n",
      "|    total_timesteps | 131072   |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 62.9       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 176        |\n",
      "|    iterations           | 513        |\n",
      "|    time_elapsed         | 742        |\n",
      "|    total_timesteps      | 131328     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.70101094 |\n",
      "|    clip_fraction        | 0.111      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0388    |\n",
      "|    explained_variance   | 0.0839     |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0613    |\n",
      "|    n_updates            | 9728       |\n",
      "|    policy_gradient_loss | -0.0463    |\n",
      "|    value_loss           | 0.0634     |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=131500, episode_reward=39.44 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 230        |\n",
      "|    mean_reward          | 39.4       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 131500     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.61540365 |\n",
      "|    clip_fraction        | 0.0997     |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0499    |\n",
      "|    explained_variance   | 0.371      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0366    |\n",
      "|    n_updates            | 9747       |\n",
      "|    policy_gradient_loss | -0.0412    |\n",
      "|    value_loss           | 0.698      |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 62.6     |\n",
      "| time/              |          |\n",
      "|    fps             | 176      |\n",
      "|    iterations      | 514      |\n",
      "|    time_elapsed    | 743      |\n",
      "|    total_timesteps | 131584   |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 62.6       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 176        |\n",
      "|    iterations           | 515        |\n",
      "|    time_elapsed         | 745        |\n",
      "|    total_timesteps      | 131840     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.98090327 |\n",
      "|    clip_fraction        | 0.169      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.08      |\n",
      "|    explained_variance   | 0.527      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0338    |\n",
      "|    n_updates            | 9766       |\n",
      "|    policy_gradient_loss | -0.0563    |\n",
      "|    value_loss           | 0.551      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=132000, episode_reward=78.20 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 78.2      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 132000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.6509339 |\n",
      "|    clip_fraction        | 0.186     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0866   |\n",
      "|    explained_variance   | 0.511     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0458   |\n",
      "|    n_updates            | 9785      |\n",
      "|    policy_gradient_loss | -0.0563   |\n",
      "|    value_loss           | 0.0272    |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 62.6     |\n",
      "| time/              |          |\n",
      "|    fps             | 176      |\n",
      "|    iterations      | 516      |\n",
      "|    time_elapsed    | 746      |\n",
      "|    total_timesteps | 132096   |\n",
      "---------------------------------\n",
      "--------------------------------------\n",
      "| rollout/                |          |\n",
      "|    ep_len_mean          | 889      |\n",
      "|    ep_rew_mean          | 62.6     |\n",
      "| time/                   |          |\n",
      "|    fps                  | 176      |\n",
      "|    iterations           | 517      |\n",
      "|    time_elapsed         | 748      |\n",
      "|    total_timesteps      | 132352   |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 0.855151 |\n",
      "|    clip_fraction        | 0.0911   |\n",
      "|    clip_range           | 0.4      |\n",
      "|    entropy_loss         | -0.0427  |\n",
      "|    explained_variance   | 0.731    |\n",
      "|    learning_rate        | 0.000389 |\n",
      "|    loss                 | -0.0951  |\n",
      "|    n_updates            | 9804     |\n",
      "|    policy_gradient_loss | -0.0373  |\n",
      "|    value_loss           | 0.189    |\n",
      "--------------------------------------\n",
      "Eval num_timesteps=132500, episode_reward=51.98 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 230        |\n",
      "|    mean_reward          | 52         |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 132500     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.66677094 |\n",
      "|    clip_fraction        | 0.129      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0506    |\n",
      "|    explained_variance   | 0.703      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0084    |\n",
      "|    n_updates            | 9823       |\n",
      "|    policy_gradient_loss | -0.0466    |\n",
      "|    value_loss           | 0.189      |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 62.5     |\n",
      "| time/              |          |\n",
      "|    fps             | 176      |\n",
      "|    iterations      | 518      |\n",
      "|    time_elapsed    | 749      |\n",
      "|    total_timesteps | 132608   |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 62.5       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 176        |\n",
      "|    iterations           | 519        |\n",
      "|    time_elapsed         | 750        |\n",
      "|    total_timesteps      | 132864     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.76916856 |\n",
      "|    clip_fraction        | 0.192      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0969    |\n",
      "|    explained_variance   | 0.472      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0199    |\n",
      "|    n_updates            | 9842       |\n",
      "|    policy_gradient_loss | -0.0477    |\n",
      "|    value_loss           | 0.204      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=133000, episode_reward=25.47 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 230        |\n",
      "|    mean_reward          | 25.5       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 133000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.95149785 |\n",
      "|    clip_fraction        | 0.139      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0445    |\n",
      "|    explained_variance   | 0.735      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0777    |\n",
      "|    n_updates            | 9861       |\n",
      "|    policy_gradient_loss | -0.0568    |\n",
      "|    value_loss           | 0.0404     |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 62.5     |\n",
      "| time/              |          |\n",
      "|    fps             | 176      |\n",
      "|    iterations      | 520      |\n",
      "|    time_elapsed    | 752      |\n",
      "|    total_timesteps | 133120   |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 62.3      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 176       |\n",
      "|    iterations           | 521       |\n",
      "|    time_elapsed         | 753       |\n",
      "|    total_timesteps      | 133376    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.9943298 |\n",
      "|    clip_fraction        | 0.128     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0635   |\n",
      "|    explained_variance   | 0.615     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.065    |\n",
      "|    n_updates            | 9880      |\n",
      "|    policy_gradient_loss | -0.0534   |\n",
      "|    value_loss           | 0.195     |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=133500, episode_reward=46.98 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "--------------------------------------\n",
      "| eval/                   |          |\n",
      "|    mean_ep_length       | 230      |\n",
      "|    mean_reward          | 47       |\n",
      "| time/                   |          |\n",
      "|    total_timesteps      | 133500   |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 0.503069 |\n",
      "|    clip_fraction        | 0.166    |\n",
      "|    clip_range           | 0.4      |\n",
      "|    entropy_loss         | -0.0724  |\n",
      "|    explained_variance   | 0.4      |\n",
      "|    learning_rate        | 0.000389 |\n",
      "|    loss                 | 0.128    |\n",
      "|    n_updates            | 9899     |\n",
      "|    policy_gradient_loss | -0.0571  |\n",
      "|    value_loss           | 0.319    |\n",
      "--------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 62.3     |\n",
      "| time/              |          |\n",
      "|    fps             | 176      |\n",
      "|    iterations      | 522      |\n",
      "|    time_elapsed    | 755      |\n",
      "|    total_timesteps | 133632   |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 62.3      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 176       |\n",
      "|    iterations           | 523       |\n",
      "|    time_elapsed         | 756       |\n",
      "|    total_timesteps      | 133888    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.8765355 |\n",
      "|    clip_fraction        | 0.203     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0826   |\n",
      "|    explained_variance   | 0.184     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0934   |\n",
      "|    n_updates            | 9918      |\n",
      "|    policy_gradient_loss | -0.0704   |\n",
      "|    value_loss           | 0.0387    |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=134000, episode_reward=40.02 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 230        |\n",
      "|    mean_reward          | 40         |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 134000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.80178714 |\n",
      "|    clip_fraction        | 0.134      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0532    |\n",
      "|    explained_variance   | 0.677      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | 0.00369    |\n",
      "|    n_updates            | 9937       |\n",
      "|    policy_gradient_loss | -0.0474    |\n",
      "|    value_loss           | 0.18       |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 62.3     |\n",
      "| time/              |          |\n",
      "|    fps             | 177      |\n",
      "|    iterations      | 524      |\n",
      "|    time_elapsed    | 757      |\n",
      "|    total_timesteps | 134144   |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 62.1      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 177       |\n",
      "|    iterations           | 525       |\n",
      "|    time_elapsed         | 759       |\n",
      "|    total_timesteps      | 134400    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.2594289 |\n",
      "|    clip_fraction        | 0.191     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0634   |\n",
      "|    explained_variance   | 0.702     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0318   |\n",
      "|    n_updates            | 9956      |\n",
      "|    policy_gradient_loss | -0.0594   |\n",
      "|    value_loss           | 0.346     |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=134500, episode_reward=47.12 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "--------------------------------------\n",
      "| eval/                   |          |\n",
      "|    mean_ep_length       | 230      |\n",
      "|    mean_reward          | 47.1     |\n",
      "| time/                   |          |\n",
      "|    total_timesteps      | 134500   |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 1.492144 |\n",
      "|    clip_fraction        | 0.278    |\n",
      "|    clip_range           | 0.4      |\n",
      "|    entropy_loss         | -0.109   |\n",
      "|    explained_variance   | 0.591    |\n",
      "|    learning_rate        | 0.000389 |\n",
      "|    loss                 | -0.0418  |\n",
      "|    n_updates            | 9975     |\n",
      "|    policy_gradient_loss | -0.0789  |\n",
      "|    value_loss           | 0.242    |\n",
      "--------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 62.1     |\n",
      "| time/              |          |\n",
      "|    fps             | 176      |\n",
      "|    iterations      | 526      |\n",
      "|    time_elapsed    | 760      |\n",
      "|    total_timesteps | 134656   |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 62.1       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 177        |\n",
      "|    iterations           | 527        |\n",
      "|    time_elapsed         | 762        |\n",
      "|    total_timesteps      | 134912     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.61970437 |\n",
      "|    clip_fraction        | 0.114      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0383    |\n",
      "|    explained_variance   | 0.685      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0514    |\n",
      "|    n_updates            | 9994       |\n",
      "|    policy_gradient_loss | -0.0412    |\n",
      "|    value_loss           | 0.0353     |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=135000, episode_reward=37.33 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 230        |\n",
      "|    mean_reward          | 37.3       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 135000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.77338487 |\n",
      "|    clip_fraction        | 0.153      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0564    |\n",
      "|    explained_variance   | 0.481      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.103     |\n",
      "|    n_updates            | 10013      |\n",
      "|    policy_gradient_loss | -0.0418    |\n",
      "|    value_loss           | 0.471      |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 62       |\n",
      "| time/              |          |\n",
      "|    fps             | 177      |\n",
      "|    iterations      | 528      |\n",
      "|    time_elapsed    | 763      |\n",
      "|    total_timesteps | 135168   |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 62        |\n",
      "| time/                   |           |\n",
      "|    fps                  | 177       |\n",
      "|    iterations           | 529       |\n",
      "|    time_elapsed         | 764       |\n",
      "|    total_timesteps      | 135424    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.6106213 |\n",
      "|    clip_fraction        | 0.173     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.081    |\n",
      "|    explained_variance   | 0.737     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.00623  |\n",
      "|    n_updates            | 10032     |\n",
      "|    policy_gradient_loss | -0.0643   |\n",
      "|    value_loss           | 0.221     |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=135500, episode_reward=54.59 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 230        |\n",
      "|    mean_reward          | 54.6       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 135500     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.76873446 |\n",
      "|    clip_fraction        | 0.161      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0755    |\n",
      "|    explained_variance   | 0.51       |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0352    |\n",
      "|    n_updates            | 10051      |\n",
      "|    policy_gradient_loss | -0.0461    |\n",
      "|    value_loss           | 0.0608     |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 62       |\n",
      "| time/              |          |\n",
      "|    fps             | 177      |\n",
      "|    iterations      | 530      |\n",
      "|    time_elapsed    | 766      |\n",
      "|    total_timesteps | 135680   |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 62         |\n",
      "| time/                   |            |\n",
      "|    fps                  | 177        |\n",
      "|    iterations           | 531        |\n",
      "|    time_elapsed         | 767        |\n",
      "|    total_timesteps      | 135936     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.68036306 |\n",
      "|    clip_fraction        | 0.0993     |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0425    |\n",
      "|    explained_variance   | 0.582      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0153    |\n",
      "|    n_updates            | 10070      |\n",
      "|    policy_gradient_loss | -0.0407    |\n",
      "|    value_loss           | 0.309      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=136000, episode_reward=68.12 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "--------------------------------------\n",
      "| eval/                   |          |\n",
      "|    mean_ep_length       | 230      |\n",
      "|    mean_reward          | 68.1     |\n",
      "| time/                   |          |\n",
      "|    total_timesteps      | 136000   |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 1.012104 |\n",
      "|    clip_fraction        | 0.136    |\n",
      "|    clip_range           | 0.4      |\n",
      "|    entropy_loss         | -0.0488  |\n",
      "|    explained_variance   | 0.875    |\n",
      "|    learning_rate        | 0.000389 |\n",
      "|    loss                 | -0.00961 |\n",
      "|    n_updates            | 10089    |\n",
      "|    policy_gradient_loss | -0.0411  |\n",
      "|    value_loss           | 0.16     |\n",
      "--------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 62.1     |\n",
      "| time/              |          |\n",
      "|    fps             | 177      |\n",
      "|    iterations      | 532      |\n",
      "|    time_elapsed    | 768      |\n",
      "|    total_timesteps | 136192   |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 62.1      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 177       |\n",
      "|    iterations           | 533       |\n",
      "|    time_elapsed         | 770       |\n",
      "|    total_timesteps      | 136448    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.3369256 |\n",
      "|    clip_fraction        | 0.221     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0833   |\n",
      "|    explained_variance   | 0.304     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0995   |\n",
      "|    n_updates            | 10108     |\n",
      "|    policy_gradient_loss | -0.0753   |\n",
      "|    value_loss           | 0.195     |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=136500, episode_reward=91.14 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 91.1      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 136500    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.8802613 |\n",
      "|    clip_fraction        | 0.126     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.037    |\n",
      "|    explained_variance   | 0.649     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0161   |\n",
      "|    n_updates            | 10127     |\n",
      "|    policy_gradient_loss | -0.047    |\n",
      "|    value_loss           | 0.0425    |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 62.1     |\n",
      "| time/              |          |\n",
      "|    fps             | 177      |\n",
      "|    iterations      | 534      |\n",
      "|    time_elapsed    | 771      |\n",
      "|    total_timesteps | 136704   |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 62         |\n",
      "| time/                   |            |\n",
      "|    fps                  | 177        |\n",
      "|    iterations           | 535        |\n",
      "|    time_elapsed         | 772        |\n",
      "|    total_timesteps      | 136960     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.82215697 |\n",
      "|    clip_fraction        | 0.151      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0473    |\n",
      "|    explained_variance   | 0.68       |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | 0.167      |\n",
      "|    n_updates            | 10146      |\n",
      "|    policy_gradient_loss | -0.0458    |\n",
      "|    value_loss           | 0.457      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=137000, episode_reward=35.49 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 230        |\n",
      "|    mean_reward          | 35.5       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 137000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.70218825 |\n",
      "|    clip_fraction        | 0.141      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0674    |\n",
      "|    explained_variance   | 0.455      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0129    |\n",
      "|    n_updates            | 10165      |\n",
      "|    policy_gradient_loss | -0.0382    |\n",
      "|    value_loss           | 0.755      |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 62       |\n",
      "| time/              |          |\n",
      "|    fps             | 177      |\n",
      "|    iterations      | 536      |\n",
      "|    time_elapsed    | 774      |\n",
      "|    total_timesteps | 137216   |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 62         |\n",
      "| time/                   |            |\n",
      "|    fps                  | 177        |\n",
      "|    iterations           | 537        |\n",
      "|    time_elapsed         | 775        |\n",
      "|    total_timesteps      | 137472     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.60773325 |\n",
      "|    clip_fraction        | 0.0962     |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0464    |\n",
      "|    explained_variance   | 0.42       |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0925    |\n",
      "|    n_updates            | 10184      |\n",
      "|    policy_gradient_loss | -0.0437    |\n",
      "|    value_loss           | 0.0495     |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=137500, episode_reward=47.11 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 47.1      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 137500    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.7488022 |\n",
      "|    clip_fraction        | 0.104     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0273   |\n",
      "|    explained_variance   | 0.521     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0927   |\n",
      "|    n_updates            | 10203     |\n",
      "|    policy_gradient_loss | -0.0371   |\n",
      "|    value_loss           | 0.564     |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 62       |\n",
      "| time/              |          |\n",
      "|    fps             | 177      |\n",
      "|    iterations      | 538      |\n",
      "|    time_elapsed    | 777      |\n",
      "|    total_timesteps | 137728   |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 62.1      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 177       |\n",
      "|    iterations           | 539       |\n",
      "|    time_elapsed         | 778       |\n",
      "|    total_timesteps      | 137984    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.4018221 |\n",
      "|    clip_fraction        | 0.146     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.051    |\n",
      "|    explained_variance   | 0.635     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0135   |\n",
      "|    n_updates            | 10222     |\n",
      "|    policy_gradient_loss | -0.0375   |\n",
      "|    value_loss           | 0.497     |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=138000, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 0         |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 138000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.0626497 |\n",
      "|    clip_fraction        | 0.19      |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0727   |\n",
      "|    explained_variance   | 0.777     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0273   |\n",
      "|    n_updates            | 10241     |\n",
      "|    policy_gradient_loss | -0.0657   |\n",
      "|    value_loss           | 0.0695    |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 62.1     |\n",
      "| time/              |          |\n",
      "|    fps             | 177      |\n",
      "|    iterations      | 540      |\n",
      "|    time_elapsed    | 779      |\n",
      "|    total_timesteps | 138240   |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 62.1      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 177       |\n",
      "|    iterations           | 541       |\n",
      "|    time_elapsed         | 781       |\n",
      "|    total_timesteps      | 138496    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.4813697 |\n",
      "|    clip_fraction        | 0.158     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0534   |\n",
      "|    explained_variance   | 0.739     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0444   |\n",
      "|    n_updates            | 10260     |\n",
      "|    policy_gradient_loss | -0.0545   |\n",
      "|    value_loss           | 0.072     |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=138500, episode_reward=8.11 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 230        |\n",
      "|    mean_reward          | 8.11       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 138500     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.84507847 |\n",
      "|    clip_fraction        | 0.118      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.036     |\n",
      "|    explained_variance   | 0.77       |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0431    |\n",
      "|    n_updates            | 10279      |\n",
      "|    policy_gradient_loss | -0.0556    |\n",
      "|    value_loss           | 0.283      |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 61.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 177      |\n",
      "|    iterations      | 542      |\n",
      "|    time_elapsed    | 782      |\n",
      "|    total_timesteps | 138752   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=139000, episode_reward=76.12 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 76.1      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 139000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.6209294 |\n",
      "|    clip_fraction        | 0.126     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.061    |\n",
      "|    explained_variance   | 0.319     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0823   |\n",
      "|    n_updates            | 10298     |\n",
      "|    policy_gradient_loss | -0.0395   |\n",
      "|    value_loss           | 0.904     |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 61.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 177      |\n",
      "|    iterations      | 543      |\n",
      "|    time_elapsed    | 784      |\n",
      "|    total_timesteps | 139008   |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 61.9       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 177        |\n",
      "|    iterations           | 544        |\n",
      "|    time_elapsed         | 785        |\n",
      "|    total_timesteps      | 139264     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.66717684 |\n",
      "|    clip_fraction        | 0.159      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0617    |\n",
      "|    explained_variance   | 0.63       |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0778    |\n",
      "|    n_updates            | 10317      |\n",
      "|    policy_gradient_loss | -0.0571    |\n",
      "|    value_loss           | 0.0373     |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=139500, episode_reward=66.90 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 66.9      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 139500    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.7075787 |\n",
      "|    clip_fraction        | 0.0711    |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0233   |\n",
      "|    explained_variance   | 0.764     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.125    |\n",
      "|    n_updates            | 10336     |\n",
      "|    policy_gradient_loss | -0.0299   |\n",
      "|    value_loss           | 0.192     |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 61.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 177      |\n",
      "|    iterations      | 545      |\n",
      "|    time_elapsed    | 786      |\n",
      "|    total_timesteps | 139520   |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 62        |\n",
      "| time/                   |           |\n",
      "|    fps                  | 177       |\n",
      "|    iterations           | 546       |\n",
      "|    time_elapsed         | 788       |\n",
      "|    total_timesteps      | 139776    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.5067071 |\n",
      "|    clip_fraction        | 0.0966    |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0509   |\n",
      "|    explained_variance   | 0.865     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.00423  |\n",
      "|    n_updates            | 10355     |\n",
      "|    policy_gradient_loss | -0.037    |\n",
      "|    value_loss           | 0.276     |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=140000, episode_reward=76.61 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 76.6      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 140000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.9108883 |\n",
      "|    clip_fraction        | 0.195     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.103    |\n",
      "|    explained_variance   | 0.333     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | 0.0597    |\n",
      "|    n_updates            | 10374     |\n",
      "|    policy_gradient_loss | -0.0648   |\n",
      "|    value_loss           | 0.108     |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 62       |\n",
      "| time/              |          |\n",
      "|    fps             | 177      |\n",
      "|    iterations      | 547      |\n",
      "|    time_elapsed    | 789      |\n",
      "|    total_timesteps | 140032   |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 62         |\n",
      "| time/                   |            |\n",
      "|    fps                  | 177        |\n",
      "|    iterations           | 548        |\n",
      "|    time_elapsed         | 790        |\n",
      "|    total_timesteps      | 140288     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.67214584 |\n",
      "|    clip_fraction        | 0.119      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0452    |\n",
      "|    explained_variance   | 0.522      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.117     |\n",
      "|    n_updates            | 10393      |\n",
      "|    policy_gradient_loss | -0.0374    |\n",
      "|    value_loss           | 0.146      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=140500, episode_reward=82.05 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 82.1      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 140500    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.7844163 |\n",
      "|    clip_fraction        | 0.134     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0421   |\n",
      "|    explained_variance   | 0.69      |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0539   |\n",
      "|    n_updates            | 10412     |\n",
      "|    policy_gradient_loss | -0.0391   |\n",
      "|    value_loss           | 0.412     |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 61.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 177      |\n",
      "|    iterations      | 549      |\n",
      "|    time_elapsed    | 792      |\n",
      "|    total_timesteps | 140544   |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 61.9      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 177       |\n",
      "|    iterations           | 550       |\n",
      "|    time_elapsed         | 793       |\n",
      "|    total_timesteps      | 140800    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 7.0492945 |\n",
      "|    clip_fraction        | 0.152     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0909   |\n",
      "|    explained_variance   | 0.758     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.00979  |\n",
      "|    n_updates            | 10431     |\n",
      "|    policy_gradient_loss | 0.00929   |\n",
      "|    value_loss           | 0.211     |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=141000, episode_reward=90.07 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 90.1      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 141000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.8061302 |\n",
      "|    clip_fraction        | 0.149     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0605   |\n",
      "|    explained_variance   | 0.609     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0877   |\n",
      "|    n_updates            | 10450     |\n",
      "|    policy_gradient_loss | -0.0512   |\n",
      "|    value_loss           | 0.0522    |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 61.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 177      |\n",
      "|    iterations      | 551      |\n",
      "|    time_elapsed    | 795      |\n",
      "|    total_timesteps | 141056   |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 61.9       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 177        |\n",
      "|    iterations           | 552        |\n",
      "|    time_elapsed         | 796        |\n",
      "|    total_timesteps      | 141312     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.81290936 |\n",
      "|    clip_fraction        | 0.0711     |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0236    |\n",
      "|    explained_variance   | 0.783      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | 0.0116     |\n",
      "|    n_updates            | 10469      |\n",
      "|    policy_gradient_loss | -0.0242    |\n",
      "|    value_loss           | 0.158      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=141500, episode_reward=70.37 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 70.4      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 141500    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.9938414 |\n",
      "|    clip_fraction        | 0.143     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0617   |\n",
      "|    explained_variance   | 0.897     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.109    |\n",
      "|    n_updates            | 10488     |\n",
      "|    policy_gradient_loss | -0.0519   |\n",
      "|    value_loss           | 0.121     |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 61.8     |\n",
      "| time/              |          |\n",
      "|    fps             | 177      |\n",
      "|    iterations      | 553      |\n",
      "|    time_elapsed    | 797      |\n",
      "|    total_timesteps | 141568   |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 61.8      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 177       |\n",
      "|    iterations           | 554       |\n",
      "|    time_elapsed         | 799       |\n",
      "|    total_timesteps      | 141824    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.6822255 |\n",
      "|    clip_fraction        | 0.214     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.104    |\n",
      "|    explained_variance   | 0.46      |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0545   |\n",
      "|    n_updates            | 10507     |\n",
      "|    policy_gradient_loss | -0.0612   |\n",
      "|    value_loss           | 0.103     |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=142000, episode_reward=57.40 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 57.4      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 142000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.4269748 |\n",
      "|    clip_fraction        | 0.147     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0301   |\n",
      "|    explained_variance   | 0.548     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0494   |\n",
      "|    n_updates            | 10526     |\n",
      "|    policy_gradient_loss | -0.0428   |\n",
      "|    value_loss           | 0.0835    |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 61.8     |\n",
      "| time/              |          |\n",
      "|    fps             | 177      |\n",
      "|    iterations      | 555      |\n",
      "|    time_elapsed    | 800      |\n",
      "|    total_timesteps | 142080   |\n",
      "---------------------------------\n",
      "--------------------------------------\n",
      "| rollout/                |          |\n",
      "|    ep_len_mean          | 889      |\n",
      "|    ep_rew_mean          | 61.9     |\n",
      "| time/                   |          |\n",
      "|    fps                  | 177      |\n",
      "|    iterations           | 556      |\n",
      "|    time_elapsed         | 801      |\n",
      "|    total_timesteps      | 142336   |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 1.848999 |\n",
      "|    clip_fraction        | 0.192    |\n",
      "|    clip_range           | 0.4      |\n",
      "|    entropy_loss         | -0.0558  |\n",
      "|    explained_variance   | 0.882    |\n",
      "|    learning_rate        | 0.000389 |\n",
      "|    loss                 | -0.0396  |\n",
      "|    n_updates            | 10545    |\n",
      "|    policy_gradient_loss | -0.0483  |\n",
      "|    value_loss           | 0.14     |\n",
      "--------------------------------------\n",
      "Eval num_timesteps=142500, episode_reward=54.13 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 230        |\n",
      "|    mean_reward          | 54.1       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 142500     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.93589354 |\n",
      "|    clip_fraction        | 0.204      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.108     |\n",
      "|    explained_variance   | 0.906      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0659    |\n",
      "|    n_updates            | 10564      |\n",
      "|    policy_gradient_loss | -0.0539    |\n",
      "|    value_loss           | 0.13       |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 61.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 177      |\n",
      "|    iterations      | 557      |\n",
      "|    time_elapsed    | 803      |\n",
      "|    total_timesteps | 142592   |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 61.9      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 177       |\n",
      "|    iterations           | 558       |\n",
      "|    time_elapsed         | 804       |\n",
      "|    total_timesteps      | 142848    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.2517009 |\n",
      "|    clip_fraction        | 0.163     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0516   |\n",
      "|    explained_variance   | 0.703     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.113    |\n",
      "|    n_updates            | 10583     |\n",
      "|    policy_gradient_loss | -0.0615   |\n",
      "|    value_loss           | 0.0334    |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=143000, episode_reward=54.74 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 230        |\n",
      "|    mean_reward          | 54.7       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 143000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.56232095 |\n",
      "|    clip_fraction        | 0.11       |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0344    |\n",
      "|    explained_variance   | 0.772      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | 0.0123     |\n",
      "|    n_updates            | 10602      |\n",
      "|    policy_gradient_loss | -0.0436    |\n",
      "|    value_loss           | 0.119      |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 61.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 177      |\n",
      "|    iterations      | 559      |\n",
      "|    time_elapsed    | 806      |\n",
      "|    total_timesteps | 143104   |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 61.7      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 177       |\n",
      "|    iterations           | 560       |\n",
      "|    time_elapsed         | 807       |\n",
      "|    total_timesteps      | 143360    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.8003791 |\n",
      "|    clip_fraction        | 0.135     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0588   |\n",
      "|    explained_variance   | 0.837     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | 0.0114    |\n",
      "|    n_updates            | 10621     |\n",
      "|    policy_gradient_loss | -0.0486   |\n",
      "|    value_loss           | 0.184     |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=143500, episode_reward=80.71 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 230        |\n",
      "|    mean_reward          | 80.7       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 143500     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.91459435 |\n",
      "|    clip_fraction        | 0.223      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0951    |\n",
      "|    explained_variance   | 0.407      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0436    |\n",
      "|    n_updates            | 10640      |\n",
      "|    policy_gradient_loss | -0.0544    |\n",
      "|    value_loss           | 0.0527     |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 61.7     |\n",
      "| time/              |          |\n",
      "|    fps             | 177      |\n",
      "|    iterations      | 561      |\n",
      "|    time_elapsed    | 809      |\n",
      "|    total_timesteps | 143616   |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 61.7      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 177       |\n",
      "|    iterations           | 562       |\n",
      "|    time_elapsed         | 810       |\n",
      "|    total_timesteps      | 143872    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.9380194 |\n",
      "|    clip_fraction        | 0.107     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0365   |\n",
      "|    explained_variance   | 0.801     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.104    |\n",
      "|    n_updates            | 10659     |\n",
      "|    policy_gradient_loss | -0.0373   |\n",
      "|    value_loss           | 0.0743    |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=144000, episode_reward=80.86 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 80.9      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 144000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.6674898 |\n",
      "|    clip_fraction        | 0.119     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0422   |\n",
      "|    explained_variance   | 0.818     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.14     |\n",
      "|    n_updates            | 10678     |\n",
      "|    policy_gradient_loss | -0.0397   |\n",
      "|    value_loss           | 0.0878    |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 61.4     |\n",
      "| time/              |          |\n",
      "|    fps             | 177      |\n",
      "|    iterations      | 563      |\n",
      "|    time_elapsed    | 811      |\n",
      "|    total_timesteps | 144128   |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 61.4      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 177       |\n",
      "|    iterations           | 564       |\n",
      "|    time_elapsed         | 813       |\n",
      "|    total_timesteps      | 144384    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.5332143 |\n",
      "|    clip_fraction        | 0.168     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.112    |\n",
      "|    explained_variance   | 0.755     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0848   |\n",
      "|    n_updates            | 10697     |\n",
      "|    policy_gradient_loss | -0.0523   |\n",
      "|    value_loss           | 0.335     |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=144500, episode_reward=37.49 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 37.5      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 144500    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.2704445 |\n",
      "|    clip_fraction        | 0.159     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0456   |\n",
      "|    explained_variance   | 0.511     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0359   |\n",
      "|    n_updates            | 10716     |\n",
      "|    policy_gradient_loss | -0.0554   |\n",
      "|    value_loss           | 0.0488    |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 61.4     |\n",
      "| time/              |          |\n",
      "|    fps             | 177      |\n",
      "|    iterations      | 565      |\n",
      "|    time_elapsed    | 814      |\n",
      "|    total_timesteps | 144640   |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 61.4       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 177        |\n",
      "|    iterations           | 566        |\n",
      "|    time_elapsed         | 815        |\n",
      "|    total_timesteps      | 144896     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.46710086 |\n",
      "|    clip_fraction        | 0.0927     |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0357    |\n",
      "|    explained_variance   | 0.802      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.00942   |\n",
      "|    n_updates            | 10735      |\n",
      "|    policy_gradient_loss | -0.0276    |\n",
      "|    value_loss           | 0.108      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=145000, episode_reward=26.43 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 230        |\n",
      "|    mean_reward          | 26.4       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 145000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.82968616 |\n",
      "|    clip_fraction        | 0.162      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0958    |\n",
      "|    explained_variance   | 0.771      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0754    |\n",
      "|    n_updates            | 10754      |\n",
      "|    policy_gradient_loss | -0.0362    |\n",
      "|    value_loss           | 0.264      |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 61.6     |\n",
      "| time/              |          |\n",
      "|    fps             | 177      |\n",
      "|    iterations      | 567      |\n",
      "|    time_elapsed    | 817      |\n",
      "|    total_timesteps | 145152   |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 61.6       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 177        |\n",
      "|    iterations           | 568        |\n",
      "|    time_elapsed         | 818        |\n",
      "|    total_timesteps      | 145408     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.68946946 |\n",
      "|    clip_fraction        | 0.174      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0906    |\n",
      "|    explained_variance   | 0.514      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0834    |\n",
      "|    n_updates            | 10773      |\n",
      "|    policy_gradient_loss | -0.0618    |\n",
      "|    value_loss           | 0.0317     |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=145500, episode_reward=5.77 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 5.77      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 145500    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.6037606 |\n",
      "|    clip_fraction        | 0.0746    |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0275   |\n",
      "|    explained_variance   | 0.818     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0911   |\n",
      "|    n_updates            | 10792     |\n",
      "|    policy_gradient_loss | -0.0423   |\n",
      "|    value_loss           | 0.0579    |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 61.6     |\n",
      "| time/              |          |\n",
      "|    fps             | 177      |\n",
      "|    iterations      | 569      |\n",
      "|    time_elapsed    | 819      |\n",
      "|    total_timesteps | 145664   |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 61.7      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 177       |\n",
      "|    iterations           | 570       |\n",
      "|    time_elapsed         | 820       |\n",
      "|    total_timesteps      | 145920    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.6672932 |\n",
      "|    clip_fraction        | 0.138     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0479   |\n",
      "|    explained_variance   | 0.831     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.159    |\n",
      "|    n_updates            | 10811     |\n",
      "|    policy_gradient_loss | -0.055    |\n",
      "|    value_loss           | 0.0948    |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=146000, episode_reward=7.02 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 7.02      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 146000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.7683439 |\n",
      "|    clip_fraction        | 0.182     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.105    |\n",
      "|    explained_variance   | 0.723     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.00243  |\n",
      "|    n_updates            | 10830     |\n",
      "|    policy_gradient_loss | -0.0535   |\n",
      "|    value_loss           | 0.456     |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 61.7     |\n",
      "| time/              |          |\n",
      "|    fps             | 177      |\n",
      "|    iterations      | 571      |\n",
      "|    time_elapsed    | 822      |\n",
      "|    total_timesteps | 146176   |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 61.7      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 177       |\n",
      "|    iterations           | 572       |\n",
      "|    time_elapsed         | 824       |\n",
      "|    total_timesteps      | 146432    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.6368497 |\n",
      "|    clip_fraction        | 0.116     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0518   |\n",
      "|    explained_variance   | 0.419     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0634   |\n",
      "|    n_updates            | 10849     |\n",
      "|    policy_gradient_loss | -0.0455   |\n",
      "|    value_loss           | 0.0943    |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=146500, episode_reward=7.64 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 7.64      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 146500    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.6426577 |\n",
      "|    clip_fraction        | 0.117     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0443   |\n",
      "|    explained_variance   | 0.806     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0305   |\n",
      "|    n_updates            | 10868     |\n",
      "|    policy_gradient_loss | -0.0461   |\n",
      "|    value_loss           | 0.0612    |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 61.6     |\n",
      "| time/              |          |\n",
      "|    fps             | 177      |\n",
      "|    iterations      | 573      |\n",
      "|    time_elapsed    | 825      |\n",
      "|    total_timesteps | 146688   |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 61.6       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 177        |\n",
      "|    iterations           | 574        |\n",
      "|    time_elapsed         | 827        |\n",
      "|    total_timesteps      | 146944     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.70533466 |\n",
      "|    clip_fraction        | 0.141      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0649    |\n",
      "|    explained_variance   | 0.869      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0199    |\n",
      "|    n_updates            | 10887      |\n",
      "|    policy_gradient_loss | -0.0567    |\n",
      "|    value_loss           | 0.134      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=147000, episode_reward=7.48 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "--------------------------------------\n",
      "| eval/                   |          |\n",
      "|    mean_ep_length       | 230      |\n",
      "|    mean_reward          | 7.48     |\n",
      "| time/                   |          |\n",
      "|    total_timesteps      | 147000   |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 0.913018 |\n",
      "|    clip_fraction        | 0.204    |\n",
      "|    clip_range           | 0.4      |\n",
      "|    entropy_loss         | -0.0893  |\n",
      "|    explained_variance   | 0.367    |\n",
      "|    learning_rate        | 0.000389 |\n",
      "|    loss                 | -0.0562  |\n",
      "|    n_updates            | 10906    |\n",
      "|    policy_gradient_loss | -0.0536  |\n",
      "|    value_loss           | 0.0367   |\n",
      "--------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 61.6     |\n",
      "| time/              |          |\n",
      "|    fps             | 177      |\n",
      "|    iterations      | 575      |\n",
      "|    time_elapsed    | 828      |\n",
      "|    total_timesteps | 147200   |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 61.6       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 177        |\n",
      "|    iterations           | 576        |\n",
      "|    time_elapsed         | 829        |\n",
      "|    total_timesteps      | 147456     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.68687993 |\n",
      "|    clip_fraction        | 0.0853     |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0389    |\n",
      "|    explained_variance   | 0.467      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.102     |\n",
      "|    n_updates            | 10925      |\n",
      "|    policy_gradient_loss | -0.0296    |\n",
      "|    value_loss           | 0.154      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=147500, episode_reward=5.17 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 5.17      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 147500    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.1009177 |\n",
      "|    clip_fraction        | 0.139     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0509   |\n",
      "|    explained_variance   | 0.871     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.116    |\n",
      "|    n_updates            | 10944     |\n",
      "|    policy_gradient_loss | -0.037    |\n",
      "|    value_loss           | 0.051     |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 61.5     |\n",
      "| time/              |          |\n",
      "|    fps             | 177      |\n",
      "|    iterations      | 577      |\n",
      "|    time_elapsed    | 831      |\n",
      "|    total_timesteps | 147712   |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 61.5       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 177        |\n",
      "|    iterations           | 578        |\n",
      "|    time_elapsed         | 832        |\n",
      "|    total_timesteps      | 147968     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.85175157 |\n",
      "|    clip_fraction        | 0.203      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.104     |\n",
      "|    explained_variance   | 0.426      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | 0.183      |\n",
      "|    n_updates            | 10963      |\n",
      "|    policy_gradient_loss | -0.0711    |\n",
      "|    value_loss           | 0.584      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=148000, episode_reward=8.71 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 230        |\n",
      "|    mean_reward          | 8.71       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 148000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.78571934 |\n",
      "|    clip_fraction        | 0.121      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0441    |\n",
      "|    explained_variance   | 0.569      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.118     |\n",
      "|    n_updates            | 10982      |\n",
      "|    policy_gradient_loss | -0.0339    |\n",
      "|    value_loss           | 0.0542     |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 61.5     |\n",
      "| time/              |          |\n",
      "|    fps             | 177      |\n",
      "|    iterations      | 579      |\n",
      "|    time_elapsed    | 834      |\n",
      "|    total_timesteps | 148224   |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 61.6      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 177       |\n",
      "|    iterations           | 580       |\n",
      "|    time_elapsed         | 835       |\n",
      "|    total_timesteps      | 148480    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.4827241 |\n",
      "|    clip_fraction        | 0.166     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0689   |\n",
      "|    explained_variance   | 0.671     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0914   |\n",
      "|    n_updates            | 11001     |\n",
      "|    policy_gradient_loss | -0.0621   |\n",
      "|    value_loss           | 0.0863    |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=148500, episode_reward=19.24 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 230        |\n",
      "|    mean_reward          | 19.2       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 148500     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.88783866 |\n",
      "|    clip_fraction        | 0.178      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.101     |\n",
      "|    explained_variance   | 0.522      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.00492   |\n",
      "|    n_updates            | 11020      |\n",
      "|    policy_gradient_loss | -0.072     |\n",
      "|    value_loss           | 0.327      |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 61.6     |\n",
      "| time/              |          |\n",
      "|    fps             | 177      |\n",
      "|    iterations      | 581      |\n",
      "|    time_elapsed    | 836      |\n",
      "|    total_timesteps | 148736   |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 61.6      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 177       |\n",
      "|    iterations           | 582       |\n",
      "|    time_elapsed         | 838       |\n",
      "|    total_timesteps      | 148992    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.5969393 |\n",
      "|    clip_fraction        | 0.174     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0763   |\n",
      "|    explained_variance   | 0.526     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.124    |\n",
      "|    n_updates            | 11039     |\n",
      "|    policy_gradient_loss | -0.0736   |\n",
      "|    value_loss           | 0.0283    |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=149000, episode_reward=67.43 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 67.4      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 149000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.5510186 |\n",
      "|    clip_fraction        | 0.153     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0465   |\n",
      "|    explained_variance   | 0.652     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | 0.00249   |\n",
      "|    n_updates            | 11058     |\n",
      "|    policy_gradient_loss | -0.0567   |\n",
      "|    value_loss           | 0.0889    |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 61.6     |\n",
      "| time/              |          |\n",
      "|    fps             | 177      |\n",
      "|    iterations      | 583      |\n",
      "|    time_elapsed    | 839      |\n",
      "|    total_timesteps | 149248   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=149500, episode_reward=58.18 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 230        |\n",
      "|    mean_reward          | 58.2       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 149500     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.48814896 |\n",
      "|    clip_fraction        | 0.136      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0603    |\n",
      "|    explained_variance   | 0.858      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.028     |\n",
      "|    n_updates            | 11077      |\n",
      "|    policy_gradient_loss | -0.0476    |\n",
      "|    value_loss           | 0.0931     |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 61.7     |\n",
      "| time/              |          |\n",
      "|    fps             | 177      |\n",
      "|    iterations      | 584      |\n",
      "|    time_elapsed    | 840      |\n",
      "|    total_timesteps | 149504   |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 61.7      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 177       |\n",
      "|    iterations           | 585       |\n",
      "|    time_elapsed         | 842       |\n",
      "|    total_timesteps      | 149760    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.7314069 |\n",
      "|    clip_fraction        | 0.233     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.124    |\n",
      "|    explained_variance   | 0.674     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | 0.0953    |\n",
      "|    n_updates            | 11096     |\n",
      "|    policy_gradient_loss | -0.0863   |\n",
      "|    value_loss           | 0.364     |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=150000, episode_reward=14.04 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 14        |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 150000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.1241733 |\n",
      "|    clip_fraction        | 0.137     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0381   |\n",
      "|    explained_variance   | 0.874     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0904   |\n",
      "|    n_updates            | 11115     |\n",
      "|    policy_gradient_loss | -0.0593   |\n",
      "|    value_loss           | 0.0203    |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 61.7     |\n",
      "| time/              |          |\n",
      "|    fps             | 177      |\n",
      "|    iterations      | 586      |\n",
      "|    time_elapsed    | 843      |\n",
      "|    total_timesteps | 150016   |\n",
      "---------------------------------\n",
      "--------------------------------------\n",
      "| rollout/                |          |\n",
      "|    ep_len_mean          | 889      |\n",
      "|    ep_rew_mean          | 61.6     |\n",
      "| time/                   |          |\n",
      "|    fps                  | 177      |\n",
      "|    iterations           | 587      |\n",
      "|    time_elapsed         | 844      |\n",
      "|    total_timesteps      | 150272   |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 1.235276 |\n",
      "|    clip_fraction        | 0.197    |\n",
      "|    clip_range           | 0.4      |\n",
      "|    entropy_loss         | -0.0668  |\n",
      "|    explained_variance   | 0.582    |\n",
      "|    learning_rate        | 0.000389 |\n",
      "|    loss                 | -0.0297  |\n",
      "|    n_updates            | 11134    |\n",
      "|    policy_gradient_loss | -0.0544  |\n",
      "|    value_loss           | 0.211    |\n",
      "--------------------------------------\n",
      "Eval num_timesteps=150500, episode_reward=55.43 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 230        |\n",
      "|    mean_reward          | 55.4       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 150500     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.67208076 |\n",
      "|    clip_fraction        | 0.138      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0882    |\n",
      "|    explained_variance   | 0.403      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | 0.016      |\n",
      "|    n_updates            | 11153      |\n",
      "|    policy_gradient_loss | -0.0436    |\n",
      "|    value_loss           | 0.39       |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 61.6     |\n",
      "| time/              |          |\n",
      "|    fps             | 177      |\n",
      "|    iterations      | 588      |\n",
      "|    time_elapsed    | 846      |\n",
      "|    total_timesteps | 150528   |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 61.6       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 177        |\n",
      "|    iterations           | 589        |\n",
      "|    time_elapsed         | 847        |\n",
      "|    total_timesteps      | 150784     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.98442787 |\n",
      "|    clip_fraction        | 0.183      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0886    |\n",
      "|    explained_variance   | 0.287      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0911    |\n",
      "|    n_updates            | 11172      |\n",
      "|    policy_gradient_loss | -0.0725    |\n",
      "|    value_loss           | 0.0459     |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=151000, episode_reward=24.62 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 230        |\n",
      "|    mean_reward          | 24.6       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 151000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.53040665 |\n",
      "|    clip_fraction        | 0.0872     |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.04      |\n",
      "|    explained_variance   | 0.432      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | 0.00471    |\n",
      "|    n_updates            | 11191      |\n",
      "|    policy_gradient_loss | -0.0434    |\n",
      "|    value_loss           | 0.164      |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 61.6     |\n",
      "| time/              |          |\n",
      "|    fps             | 177      |\n",
      "|    iterations      | 590      |\n",
      "|    time_elapsed    | 849      |\n",
      "|    total_timesteps | 151040   |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 61.4      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 177       |\n",
      "|    iterations           | 591       |\n",
      "|    time_elapsed         | 850       |\n",
      "|    total_timesteps      | 151296    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.1199967 |\n",
      "|    clip_fraction        | 0.157     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0629   |\n",
      "|    explained_variance   | 0.796     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.139    |\n",
      "|    n_updates            | 11210     |\n",
      "|    policy_gradient_loss | -0.0513   |\n",
      "|    value_loss           | 0.157     |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=151500, episode_reward=29.23 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 230        |\n",
      "|    mean_reward          | 29.2       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 151500     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.85754025 |\n",
      "|    clip_fraction        | 0.208      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.135     |\n",
      "|    explained_variance   | 0.425      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | 0.127      |\n",
      "|    n_updates            | 11229      |\n",
      "|    policy_gradient_loss | -0.0698    |\n",
      "|    value_loss           | 0.313      |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 61.4     |\n",
      "| time/              |          |\n",
      "|    fps             | 177      |\n",
      "|    iterations      | 592      |\n",
      "|    time_elapsed    | 852      |\n",
      "|    total_timesteps | 151552   |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 61.4      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 177       |\n",
      "|    iterations           | 593       |\n",
      "|    time_elapsed         | 853       |\n",
      "|    total_timesteps      | 151808    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.1040511 |\n",
      "|    clip_fraction        | 0.124     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0381   |\n",
      "|    explained_variance   | 0.842     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.138    |\n",
      "|    n_updates            | 11248     |\n",
      "|    policy_gradient_loss | -0.0435   |\n",
      "|    value_loss           | 0.0227    |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=152000, episode_reward=35.47 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 35.5      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 152000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.0444055 |\n",
      "|    clip_fraction        | 0.147     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.058    |\n",
      "|    explained_variance   | 0.13      |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0499   |\n",
      "|    n_updates            | 11267     |\n",
      "|    policy_gradient_loss | -0.0325   |\n",
      "|    value_loss           | 0.294     |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 61.4     |\n",
      "| time/              |          |\n",
      "|    fps             | 177      |\n",
      "|    iterations      | 594      |\n",
      "|    time_elapsed    | 854      |\n",
      "|    total_timesteps | 152064   |\n",
      "---------------------------------\n",
      "--------------------------------------\n",
      "| rollout/                |          |\n",
      "|    ep_len_mean          | 889      |\n",
      "|    ep_rew_mean          | 61.4     |\n",
      "| time/                   |          |\n",
      "|    fps                  | 177      |\n",
      "|    iterations           | 595      |\n",
      "|    time_elapsed         | 856      |\n",
      "|    total_timesteps      | 152320   |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 0.802927 |\n",
      "|    clip_fraction        | 0.214    |\n",
      "|    clip_range           | 0.4      |\n",
      "|    entropy_loss         | -0.127   |\n",
      "|    explained_variance   | 0.824    |\n",
      "|    learning_rate        | 0.000389 |\n",
      "|    loss                 | -0.0743  |\n",
      "|    n_updates            | 11286    |\n",
      "|    policy_gradient_loss | -0.0785  |\n",
      "|    value_loss           | 0.142    |\n",
      "--------------------------------------\n",
      "Eval num_timesteps=152500, episode_reward=70.18 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 70.2      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 152500    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.5767473 |\n",
      "|    clip_fraction        | 0.137     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0843   |\n",
      "|    explained_variance   | 0.634     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0631   |\n",
      "|    n_updates            | 11305     |\n",
      "|    policy_gradient_loss | -0.051    |\n",
      "|    value_loss           | 0.0391    |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 61.4     |\n",
      "| time/              |          |\n",
      "|    fps             | 177      |\n",
      "|    iterations      | 596      |\n",
      "|    time_elapsed    | 857      |\n",
      "|    total_timesteps | 152576   |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 61.4      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 177       |\n",
      "|    iterations           | 597       |\n",
      "|    time_elapsed         | 858       |\n",
      "|    total_timesteps      | 152832    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.8512877 |\n",
      "|    clip_fraction        | 0.106     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0374   |\n",
      "|    explained_variance   | 0.723     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.002    |\n",
      "|    n_updates            | 11324     |\n",
      "|    policy_gradient_loss | -0.0347   |\n",
      "|    value_loss           | 0.116     |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=153000, episode_reward=21.96 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 22        |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 153000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.0342304 |\n",
      "|    clip_fraction        | 0.162     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0651   |\n",
      "|    explained_variance   | 0.837     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0738   |\n",
      "|    n_updates            | 11343     |\n",
      "|    policy_gradient_loss | -0.0343   |\n",
      "|    value_loss           | 0.116     |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 61.3     |\n",
      "| time/              |          |\n",
      "|    fps             | 177      |\n",
      "|    iterations      | 598      |\n",
      "|    time_elapsed    | 860      |\n",
      "|    total_timesteps | 153088   |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 61.3       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 178        |\n",
      "|    iterations           | 599        |\n",
      "|    time_elapsed         | 861        |\n",
      "|    total_timesteps      | 153344     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.77464986 |\n",
      "|    clip_fraction        | 0.215      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.104     |\n",
      "|    explained_variance   | 0.691      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.103     |\n",
      "|    n_updates            | 11362      |\n",
      "|    policy_gradient_loss | -0.0682    |\n",
      "|    value_loss           | 0.0873     |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=153500, episode_reward=9.20 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "--------------------------------------\n",
      "| eval/                   |          |\n",
      "|    mean_ep_length       | 230      |\n",
      "|    mean_reward          | 9.2      |\n",
      "| time/                   |          |\n",
      "|    total_timesteps      | 153500   |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 0.609686 |\n",
      "|    clip_fraction        | 0.129    |\n",
      "|    clip_range           | 0.4      |\n",
      "|    entropy_loss         | -0.051   |\n",
      "|    explained_variance   | 0.78     |\n",
      "|    learning_rate        | 0.000389 |\n",
      "|    loss                 | -0.108   |\n",
      "|    n_updates            | 11381    |\n",
      "|    policy_gradient_loss | -0.0579  |\n",
      "|    value_loss           | 0.0538   |\n",
      "--------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 61.3     |\n",
      "| time/              |          |\n",
      "|    fps             | 178      |\n",
      "|    iterations      | 600      |\n",
      "|    time_elapsed    | 862      |\n",
      "|    total_timesteps | 153600   |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 61.1      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 178       |\n",
      "|    iterations           | 601       |\n",
      "|    time_elapsed         | 864       |\n",
      "|    total_timesteps      | 153856    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.5213146 |\n",
      "|    clip_fraction        | 0.17      |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.066    |\n",
      "|    explained_variance   | 0.506     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.12     |\n",
      "|    n_updates            | 11400     |\n",
      "|    policy_gradient_loss | -0.0418   |\n",
      "|    value_loss           | 0.134     |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=154000, episode_reward=16.12 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 16.1      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 154000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.9968659 |\n",
      "|    clip_fraction        | 0.143     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0934   |\n",
      "|    explained_variance   | 0.17      |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | 0.0142    |\n",
      "|    n_updates            | 11419     |\n",
      "|    policy_gradient_loss | -0.0469   |\n",
      "|    value_loss           | 0.275     |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 61.1     |\n",
      "| time/              |          |\n",
      "|    fps             | 178      |\n",
      "|    iterations      | 602      |\n",
      "|    time_elapsed    | 865      |\n",
      "|    total_timesteps | 154112   |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 61.1       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 178        |\n",
      "|    iterations           | 603        |\n",
      "|    time_elapsed         | 867        |\n",
      "|    total_timesteps      | 154368     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.89238226 |\n",
      "|    clip_fraction        | 0.231      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0981    |\n",
      "|    explained_variance   | 0.4        |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0882    |\n",
      "|    n_updates            | 11438      |\n",
      "|    policy_gradient_loss | -0.0709    |\n",
      "|    value_loss           | 0.0546     |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=154500, episode_reward=25.41 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 230        |\n",
      "|    mean_reward          | 25.4       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 154500     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.52420235 |\n",
      "|    clip_fraction        | 0.0989     |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0371    |\n",
      "|    explained_variance   | 0.38       |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0351    |\n",
      "|    n_updates            | 11457      |\n",
      "|    policy_gradient_loss | -0.0333    |\n",
      "|    value_loss           | 0.204      |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 61.1     |\n",
      "| time/              |          |\n",
      "|    fps             | 178      |\n",
      "|    iterations      | 604      |\n",
      "|    time_elapsed    | 868      |\n",
      "|    total_timesteps | 154624   |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 61        |\n",
      "| time/                   |           |\n",
      "|    fps                  | 178       |\n",
      "|    iterations           | 605       |\n",
      "|    time_elapsed         | 869       |\n",
      "|    total_timesteps      | 154880    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.1574004 |\n",
      "|    clip_fraction        | 0.184     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0896   |\n",
      "|    explained_variance   | 0.75      |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.17     |\n",
      "|    n_updates            | 11476     |\n",
      "|    policy_gradient_loss | -0.0689   |\n",
      "|    value_loss           | 0.143     |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=155000, episode_reward=42.77 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 42.8      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 155000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.5804605 |\n",
      "|    clip_fraction        | 0.201     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.122    |\n",
      "|    explained_variance   | 0.705     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | 0.0516    |\n",
      "|    n_updates            | 11495     |\n",
      "|    policy_gradient_loss | -0.0537   |\n",
      "|    value_loss           | 0.186     |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 61       |\n",
      "| time/              |          |\n",
      "|    fps             | 178      |\n",
      "|    iterations      | 606      |\n",
      "|    time_elapsed    | 871      |\n",
      "|    total_timesteps | 155136   |\n",
      "---------------------------------\n",
      "--------------------------------------\n",
      "| rollout/                |          |\n",
      "|    ep_len_mean          | 889      |\n",
      "|    ep_rew_mean          | 61       |\n",
      "| time/                   |          |\n",
      "|    fps                  | 178      |\n",
      "|    iterations           | 607      |\n",
      "|    time_elapsed         | 872      |\n",
      "|    total_timesteps      | 155392   |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 0.666486 |\n",
      "|    clip_fraction        | 0.14     |\n",
      "|    clip_range           | 0.4      |\n",
      "|    entropy_loss         | -0.0491  |\n",
      "|    explained_variance   | 0.754    |\n",
      "|    learning_rate        | 0.000389 |\n",
      "|    loss                 | -0.084   |\n",
      "|    n_updates            | 11514    |\n",
      "|    policy_gradient_loss | -0.0439  |\n",
      "|    value_loss           | 0.0711   |\n",
      "--------------------------------------\n",
      "Eval num_timesteps=155500, episode_reward=8.75 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 8.75      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 155500    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.9093275 |\n",
      "|    clip_fraction        | 0.161     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0587   |\n",
      "|    explained_variance   | 0.432     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0568   |\n",
      "|    n_updates            | 11533     |\n",
      "|    policy_gradient_loss | -0.0678   |\n",
      "|    value_loss           | 0.217     |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 60.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 178      |\n",
      "|    iterations      | 608      |\n",
      "|    time_elapsed    | 873      |\n",
      "|    total_timesteps | 155648   |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 60.9      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 178       |\n",
      "|    iterations           | 609       |\n",
      "|    time_elapsed         | 875       |\n",
      "|    total_timesteps      | 155904    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.9917356 |\n",
      "|    clip_fraction        | 0.189     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.101    |\n",
      "|    explained_variance   | 0.66      |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0805   |\n",
      "|    n_updates            | 11552     |\n",
      "|    policy_gradient_loss | -0.0676   |\n",
      "|    value_loss           | 0.372     |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=156000, episode_reward=54.53 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 54.5      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 156000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.2561626 |\n",
      "|    clip_fraction        | 0.209     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.082    |\n",
      "|    explained_variance   | 0.482     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.106    |\n",
      "|    n_updates            | 11571     |\n",
      "|    policy_gradient_loss | -0.07     |\n",
      "|    value_loss           | 0.0549    |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 60.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 178      |\n",
      "|    iterations      | 610      |\n",
      "|    time_elapsed    | 876      |\n",
      "|    total_timesteps | 156160   |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 60.9       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 178        |\n",
      "|    iterations           | 611        |\n",
      "|    time_elapsed         | 877        |\n",
      "|    total_timesteps      | 156416     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.62667036 |\n",
      "|    clip_fraction        | 0.0824     |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0414    |\n",
      "|    explained_variance   | 0.52       |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | 0.000715   |\n",
      "|    n_updates            | 11590      |\n",
      "|    policy_gradient_loss | -0.0403    |\n",
      "|    value_loss           | 0.224      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=156500, episode_reward=66.59 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 66.6      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 156500    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.9861127 |\n",
      "|    clip_fraction        | 0.139     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0594   |\n",
      "|    explained_variance   | 0.817     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0361   |\n",
      "|    n_updates            | 11609     |\n",
      "|    policy_gradient_loss | -0.0538   |\n",
      "|    value_loss           | 0.141     |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 60.7     |\n",
      "| time/              |          |\n",
      "|    fps             | 178      |\n",
      "|    iterations      | 612      |\n",
      "|    time_elapsed    | 879      |\n",
      "|    total_timesteps | 156672   |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 60.7       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 178        |\n",
      "|    iterations           | 613        |\n",
      "|    time_elapsed         | 881        |\n",
      "|    total_timesteps      | 156928     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.68707913 |\n",
      "|    clip_fraction        | 0.17       |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0791    |\n",
      "|    explained_variance   | 0.652      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0806    |\n",
      "|    n_updates            | 11628      |\n",
      "|    policy_gradient_loss | -0.0591    |\n",
      "|    value_loss           | 0.275      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=157000, episode_reward=15.21 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 230        |\n",
      "|    mean_reward          | 15.2       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 157000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.38480496 |\n",
      "|    clip_fraction        | 0.0697     |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0292    |\n",
      "|    explained_variance   | 0.822      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | 0.00112    |\n",
      "|    n_updates            | 11647      |\n",
      "|    policy_gradient_loss | -0.028     |\n",
      "|    value_loss           | 0.0488     |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 60.7     |\n",
      "| time/              |          |\n",
      "|    fps             | 178      |\n",
      "|    iterations      | 614      |\n",
      "|    time_elapsed    | 882      |\n",
      "|    total_timesteps | 157184   |\n",
      "---------------------------------\n",
      "--------------------------------------\n",
      "| rollout/                |          |\n",
      "|    ep_len_mean          | 889      |\n",
      "|    ep_rew_mean          | 60.3     |\n",
      "| time/                   |          |\n",
      "|    fps                  | 178      |\n",
      "|    iterations           | 615      |\n",
      "|    time_elapsed         | 883      |\n",
      "|    total_timesteps      | 157440   |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 1.416688 |\n",
      "|    clip_fraction        | 0.13     |\n",
      "|    clip_range           | 0.4      |\n",
      "|    entropy_loss         | -0.0453  |\n",
      "|    explained_variance   | 0.392    |\n",
      "|    learning_rate        | 0.000389 |\n",
      "|    loss                 | -0.0702  |\n",
      "|    n_updates            | 11666    |\n",
      "|    policy_gradient_loss | -0.0615  |\n",
      "|    value_loss           | 0.118    |\n",
      "--------------------------------------\n",
      "Eval num_timesteps=157500, episode_reward=36.31 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 230        |\n",
      "|    mean_reward          | 36.3       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 157500     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.71230596 |\n",
      "|    clip_fraction        | 0.207      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.125     |\n",
      "|    explained_variance   | 0.163      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.111     |\n",
      "|    n_updates            | 11685      |\n",
      "|    policy_gradient_loss | -0.0552    |\n",
      "|    value_loss           | 0.395      |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 60.3     |\n",
      "| time/              |          |\n",
      "|    fps             | 178      |\n",
      "|    iterations      | 616      |\n",
      "|    time_elapsed    | 885      |\n",
      "|    total_timesteps | 157696   |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 60.3      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 178       |\n",
      "|    iterations           | 617       |\n",
      "|    time_elapsed         | 887       |\n",
      "|    total_timesteps      | 157952    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.4108571 |\n",
      "|    clip_fraction        | 0.171     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0552   |\n",
      "|    explained_variance   | 0.39      |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | 0.0103    |\n",
      "|    n_updates            | 11704     |\n",
      "|    policy_gradient_loss | -0.0704   |\n",
      "|    value_loss           | 0.0534    |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=158000, episode_reward=26.06 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 26.1      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 158000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.5945031 |\n",
      "|    clip_fraction        | 0.0979    |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0422   |\n",
      "|    explained_variance   | 0.545     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0383   |\n",
      "|    n_updates            | 11723     |\n",
      "|    policy_gradient_loss | -0.0379   |\n",
      "|    value_loss           | 0.115     |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 60.3     |\n",
      "| time/              |          |\n",
      "|    fps             | 177      |\n",
      "|    iterations      | 618      |\n",
      "|    time_elapsed    | 888      |\n",
      "|    total_timesteps | 158208   |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 60.2       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 177        |\n",
      "|    iterations           | 619        |\n",
      "|    time_elapsed         | 890        |\n",
      "|    total_timesteps      | 158464     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.74652535 |\n",
      "|    clip_fraction        | 0.153      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0682    |\n",
      "|    explained_variance   | 0.669      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0577    |\n",
      "|    n_updates            | 11742      |\n",
      "|    policy_gradient_loss | -0.0498    |\n",
      "|    value_loss           | 0.189      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=158500, episode_reward=50.45 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 230        |\n",
      "|    mean_reward          | 50.4       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 158500     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.65771735 |\n",
      "|    clip_fraction        | 0.153      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0759    |\n",
      "|    explained_variance   | 0.708      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.11      |\n",
      "|    n_updates            | 11761      |\n",
      "|    policy_gradient_loss | -0.0579    |\n",
      "|    value_loss           | 0.0856     |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 60.2     |\n",
      "| time/              |          |\n",
      "|    fps             | 177      |\n",
      "|    iterations      | 620      |\n",
      "|    time_elapsed    | 892      |\n",
      "|    total_timesteps | 158720   |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 60.2      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 177       |\n",
      "|    iterations           | 621       |\n",
      "|    time_elapsed         | 894       |\n",
      "|    total_timesteps      | 158976    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.8031693 |\n",
      "|    clip_fraction        | 0.115     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0432   |\n",
      "|    explained_variance   | 0.776     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0796   |\n",
      "|    n_updates            | 11780     |\n",
      "|    policy_gradient_loss | -0.0566   |\n",
      "|    value_loss           | 0.0491    |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=159000, episode_reward=54.61 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 230        |\n",
      "|    mean_reward          | 54.6       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 159000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.65270627 |\n",
      "|    clip_fraction        | 0.097      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0372    |\n",
      "|    explained_variance   | 0.505      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.089     |\n",
      "|    n_updates            | 11799      |\n",
      "|    policy_gradient_loss | -0.0468    |\n",
      "|    value_loss           | 0.107      |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 60.1     |\n",
      "| time/              |          |\n",
      "|    fps             | 177      |\n",
      "|    iterations      | 622      |\n",
      "|    time_elapsed    | 896      |\n",
      "|    total_timesteps | 159232   |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 60.1       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 177        |\n",
      "|    iterations           | 623        |\n",
      "|    time_elapsed         | 898        |\n",
      "|    total_timesteps      | 159488     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.88535553 |\n",
      "|    clip_fraction        | 0.19       |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.104     |\n",
      "|    explained_variance   | 0.687      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0952    |\n",
      "|    n_updates            | 11818      |\n",
      "|    policy_gradient_loss | -0.0694    |\n",
      "|    value_loss           | 0.224      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=159500, episode_reward=67.58 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 67.6      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 159500    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.6726364 |\n",
      "|    clip_fraction        | 0.145     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0625   |\n",
      "|    explained_variance   | 0.158     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0589   |\n",
      "|    n_updates            | 11837     |\n",
      "|    policy_gradient_loss | -0.0513   |\n",
      "|    value_loss           | 0.0883    |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 60.1     |\n",
      "| time/              |          |\n",
      "|    fps             | 177      |\n",
      "|    iterations      | 624      |\n",
      "|    time_elapsed    | 900      |\n",
      "|    total_timesteps | 159744   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=160000, episode_reward=53.31 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "--------------------------------------\n",
      "| eval/                   |          |\n",
      "|    mean_ep_length       | 230      |\n",
      "|    mean_reward          | 53.3     |\n",
      "| time/                   |          |\n",
      "|    total_timesteps      | 160000   |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 0.795122 |\n",
      "|    clip_fraction        | 0.119    |\n",
      "|    clip_range           | 0.4      |\n",
      "|    entropy_loss         | -0.0473  |\n",
      "|    explained_variance   | 0.677    |\n",
      "|    learning_rate        | 0.000389 |\n",
      "|    loss                 | 0.0735   |\n",
      "|    n_updates            | 11856    |\n",
      "|    policy_gradient_loss | -0.0363  |\n",
      "|    value_loss           | 0.213    |\n",
      "--------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 60.1     |\n",
      "| time/              |          |\n",
      "|    fps             | 177      |\n",
      "|    iterations      | 625      |\n",
      "|    time_elapsed    | 901      |\n",
      "|    total_timesteps | 160000   |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 59.9      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 177       |\n",
      "|    iterations           | 626       |\n",
      "|    time_elapsed         | 903       |\n",
      "|    total_timesteps      | 160256    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.7583107 |\n",
      "|    clip_fraction        | 0.137     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0752   |\n",
      "|    explained_variance   | 0.786     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0902   |\n",
      "|    n_updates            | 11875     |\n",
      "|    policy_gradient_loss | -0.0424   |\n",
      "|    value_loss           | 0.175     |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=160500, episode_reward=63.80 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 230        |\n",
      "|    mean_reward          | 63.8       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 160500     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.56476575 |\n",
      "|    clip_fraction        | 0.187      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0902    |\n",
      "|    explained_variance   | 0.703      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.087     |\n",
      "|    n_updates            | 11894      |\n",
      "|    policy_gradient_loss | -0.06      |\n",
      "|    value_loss           | 0.0389     |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 59.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 177      |\n",
      "|    iterations      | 627      |\n",
      "|    time_elapsed    | 904      |\n",
      "|    total_timesteps | 160512   |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 59.9       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 177        |\n",
      "|    iterations           | 628        |\n",
      "|    time_elapsed         | 906        |\n",
      "|    total_timesteps      | 160768     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.73852885 |\n",
      "|    clip_fraction        | 0.131      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0478    |\n",
      "|    explained_variance   | 0.653      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.089     |\n",
      "|    n_updates            | 11913      |\n",
      "|    policy_gradient_loss | -0.0538    |\n",
      "|    value_loss           | 0.106      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=161000, episode_reward=73.16 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 73.2      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 161000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.9951518 |\n",
      "|    clip_fraction        | 0.148     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0464   |\n",
      "|    explained_variance   | 0.722     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0587   |\n",
      "|    n_updates            | 11932     |\n",
      "|    policy_gradient_loss | -0.0522   |\n",
      "|    value_loss           | 0.105     |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 59.7     |\n",
      "| time/              |          |\n",
      "|    fps             | 177      |\n",
      "|    iterations      | 629      |\n",
      "|    time_elapsed    | 908      |\n",
      "|    total_timesteps | 161024   |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 59.7      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 177       |\n",
      "|    iterations           | 630       |\n",
      "|    time_elapsed         | 910       |\n",
      "|    total_timesteps      | 161280    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.7627131 |\n",
      "|    clip_fraction        | 0.215     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.135    |\n",
      "|    explained_variance   | 0.846     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0473   |\n",
      "|    n_updates            | 11951     |\n",
      "|    policy_gradient_loss | -0.0507   |\n",
      "|    value_loss           | 0.179     |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=161500, episode_reward=49.90 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 230        |\n",
      "|    mean_reward          | 49.9       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 161500     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.94936335 |\n",
      "|    clip_fraction        | 0.163      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0598    |\n",
      "|    explained_variance   | 0.121      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0345    |\n",
      "|    n_updates            | 11970      |\n",
      "|    policy_gradient_loss | -0.0408    |\n",
      "|    value_loss           | 0.0646     |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 59.7     |\n",
      "| time/              |          |\n",
      "|    fps             | 177      |\n",
      "|    iterations      | 631      |\n",
      "|    time_elapsed    | 912      |\n",
      "|    total_timesteps | 161536   |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 59.7       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 177        |\n",
      "|    iterations           | 632        |\n",
      "|    time_elapsed         | 913        |\n",
      "|    total_timesteps      | 161792     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.56331635 |\n",
      "|    clip_fraction        | 0.121      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0531    |\n",
      "|    explained_variance   | 0.778      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.043     |\n",
      "|    n_updates            | 11989      |\n",
      "|    policy_gradient_loss | -0.045     |\n",
      "|    value_loss           | 0.0906     |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=162000, episode_reward=56.26 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 230        |\n",
      "|    mean_reward          | 56.3       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 162000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.84748673 |\n",
      "|    clip_fraction        | 0.133      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0648    |\n",
      "|    explained_variance   | 0.285      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0498    |\n",
      "|    n_updates            | 12008      |\n",
      "|    policy_gradient_loss | -0.0472    |\n",
      "|    value_loss           | 0.657      |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 59.7     |\n",
      "| time/              |          |\n",
      "|    fps             | 177      |\n",
      "|    iterations      | 633      |\n",
      "|    time_elapsed    | 915      |\n",
      "|    total_timesteps | 162048   |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 59.7       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 176        |\n",
      "|    iterations           | 634        |\n",
      "|    time_elapsed         | 917        |\n",
      "|    total_timesteps      | 162304     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.60653305 |\n",
      "|    clip_fraction        | 0.169      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0952    |\n",
      "|    explained_variance   | 0.41       |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0775    |\n",
      "|    n_updates            | 12027      |\n",
      "|    policy_gradient_loss | -0.0685    |\n",
      "|    value_loss           | 0.0421     |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=162500, episode_reward=40.03 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 40        |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 162500    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 2.0694947 |\n",
      "|    clip_fraction        | 0.17      |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0439   |\n",
      "|    explained_variance   | 0.507     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | 0.0108    |\n",
      "|    n_updates            | 12046     |\n",
      "|    policy_gradient_loss | -0.0601   |\n",
      "|    value_loss           | 0.105     |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 59.7     |\n",
      "| time/              |          |\n",
      "|    fps             | 176      |\n",
      "|    iterations      | 635      |\n",
      "|    time_elapsed    | 918      |\n",
      "|    total_timesteps | 162560   |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 59.5      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 176       |\n",
      "|    iterations           | 636       |\n",
      "|    time_elapsed         | 920       |\n",
      "|    total_timesteps      | 162816    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.1373317 |\n",
      "|    clip_fraction        | 0.13      |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0425   |\n",
      "|    explained_variance   | 0.647     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0213   |\n",
      "|    n_updates            | 12065     |\n",
      "|    policy_gradient_loss | -0.0517   |\n",
      "|    value_loss           | 0.126     |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=163000, episode_reward=44.93 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 230        |\n",
      "|    mean_reward          | 44.9       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 163000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.40685904 |\n",
      "|    clip_fraction        | 0.213      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.135     |\n",
      "|    explained_variance   | 0.449      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0979    |\n",
      "|    n_updates            | 12084      |\n",
      "|    policy_gradient_loss | -0.069     |\n",
      "|    value_loss           | 0.348      |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 59.5     |\n",
      "| time/              |          |\n",
      "|    fps             | 176      |\n",
      "|    iterations      | 637      |\n",
      "|    time_elapsed    | 922      |\n",
      "|    total_timesteps | 163072   |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 59.5       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 176        |\n",
      "|    iterations           | 638        |\n",
      "|    time_elapsed         | 924        |\n",
      "|    total_timesteps      | 163328     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.74484676 |\n",
      "|    clip_fraction        | 0.153      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0703    |\n",
      "|    explained_variance   | 0.411      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.109     |\n",
      "|    n_updates            | 12103      |\n",
      "|    policy_gradient_loss | -0.0505    |\n",
      "|    value_loss           | 0.0514     |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=163500, episode_reward=37.29 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 230        |\n",
      "|    mean_reward          | 37.3       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 163500     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.39493644 |\n",
      "|    clip_fraction        | 0.0631     |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0273    |\n",
      "|    explained_variance   | 0.593      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.103     |\n",
      "|    n_updates            | 12122      |\n",
      "|    policy_gradient_loss | -0.0253    |\n",
      "|    value_loss           | 0.105      |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 59.4     |\n",
      "| time/              |          |\n",
      "|    fps             | 176      |\n",
      "|    iterations      | 639      |\n",
      "|    time_elapsed    | 926      |\n",
      "|    total_timesteps | 163584   |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 59.4      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 176       |\n",
      "|    iterations           | 640       |\n",
      "|    time_elapsed         | 928       |\n",
      "|    total_timesteps      | 163840    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.6451558 |\n",
      "|    clip_fraction        | 0.116     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0648   |\n",
      "|    explained_variance   | 0.357     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0376   |\n",
      "|    n_updates            | 12141     |\n",
      "|    policy_gradient_loss | -0.0273   |\n",
      "|    value_loss           | 0.989     |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=164000, episode_reward=36.82 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 36.8      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 164000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.8649523 |\n",
      "|    clip_fraction        | 0.218     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.103    |\n",
      "|    explained_variance   | 0.0709    |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0394   |\n",
      "|    n_updates            | 12160     |\n",
      "|    policy_gradient_loss | -0.072    |\n",
      "|    value_loss           | 0.0893    |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 59.4     |\n",
      "| time/              |          |\n",
      "|    fps             | 176      |\n",
      "|    iterations      | 641      |\n",
      "|    time_elapsed    | 930      |\n",
      "|    total_timesteps | 164096   |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 59.4       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 176        |\n",
      "|    iterations           | 642        |\n",
      "|    time_elapsed         | 931        |\n",
      "|    total_timesteps      | 164352     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.70617855 |\n",
      "|    clip_fraction        | 0.118      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0416    |\n",
      "|    explained_variance   | 0.756      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0138    |\n",
      "|    n_updates            | 12179      |\n",
      "|    policy_gradient_loss | -0.0406    |\n",
      "|    value_loss           | 0.111      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=164500, episode_reward=82.71 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 82.7      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 164500    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.6765313 |\n",
      "|    clip_fraction        | 0.113     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0553   |\n",
      "|    explained_variance   | 0.737     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0562   |\n",
      "|    n_updates            | 12198     |\n",
      "|    policy_gradient_loss | -0.0442   |\n",
      "|    value_loss           | 0.0891    |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 59.2     |\n",
      "| time/              |          |\n",
      "|    fps             | 176      |\n",
      "|    iterations      | 643      |\n",
      "|    time_elapsed    | 933      |\n",
      "|    total_timesteps | 164608   |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 59.2      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 176       |\n",
      "|    iterations           | 644       |\n",
      "|    time_elapsed         | 935       |\n",
      "|    total_timesteps      | 164864    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.5610889 |\n",
      "|    clip_fraction        | 0.21      |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.137    |\n",
      "|    explained_variance   | 0.662     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | 0.0183    |\n",
      "|    n_updates            | 12217     |\n",
      "|    policy_gradient_loss | -0.0759   |\n",
      "|    value_loss           | 0.419     |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=165000, episode_reward=49.81 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 230        |\n",
      "|    mean_reward          | 49.8       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 165000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.90249467 |\n",
      "|    clip_fraction        | 0.132      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0528    |\n",
      "|    explained_variance   | 0.498      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0441    |\n",
      "|    n_updates            | 12236      |\n",
      "|    policy_gradient_loss | -0.055     |\n",
      "|    value_loss           | 0.0419     |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 59.2     |\n",
      "| time/              |          |\n",
      "|    fps             | 176      |\n",
      "|    iterations      | 645      |\n",
      "|    time_elapsed    | 937      |\n",
      "|    total_timesteps | 165120   |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 59        |\n",
      "| time/                   |           |\n",
      "|    fps                  | 176       |\n",
      "|    iterations           | 646       |\n",
      "|    time_elapsed         | 938       |\n",
      "|    total_timesteps      | 165376    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.0090768 |\n",
      "|    clip_fraction        | 0.159     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0574   |\n",
      "|    explained_variance   | 0.601     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0778   |\n",
      "|    n_updates            | 12255     |\n",
      "|    policy_gradient_loss | -0.0608   |\n",
      "|    value_loss           | 0.163     |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=165500, episode_reward=76.22 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 230        |\n",
      "|    mean_reward          | 76.2       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 165500     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.82705826 |\n",
      "|    clip_fraction        | 0.208      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0926    |\n",
      "|    explained_variance   | 0.51       |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0675    |\n",
      "|    n_updates            | 12274      |\n",
      "|    policy_gradient_loss | -0.0743    |\n",
      "|    value_loss           | 0.195      |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 59       |\n",
      "| time/              |          |\n",
      "|    fps             | 176      |\n",
      "|    iterations      | 647      |\n",
      "|    time_elapsed    | 940      |\n",
      "|    total_timesteps | 165632   |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 59         |\n",
      "| time/                   |            |\n",
      "|    fps                  | 176        |\n",
      "|    iterations           | 648        |\n",
      "|    time_elapsed         | 941        |\n",
      "|    total_timesteps      | 165888     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.86118305 |\n",
      "|    clip_fraction        | 0.185      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0914    |\n",
      "|    explained_variance   | 0.485      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.041     |\n",
      "|    n_updates            | 12293      |\n",
      "|    policy_gradient_loss | -0.0719    |\n",
      "|    value_loss           | 0.0502     |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=166000, episode_reward=62.79 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 62.8      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 166000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.9457897 |\n",
      "|    clip_fraction        | 0.104     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0432   |\n",
      "|    explained_variance   | 0.607     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0286   |\n",
      "|    n_updates            | 12312     |\n",
      "|    policy_gradient_loss | -0.0424   |\n",
      "|    value_loss           | 0.303     |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 59       |\n",
      "| time/              |          |\n",
      "|    fps             | 176      |\n",
      "|    iterations      | 649      |\n",
      "|    time_elapsed    | 943      |\n",
      "|    total_timesteps | 166144   |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 58.8       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 176        |\n",
      "|    iterations           | 650        |\n",
      "|    time_elapsed         | 944        |\n",
      "|    total_timesteps      | 166400     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.65299237 |\n",
      "|    clip_fraction        | 0.135      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.055     |\n",
      "|    explained_variance   | 0.65       |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0963    |\n",
      "|    n_updates            | 12331      |\n",
      "|    policy_gradient_loss | -0.0567    |\n",
      "|    value_loss           | 0.144      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=166500, episode_reward=73.18 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 73.2      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 166500    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.0095332 |\n",
      "|    clip_fraction        | 0.2       |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.107    |\n",
      "|    explained_variance   | 0.473     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0921   |\n",
      "|    n_updates            | 12350     |\n",
      "|    policy_gradient_loss | -0.0656   |\n",
      "|    value_loss           | 0.29      |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 58.8     |\n",
      "| time/              |          |\n",
      "|    fps             | 175      |\n",
      "|    iterations      | 651      |\n",
      "|    time_elapsed    | 947      |\n",
      "|    total_timesteps | 166656   |\n",
      "---------------------------------\n",
      "--------------------------------------\n",
      "| rollout/                |          |\n",
      "|    ep_len_mean          | 889      |\n",
      "|    ep_rew_mean          | 58.8     |\n",
      "| time/                   |          |\n",
      "|    fps                  | 175      |\n",
      "|    iterations           | 652      |\n",
      "|    time_elapsed         | 948      |\n",
      "|    total_timesteps      | 166912   |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 0.663993 |\n",
      "|    clip_fraction        | 0.147    |\n",
      "|    clip_range           | 0.4      |\n",
      "|    entropy_loss         | -0.058   |\n",
      "|    explained_variance   | 0.838    |\n",
      "|    learning_rate        | 0.000389 |\n",
      "|    loss                 | -0.0694  |\n",
      "|    n_updates            | 12369    |\n",
      "|    policy_gradient_loss | -0.0512  |\n",
      "|    value_loss           | 0.0253   |\n",
      "--------------------------------------\n",
      "Eval num_timesteps=167000, episode_reward=82.30 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 230        |\n",
      "|    mean_reward          | 82.3       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 167000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.69048744 |\n",
      "|    clip_fraction        | 0.168      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0708    |\n",
      "|    explained_variance   | 0.636      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0956    |\n",
      "|    n_updates            | 12388      |\n",
      "|    policy_gradient_loss | -0.0601    |\n",
      "|    value_loss           | 0.313      |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 58.8     |\n",
      "| time/              |          |\n",
      "|    fps             | 175      |\n",
      "|    iterations      | 653      |\n",
      "|    time_elapsed    | 950      |\n",
      "|    total_timesteps | 167168   |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 58.8       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 175        |\n",
      "|    iterations           | 654        |\n",
      "|    time_elapsed         | 951        |\n",
      "|    total_timesteps      | 167424     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.82581794 |\n",
      "|    clip_fraction        | 0.175      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0713    |\n",
      "|    explained_variance   | 0.632      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0496    |\n",
      "|    n_updates            | 12407      |\n",
      "|    policy_gradient_loss | -0.0704    |\n",
      "|    value_loss           | 0.18       |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=167500, episode_reward=42.67 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 42.7      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 167500    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.8524309 |\n",
      "|    clip_fraction        | 0.202     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.092    |\n",
      "|    explained_variance   | 0.431     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.113    |\n",
      "|    n_updates            | 12426     |\n",
      "|    policy_gradient_loss | -0.0723   |\n",
      "|    value_loss           | 0.034     |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 58.8     |\n",
      "| time/              |          |\n",
      "|    fps             | 175      |\n",
      "|    iterations      | 655      |\n",
      "|    time_elapsed    | 953      |\n",
      "|    total_timesteps | 167680   |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 58.8       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 175        |\n",
      "|    iterations           | 656        |\n",
      "|    time_elapsed         | 954        |\n",
      "|    total_timesteps      | 167936     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.42517215 |\n",
      "|    clip_fraction        | 0.0983     |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0546    |\n",
      "|    explained_variance   | 0.852      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0369    |\n",
      "|    n_updates            | 12445      |\n",
      "|    policy_gradient_loss | -0.0322    |\n",
      "|    value_loss           | 0.0831     |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=168000, episode_reward=36.50 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 36.5      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 168000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.8530363 |\n",
      "|    clip_fraction        | 0.14      |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0523   |\n",
      "|    explained_variance   | 0.878     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0562   |\n",
      "|    n_updates            | 12464     |\n",
      "|    policy_gradient_loss | -0.0542   |\n",
      "|    value_loss           | 0.0768    |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 58.8     |\n",
      "| time/              |          |\n",
      "|    fps             | 175      |\n",
      "|    iterations      | 657      |\n",
      "|    time_elapsed    | 956      |\n",
      "|    total_timesteps | 168192   |\n",
      "---------------------------------\n",
      "--------------------------------------\n",
      "| rollout/                |          |\n",
      "|    ep_len_mean          | 889      |\n",
      "|    ep_rew_mean          | 58.8     |\n",
      "| time/                   |          |\n",
      "|    fps                  | 175      |\n",
      "|    iterations           | 658      |\n",
      "|    time_elapsed         | 958      |\n",
      "|    total_timesteps      | 168448   |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 43.75977 |\n",
      "|    clip_fraction        | 0.245    |\n",
      "|    clip_range           | 0.4      |\n",
      "|    entropy_loss         | -0.111   |\n",
      "|    explained_variance   | 0.24     |\n",
      "|    learning_rate        | 0.000389 |\n",
      "|    loss                 | -0.1     |\n",
      "|    n_updates            | 12483    |\n",
      "|    policy_gradient_loss | -0.0787  |\n",
      "|    value_loss           | 0.27     |\n",
      "--------------------------------------\n",
      "Eval num_timesteps=168500, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 0         |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 168500    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.7353046 |\n",
      "|    clip_fraction        | 0.115     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0392   |\n",
      "|    explained_variance   | 0.892     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0187   |\n",
      "|    n_updates            | 12502     |\n",
      "|    policy_gradient_loss | -0.0363   |\n",
      "|    value_loss           | 0.0376    |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 58.8     |\n",
      "| time/              |          |\n",
      "|    fps             | 175      |\n",
      "|    iterations      | 659      |\n",
      "|    time_elapsed    | 959      |\n",
      "|    total_timesteps | 168704   |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 58.9      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 175       |\n",
      "|    iterations           | 660       |\n",
      "|    time_elapsed         | 961       |\n",
      "|    total_timesteps      | 168960    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.6419493 |\n",
      "|    clip_fraction        | 0.124     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.055    |\n",
      "|    explained_variance   | 0.652     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | 0.164     |\n",
      "|    n_updates            | 12521     |\n",
      "|    policy_gradient_loss | -0.044    |\n",
      "|    value_loss           | 0.284     |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=169000, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 230        |\n",
      "|    mean_reward          | 0          |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 169000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.50238645 |\n",
      "|    clip_fraction        | 0.176      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.102     |\n",
      "|    explained_variance   | 0.658      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.039     |\n",
      "|    n_updates            | 12540      |\n",
      "|    policy_gradient_loss | -0.0666    |\n",
      "|    value_loss           | 0.518      |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 58.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 175      |\n",
      "|    iterations      | 661      |\n",
      "|    time_elapsed    | 963      |\n",
      "|    total_timesteps | 169216   |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 58.9      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 175       |\n",
      "|    iterations           | 662       |\n",
      "|    time_elapsed         | 964       |\n",
      "|    total_timesteps      | 169472    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.8785533 |\n",
      "|    clip_fraction        | 0.147     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0675   |\n",
      "|    explained_variance   | 0.593     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.114    |\n",
      "|    n_updates            | 12559     |\n",
      "|    policy_gradient_loss | -0.0533   |\n",
      "|    value_loss           | 0.0531    |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=169500, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 0         |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 169500    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.5230347 |\n",
      "|    clip_fraction        | 0.0894    |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0404   |\n",
      "|    explained_variance   | 0.854     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.115    |\n",
      "|    n_updates            | 12578     |\n",
      "|    policy_gradient_loss | -0.0394   |\n",
      "|    value_loss           | 0.117     |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 58.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 175      |\n",
      "|    iterations      | 663      |\n",
      "|    time_elapsed    | 966      |\n",
      "|    total_timesteps | 169728   |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 58.9       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 175        |\n",
      "|    iterations           | 664        |\n",
      "|    time_elapsed         | 967        |\n",
      "|    total_timesteps      | 169984     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.69291925 |\n",
      "|    clip_fraction        | 0.101      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.041     |\n",
      "|    explained_variance   | 0.477      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0445    |\n",
      "|    n_updates            | 12597      |\n",
      "|    policy_gradient_loss | -0.0459    |\n",
      "|    value_loss           | 0.672      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=170000, episode_reward=52.39 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 52.4      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 170000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.0771585 |\n",
      "|    clip_fraction        | 0.243     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.143    |\n",
      "|    explained_variance   | 0.287     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.116    |\n",
      "|    n_updates            | 12616     |\n",
      "|    policy_gradient_loss | -0.0724   |\n",
      "|    value_loss           | 0.181     |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 58.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 175      |\n",
      "|    iterations      | 665      |\n",
      "|    time_elapsed    | 969      |\n",
      "|    total_timesteps | 170240   |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 58.9      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 175       |\n",
      "|    iterations           | 666       |\n",
      "|    time_elapsed         | 970       |\n",
      "|    total_timesteps      | 170496    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.9161286 |\n",
      "|    clip_fraction        | 0.106     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0344   |\n",
      "|    explained_variance   | 0.785     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | 0.00322   |\n",
      "|    n_updates            | 12635     |\n",
      "|    policy_gradient_loss | -0.0432   |\n",
      "|    value_loss           | 0.146     |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=170500, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 0         |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 170500    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.4436214 |\n",
      "|    clip_fraction        | 0.146     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0655   |\n",
      "|    explained_variance   | 0.828     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0419   |\n",
      "|    n_updates            | 12654     |\n",
      "|    policy_gradient_loss | -0.0546   |\n",
      "|    value_loss           | 0.131     |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 59       |\n",
      "| time/              |          |\n",
      "|    fps             | 175      |\n",
      "|    iterations      | 667      |\n",
      "|    time_elapsed    | 971      |\n",
      "|    total_timesteps | 170752   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=171000, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 230        |\n",
      "|    mean_reward          | 0          |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 171000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.56654024 |\n",
      "|    clip_fraction        | 0.196      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.113     |\n",
      "|    explained_variance   | 0.697      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0863    |\n",
      "|    n_updates            | 12673      |\n",
      "|    policy_gradient_loss | -0.0599    |\n",
      "|    value_loss           | 0.338      |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 59       |\n",
      "| time/              |          |\n",
      "|    fps             | 175      |\n",
      "|    iterations      | 668      |\n",
      "|    time_elapsed    | 973      |\n",
      "|    total_timesteps | 171008   |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 59        |\n",
      "| time/                   |           |\n",
      "|    fps                  | 175       |\n",
      "|    iterations           | 669       |\n",
      "|    time_elapsed         | 975       |\n",
      "|    total_timesteps      | 171264    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.9373162 |\n",
      "|    clip_fraction        | 0.171     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0712   |\n",
      "|    explained_variance   | 0.513     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0486   |\n",
      "|    n_updates            | 12692     |\n",
      "|    policy_gradient_loss | -0.0583   |\n",
      "|    value_loss           | 0.0648    |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=171500, episode_reward=31.83 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 230        |\n",
      "|    mean_reward          | 31.8       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 171500     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.21009041 |\n",
      "|    clip_fraction        | 0.0547     |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0327    |\n",
      "|    explained_variance   | 0.797      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0205    |\n",
      "|    n_updates            | 12711      |\n",
      "|    policy_gradient_loss | -0.0222    |\n",
      "|    value_loss           | 0.28       |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 59       |\n",
      "| time/              |          |\n",
      "|    fps             | 175      |\n",
      "|    iterations      | 670      |\n",
      "|    time_elapsed    | 976      |\n",
      "|    total_timesteps | 171520   |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 59        |\n",
      "| time/                   |           |\n",
      "|    fps                  | 175       |\n",
      "|    iterations           | 671       |\n",
      "|    time_elapsed         | 978       |\n",
      "|    total_timesteps      | 171776    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.1012855 |\n",
      "|    clip_fraction        | 0.126     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0581   |\n",
      "|    explained_variance   | 0.712     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0651   |\n",
      "|    n_updates            | 12730     |\n",
      "|    policy_gradient_loss | -0.0465   |\n",
      "|    value_loss           | 0.436     |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=172000, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 230        |\n",
      "|    mean_reward          | 0          |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 172000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.31949627 |\n",
      "|    clip_fraction        | 0.146      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.11      |\n",
      "|    explained_variance   | 0.347      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.153     |\n",
      "|    n_updates            | 12749      |\n",
      "|    policy_gradient_loss | -0.0486    |\n",
      "|    value_loss           | 0.117      |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 59       |\n",
      "| time/              |          |\n",
      "|    fps             | 175      |\n",
      "|    iterations      | 672      |\n",
      "|    time_elapsed    | 980      |\n",
      "|    total_timesteps | 172032   |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 59        |\n",
      "| time/                   |           |\n",
      "|    fps                  | 175       |\n",
      "|    iterations           | 673       |\n",
      "|    time_elapsed         | 981       |\n",
      "|    total_timesteps      | 172288    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.9736186 |\n",
      "|    clip_fraction        | 0.138     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0461   |\n",
      "|    explained_variance   | 0.957     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | 0.00333   |\n",
      "|    n_updates            | 12768     |\n",
      "|    policy_gradient_loss | -0.0599   |\n",
      "|    value_loss           | 0.0374    |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=172500, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 0         |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 172500    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.1186972 |\n",
      "|    clip_fraction        | 0.137     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0457   |\n",
      "|    explained_variance   | 0.759     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | 0.0255    |\n",
      "|    n_updates            | 12787     |\n",
      "|    policy_gradient_loss | -0.0478   |\n",
      "|    value_loss           | 0.241     |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 58.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 175      |\n",
      "|    iterations      | 674      |\n",
      "|    time_elapsed    | 983      |\n",
      "|    total_timesteps | 172544   |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 58.9      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 175       |\n",
      "|    iterations           | 675       |\n",
      "|    time_elapsed         | 984       |\n",
      "|    total_timesteps      | 172800    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.7034941 |\n",
      "|    clip_fraction        | 0.24      |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.123    |\n",
      "|    explained_variance   | 0.627     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0495   |\n",
      "|    n_updates            | 12806     |\n",
      "|    policy_gradient_loss | -0.0645   |\n",
      "|    value_loss           | 0.278     |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=173000, episode_reward=51.48 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 230        |\n",
      "|    mean_reward          | 51.5       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 173000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.75510716 |\n",
      "|    clip_fraction        | 0.145      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0541    |\n",
      "|    explained_variance   | 0.496      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.127     |\n",
      "|    n_updates            | 12825      |\n",
      "|    policy_gradient_loss | -0.0635    |\n",
      "|    value_loss           | 0.0393     |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 58.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 175      |\n",
      "|    iterations      | 676      |\n",
      "|    time_elapsed    | 986      |\n",
      "|    total_timesteps | 173056   |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 58.9      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 175       |\n",
      "|    iterations           | 677       |\n",
      "|    time_elapsed         | 987       |\n",
      "|    total_timesteps      | 173312    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.9185993 |\n",
      "|    clip_fraction        | 0.096     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.028    |\n",
      "|    explained_variance   | 0.864     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0432   |\n",
      "|    n_updates            | 12844     |\n",
      "|    policy_gradient_loss | -0.0344   |\n",
      "|    value_loss           | 0.172     |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=173500, episode_reward=56.88 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 56.9      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 173500    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.7003273 |\n",
      "|    clip_fraction        | 0.159     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0763   |\n",
      "|    explained_variance   | 0.478     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | 0.0296    |\n",
      "|    n_updates            | 12863     |\n",
      "|    policy_gradient_loss | -0.0475   |\n",
      "|    value_loss           | 0.363     |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 59       |\n",
      "| time/              |          |\n",
      "|    fps             | 175      |\n",
      "|    iterations      | 678      |\n",
      "|    time_elapsed    | 989      |\n",
      "|    total_timesteps | 173568   |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 59        |\n",
      "| time/                   |           |\n",
      "|    fps                  | 175       |\n",
      "|    iterations           | 679       |\n",
      "|    time_elapsed         | 991       |\n",
      "|    total_timesteps      | 173824    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.1595844 |\n",
      "|    clip_fraction        | 0.198     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.104    |\n",
      "|    explained_variance   | 0.0382    |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0944   |\n",
      "|    n_updates            | 12882     |\n",
      "|    policy_gradient_loss | -0.0731   |\n",
      "|    value_loss           | 0.14      |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=174000, episode_reward=31.83 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 31.8      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 174000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.6454782 |\n",
      "|    clip_fraction        | 0.0835    |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0273   |\n",
      "|    explained_variance   | 0.888     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.049    |\n",
      "|    n_updates            | 12901     |\n",
      "|    policy_gradient_loss | -0.0285   |\n",
      "|    value_loss           | 0.0754    |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 59       |\n",
      "| time/              |          |\n",
      "|    fps             | 175      |\n",
      "|    iterations      | 680      |\n",
      "|    time_elapsed    | 993      |\n",
      "|    total_timesteps | 174080   |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 59         |\n",
      "| time/                   |            |\n",
      "|    fps                  | 175        |\n",
      "|    iterations           | 681        |\n",
      "|    time_elapsed         | 994        |\n",
      "|    total_timesteps      | 174336     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.28113717 |\n",
      "|    clip_fraction        | 0.0389     |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0134    |\n",
      "|    explained_variance   | 0.63       |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0265    |\n",
      "|    n_updates            | 12920      |\n",
      "|    policy_gradient_loss | -0.0124    |\n",
      "|    value_loss           | 0.146      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=174500, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 0         |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 174500    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.9178735 |\n",
      "|    clip_fraction        | 0.19      |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.115    |\n",
      "|    explained_variance   | 0.778     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0572   |\n",
      "|    n_updates            | 12939     |\n",
      "|    policy_gradient_loss | -0.0548   |\n",
      "|    value_loss           | 0.183     |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 59       |\n",
      "| time/              |          |\n",
      "|    fps             | 175      |\n",
      "|    iterations      | 682      |\n",
      "|    time_elapsed    | 996      |\n",
      "|    total_timesteps | 174592   |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 59         |\n",
      "| time/                   |            |\n",
      "|    fps                  | 175        |\n",
      "|    iterations           | 683        |\n",
      "|    time_elapsed         | 997        |\n",
      "|    total_timesteps      | 174848     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.69739777 |\n",
      "|    clip_fraction        | 0.139      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0441    |\n",
      "|    explained_variance   | 0.487      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0462    |\n",
      "|    n_updates            | 12958      |\n",
      "|    policy_gradient_loss | -0.0495    |\n",
      "|    value_loss           | 0.0577     |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=175000, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 230        |\n",
      "|    mean_reward          | 0          |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 175000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.42619938 |\n",
      "|    clip_fraction        | 0.0703     |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0243    |\n",
      "|    explained_variance   | 0.66       |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | 0.00331    |\n",
      "|    n_updates            | 12977      |\n",
      "|    policy_gradient_loss | -0.0295    |\n",
      "|    value_loss           | 0.212      |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 59       |\n",
      "| time/              |          |\n",
      "|    fps             | 175      |\n",
      "|    iterations      | 684      |\n",
      "|    time_elapsed    | 998      |\n",
      "|    total_timesteps | 175104   |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 58.8      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 175       |\n",
      "|    iterations           | 685       |\n",
      "|    time_elapsed         | 1000      |\n",
      "|    total_timesteps      | 175360    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.6377164 |\n",
      "|    clip_fraction        | 0.105     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0531   |\n",
      "|    explained_variance   | 0.655     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0119   |\n",
      "|    n_updates            | 12996     |\n",
      "|    policy_gradient_loss | -0.0431   |\n",
      "|    value_loss           | 0.168     |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=175500, episode_reward=8.51 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 8.51      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 175500    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.5246638 |\n",
      "|    clip_fraction        | 0.142     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0722   |\n",
      "|    explained_variance   | 0.254     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0518   |\n",
      "|    n_updates            | 13015     |\n",
      "|    policy_gradient_loss | -0.0473   |\n",
      "|    value_loss           | 0.0829    |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 58.8     |\n",
      "| time/              |          |\n",
      "|    fps             | 175      |\n",
      "|    iterations      | 686      |\n",
      "|    time_elapsed    | 1002     |\n",
      "|    total_timesteps | 175616   |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 58.8       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 175        |\n",
      "|    iterations           | 687        |\n",
      "|    time_elapsed         | 1003       |\n",
      "|    total_timesteps      | 175872     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.94591284 |\n",
      "|    clip_fraction        | 0.123      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0415    |\n",
      "|    explained_variance   | 0.677      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0859    |\n",
      "|    n_updates            | 13034      |\n",
      "|    policy_gradient_loss | -0.0483    |\n",
      "|    value_loss           | 0.156      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=176000, episode_reward=8.51 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 230        |\n",
      "|    mean_reward          | 8.51       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 176000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.72583336 |\n",
      "|    clip_fraction        | 0.0685     |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0178    |\n",
      "|    explained_variance   | 0.792      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | 0.00225    |\n",
      "|    n_updates            | 13053      |\n",
      "|    policy_gradient_loss | -0.0254    |\n",
      "|    value_loss           | 0.0687     |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 58.5     |\n",
      "| time/              |          |\n",
      "|    fps             | 175      |\n",
      "|    iterations      | 688      |\n",
      "|    time_elapsed    | 1004     |\n",
      "|    total_timesteps | 176128   |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 58.5       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 175        |\n",
      "|    iterations           | 689        |\n",
      "|    time_elapsed         | 1006       |\n",
      "|    total_timesteps      | 176384     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.26173186 |\n",
      "|    clip_fraction        | 0.113      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0891    |\n",
      "|    explained_variance   | 0.406      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.106     |\n",
      "|    n_updates            | 13072      |\n",
      "|    policy_gradient_loss | -0.0552    |\n",
      "|    value_loss           | 0.398      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=176500, episode_reward=32.58 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 32.6      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 176500    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.3127246 |\n",
      "|    clip_fraction        | 0.162     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0395   |\n",
      "|    explained_variance   | 0.79      |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0985   |\n",
      "|    n_updates            | 13091     |\n",
      "|    policy_gradient_loss | -0.0584   |\n",
      "|    value_loss           | 0.0249    |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 58.5     |\n",
      "| time/              |          |\n",
      "|    fps             | 175      |\n",
      "|    iterations      | 690      |\n",
      "|    time_elapsed    | 1007     |\n",
      "|    total_timesteps | 176640   |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 58.5      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 175       |\n",
      "|    iterations           | 691       |\n",
      "|    time_elapsed         | 1008      |\n",
      "|    total_timesteps      | 176896    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.6582614 |\n",
      "|    clip_fraction        | 0.0397    |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0215   |\n",
      "|    explained_variance   | 0.669     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0855   |\n",
      "|    n_updates            | 13110     |\n",
      "|    policy_gradient_loss | -0.0187   |\n",
      "|    value_loss           | 0.137     |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=177000, episode_reward=28.96 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 230        |\n",
      "|    mean_reward          | 29         |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 177000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.83193547 |\n",
      "|    clip_fraction        | 0.125      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0602    |\n",
      "|    explained_variance   | 0.675      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.00732   |\n",
      "|    n_updates            | 13129      |\n",
      "|    policy_gradient_loss | -0.0421    |\n",
      "|    value_loss           | 0.218      |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 58.2     |\n",
      "| time/              |          |\n",
      "|    fps             | 175      |\n",
      "|    iterations      | 692      |\n",
      "|    time_elapsed    | 1010     |\n",
      "|    total_timesteps | 177152   |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 58.2      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 175       |\n",
      "|    iterations           | 693       |\n",
      "|    time_elapsed         | 1011      |\n",
      "|    total_timesteps      | 177408    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.0805397 |\n",
      "|    clip_fraction        | 0.206     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0849   |\n",
      "|    explained_variance   | 0.782     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.139    |\n",
      "|    n_updates            | 13148     |\n",
      "|    policy_gradient_loss | -0.0705   |\n",
      "|    value_loss           | 0.0268    |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=177500, episode_reward=29.60 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 230        |\n",
      "|    mean_reward          | 29.6       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 177500     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.48060128 |\n",
      "|    clip_fraction        | 0.0493     |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.028     |\n",
      "|    explained_variance   | 0.423      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0215    |\n",
      "|    n_updates            | 13167      |\n",
      "|    policy_gradient_loss | -0.0149    |\n",
      "|    value_loss           | 0.39       |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 58.2     |\n",
      "| time/              |          |\n",
      "|    fps             | 175      |\n",
      "|    iterations      | 694      |\n",
      "|    time_elapsed    | 1013     |\n",
      "|    total_timesteps | 177664   |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 58.2       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 175        |\n",
      "|    iterations           | 695        |\n",
      "|    time_elapsed         | 1014       |\n",
      "|    total_timesteps      | 177920     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.12571181 |\n",
      "|    clip_fraction        | 0.0241     |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.00933   |\n",
      "|    explained_variance   | 0.904      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | 0.000575   |\n",
      "|    n_updates            | 13186      |\n",
      "|    policy_gradient_loss | -0.0114    |\n",
      "|    value_loss           | 0.0272     |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=178000, episode_reward=29.79 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 29.8      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 178000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.6322556 |\n",
      "|    clip_fraction        | 0.171     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.11     |\n",
      "|    explained_variance   | 0.659     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0693   |\n",
      "|    n_updates            | 13205     |\n",
      "|    policy_gradient_loss | -0.0543   |\n",
      "|    value_loss           | 0.127     |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 58.2     |\n",
      "| time/              |          |\n",
      "|    fps             | 175      |\n",
      "|    iterations      | 696      |\n",
      "|    time_elapsed    | 1016     |\n",
      "|    total_timesteps | 178176   |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 58.2       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 175        |\n",
      "|    iterations           | 697        |\n",
      "|    time_elapsed         | 1017       |\n",
      "|    total_timesteps      | 178432     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.85905623 |\n",
      "|    clip_fraction        | 0.0894     |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.035     |\n",
      "|    explained_variance   | 0.844      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | 0.00931    |\n",
      "|    n_updates            | 13224      |\n",
      "|    policy_gradient_loss | -0.0229    |\n",
      "|    value_loss           | 0.0259     |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=178500, episode_reward=30.87 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 230        |\n",
      "|    mean_reward          | 30.9       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 178500     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.11913341 |\n",
      "|    clip_fraction        | 0.0337     |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0131    |\n",
      "|    explained_variance   | 0.669      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | 0.000613   |\n",
      "|    n_updates            | 13243      |\n",
      "|    policy_gradient_loss | -0.00966   |\n",
      "|    value_loss           | 0.527      |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 58.2     |\n",
      "| time/              |          |\n",
      "|    fps             | 175      |\n",
      "|    iterations      | 698      |\n",
      "|    time_elapsed    | 1019     |\n",
      "|    total_timesteps | 178688   |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 58.2      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 175       |\n",
      "|    iterations           | 699       |\n",
      "|    time_elapsed         | 1020      |\n",
      "|    total_timesteps      | 178944    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.6971515 |\n",
      "|    clip_fraction        | 0.146     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0916   |\n",
      "|    explained_variance   | 0.261     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0438   |\n",
      "|    n_updates            | 13262     |\n",
      "|    policy_gradient_loss | -0.0601   |\n",
      "|    value_loss           | 0.68      |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=179000, episode_reward=31.03 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 230        |\n",
      "|    mean_reward          | 31         |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 179000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.93158495 |\n",
      "|    clip_fraction        | 0.209      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0912    |\n",
      "|    explained_variance   | 0.363      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.113     |\n",
      "|    n_updates            | 13281      |\n",
      "|    policy_gradient_loss | -0.0682    |\n",
      "|    value_loss           | 0.0565     |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 58.2     |\n",
      "| time/              |          |\n",
      "|    fps             | 175      |\n",
      "|    iterations      | 700      |\n",
      "|    time_elapsed    | 1022     |\n",
      "|    total_timesteps | 179200   |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 58.2       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 175        |\n",
      "|    iterations           | 701        |\n",
      "|    time_elapsed         | 1023       |\n",
      "|    total_timesteps      | 179456     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.37147436 |\n",
      "|    clip_fraction        | 0.0539     |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0131    |\n",
      "|    explained_variance   | 0.771      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | 0.00814    |\n",
      "|    n_updates            | 13300      |\n",
      "|    policy_gradient_loss | -0.0151    |\n",
      "|    value_loss           | 0.152      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=179500, episode_reward=29.23 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 29.2      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 179500    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.3182223 |\n",
      "|    clip_fraction        | 0.0681    |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.024    |\n",
      "|    explained_variance   | 0.925     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0995   |\n",
      "|    n_updates            | 13319     |\n",
      "|    policy_gradient_loss | -0.0299   |\n",
      "|    value_loss           | 0.0385    |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 58.3     |\n",
      "| time/              |          |\n",
      "|    fps             | 175      |\n",
      "|    iterations      | 702      |\n",
      "|    time_elapsed    | 1025     |\n",
      "|    total_timesteps | 179712   |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 58.3       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 175        |\n",
      "|    iterations           | 703        |\n",
      "|    time_elapsed         | 1026       |\n",
      "|    total_timesteps      | 179968     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.94156945 |\n",
      "|    clip_fraction        | 0.209      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.111     |\n",
      "|    explained_variance   | 0.783      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0159    |\n",
      "|    n_updates            | 13338      |\n",
      "|    policy_gradient_loss | -0.0629    |\n",
      "|    value_loss           | 0.2        |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=180000, episode_reward=28.30 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 28.3      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 180000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.9415919 |\n",
      "|    clip_fraction        | 0.121     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0369   |\n",
      "|    explained_variance   | 0.687     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0791   |\n",
      "|    n_updates            | 13357     |\n",
      "|    policy_gradient_loss | -0.0466   |\n",
      "|    value_loss           | 0.0306    |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 58.3     |\n",
      "| time/              |          |\n",
      "|    fps             | 175      |\n",
      "|    iterations      | 704      |\n",
      "|    time_elapsed    | 1028     |\n",
      "|    total_timesteps | 180224   |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 58.3       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 175        |\n",
      "|    iterations           | 705        |\n",
      "|    time_elapsed         | 1029       |\n",
      "|    total_timesteps      | 180480     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.49881724 |\n",
      "|    clip_fraction        | 0.0761     |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0256    |\n",
      "|    explained_variance   | 0.681      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0237    |\n",
      "|    n_updates            | 13376      |\n",
      "|    policy_gradient_loss | -0.0284    |\n",
      "|    value_loss           | 0.317      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=180500, episode_reward=36.87 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 230        |\n",
      "|    mean_reward          | 36.9       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 180500     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.58338535 |\n",
      "|    clip_fraction        | 0.15       |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0716    |\n",
      "|    explained_variance   | 0.762      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0286    |\n",
      "|    n_updates            | 13395      |\n",
      "|    policy_gradient_loss | -0.0502    |\n",
      "|    value_loss           | 0.311      |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 58.3     |\n",
      "| time/              |          |\n",
      "|    fps             | 175      |\n",
      "|    iterations      | 706      |\n",
      "|    time_elapsed    | 1031     |\n",
      "|    total_timesteps | 180736   |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 58.3      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 175       |\n",
      "|    iterations           | 707       |\n",
      "|    time_elapsed         | 1032      |\n",
      "|    total_timesteps      | 180992    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.2025728 |\n",
      "|    clip_fraction        | 0.209     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0814   |\n",
      "|    explained_variance   | 0.258     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0943   |\n",
      "|    n_updates            | 13414     |\n",
      "|    policy_gradient_loss | -0.0757   |\n",
      "|    value_loss           | 0.0553    |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=181000, episode_reward=81.73 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 230        |\n",
      "|    mean_reward          | 81.7       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 181000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.67033035 |\n",
      "|    clip_fraction        | 0.0668     |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.018     |\n",
      "|    explained_variance   | 0.709      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0494    |\n",
      "|    n_updates            | 13433      |\n",
      "|    policy_gradient_loss | -0.0218    |\n",
      "|    value_loss           | 0.273      |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 58.3     |\n",
      "| time/              |          |\n",
      "|    fps             | 175      |\n",
      "|    iterations      | 708      |\n",
      "|    time_elapsed    | 1033     |\n",
      "|    total_timesteps | 181248   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=181500, episode_reward=83.90 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 83.9      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 181500    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.7611592 |\n",
      "|    clip_fraction        | 0.127     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0327   |\n",
      "|    explained_variance   | 0.866     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.115    |\n",
      "|    n_updates            | 13452     |\n",
      "|    policy_gradient_loss | -0.0546   |\n",
      "|    value_loss           | 0.0735    |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 58.2     |\n",
      "| time/              |          |\n",
      "|    fps             | 175      |\n",
      "|    iterations      | 709      |\n",
      "|    time_elapsed    | 1035     |\n",
      "|    total_timesteps | 181504   |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 58.2       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 175        |\n",
      "|    iterations           | 710        |\n",
      "|    time_elapsed         | 1036       |\n",
      "|    total_timesteps      | 181760     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.79780793 |\n",
      "|    clip_fraction        | 0.183      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0865    |\n",
      "|    explained_variance   | 0.825      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0284    |\n",
      "|    n_updates            | 13471      |\n",
      "|    policy_gradient_loss | -0.0514    |\n",
      "|    value_loss           | 0.174      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=182000, episode_reward=50.77 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 50.8      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 182000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.8744035 |\n",
      "|    clip_fraction        | 0.115     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0304   |\n",
      "|    explained_variance   | 0.566     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0808   |\n",
      "|    n_updates            | 13490     |\n",
      "|    policy_gradient_loss | -0.0421   |\n",
      "|    value_loss           | 0.071     |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 58.2     |\n",
      "| time/              |          |\n",
      "|    fps             | 175      |\n",
      "|    iterations      | 711      |\n",
      "|    time_elapsed    | 1038     |\n",
      "|    total_timesteps | 182016   |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 58         |\n",
      "| time/                   |            |\n",
      "|    fps                  | 175        |\n",
      "|    iterations           | 712        |\n",
      "|    time_elapsed         | 1039       |\n",
      "|    total_timesteps      | 182272     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.50942194 |\n",
      "|    clip_fraction        | 0.0979     |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.046     |\n",
      "|    explained_variance   | 0.622      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | 0.00735    |\n",
      "|    n_updates            | 13509      |\n",
      "|    policy_gradient_loss | -0.0258    |\n",
      "|    value_loss           | 0.481      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=182500, episode_reward=51.23 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 51.2      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 182500    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 2.8285432 |\n",
      "|    clip_fraction        | 0.161     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0586   |\n",
      "|    explained_variance   | 0.228     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | 0.0512    |\n",
      "|    n_updates            | 13528     |\n",
      "|    policy_gradient_loss | -0.0661   |\n",
      "|    value_loss           | 0.251     |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 58       |\n",
      "| time/              |          |\n",
      "|    fps             | 175      |\n",
      "|    iterations      | 713      |\n",
      "|    time_elapsed    | 1040     |\n",
      "|    total_timesteps | 182528   |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 58        |\n",
      "| time/                   |           |\n",
      "|    fps                  | 175       |\n",
      "|    iterations           | 714       |\n",
      "|    time_elapsed         | 1042      |\n",
      "|    total_timesteps      | 182784    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.0691309 |\n",
      "|    clip_fraction        | 0.189     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0805   |\n",
      "|    explained_variance   | 0.543     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.101    |\n",
      "|    n_updates            | 13547     |\n",
      "|    policy_gradient_loss | -0.065    |\n",
      "|    value_loss           | 0.0684    |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=183000, episode_reward=50.95 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 51        |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 183000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.1031651 |\n",
      "|    clip_fraction        | 0.0962    |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0241   |\n",
      "|    explained_variance   | 0.841     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | 0.0101    |\n",
      "|    n_updates            | 13566     |\n",
      "|    policy_gradient_loss | -0.0421   |\n",
      "|    value_loss           | 0.132     |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 58       |\n",
      "| time/              |          |\n",
      "|    fps             | 175      |\n",
      "|    iterations      | 715      |\n",
      "|    time_elapsed    | 1043     |\n",
      "|    total_timesteps | 183040   |\n",
      "---------------------------------\n",
      "--------------------------------------\n",
      "| rollout/                |          |\n",
      "|    ep_len_mean          | 889      |\n",
      "|    ep_rew_mean          | 58.2     |\n",
      "| time/                   |          |\n",
      "|    fps                  | 175      |\n",
      "|    iterations           | 716      |\n",
      "|    time_elapsed         | 1045     |\n",
      "|    total_timesteps      | 183296   |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 1.147408 |\n",
      "|    clip_fraction        | 0.137    |\n",
      "|    clip_range           | 0.4      |\n",
      "|    entropy_loss         | -0.0489  |\n",
      "|    explained_variance   | 0.808    |\n",
      "|    learning_rate        | 0.000389 |\n",
      "|    loss                 | -0.041   |\n",
      "|    n_updates            | 13585    |\n",
      "|    policy_gradient_loss | -0.0449  |\n",
      "|    value_loss           | 0.0779   |\n",
      "--------------------------------------\n",
      "Eval num_timesteps=183500, episode_reward=6.17 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 6.17      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 183500    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.6132709 |\n",
      "|    clip_fraction        | 0.224     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.115    |\n",
      "|    explained_variance   | 0.331     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.132    |\n",
      "|    n_updates            | 13604     |\n",
      "|    policy_gradient_loss | -0.0467   |\n",
      "|    value_loss           | 0.267     |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 58.2     |\n",
      "| time/              |          |\n",
      "|    fps             | 175      |\n",
      "|    iterations      | 717      |\n",
      "|    time_elapsed    | 1047     |\n",
      "|    total_timesteps | 183552   |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 58.2       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 175        |\n",
      "|    iterations           | 718        |\n",
      "|    time_elapsed         | 1048       |\n",
      "|    total_timesteps      | 183808     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.31859314 |\n",
      "|    clip_fraction        | 0.0676     |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0325    |\n",
      "|    explained_variance   | 0.389      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0346    |\n",
      "|    n_updates            | 13623      |\n",
      "|    policy_gradient_loss | -0.0219    |\n",
      "|    value_loss           | 0.135      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=184000, episode_reward=9.01 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 230        |\n",
      "|    mean_reward          | 9.01       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 184000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.38116878 |\n",
      "|    clip_fraction        | 0.0952     |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0464    |\n",
      "|    explained_variance   | 0.63       |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0511    |\n",
      "|    n_updates            | 13642      |\n",
      "|    policy_gradient_loss | -0.0282    |\n",
      "|    value_loss           | 0.252      |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 58.2     |\n",
      "| time/              |          |\n",
      "|    fps             | 175      |\n",
      "|    iterations      | 719      |\n",
      "|    time_elapsed    | 1050     |\n",
      "|    total_timesteps | 184064   |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 58.2       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 175        |\n",
      "|    iterations           | 720        |\n",
      "|    time_elapsed         | 1052       |\n",
      "|    total_timesteps      | 184320     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.63605416 |\n",
      "|    clip_fraction        | 0.135      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0722    |\n",
      "|    explained_variance   | 0.656      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.105     |\n",
      "|    n_updates            | 13661      |\n",
      "|    policy_gradient_loss | -0.0527    |\n",
      "|    value_loss           | 0.275      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=184500, episode_reward=6.14 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 6.14      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 184500    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.8526103 |\n",
      "|    clip_fraction        | 0.181     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0749   |\n",
      "|    explained_variance   | 0.4       |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0515   |\n",
      "|    n_updates            | 13680     |\n",
      "|    policy_gradient_loss | -0.0425   |\n",
      "|    value_loss           | 0.0637    |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 58.2     |\n",
      "| time/              |          |\n",
      "|    fps             | 175      |\n",
      "|    iterations      | 721      |\n",
      "|    time_elapsed    | 1053     |\n",
      "|    total_timesteps | 184576   |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 58.2       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 175        |\n",
      "|    iterations           | 722        |\n",
      "|    time_elapsed         | 1055       |\n",
      "|    total_timesteps      | 184832     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.29433215 |\n",
      "|    clip_fraction        | 0.0648     |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0267    |\n",
      "|    explained_variance   | 0.745      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0419    |\n",
      "|    n_updates            | 13699      |\n",
      "|    policy_gradient_loss | -0.0377    |\n",
      "|    value_loss           | 0.249      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=185000, episode_reward=9.25 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 9.25      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 185000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.5968471 |\n",
      "|    clip_fraction        | 0.103     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0412   |\n",
      "|    explained_variance   | 0.909     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.11     |\n",
      "|    n_updates            | 13718     |\n",
      "|    policy_gradient_loss | -0.0343   |\n",
      "|    value_loss           | 0.084     |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 58.3     |\n",
      "| time/              |          |\n",
      "|    fps             | 175      |\n",
      "|    iterations      | 723      |\n",
      "|    time_elapsed    | 1057     |\n",
      "|    total_timesteps | 185088   |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 58.3      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 174       |\n",
      "|    iterations           | 724       |\n",
      "|    time_elapsed         | 1059      |\n",
      "|    total_timesteps      | 185344    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.6010158 |\n",
      "|    clip_fraction        | 0.17      |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0999   |\n",
      "|    explained_variance   | 0.312     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.00922  |\n",
      "|    n_updates            | 13737     |\n",
      "|    policy_gradient_loss | -0.0573   |\n",
      "|    value_loss           | 0.187     |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=185500, episode_reward=9.09 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 9.09      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 185500    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.6655998 |\n",
      "|    clip_fraction        | 0.117     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0385   |\n",
      "|    explained_variance   | 0.893     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0475   |\n",
      "|    n_updates            | 13756     |\n",
      "|    policy_gradient_loss | -0.042    |\n",
      "|    value_loss           | 0.0598    |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 58.3     |\n",
      "| time/              |          |\n",
      "|    fps             | 174      |\n",
      "|    iterations      | 725      |\n",
      "|    time_elapsed    | 1061     |\n",
      "|    total_timesteps | 185600   |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 58.4      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 174       |\n",
      "|    iterations           | 726       |\n",
      "|    time_elapsed         | 1062      |\n",
      "|    total_timesteps      | 185856    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.6875316 |\n",
      "|    clip_fraction        | 0.133     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0602   |\n",
      "|    explained_variance   | 0.836     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0375   |\n",
      "|    n_updates            | 13775     |\n",
      "|    policy_gradient_loss | -0.0429   |\n",
      "|    value_loss           | 0.0689    |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=186000, episode_reward=84.87 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 230        |\n",
      "|    mean_reward          | 84.9       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 186000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.85512364 |\n",
      "|    clip_fraction        | 0.169      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0836    |\n",
      "|    explained_variance   | 0.767      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0877    |\n",
      "|    n_updates            | 13794      |\n",
      "|    policy_gradient_loss | -0.0669    |\n",
      "|    value_loss           | 0.247      |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 58.4     |\n",
      "| time/              |          |\n",
      "|    fps             | 174      |\n",
      "|    iterations      | 727      |\n",
      "|    time_elapsed    | 1064     |\n",
      "|    total_timesteps | 186112   |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 58.4      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 174       |\n",
      "|    iterations           | 728       |\n",
      "|    time_elapsed         | 1066      |\n",
      "|    total_timesteps      | 186368    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.1660743 |\n",
      "|    clip_fraction        | 0.2       |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0756   |\n",
      "|    explained_variance   | 0.28      |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0226   |\n",
      "|    n_updates            | 13813     |\n",
      "|    policy_gradient_loss | -0.0657   |\n",
      "|    value_loss           | 0.0533    |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=186500, episode_reward=84.86 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 230        |\n",
      "|    mean_reward          | 84.9       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 186500     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.57940024 |\n",
      "|    clip_fraction        | 0.0814     |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0374    |\n",
      "|    explained_variance   | 0.555      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | 0.248      |\n",
      "|    n_updates            | 13832      |\n",
      "|    policy_gradient_loss | -0.0402    |\n",
      "|    value_loss           | 0.62       |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 58.4     |\n",
      "| time/              |          |\n",
      "|    fps             | 174      |\n",
      "|    iterations      | 729      |\n",
      "|    time_elapsed    | 1067     |\n",
      "|    total_timesteps | 186624   |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 58.3      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 174       |\n",
      "|    iterations           | 730       |\n",
      "|    time_elapsed         | 1069      |\n",
      "|    total_timesteps      | 186880    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.3740137 |\n",
      "|    clip_fraction        | 0.183     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0586   |\n",
      "|    explained_variance   | 0.794     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0943   |\n",
      "|    n_updates            | 13851     |\n",
      "|    policy_gradient_loss | -0.0598   |\n",
      "|    value_loss           | 0.183     |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=187000, episode_reward=5.17 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 5.17      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 187000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.9735602 |\n",
      "|    clip_fraction        | 0.182     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0938   |\n",
      "|    explained_variance   | 0.375     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0846   |\n",
      "|    n_updates            | 13870     |\n",
      "|    policy_gradient_loss | -0.0672   |\n",
      "|    value_loss           | 0.129     |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 58.3     |\n",
      "| time/              |          |\n",
      "|    fps             | 174      |\n",
      "|    iterations      | 731      |\n",
      "|    time_elapsed    | 1071     |\n",
      "|    total_timesteps | 187136   |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 58.3       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 174        |\n",
      "|    iterations           | 732        |\n",
      "|    time_elapsed         | 1072       |\n",
      "|    total_timesteps      | 187392     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.40538606 |\n",
      "|    clip_fraction        | 0.0646     |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0286    |\n",
      "|    explained_variance   | 0.859      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0194    |\n",
      "|    n_updates            | 13889      |\n",
      "|    policy_gradient_loss | -0.0266    |\n",
      "|    value_loss           | 0.102      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=187500, episode_reward=85.33 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 85.3      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 187500    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.4146469 |\n",
      "|    clip_fraction        | 0.132     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0341   |\n",
      "|    explained_variance   | 0.754     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.028    |\n",
      "|    n_updates            | 13908     |\n",
      "|    policy_gradient_loss | -0.0361   |\n",
      "|    value_loss           | 0.091     |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 58.4     |\n",
      "| time/              |          |\n",
      "|    fps             | 174      |\n",
      "|    iterations      | 733      |\n",
      "|    time_elapsed    | 1074     |\n",
      "|    total_timesteps | 187648   |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 58.4       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 174        |\n",
      "|    iterations           | 734        |\n",
      "|    time_elapsed         | 1075       |\n",
      "|    total_timesteps      | 187904     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.43324727 |\n",
      "|    clip_fraction        | 0.148      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0975    |\n",
      "|    explained_variance   | 0.829      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0244    |\n",
      "|    n_updates            | 13927      |\n",
      "|    policy_gradient_loss | -0.052     |\n",
      "|    value_loss           | 0.223      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=188000, episode_reward=89.31 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 230        |\n",
      "|    mean_reward          | 89.3       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 188000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.84287155 |\n",
      "|    clip_fraction        | 0.174      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0644    |\n",
      "|    explained_variance   | 0.586      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0256    |\n",
      "|    n_updates            | 13946      |\n",
      "|    policy_gradient_loss | -0.0703    |\n",
      "|    value_loss           | 0.0484     |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 58.4     |\n",
      "| time/              |          |\n",
      "|    fps             | 174      |\n",
      "|    iterations      | 735      |\n",
      "|    time_elapsed    | 1077     |\n",
      "|    total_timesteps | 188160   |\n",
      "---------------------------------\n",
      "--------------------------------------\n",
      "| rollout/                |          |\n",
      "|    ep_len_mean          | 889      |\n",
      "|    ep_rew_mean          | 58.4     |\n",
      "| time/                   |          |\n",
      "|    fps                  | 174      |\n",
      "|    iterations           | 736      |\n",
      "|    time_elapsed         | 1079     |\n",
      "|    total_timesteps      | 188416   |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 0.600352 |\n",
      "|    clip_fraction        | 0.0886   |\n",
      "|    clip_range           | 0.4      |\n",
      "|    entropy_loss         | -0.0287  |\n",
      "|    explained_variance   | 0.899    |\n",
      "|    learning_rate        | 0.000389 |\n",
      "|    loss                 | -0.0177  |\n",
      "|    n_updates            | 13965    |\n",
      "|    policy_gradient_loss | -0.0279  |\n",
      "|    value_loss           | 0.117    |\n",
      "--------------------------------------\n",
      "Eval num_timesteps=188500, episode_reward=85.22 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 85.2      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 188500    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.3618892 |\n",
      "|    clip_fraction        | 0.153     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0503   |\n",
      "|    explained_variance   | 0.834     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0389   |\n",
      "|    n_updates            | 13984     |\n",
      "|    policy_gradient_loss | -0.0583   |\n",
      "|    value_loss           | 0.258     |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 58.4     |\n",
      "| time/              |          |\n",
      "|    fps             | 174      |\n",
      "|    iterations      | 737      |\n",
      "|    time_elapsed    | 1080     |\n",
      "|    total_timesteps | 188672   |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 58.4       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 174        |\n",
      "|    iterations           | 738        |\n",
      "|    time_elapsed         | 1082       |\n",
      "|    total_timesteps      | 188928     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.99002403 |\n",
      "|    clip_fraction        | 0.254      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.12      |\n",
      "|    explained_variance   | -0.201     |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | 0.0182     |\n",
      "|    n_updates            | 14003      |\n",
      "|    policy_gradient_loss | -0.0661    |\n",
      "|    value_loss           | 0.0954     |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=189000, episode_reward=47.10 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 47.1      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 189000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.6041817 |\n",
      "|    clip_fraction        | 0.104     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.044    |\n",
      "|    explained_variance   | 0.874     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0364   |\n",
      "|    n_updates            | 14022     |\n",
      "|    policy_gradient_loss | -0.0385   |\n",
      "|    value_loss           | 0.127     |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 58.4     |\n",
      "| time/              |          |\n",
      "|    fps             | 174      |\n",
      "|    iterations      | 739      |\n",
      "|    time_elapsed    | 1084     |\n",
      "|    total_timesteps | 189184   |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 58.3       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 174        |\n",
      "|    iterations           | 740        |\n",
      "|    time_elapsed         | 1085       |\n",
      "|    total_timesteps      | 189440     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.70396554 |\n",
      "|    clip_fraction        | 0.0773     |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0291    |\n",
      "|    explained_variance   | 0.836      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.113     |\n",
      "|    n_updates            | 14041      |\n",
      "|    policy_gradient_loss | -0.0283    |\n",
      "|    value_loss           | 0.085      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=189500, episode_reward=74.74 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 74.7      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 189500    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.0850912 |\n",
      "|    clip_fraction        | 0.166     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0981   |\n",
      "|    explained_variance   | 0.789     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | 0.0794    |\n",
      "|    n_updates            | 14060     |\n",
      "|    policy_gradient_loss | -0.0527   |\n",
      "|    value_loss           | 0.249     |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 58.3     |\n",
      "| time/              |          |\n",
      "|    fps             | 174      |\n",
      "|    iterations      | 741      |\n",
      "|    time_elapsed    | 1087     |\n",
      "|    total_timesteps | 189696   |\n",
      "---------------------------------\n",
      "--------------------------------------\n",
      "| rollout/                |          |\n",
      "|    ep_len_mean          | 889      |\n",
      "|    ep_rew_mean          | 58.3     |\n",
      "| time/                   |          |\n",
      "|    fps                  | 174      |\n",
      "|    iterations           | 742      |\n",
      "|    time_elapsed         | 1088     |\n",
      "|    total_timesteps      | 189952   |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 1.460278 |\n",
      "|    clip_fraction        | 0.155    |\n",
      "|    clip_range           | 0.4      |\n",
      "|    entropy_loss         | -0.0549  |\n",
      "|    explained_variance   | 0.617    |\n",
      "|    learning_rate        | 0.000389 |\n",
      "|    loss                 | 0.00636  |\n",
      "|    n_updates            | 14079    |\n",
      "|    policy_gradient_loss | -0.0672  |\n",
      "|    value_loss           | 0.0514   |\n",
      "--------------------------------------\n",
      "Eval num_timesteps=190000, episode_reward=16.33 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 230        |\n",
      "|    mean_reward          | 16.3       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 190000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.90808463 |\n",
      "|    clip_fraction        | 0.0954     |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0304    |\n",
      "|    explained_variance   | 0.841      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0379    |\n",
      "|    n_updates            | 14098      |\n",
      "|    policy_gradient_loss | -0.0412    |\n",
      "|    value_loss           | 0.185      |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 58.3     |\n",
      "| time/              |          |\n",
      "|    fps             | 174      |\n",
      "|    iterations      | 743      |\n",
      "|    time_elapsed    | 1090     |\n",
      "|    total_timesteps | 190208   |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 57.9      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 174       |\n",
      "|    iterations           | 744       |\n",
      "|    time_elapsed         | 1092      |\n",
      "|    total_timesteps      | 190464    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.7415129 |\n",
      "|    clip_fraction        | 0.0792    |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0378   |\n",
      "|    explained_variance   | 0.843     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.143    |\n",
      "|    n_updates            | 14117     |\n",
      "|    policy_gradient_loss | -0.031    |\n",
      "|    value_loss           | 0.195     |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=190500, episode_reward=15.78 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 230        |\n",
      "|    mean_reward          | 15.8       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 190500     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.94559574 |\n",
      "|    clip_fraction        | 0.178      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0816    |\n",
      "|    explained_variance   | 0.478      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.185     |\n",
      "|    n_updates            | 14136      |\n",
      "|    policy_gradient_loss | -0.0461    |\n",
      "|    value_loss           | 0.168      |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 57.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 174      |\n",
      "|    iterations      | 745      |\n",
      "|    time_elapsed    | 1094     |\n",
      "|    total_timesteps | 190720   |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 57.9      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 174       |\n",
      "|    iterations           | 746       |\n",
      "|    time_elapsed         | 1095      |\n",
      "|    total_timesteps      | 190976    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.0630879 |\n",
      "|    clip_fraction        | 0.115     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0352   |\n",
      "|    explained_variance   | 0.945     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0286   |\n",
      "|    n_updates            | 14155     |\n",
      "|    policy_gradient_loss | -0.0491   |\n",
      "|    value_loss           | 0.0488    |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=191000, episode_reward=17.58 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 17.6      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 191000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.1914859 |\n",
      "|    clip_fraction        | 0.0703    |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0229   |\n",
      "|    explained_variance   | 0.731     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.119    |\n",
      "|    n_updates            | 14174     |\n",
      "|    policy_gradient_loss | -0.0286   |\n",
      "|    value_loss           | 0.0732    |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 57.7     |\n",
      "| time/              |          |\n",
      "|    fps             | 174      |\n",
      "|    iterations      | 747      |\n",
      "|    time_elapsed    | 1096     |\n",
      "|    total_timesteps | 191232   |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 57.7       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 174        |\n",
      "|    iterations           | 748        |\n",
      "|    time_elapsed         | 1098       |\n",
      "|    total_timesteps      | 191488     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.69845396 |\n",
      "|    clip_fraction        | 0.179      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.119     |\n",
      "|    explained_variance   | 0.855      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0017    |\n",
      "|    n_updates            | 14193      |\n",
      "|    policy_gradient_loss | -0.0574    |\n",
      "|    value_loss           | 0.166      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=191500, episode_reward=21.39 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 21.4      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 191500    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.2874119 |\n",
      "|    clip_fraction        | 0.18      |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0569   |\n",
      "|    explained_variance   | 0.351     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0879   |\n",
      "|    n_updates            | 14212     |\n",
      "|    policy_gradient_loss | -0.0607   |\n",
      "|    value_loss           | 0.114     |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 57.7     |\n",
      "| time/              |          |\n",
      "|    fps             | 174      |\n",
      "|    iterations      | 749      |\n",
      "|    time_elapsed    | 1099     |\n",
      "|    total_timesteps | 191744   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=192000, episode_reward=20.06 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 230        |\n",
      "|    mean_reward          | 20.1       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 192000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.34683016 |\n",
      "|    clip_fraction        | 0.0798     |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0271    |\n",
      "|    explained_variance   | 0.699      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0174    |\n",
      "|    n_updates            | 14231      |\n",
      "|    policy_gradient_loss | -0.0316    |\n",
      "|    value_loss           | 0.543      |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 57.7     |\n",
      "| time/              |          |\n",
      "|    fps             | 174      |\n",
      "|    iterations      | 750      |\n",
      "|    time_elapsed    | 1101     |\n",
      "|    total_timesteps | 192000   |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 57.7      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 174       |\n",
      "|    iterations           | 751       |\n",
      "|    time_elapsed         | 1102      |\n",
      "|    total_timesteps      | 192256    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.8055253 |\n",
      "|    clip_fraction        | 0.164     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0777   |\n",
      "|    explained_variance   | 0.903     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0809   |\n",
      "|    n_updates            | 14250     |\n",
      "|    policy_gradient_loss | -0.0603   |\n",
      "|    value_loss           | 0.129     |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=192500, episode_reward=7.33 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 7.33      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 192500    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.7911271 |\n",
      "|    clip_fraction        | 0.203     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0784   |\n",
      "|    explained_variance   | 0.71      |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.167    |\n",
      "|    n_updates            | 14269     |\n",
      "|    policy_gradient_loss | -0.0715   |\n",
      "|    value_loss           | 0.0519    |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 57.7     |\n",
      "| time/              |          |\n",
      "|    fps             | 174      |\n",
      "|    iterations      | 752      |\n",
      "|    time_elapsed    | 1104     |\n",
      "|    total_timesteps | 192512   |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 57.7      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 174       |\n",
      "|    iterations           | 753       |\n",
      "|    time_elapsed         | 1105      |\n",
      "|    total_timesteps      | 192768    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.4292166 |\n",
      "|    clip_fraction        | 0.0689    |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0169   |\n",
      "|    explained_variance   | 0.888     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0205   |\n",
      "|    n_updates            | 14288     |\n",
      "|    policy_gradient_loss | -0.0245   |\n",
      "|    value_loss           | 0.0765    |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=193000, episode_reward=8.85 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 8.85      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 193000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.4357047 |\n",
      "|    clip_fraction        | 0.0833    |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.031    |\n",
      "|    explained_variance   | 0.696     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0452   |\n",
      "|    n_updates            | 14307     |\n",
      "|    policy_gradient_loss | -0.0421   |\n",
      "|    value_loss           | 0.192     |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 57.7     |\n",
      "| time/              |          |\n",
      "|    fps             | 174      |\n",
      "|    iterations      | 754      |\n",
      "|    time_elapsed    | 1107     |\n",
      "|    total_timesteps | 193024   |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 57.7      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 174       |\n",
      "|    iterations           | 755       |\n",
      "|    time_elapsed         | 1108      |\n",
      "|    total_timesteps      | 193280    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.7802816 |\n",
      "|    clip_fraction        | 0.171     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0897   |\n",
      "|    explained_variance   | 0.925     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.132    |\n",
      "|    n_updates            | 14326     |\n",
      "|    policy_gradient_loss | -0.0637   |\n",
      "|    value_loss           | 0.101     |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=193500, episode_reward=16.53 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "--------------------------------------\n",
      "| eval/                   |          |\n",
      "|    mean_ep_length       | 230      |\n",
      "|    mean_reward          | 16.5     |\n",
      "| time/                   |          |\n",
      "|    total_timesteps      | 193500   |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 0.609261 |\n",
      "|    clip_fraction        | 0.11     |\n",
      "|    clip_range           | 0.4      |\n",
      "|    entropy_loss         | -0.0339  |\n",
      "|    explained_variance   | 0.537    |\n",
      "|    learning_rate        | 0.000389 |\n",
      "|    loss                 | -0.0181  |\n",
      "|    n_updates            | 14345    |\n",
      "|    policy_gradient_loss | -0.0396  |\n",
      "|    value_loss           | 0.064    |\n",
      "--------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 57.7     |\n",
      "| time/              |          |\n",
      "|    fps             | 174      |\n",
      "|    iterations      | 756      |\n",
      "|    time_elapsed    | 1109     |\n",
      "|    total_timesteps | 193536   |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 57.7       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 174        |\n",
      "|    iterations           | 757        |\n",
      "|    time_elapsed         | 1110       |\n",
      "|    total_timesteps      | 193792     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.27738038 |\n",
      "|    clip_fraction        | 0.0596     |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.029     |\n",
      "|    explained_variance   | 0.804      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | 0.0198     |\n",
      "|    n_updates            | 14364      |\n",
      "|    policy_gradient_loss | -0.0213    |\n",
      "|    value_loss           | 0.343      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=194000, episode_reward=14.27 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 230        |\n",
      "|    mean_reward          | 14.3       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 194000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.93758905 |\n",
      "|    clip_fraction        | 0.154      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0665    |\n",
      "|    explained_variance   | 0.329      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | 0.00932    |\n",
      "|    n_updates            | 14383      |\n",
      "|    policy_gradient_loss | -0.0403    |\n",
      "|    value_loss           | 0.45       |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 57.7     |\n",
      "| time/              |          |\n",
      "|    fps             | 174      |\n",
      "|    iterations      | 758      |\n",
      "|    time_elapsed    | 1112     |\n",
      "|    total_timesteps | 194048   |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 57.7      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 174       |\n",
      "|    iterations           | 759       |\n",
      "|    time_elapsed         | 1113      |\n",
      "|    total_timesteps      | 194304    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.3158026 |\n",
      "|    clip_fraction        | 0.202     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0716   |\n",
      "|    explained_variance   | 0.37      |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.128    |\n",
      "|    n_updates            | 14402     |\n",
      "|    policy_gradient_loss | -0.0688   |\n",
      "|    value_loss           | 0.0444    |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=194500, episode_reward=13.17 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 230        |\n",
      "|    mean_reward          | 13.2       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 194500     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.70121145 |\n",
      "|    clip_fraction        | 0.0785     |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.024     |\n",
      "|    explained_variance   | 0.803      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0144    |\n",
      "|    n_updates            | 14421      |\n",
      "|    policy_gradient_loss | -0.0297    |\n",
      "|    value_loss           | 0.22       |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 57.7     |\n",
      "| time/              |          |\n",
      "|    fps             | 174      |\n",
      "|    iterations      | 760      |\n",
      "|    time_elapsed    | 1115     |\n",
      "|    total_timesteps | 194560   |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 57.5      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 174       |\n",
      "|    iterations           | 761       |\n",
      "|    time_elapsed         | 1116      |\n",
      "|    total_timesteps      | 194816    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.4748197 |\n",
      "|    clip_fraction        | 0.106     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0404   |\n",
      "|    explained_variance   | 0.729     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0149   |\n",
      "|    n_updates            | 14440     |\n",
      "|    policy_gradient_loss | -0.0383   |\n",
      "|    value_loss           | 0.431     |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=195000, episode_reward=20.12 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 20.1      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 195000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.3278259 |\n",
      "|    clip_fraction        | 0.127     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0806   |\n",
      "|    explained_variance   | 0.774     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | 0.0172    |\n",
      "|    n_updates            | 14459     |\n",
      "|    policy_gradient_loss | -0.0476   |\n",
      "|    value_loss           | 0.172     |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 57.5     |\n",
      "| time/              |          |\n",
      "|    fps             | 174      |\n",
      "|    iterations      | 762      |\n",
      "|    time_elapsed    | 1118     |\n",
      "|    total_timesteps | 195072   |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 57.5      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 174       |\n",
      "|    iterations           | 763       |\n",
      "|    time_elapsed         | 1119      |\n",
      "|    total_timesteps      | 195328    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.5839742 |\n",
      "|    clip_fraction        | 0.0718    |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.03     |\n",
      "|    explained_variance   | 0.767     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0135   |\n",
      "|    n_updates            | 14478     |\n",
      "|    policy_gradient_loss | -0.0289   |\n",
      "|    value_loss           | 0.0343    |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=195500, episode_reward=12.88 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 12.9      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 195500    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.2143457 |\n",
      "|    clip_fraction        | 0.0666    |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0311   |\n",
      "|    explained_variance   | 0.613     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0027   |\n",
      "|    n_updates            | 14497     |\n",
      "|    policy_gradient_loss | -0.0348   |\n",
      "|    value_loss           | 1.15      |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 57.5     |\n",
      "| time/              |          |\n",
      "|    fps             | 174      |\n",
      "|    iterations      | 764      |\n",
      "|    time_elapsed    | 1120     |\n",
      "|    total_timesteps | 195584   |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 57.5       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 174        |\n",
      "|    iterations           | 765        |\n",
      "|    time_elapsed         | 1121       |\n",
      "|    total_timesteps      | 195840     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.53171134 |\n",
      "|    clip_fraction        | 0.16       |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0802    |\n",
      "|    explained_variance   | 0.837      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.032     |\n",
      "|    n_updates            | 14516      |\n",
      "|    policy_gradient_loss | -0.0644    |\n",
      "|    value_loss           | 0.139      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=196000, episode_reward=13.97 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "--------------------------------------\n",
      "| eval/                   |          |\n",
      "|    mean_ep_length       | 230      |\n",
      "|    mean_reward          | 14       |\n",
      "| time/                   |          |\n",
      "|    total_timesteps      | 196000   |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 0.750744 |\n",
      "|    clip_fraction        | 0.164    |\n",
      "|    clip_range           | 0.4      |\n",
      "|    entropy_loss         | -0.0671  |\n",
      "|    explained_variance   | 0.554    |\n",
      "|    learning_rate        | 0.000389 |\n",
      "|    loss                 | -0.05    |\n",
      "|    n_updates            | 14535    |\n",
      "|    policy_gradient_loss | -0.0424  |\n",
      "|    value_loss           | 0.0603   |\n",
      "--------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 57.5     |\n",
      "| time/              |          |\n",
      "|    fps             | 174      |\n",
      "|    iterations      | 766      |\n",
      "|    time_elapsed    | 1123     |\n",
      "|    total_timesteps | 196096   |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 57.5      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 174       |\n",
      "|    iterations           | 767       |\n",
      "|    time_elapsed         | 1124      |\n",
      "|    total_timesteps      | 196352    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.5738456 |\n",
      "|    clip_fraction        | 0.0892    |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0412   |\n",
      "|    explained_variance   | 0.747     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0388   |\n",
      "|    n_updates            | 14554     |\n",
      "|    policy_gradient_loss | -0.0314   |\n",
      "|    value_loss           | 0.409     |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=196500, episode_reward=10.28 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 10.3      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 196500    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.1273062 |\n",
      "|    clip_fraction        | 0.111     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0451   |\n",
      "|    explained_variance   | 0.867     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.108    |\n",
      "|    n_updates            | 14573     |\n",
      "|    policy_gradient_loss | -0.0438   |\n",
      "|    value_loss           | 0.109     |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 57.4     |\n",
      "| time/              |          |\n",
      "|    fps             | 174      |\n",
      "|    iterations      | 768      |\n",
      "|    time_elapsed    | 1125     |\n",
      "|    total_timesteps | 196608   |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 57.4      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 174       |\n",
      "|    iterations           | 769       |\n",
      "|    time_elapsed         | 1127      |\n",
      "|    total_timesteps      | 196864    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.6817341 |\n",
      "|    clip_fraction        | 0.174     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0918   |\n",
      "|    explained_variance   | 0.676     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0745   |\n",
      "|    n_updates            | 14592     |\n",
      "|    policy_gradient_loss | -0.0585   |\n",
      "|    value_loss           | 0.273     |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=197000, episode_reward=13.85 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 230        |\n",
      "|    mean_reward          | 13.9       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 197000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.76359844 |\n",
      "|    clip_fraction        | 0.125      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0413    |\n",
      "|    explained_variance   | 0.389      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0795    |\n",
      "|    n_updates            | 14611      |\n",
      "|    policy_gradient_loss | -0.0321    |\n",
      "|    value_loss           | 0.0771     |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 57.4     |\n",
      "| time/              |          |\n",
      "|    fps             | 174      |\n",
      "|    iterations      | 770      |\n",
      "|    time_elapsed    | 1128     |\n",
      "|    total_timesteps | 197120   |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 57.6       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 174        |\n",
      "|    iterations           | 771        |\n",
      "|    time_elapsed         | 1129       |\n",
      "|    total_timesteps      | 197376     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.26045328 |\n",
      "|    clip_fraction        | 0.06       |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0309    |\n",
      "|    explained_variance   | 0.791      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.00199   |\n",
      "|    n_updates            | 14630      |\n",
      "|    policy_gradient_loss | -0.0256    |\n",
      "|    value_loss           | 0.465      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=197500, episode_reward=6.73 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 6.73      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 197500    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 1.0424166 |\n",
      "|    clip_fraction        | 0.143     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0681   |\n",
      "|    explained_variance   | 0.728     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.09     |\n",
      "|    n_updates            | 14649     |\n",
      "|    policy_gradient_loss | -0.0544   |\n",
      "|    value_loss           | 0.288     |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 57.6     |\n",
      "| time/              |          |\n",
      "|    fps             | 174      |\n",
      "|    iterations      | 772      |\n",
      "|    time_elapsed    | 1131     |\n",
      "|    total_timesteps | 197632   |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 57.6      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 174       |\n",
      "|    iterations           | 773       |\n",
      "|    time_elapsed         | 1132      |\n",
      "|    total_timesteps      | 197888    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.7638707 |\n",
      "|    clip_fraction        | 0.155     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0691   |\n",
      "|    explained_variance   | 0.483     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0289   |\n",
      "|    n_updates            | 14668     |\n",
      "|    policy_gradient_loss | -0.0571   |\n",
      "|    value_loss           | 0.115     |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=198000, episode_reward=39.39 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 230        |\n",
      "|    mean_reward          | 39.4       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 198000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.29261273 |\n",
      "|    clip_fraction        | 0.0701     |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0318    |\n",
      "|    explained_variance   | 0.906      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.00828   |\n",
      "|    n_updates            | 14687      |\n",
      "|    policy_gradient_loss | -0.0181    |\n",
      "|    value_loss           | 0.125      |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 57.6     |\n",
      "| time/              |          |\n",
      "|    fps             | 174      |\n",
      "|    iterations      | 774      |\n",
      "|    time_elapsed    | 1133     |\n",
      "|    total_timesteps | 198144   |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 57.6       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 174        |\n",
      "|    iterations           | 775        |\n",
      "|    time_elapsed         | 1135       |\n",
      "|    total_timesteps      | 198400     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.47616237 |\n",
      "|    clip_fraction        | 0.0975     |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0439    |\n",
      "|    explained_variance   | 0.845      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0191    |\n",
      "|    n_updates            | 14706      |\n",
      "|    policy_gradient_loss | -0.0233    |\n",
      "|    value_loss           | 0.233      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=198500, episode_reward=31.77 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 230        |\n",
      "|    mean_reward          | 31.8       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 198500     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.79493356 |\n",
      "|    clip_fraction        | 0.193      |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0704    |\n",
      "|    explained_variance   | 0.786      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.175     |\n",
      "|    n_updates            | 14725      |\n",
      "|    policy_gradient_loss | -0.0694    |\n",
      "|    value_loss           | 0.265      |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 57.6     |\n",
      "| time/              |          |\n",
      "|    fps             | 174      |\n",
      "|    iterations      | 776      |\n",
      "|    time_elapsed    | 1137     |\n",
      "|    total_timesteps | 198656   |\n",
      "---------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 889       |\n",
      "|    ep_rew_mean          | 57.6      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 174       |\n",
      "|    iterations           | 777       |\n",
      "|    time_elapsed         | 1138      |\n",
      "|    total_timesteps      | 198912    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.8393789 |\n",
      "|    clip_fraction        | 0.122     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0395   |\n",
      "|    explained_variance   | 0.565     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.013    |\n",
      "|    n_updates            | 14744     |\n",
      "|    policy_gradient_loss | -0.0403   |\n",
      "|    value_loss           | 0.0474    |\n",
      "---------------------------------------\n",
      "Eval num_timesteps=199000, episode_reward=17.67 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 230        |\n",
      "|    mean_reward          | 17.7       |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 199000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.30610138 |\n",
      "|    clip_fraction        | 0.0894     |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.038     |\n",
      "|    explained_variance   | 0.806      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | 0.162      |\n",
      "|    n_updates            | 14763      |\n",
      "|    policy_gradient_loss | -0.0231    |\n",
      "|    value_loss           | 0.464      |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 57.5     |\n",
      "| time/              |          |\n",
      "|    fps             | 174      |\n",
      "|    iterations      | 778      |\n",
      "|    time_elapsed    | 1139     |\n",
      "|    total_timesteps | 199168   |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 57.5       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 174        |\n",
      "|    iterations           | 779        |\n",
      "|    time_elapsed         | 1141       |\n",
      "|    total_timesteps      | 199424     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.66566443 |\n",
      "|    clip_fraction        | 0.13       |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0496    |\n",
      "|    explained_variance   | 0.848      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0579    |\n",
      "|    n_updates            | 14782      |\n",
      "|    policy_gradient_loss | -0.0414    |\n",
      "|    value_loss           | 0.169      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=199500, episode_reward=28.50 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 28.5      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 199500    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.5923333 |\n",
      "|    clip_fraction        | 0.11      |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0569   |\n",
      "|    explained_variance   | 0.351     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.0863   |\n",
      "|    n_updates            | 14801     |\n",
      "|    policy_gradient_loss | -0.0381   |\n",
      "|    value_loss           | 0.0627    |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 57.5     |\n",
      "| time/              |          |\n",
      "|    fps             | 174      |\n",
      "|    iterations      | 780      |\n",
      "|    time_elapsed    | 1142     |\n",
      "|    total_timesteps | 199680   |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 889        |\n",
      "|    ep_rew_mean          | 57.5       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 174        |\n",
      "|    iterations           | 781        |\n",
      "|    time_elapsed         | 1143       |\n",
      "|    total_timesteps      | 199936     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.43389827 |\n",
      "|    clip_fraction        | 0.0711     |\n",
      "|    clip_range           | 0.4        |\n",
      "|    entropy_loss         | -0.0293    |\n",
      "|    explained_variance   | 0.863      |\n",
      "|    learning_rate        | 0.000389   |\n",
      "|    loss                 | -0.0573    |\n",
      "|    n_updates            | 14820      |\n",
      "|    policy_gradient_loss | -0.0319    |\n",
      "|    value_loss           | 0.179      |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=200000, episode_reward=49.60 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "---------------------------------------\n",
      "| eval/                   |           |\n",
      "|    mean_ep_length       | 230       |\n",
      "|    mean_reward          | 49.6      |\n",
      "| time/                   |           |\n",
      "|    total_timesteps      | 200000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.7815335 |\n",
      "|    clip_fraction        | 0.106     |\n",
      "|    clip_range           | 0.4       |\n",
      "|    entropy_loss         | -0.0415   |\n",
      "|    explained_variance   | 0.923     |\n",
      "|    learning_rate        | 0.000389  |\n",
      "|    loss                 | -0.00466  |\n",
      "|    n_updates            | 14839     |\n",
      "|    policy_gradient_loss | -0.0497   |\n",
      "|    value_loss           | 0.15      |\n",
      "---------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 57.4     |\n",
      "| time/              |          |\n",
      "|    fps             | 174      |\n",
      "|    iterations      | 782      |\n",
      "|    time_elapsed    | 1145     |\n",
      "|    total_timesteps | 200192   |\n",
      "---------------------------------\n",
      "Final model evaluation on validation set: Mean Reward = 48.33 ± 10.40\n"
     ]
    }
   ],
   "source": [
    "train_lstm_env = CustomStocksEnv(df=train_df, window_size=window_size, frame_bound=(window_size, len(train_df)))\n",
    "val_lstm_env = CustomStocksEnv(df=val_df, window_size=window_size, frame_bound=(window_size, len(val_df)))\n",
    "val_lstm_env = Monitor(val_lstm_env)\n",
    "\n",
    "lstm_eval_callback = EvalCallback(\n",
    "        val_lstm_env,\n",
    "        best_model_save_path='./logs/best_model_lstm/',\n",
    "        log_path='./logs/results/',\n",
    "        eval_freq=500,  # Evaluate every 500 timesteps.\n",
    "        deterministic=True,\n",
    "        render=False\n",
    "    )\n",
    "\n",
    "final_policy_kwargs = dict(\n",
    "    n_lstm_layers=lstm_best_params[\"n_lstm_layers\"],\n",
    "    lstm_hidden_size = lstm_best_params[\"hidden_size\"],\n",
    "    lstm_kwargs=dict(\n",
    "        dropout=lstm_best_params[\"dropout\"]\n",
    "    ),\n",
    "    net_arch=dict(pi=[64], vf=[64])\n",
    ")\n",
    "\n",
    "final_lstm_model = RecurrentPPO(\n",
    "    policy=\"MlpLstmPolicy\",\n",
    "    env=train_lstm_env,\n",
    "    learning_rate=lstm_best_params[\"learning_rate\"],\n",
    "    n_steps=lstm_best_params[\"n_steps\"],\n",
    "    gamma=lstm_best_params[\"gamma\"],\n",
    "    ent_coef=lstm_best_params[\"ent_coef\"],\n",
    "    clip_range=lstm_best_params[\"clip_range\"],\n",
    "    batch_size=lstm_best_params[\"batch_size\"],\n",
    "    gae_lambda=lstm_best_params[\"gae_lambda\"],\n",
    "    n_epochs=lstm_best_params[\"n_epochs\"],\n",
    "    policy_kwargs=final_policy_kwargs,\n",
    "    seed=SEED,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "\n",
    "final_lstm_model.learn(total_timesteps=200_000, callback= lstm_eval_callback)\n",
    "final_lstm_model.save(\"recurrent_ppo_stocks_model\")\n",
    "\n",
    "final_model = RecurrentPPO.load(\"./logs/best_model_lstm/best_model.zip\")\n",
    "\n",
    "#val_lstm_env = CustomStocksEnv(df=val_df, window_size=window_size, frame_bound=(window_size, len(val_df)))\n",
    "mean_reward, std_reward = evaluate_policy(final_lstm_model, val_lstm_env, n_eval_episodes=20, deterministic=False)\n",
    "\n",
    "print(f\"Final model evaluation on validation set: Mean Reward = {mean_reward:.2f} ± {std_reward:.2f}\")\n",
    "\n",
    "train_lstm_env.close()\n",
    "val_lstm_env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6272b0f8-0d07-4d58-9460-789913b5b442",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "64c1c288-2b1b-4dec-a73b-1846591b0a1f",
   "metadata": {},
   "source": [
    "## c. Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0473dc61-1942-4784-9549-dc9385f6b494",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final info: {'total_reward': np.float32(39.389053), 'total_profit': np.float32(1.3771601), 'position': <Positions.Long: 1>}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/self-learning/lib/python3.9/site-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_lstm_env = CustomStocksEnv(df=test_df, window_size=window_size, frame_bound=(window_size, len(test_df)))\n",
    "final_lstm_model = RecurrentPPO.load(\"./logs/best_model_lstm/best_model.zip\")\n",
    "\n",
    "obs, info = test_lstm_env.reset()\n",
    "rewards = []\n",
    "while True:\n",
    "    action, _states = final_lstm_model.predict(obs, deterministic=True)\n",
    "    \n",
    "    obs, reward, terminated, truncated, info = test_lstm_env.step(action)\n",
    "    done = terminated or truncated\n",
    "    if done:\n",
    "        print(\"Final info:\", info)\n",
    "        break\n",
    "\n",
    "\n",
    "plt.figure(figsize=(15, 6))\n",
    "plt.cla()\n",
    "test_lstm_env.unwrapped.render_all()\n",
    "plt.show()\n",
    "\n",
    "test_lstm_env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7d92d34-4939-4cb3-9b6a-22d34ce226c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2ed9ebf9-2d7a-4d62-9cb3-061a6fe3358e",
   "metadata": {},
   "source": [
    "# 5. A2C Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d754884d-5167-44e1-b45e-4bc3d4c762c3",
   "metadata": {},
   "source": [
    "## a. Fine tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a1c87899-1087-4faa-ad7d-f46ed20b14b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def a2c_objective(trial: optuna.Trial):\n",
    "    \"\"\"\n",
    "    Fine tuning using Bayesian Optimization from optuna\n",
    "    \"\"\"\n",
    "\n",
    "    learning_rate = trial.suggest_float(\"learning_rate\", 1e-5, 1e-2, log=True)\n",
    "    # A2C typically uses smaller n_steps than PPO since it updates synchronously\n",
    "    n_steps       = trial.suggest_int(\"n_steps\", 5, 130, step=5)\n",
    "    gamma         = trial.suggest_float(\"gamma\", 0.90, 0.99, step=0.01)\n",
    "    ent_coef      = trial.suggest_float(\"ent_coef\", 1e-8, 0.1, log=True)\n",
    "    gae_lambda    = trial.suggest_float(\"gae_lambda\", 0.8, 0.98)\n",
    "    # A2C-specific hyperparameters:\n",
    "    vf_coef       = trial.suggest_float(\"vf_coef\", 0.1, 1.0, step=0.1)\n",
    "    max_grad_norm = trial.suggest_float(\"max_grad_norm\", 0.3, 5.0, step=0.1)\n",
    "\n",
    "\n",
    "    # Create the environment\n",
    "    train_env = CustomStocksEnv(df=train_df, window_size=window_size, frame_bound=(window_size, len(train_df)))\n",
    "    val_env = CustomStocksEnv(df=val_df, window_size=window_size, frame_bound=(window_size, len(val_df)))\n",
    "    val_env = Monitor(val_env)\n",
    "\n",
    "    eval_callback_ft = EvalCallback(\n",
    "        val_env,\n",
    "        best_model_save_path='./logs/best_model_ft_a2c/',\n",
    "        log_path='./logs/results_ft_a2c/',\n",
    "        eval_freq=500,  # Evaluate every 500 timesteps.\n",
    "        deterministic=True,\n",
    "        render=False,\n",
    "        verbose=0\n",
    "    )\n",
    "    \n",
    "    # Build the A2C model\n",
    "    model = A2C(\n",
    "        policy=\"MlpPolicy\",\n",
    "        env=train_env,\n",
    "        verbose=0,\n",
    "        learning_rate=learning_rate,\n",
    "        n_steps=n_steps,\n",
    "        gamma=gamma,\n",
    "        gae_lambda=gae_lambda,\n",
    "        ent_coef=ent_coef,\n",
    "        vf_coef=vf_coef,\n",
    "        max_grad_norm=max_grad_norm\n",
    "    )\n",
    "\n",
    "    #Keep it short - 50,000 steps also make sense\n",
    "    model.learn(total_timesteps=100_000, callback=eval_callback_ft)\n",
    "\n",
    "    model = PPO.load(\"./logs/best_model_ft_a2c/best_model.zip\")\n",
    "    mean_reward, std_reward = evaluate_policy(model, val_env, n_eval_episodes=20, deterministic=False)\n",
    "\n",
    "    # Cleanup the environments\n",
    "    train_env.close()\n",
    "    val_env.close()\n",
    "\n",
    "    return mean_reward\n",
    "\n",
    "def run_a2c_optimization():\n",
    "    study = optuna.create_study(direction=\"maximize\")  \n",
    "    study.optimize(a2c_objective, n_trials=200)\n",
    "\n",
    "    print(\"Best hyperparameters:\", study.best_params)\n",
    "    print(\"Best value (objective):\", study.best_value)\n",
    "\n",
    "    return study\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f7176e9b-1c3d-4cd7-b409-b6d542e90270",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-06 23:09:46,675] A new study created in memory with name: no-name-568a7912-6ae3-46f6-9950-72243c6bb1c3\n",
      "[I 2025-02-06 23:10:42,795] Trial 0 finished with value: 36.64268954999999 and parameters: {'learning_rate': 4.147057249781011e-05, 'n_steps': 25, 'gamma': 0.93, 'ent_coef': 1.2511054224072445e-05, 'gae_lambda': 0.9665164992798346, 'vf_coef': 0.6, 'max_grad_norm': 0.3}. Best is trial 0 with value: 36.64268954999999.\n",
      "[I 2025-02-06 23:11:34,878] Trial 1 finished with value: 71.12198179999999 and parameters: {'learning_rate': 0.00026120807103009074, 'n_steps': 40, 'gamma': 0.91, 'ent_coef': 3.541855371354562e-05, 'gae_lambda': 0.8721807657858502, 'vf_coef': 0.8, 'max_grad_norm': 4.4}. Best is trial 1 with value: 71.12198179999999.\n",
      "[I 2025-02-06 23:13:02,680] Trial 2 finished with value: 72.5048341 and parameters: {'learning_rate': 0.00010872769351537924, 'n_steps': 5, 'gamma': 0.97, 'ent_coef': 0.0012065160455882603, 'gae_lambda': 0.9713902470248995, 'vf_coef': 0.2, 'max_grad_norm': 3.6}. Best is trial 2 with value: 72.5048341.\n",
      "[I 2025-02-06 23:13:53,694] Trial 3 finished with value: 30.5415508 and parameters: {'learning_rate': 0.00025263715212197355, 'n_steps': 80, 'gamma': 0.93, 'ent_coef': 1.2252501005053125e-06, 'gae_lambda': 0.8827226035478942, 'vf_coef': 0.8, 'max_grad_norm': 0.5}. Best is trial 2 with value: 72.5048341.\n",
      "[I 2025-02-06 23:14:41,411] Trial 4 finished with value: 46.984905999999995 and parameters: {'learning_rate': 0.000572782185181415, 'n_steps': 130, 'gamma': 0.91, 'ent_coef': 0.009489234885980464, 'gae_lambda': 0.9495350110588597, 'vf_coef': 0.7000000000000001, 'max_grad_norm': 0.8}. Best is trial 2 with value: 72.5048341.\n",
      "[I 2025-02-06 23:15:31,093] Trial 5 finished with value: 72.38659715000001 and parameters: {'learning_rate': 0.0019446636348267208, 'n_steps': 55, 'gamma': 0.98, 'ent_coef': 0.0009543686910258956, 'gae_lambda': 0.9126207386762039, 'vf_coef': 0.8, 'max_grad_norm': 1.6}. Best is trial 2 with value: 72.5048341.\n",
      "[I 2025-02-06 23:16:24,030] Trial 6 finished with value: 75.50150025 and parameters: {'learning_rate': 0.003867388206181347, 'n_steps': 25, 'gamma': 0.9, 'ent_coef': 4.319241435202112e-06, 'gae_lambda': 0.9679740530057444, 'vf_coef': 0.4, 'max_grad_norm': 4.1000000000000005}. Best is trial 6 with value: 75.50150025.\n",
      "[I 2025-02-06 23:17:11,864] Trial 7 finished with value: 40.88054895 and parameters: {'learning_rate': 0.00024329141214691688, 'n_steps': 115, 'gamma': 0.91, 'ent_coef': 5.418577378265853e-05, 'gae_lambda': 0.9190281569648988, 'vf_coef': 0.2, 'max_grad_norm': 1.5000000000000002}. Best is trial 6 with value: 75.50150025.\n",
      "[I 2025-02-06 23:18:01,543] Trial 8 finished with value: 71.49940985 and parameters: {'learning_rate': 0.0015907937311630538, 'n_steps': 120, 'gamma': 0.98, 'ent_coef': 0.035837383439281514, 'gae_lambda': 0.8319642153706287, 'vf_coef': 0.1, 'max_grad_norm': 1.1}. Best is trial 6 with value: 75.50150025.\n",
      "[I 2025-02-06 23:18:49,915] Trial 9 finished with value: 27.03009015 and parameters: {'learning_rate': 0.00010294355618293657, 'n_steps': 80, 'gamma': 0.9400000000000001, 'ent_coef': 4.0779069249577314e-07, 'gae_lambda': 0.9344077123206791, 'vf_coef': 0.4, 'max_grad_norm': 0.7}. Best is trial 6 with value: 75.50150025.\n",
      "[I 2025-02-06 23:20:06,855] Trial 10 finished with value: 56.095004900000006 and parameters: {'learning_rate': 0.0088278638052193, 'n_steps': 5, 'gamma': 0.96, 'ent_coef': 2.3596096687028307e-08, 'gae_lambda': 0.8305576246598706, 'vf_coef': 0.4, 'max_grad_norm': 5.0}. Best is trial 6 with value: 75.50150025.\n",
      "[I 2025-02-06 23:21:07,449] Trial 11 finished with value: 39.72212605 and parameters: {'learning_rate': 2.6810138642034826e-05, 'n_steps': 10, 'gamma': 0.96, 'ent_coef': 0.001719694146215896, 'gae_lambda': 0.9767385280579666, 'vf_coef': 0.30000000000000004, 'max_grad_norm': 3.6}. Best is trial 6 with value: 75.50150025.\n",
      "[I 2025-02-06 23:21:59,338] Trial 12 finished with value: 85.58430725 and parameters: {'learning_rate': 0.009960869214942719, 'n_steps': 30, 'gamma': 0.96, 'ent_coef': 0.0002592474661674927, 'gae_lambda': 0.950438086902761, 'vf_coef': 0.5, 'max_grad_norm': 3.3}. Best is trial 12 with value: 85.58430725.\n",
      "[I 2025-02-06 23:22:50,108] Trial 13 finished with value: 82.1796428 and parameters: {'learning_rate': 0.008591895888200168, 'n_steps': 40, 'gamma': 0.9500000000000001, 'ent_coef': 2.7371384352616216e-06, 'gae_lambda': 0.9419205033531152, 'vf_coef': 1.0, 'max_grad_norm': 2.7}. Best is trial 12 with value: 85.58430725.\n",
      "[I 2025-02-06 23:23:38,137] Trial 14 finished with value: 69.3139803 and parameters: {'learning_rate': 0.009920448148258336, 'n_steps': 60, 'gamma': 0.9500000000000001, 'ent_coef': 0.00015533932360888133, 'gae_lambda': 0.9003928673922703, 'vf_coef': 1.0, 'max_grad_norm': 2.3}. Best is trial 12 with value: 85.58430725.\n",
      "[I 2025-02-06 23:24:29,388] Trial 15 finished with value: 67.67902425 and parameters: {'learning_rate': 0.0029073938248860755, 'n_steps': 40, 'gamma': 0.99, 'ent_coef': 8.554523024757355e-08, 'gae_lambda': 0.9386931432269627, 'vf_coef': 0.6, 'max_grad_norm': 2.7}. Best is trial 12 with value: 85.58430725.\n",
      "[I 2025-02-06 23:25:19,287] Trial 16 finished with value: 86.72005190000002 and parameters: {'learning_rate': 0.0007865482300862252, 'n_steps': 40, 'gamma': 0.9500000000000001, 'ent_coef': 0.00029943092911532856, 'gae_lambda': 0.8728915211197849, 'vf_coef': 1.0, 'max_grad_norm': 2.7}. Best is trial 16 with value: 86.72005190000002.\n",
      "[I 2025-02-06 23:26:06,834] Trial 17 finished with value: 32.255693099999995 and parameters: {'learning_rate': 1.0582119893994548e-05, 'n_steps': 85, 'gamma': 0.93, 'ent_coef': 0.00014727871865837612, 'gae_lambda': 0.8609353439008107, 'vf_coef': 0.5, 'max_grad_norm': 3.4}. Best is trial 16 with value: 86.72005190000002.\n",
      "[I 2025-02-06 23:27:00,071] Trial 18 finished with value: 77.39256499999999 and parameters: {'learning_rate': 0.0011142698153620931, 'n_steps': 25, 'gamma': 0.96, 'ent_coef': 0.007286349551761724, 'gae_lambda': 0.8050481883600376, 'vf_coef': 0.9, 'max_grad_norm': 2.9}. Best is trial 16 with value: 86.72005190000002.\n",
      "[I 2025-02-06 23:27:53,406] Trial 19 finished with value: 74.43210765 and parameters: {'learning_rate': 0.0008890979686122854, 'n_steps': 50, 'gamma': 0.9400000000000001, 'ent_coef': 0.00031090513867312994, 'gae_lambda': 0.8556176761196453, 'vf_coef': 0.5, 'max_grad_norm': 2.1}. Best is trial 16 with value: 86.72005190000002.\n",
      "[I 2025-02-06 23:28:45,643] Trial 20 finished with value: 70.90148985 and parameters: {'learning_rate': 0.004786961336756686, 'n_steps': 70, 'gamma': 0.97, 'ent_coef': 0.0436394379539724, 'gae_lambda': 0.8884091345261806, 'vf_coef': 0.7000000000000001, 'max_grad_norm': 3.2}. Best is trial 16 with value: 86.72005190000002.\n",
      "[I 2025-02-06 23:29:40,219] Trial 21 finished with value: 55.3393599 and parameters: {'learning_rate': 0.005615543957695293, 'n_steps': 40, 'gamma': 0.9500000000000001, 'ent_coef': 5.820560799404619e-06, 'gae_lambda': 0.9233445807439782, 'vf_coef': 1.0, 'max_grad_norm': 2.3}. Best is trial 16 with value: 86.72005190000002.\n",
      "[I 2025-02-06 23:30:38,890] Trial 22 finished with value: 75.87382435 and parameters: {'learning_rate': 0.0006033936417159843, 'n_steps': 20, 'gamma': 0.9500000000000001, 'ent_coef': 9.838293715657887e-07, 'gae_lambda': 0.9502226888818701, 'vf_coef': 0.9, 'max_grad_norm': 3.1}. Best is trial 16 with value: 86.72005190000002.\n",
      "[I 2025-02-06 23:31:33,736] Trial 23 finished with value: 90.02189089999999 and parameters: {'learning_rate': 0.0027952358035528103, 'n_steps': 45, 'gamma': 0.9400000000000001, 'ent_coef': 1.882299747702415e-05, 'gae_lambda': 0.8970309360322143, 'vf_coef': 1.0, 'max_grad_norm': 2.0}. Best is trial 23 with value: 90.02189089999999.\n",
      "[I 2025-02-06 23:32:25,716] Trial 24 finished with value: 87.7270017 and parameters: {'learning_rate': 0.0025663807820984234, 'n_steps': 65, 'gamma': 0.9400000000000001, 'ent_coef': 2.9833643270167172e-05, 'gae_lambda': 0.9021500046705663, 'vf_coef': 0.9, 'max_grad_norm': 1.9000000000000001}. Best is trial 23 with value: 90.02189089999999.\n",
      "[I 2025-02-06 23:33:15,928] Trial 25 finished with value: 86.63658694999998 and parameters: {'learning_rate': 0.0019499637703670372, 'n_steps': 95, 'gamma': 0.92, 'ent_coef': 1.5307200066305598e-05, 'gae_lambda': 0.9005616746654992, 'vf_coef': 0.9, 'max_grad_norm': 1.8}. Best is trial 23 with value: 90.02189089999999.\n",
      "[I 2025-02-06 23:34:06,848] Trial 26 finished with value: 72.55642724999998 and parameters: {'learning_rate': 0.002780182070718901, 'n_steps': 65, 'gamma': 0.9400000000000001, 'ent_coef': 7.207110751811187e-05, 'gae_lambda': 0.8688636434201976, 'vf_coef': 1.0, 'max_grad_norm': 1.2}. Best is trial 23 with value: 90.02189089999999.\n",
      "[I 2025-02-06 23:34:58,083] Trial 27 finished with value: 53.560558549999996 and parameters: {'learning_rate': 0.0006008157764131646, 'n_steps': 95, 'gamma': 0.92, 'ent_coef': 1.922554515850382e-05, 'gae_lambda': 0.8415907835335017, 'vf_coef': 0.9, 'max_grad_norm': 2.0}. Best is trial 23 with value: 90.02189089999999.\n",
      "[I 2025-02-06 23:35:50,803] Trial 28 finished with value: 80.18334469999999 and parameters: {'learning_rate': 0.0011440192897096098, 'n_steps': 50, 'gamma': 0.93, 'ent_coef': 0.0004248670308606709, 'gae_lambda': 0.9040215303049295, 'vf_coef': 0.7000000000000001, 'max_grad_norm': 2.4}. Best is trial 23 with value: 90.02189089999999.\n",
      "[I 2025-02-06 23:36:41,771] Trial 29 finished with value: 42.882394649999995 and parameters: {'learning_rate': 0.00037890021094840875, 'n_steps': 75, 'gamma': 0.9400000000000001, 'ent_coef': 1.8887424762315367e-07, 'gae_lambda': 0.8830481741256272, 'vf_coef': 0.8, 'max_grad_norm': 1.5000000000000002}. Best is trial 23 with value: 90.02189089999999.\n",
      "[I 2025-02-06 23:37:33,963] Trial 30 finished with value: 37.70800955 and parameters: {'learning_rate': 0.0001536003516777101, 'n_steps': 60, 'gamma': 0.92, 'ent_coef': 0.004261044072463582, 'gae_lambda': 0.8547028930412838, 'vf_coef': 1.0, 'max_grad_norm': 1.9000000000000001}. Best is trial 23 with value: 90.02189089999999.\n",
      "[I 2025-02-06 23:38:25,239] Trial 31 finished with value: 69.91323134999999 and parameters: {'learning_rate': 0.002001436824988908, 'n_steps': 95, 'gamma': 0.92, 'ent_coef': 1.181751210555317e-05, 'gae_lambda': 0.9025476527781214, 'vf_coef': 0.9, 'max_grad_norm': 1.8}. Best is trial 23 with value: 90.02189089999999.\n",
      "[I 2025-02-06 23:39:14,363] Trial 32 finished with value: 82.41557935 and parameters: {'learning_rate': 0.0016304367488617945, 'n_steps': 100, 'gamma': 0.93, 'ent_coef': 1.8290172459730076e-05, 'gae_lambda': 0.8950470710453216, 'vf_coef': 0.9, 'max_grad_norm': 1.3}. Best is trial 23 with value: 90.02189089999999.\n",
      "[I 2025-02-06 23:40:03,421] Trial 33 finished with value: 100.89581374999999 and parameters: {'learning_rate': 0.0028126632631517677, 'n_steps': 50, 'gamma': 0.9, 'ent_coef': 3.081984679463681e-05, 'gae_lambda': 0.8741255082623497, 'vf_coef': 0.8, 'max_grad_norm': 2.5}. Best is trial 33 with value: 100.89581374999999.\n",
      "[I 2025-02-06 23:40:53,025] Trial 34 finished with value: 56.97110439999999 and parameters: {'learning_rate': 0.003273604834480659, 'n_steps': 35, 'gamma': 0.97, 'ent_coef': 0.00010354020622217761, 'gae_lambda': 0.8792693617290395, 'vf_coef': 0.8, 'max_grad_norm': 2.5}. Best is trial 33 with value: 100.89581374999999.\n",
      "[I 2025-02-06 23:41:45,717] Trial 35 finished with value: 72.22614595 and parameters: {'learning_rate': 0.005660489617294133, 'n_steps': 50, 'gamma': 0.9, 'ent_coef': 0.0006869091594902142, 'gae_lambda': 0.8729738869080522, 'vf_coef': 1.0, 'max_grad_norm': 2.9}. Best is trial 33 with value: 100.89581374999999.\n",
      "[I 2025-02-06 23:42:37,783] Trial 36 finished with value: 66.72571785 and parameters: {'learning_rate': 0.0008351997032830575, 'n_steps': 45, 'gamma': 0.9400000000000001, 'ent_coef': 3.3894679840138826e-05, 'gae_lambda': 0.912780086794899, 'vf_coef': 0.8, 'max_grad_norm': 3.9}. Best is trial 33 with value: 100.89581374999999.\n",
      "[I 2025-02-06 23:43:42,783] Trial 37 finished with value: 62.71662655 and parameters: {'learning_rate': 0.00039077677915406767, 'n_steps': 15, 'gamma': 0.9, 'ent_coef': 2.0580176019591562e-06, 'gae_lambda': 0.8672882640116997, 'vf_coef': 0.7000000000000001, 'max_grad_norm': 2.2}. Best is trial 33 with value: 100.89581374999999.\n",
      "[I 2025-02-06 23:44:36,036] Trial 38 finished with value: 92.67919855 and parameters: {'learning_rate': 0.0024495649396408413, 'n_steps': 65, 'gamma': 0.91, 'ent_coef': 8.880522427004356e-06, 'gae_lambda': 0.8455151932032782, 'vf_coef': 0.8, 'max_grad_norm': 2.6}. Best is trial 33 with value: 100.89581374999999.\n",
      "[I 2025-02-06 23:45:29,739] Trial 39 finished with value: 85.35802190000001 and parameters: {'learning_rate': 0.004526795026088439, 'n_steps': 65, 'gamma': 0.91, 'ent_coef': 7.623711664982589e-06, 'gae_lambda': 0.8460062177825046, 'vf_coef': 0.6, 'max_grad_norm': 1.0}. Best is trial 33 with value: 100.89581374999999.\n",
      "[I 2025-02-06 23:46:23,500] Trial 40 finished with value: 73.07507055 and parameters: {'learning_rate': 0.0026285045739473257, 'n_steps': 55, 'gamma': 0.9, 'ent_coef': 6.347736630936252e-07, 'gae_lambda': 0.813537175730412, 'vf_coef': 0.7000000000000001, 'max_grad_norm': 0.3}. Best is trial 33 with value: 100.89581374999999.\n",
      "[I 2025-02-06 23:47:16,986] Trial 41 finished with value: 69.33749275 and parameters: {'learning_rate': 0.0013332136955435357, 'n_steps': 70, 'gamma': 0.91, 'ent_coef': 2.5561803739813692e-05, 'gae_lambda': 0.890989554642327, 'vf_coef': 0.9, 'max_grad_norm': 2.5}. Best is trial 33 with value: 100.89581374999999.\n",
      "[I 2025-02-06 23:48:08,610] Trial 42 finished with value: 69.06827560000002 and parameters: {'learning_rate': 0.0022524164007791275, 'n_steps': 60, 'gamma': 0.9, 'ent_coef': 5.5918293365435543e-05, 'gae_lambda': 0.8784183118504758, 'vf_coef': 0.8, 'max_grad_norm': 2.9}. Best is trial 33 with value: 100.89581374999999.\n",
      "[I 2025-02-06 23:49:06,691] Trial 43 finished with value: 65.69656269999999 and parameters: {'learning_rate': 0.003445743751173125, 'n_steps': 30, 'gamma': 0.91, 'ent_coef': 0.0019261619146661133, 'gae_lambda': 0.8224666911877003, 'vf_coef': 1.0, 'max_grad_norm': 1.6}. Best is trial 33 with value: 100.89581374999999.\n",
      "[I 2025-02-06 23:50:01,091] Trial 44 finished with value: 61.68446504999999 and parameters: {'learning_rate': 0.0008493321206108029, 'n_steps': 45, 'gamma': 0.93, 'ent_coef': 3.588894418247705e-06, 'gae_lambda': 0.843051040386534, 'vf_coef': 0.8, 'max_grad_norm': 2.6}. Best is trial 33 with value: 100.89581374999999.\n",
      "[I 2025-02-06 23:50:51,587] Trial 45 finished with value: 61.788081999999996 and parameters: {'learning_rate': 0.006409952103875558, 'n_steps': 80, 'gamma': 0.9500000000000001, 'ent_coef': 4.4225037773829885e-05, 'gae_lambda': 0.9178283173445791, 'vf_coef': 0.9, 'max_grad_norm': 2.1}. Best is trial 33 with value: 100.89581374999999.\n",
      "[I 2025-02-06 23:51:44,924] Trial 46 finished with value: 70.0297382 and parameters: {'learning_rate': 0.0014976084913960724, 'n_steps': 55, 'gamma': 0.91, 'ent_coef': 6.0569439227106685e-06, 'gae_lambda': 0.8583682414257077, 'vf_coef': 1.0, 'max_grad_norm': 1.7000000000000002}. Best is trial 33 with value: 100.89581374999999.\n",
      "[I 2025-02-06 23:52:41,922] Trial 47 finished with value: 62.35005875 and parameters: {'learning_rate': 0.0037742067504074437, 'n_steps': 35, 'gamma': 0.93, 'ent_coef': 1.8876240596702674e-06, 'gae_lambda': 0.8330761420897789, 'vf_coef': 0.8, 'max_grad_norm': 2.8}. Best is trial 33 with value: 100.89581374999999.\n",
      "[I 2025-02-06 23:53:34,623] Trial 48 finished with value: 34.916571450000006 and parameters: {'learning_rate': 6.607770516069536e-05, 'n_steps': 75, 'gamma': 0.96, 'ent_coef': 9.369100959347621e-05, 'gae_lambda': 0.8844248365407451, 'vf_coef': 0.9, 'max_grad_norm': 3.6}. Best is trial 33 with value: 100.89581374999999.\n",
      "[I 2025-02-06 23:54:24,646] Trial 49 finished with value: 54.5221273 and parameters: {'learning_rate': 0.00025425578749629506, 'n_steps': 85, 'gamma': 0.92, 'ent_coef': 0.0005012872345377928, 'gae_lambda': 0.9269917028658617, 'vf_coef': 0.7000000000000001, 'max_grad_norm': 4.5}. Best is trial 33 with value: 100.89581374999999.\n",
      "[I 2025-02-06 23:55:19,266] Trial 50 finished with value: 79.2276894 and parameters: {'learning_rate': 0.007157055077563276, 'n_steps': 45, 'gamma': 0.98, 'ent_coef': 0.0001972740054731655, 'gae_lambda': 0.9090647367341101, 'vf_coef': 1.0, 'max_grad_norm': 3.1}. Best is trial 33 with value: 100.89581374999999.\n",
      "[I 2025-02-06 23:56:09,152] Trial 51 finished with value: 66.53247335 and parameters: {'learning_rate': 0.0018295095185549498, 'n_steps': 125, 'gamma': 0.91, 'ent_coef': 1.0573546246873418e-05, 'gae_lambda': 0.894353666069476, 'vf_coef': 0.9, 'max_grad_norm': 1.9000000000000001}. Best is trial 33 with value: 100.89581374999999.\n",
      "[I 2025-02-06 23:56:59,379] Trial 52 finished with value: 72.1590502 and parameters: {'learning_rate': 0.0022239260884990035, 'n_steps': 105, 'gamma': 0.92, 'ent_coef': 2.2452996755058805e-05, 'gae_lambda': 0.8726107096442014, 'vf_coef': 0.9, 'max_grad_norm': 1.4000000000000001}. Best is trial 33 with value: 100.89581374999999.\n",
      "[I 2025-02-06 23:57:48,679] Trial 53 finished with value: 55.333241099999995 and parameters: {'learning_rate': 0.0010393147122408908, 'n_steps': 110, 'gamma': 0.9400000000000001, 'ent_coef': 4.0131108479758704e-05, 'gae_lambda': 0.8982047729696778, 'vf_coef': 1.0, 'max_grad_norm': 2.2}. Best is trial 33 with value: 100.89581374999999.\n",
      "[I 2025-02-06 23:58:40,155] Trial 54 finished with value: 67.37251144999999 and parameters: {'learning_rate': 0.004283816912080273, 'n_steps': 60, 'gamma': 0.9, 'ent_coef': 1.2979582332993211e-05, 'gae_lambda': 0.908500799210292, 'vf_coef': 0.1, 'max_grad_norm': 2.4}. Best is trial 33 with value: 100.89581374999999.\n",
      "[I 2025-02-06 23:59:34,060] Trial 55 finished with value: 61.29343115 and parameters: {'learning_rate': 0.0007178372459175228, 'n_steps': 55, 'gamma': 0.9500000000000001, 'ent_coef': 0.00012319405478463275, 'gae_lambda': 0.8650887804247009, 'vf_coef': 0.8, 'max_grad_norm': 1.7000000000000002}. Best is trial 33 with value: 100.89581374999999.\n",
      "[I 2025-02-07 00:00:24,717] Trial 56 finished with value: 67.77606129999998 and parameters: {'learning_rate': 0.0025601740054707143, 'n_steps': 90, 'gamma': 0.92, 'ent_coef': 4.907854095621486e-06, 'gae_lambda': 0.8866260189680895, 'vf_coef': 0.9, 'max_grad_norm': 2.7}. Best is trial 33 with value: 100.89581374999999.\n",
      "[I 2025-02-07 00:01:16,628] Trial 57 finished with value: 63.89328069999999 and parameters: {'learning_rate': 0.0013985347690652877, 'n_steps': 75, 'gamma': 0.91, 'ent_coef': 1.403461116783725e-06, 'gae_lambda': 0.8479465407454547, 'vf_coef': 1.0, 'max_grad_norm': 1.0}. Best is trial 33 with value: 100.89581374999999.\n",
      "[I 2025-02-07 00:02:11,807] Trial 58 finished with value: 79.1480291 and parameters: {'learning_rate': 0.0019240417514307228, 'n_steps': 35, 'gamma': 0.9400000000000001, 'ent_coef': 6.429506848611263e-05, 'gae_lambda': 0.8777027313684269, 'vf_coef': 0.2, 'max_grad_norm': 2.0}. Best is trial 33 with value: 100.89581374999999.\n",
      "[I 2025-02-07 00:03:04,976] Trial 59 finished with value: 52.13848660000001 and parameters: {'learning_rate': 0.00046418679665307224, 'n_steps': 65, 'gamma': 0.9500000000000001, 'ent_coef': 0.0002594061655246066, 'gae_lambda': 0.9309882158660533, 'vf_coef': 0.9, 'max_grad_norm': 2.2}. Best is trial 33 with value: 100.89581374999999.\n",
      "[I 2025-02-07 00:03:56,449] Trial 60 finished with value: 67.37834910000001 and parameters: {'learning_rate': 0.0011605301662700061, 'n_steps': 50, 'gamma': 0.93, 'ent_coef': 1.0296738899467715e-05, 'gae_lambda': 0.9175420629208365, 'vf_coef': 0.8, 'max_grad_norm': 3.0}. Best is trial 33 with value: 100.89581374999999.\n",
      "[I 2025-02-07 00:04:52,452] Trial 61 finished with value: 90.10599079999999 and parameters: {'learning_rate': 0.00912853392175198, 'n_steps': 30, 'gamma': 0.96, 'ent_coef': 0.0022454305537258534, 'gae_lambda': 0.9525533804159568, 'vf_coef': 0.5, 'max_grad_norm': 3.9}. Best is trial 33 with value: 100.89581374999999.\n",
      "[I 2025-02-07 00:05:45,789] Trial 62 finished with value: 69.6737572 and parameters: {'learning_rate': 0.007708248293172173, 'n_steps': 25, 'gamma': 0.96, 'ent_coef': 0.012670391982721237, 'gae_lambda': 0.966365656851679, 'vf_coef': 0.4, 'max_grad_norm': 5.0}. Best is trial 33 with value: 100.89581374999999.\n",
      "[I 2025-02-07 00:06:43,862] Trial 63 finished with value: 69.39545089999999 and parameters: {'learning_rate': 0.0055272414371753805, 'n_steps': 30, 'gamma': 0.97, 'ent_coef': 0.0016139737794325386, 'gae_lambda': 0.8904835143375807, 'vf_coef': 0.6, 'max_grad_norm': 3.4}. Best is trial 33 with value: 100.89581374999999.\n",
      "[I 2025-02-07 00:07:38,134] Trial 64 finished with value: 40.91557145 and parameters: {'learning_rate': 0.0031760055776031554, 'n_steps': 40, 'gamma': 0.96, 'ent_coef': 1.0623130333080675e-08, 'gae_lambda': 0.9790471586774339, 'vf_coef': 0.5, 'max_grad_norm': 4.2}. Best is trial 33 with value: 100.89581374999999.\n",
      "[I 2025-02-07 00:08:40,792] Trial 65 finished with value: 84.29386685 and parameters: {'learning_rate': 0.003909738275657909, 'n_steps': 15, 'gamma': 0.96, 'ent_coef': 0.005492647023598325, 'gae_lambda': 0.9562728538342895, 'vf_coef': 0.30000000000000004, 'max_grad_norm': 3.8}. Best is trial 33 with value: 100.89581374999999.\n",
      "[I 2025-02-07 00:09:36,369] Trial 66 finished with value: 60.8911476 and parameters: {'learning_rate': 0.002542522490674648, 'n_steps': 45, 'gamma': 0.9500000000000001, 'ent_coef': 0.0007553829647647433, 'gae_lambda': 0.8535983946807351, 'vf_coef': 0.6, 'max_grad_norm': 4.8}. Best is trial 33 with value: 100.89581374999999.\n",
      "[I 2025-02-07 00:10:35,505] Trial 67 finished with value: 53.24872535000001 and parameters: {'learning_rate': 0.00016600390282562854, 'n_steps': 20, 'gamma': 0.97, 'ent_coef': 0.02034098853419672, 'gae_lambda': 0.8347839721799408, 'vf_coef': 0.5, 'max_grad_norm': 1.9000000000000001}. Best is trial 33 with value: 100.89581374999999.\n",
      "[I 2025-02-07 00:11:28,629] Trial 68 finished with value: 56.532754350000005 and parameters: {'learning_rate': 0.005020974129939846, 'n_steps': 55, 'gamma': 0.99, 'ent_coef': 0.0032157052578451837, 'gae_lambda': 0.8627899198729175, 'vf_coef': 1.0, 'max_grad_norm': 2.4}. Best is trial 33 with value: 100.89581374999999.\n",
      "[I 2025-02-07 00:12:23,529] Trial 69 finished with value: 45.6658157 and parameters: {'learning_rate': 0.001660939743296843, 'n_steps': 30, 'gamma': 0.9, 'ent_coef': 0.0704577788937984, 'gae_lambda': 0.9617110567523119, 'vf_coef': 0.4, 'max_grad_norm': 2.6}. Best is trial 33 with value: 100.89581374999999.\n",
      "[I 2025-02-07 00:13:14,685] Trial 70 finished with value: 40.952464750000004 and parameters: {'learning_rate': 0.0006660089466666867, 'n_steps': 70, 'gamma': 0.9400000000000001, 'ent_coef': 1.6277891286785563e-05, 'gae_lambda': 0.945266484435329, 'vf_coef': 0.30000000000000004, 'max_grad_norm': 1.4000000000000001}. Best is trial 33 with value: 100.89581374999999.\n",
      "[I 2025-02-07 00:14:09,676] Trial 71 finished with value: 68.4055685 and parameters: {'learning_rate': 0.008800249523198328, 'n_steps': 35, 'gamma': 0.96, 'ent_coef': 3.126489738168109e-05, 'gae_lambda': 0.9374090184672935, 'vf_coef': 0.5, 'max_grad_norm': 3.2}. Best is trial 33 with value: 100.89581374999999.\n",
      "[I 2025-02-07 00:15:03,496] Trial 72 finished with value: 78.92263315 and parameters: {'learning_rate': 0.006621398634599073, 'n_steps': 40, 'gamma': 0.9500000000000001, 'ent_coef': 0.00019277189409342958, 'gae_lambda': 0.9591382934011944, 'vf_coef': 0.5, 'max_grad_norm': 3.9}. Best is trial 33 with value: 100.89581374999999.\n",
      "[I 2025-02-07 00:16:01,166] Trial 73 finished with value: 68.10703180000002 and parameters: {'learning_rate': 0.0029398867669245764, 'n_steps': 25, 'gamma': 0.97, 'ent_coef': 0.0004004686113785126, 'gae_lambda': 0.9488158412872154, 'vf_coef': 0.6, 'max_grad_norm': 4.2}. Best is trial 33 with value: 100.89581374999999.\n",
      "[I 2025-02-07 00:16:56,956] Trial 74 finished with value: 61.464370599999995 and parameters: {'learning_rate': 0.008198215832217432, 'n_steps': 30, 'gamma': 0.96, 'ent_coef': 0.0011902726205370473, 'gae_lambda': 0.9044418586014068, 'vf_coef': 0.4, 'max_grad_norm': 3.4}. Best is trial 33 with value: 100.89581374999999.\n",
      "[I 2025-02-07 00:17:51,425] Trial 75 finished with value: 46.95868684999999 and parameters: {'learning_rate': 0.009577443136459236, 'n_steps': 50, 'gamma': 0.9500000000000001, 'ent_coef': 3.7511895794893684e-06, 'gae_lambda': 0.9732426739322773, 'vf_coef': 1.0, 'max_grad_norm': 2.8}. Best is trial 33 with value: 100.89581374999999.\n",
      "[I 2025-02-07 00:18:50,705] Trial 76 finished with value: 72.8705005 and parameters: {'learning_rate': 0.004433482065670376, 'n_steps': 20, 'gamma': 0.9400000000000001, 'ent_coef': 8.000019868351375e-05, 'gae_lambda': 0.8747185202473695, 'vf_coef': 0.9, 'max_grad_norm': 3.6}. Best is trial 33 with value: 100.89581374999999.\n",
      "[I 2025-02-07 00:19:43,167] Trial 77 finished with value: 64.30452899999999 and parameters: {'learning_rate': 0.002263057031495133, 'n_steps': 60, 'gamma': 0.91, 'ent_coef': 2.2522155421005722e-05, 'gae_lambda': 0.9224974222438954, 'vf_coef': 0.8, 'max_grad_norm': 2.1}. Best is trial 33 with value: 100.89581374999999.\n",
      "[I 2025-02-07 00:21:09,664] Trial 78 finished with value: 83.98007450000001 and parameters: {'learning_rate': 0.0005209223402874122, 'n_steps': 5, 'gamma': 0.96, 'ent_coef': 4.653981619193231e-05, 'gae_lambda': 0.8985300663744474, 'vf_coef': 0.5, 'max_grad_norm': 2.3}. Best is trial 33 with value: 100.89581374999999.\n",
      "[I 2025-02-07 00:22:04,045] Trial 79 finished with value: 48.55955285 and parameters: {'learning_rate': 1.0855874235934428e-05, 'n_steps': 45, 'gamma': 0.9, 'ent_coef': 9.492129228971011e-06, 'gae_lambda': 0.9126159847866074, 'vf_coef': 0.4, 'max_grad_norm': 1.7000000000000002}. Best is trial 33 with value: 100.89581374999999.\n",
      "[I 2025-02-07 00:22:55,066] Trial 80 finished with value: 56.413758099999995 and parameters: {'learning_rate': 0.006295187791684673, 'n_steps': 80, 'gamma': 0.9500000000000001, 'ent_coef': 0.00010796805479008585, 'gae_lambda': 0.8224766994446101, 'vf_coef': 0.9, 'max_grad_norm': 2.5}. Best is trial 33 with value: 100.89581374999999.\n",
      "[I 2025-02-07 00:23:48,161] Trial 81 finished with value: 55.29846359999999 and parameters: {'learning_rate': 0.00476918945746217, 'n_steps': 65, 'gamma': 0.91, 'ent_coef': 7.267143357591194e-06, 'gae_lambda': 0.8494490166778987, 'vf_coef': 0.6, 'max_grad_norm': 0.4}. Best is trial 33 with value: 100.89581374999999.\n",
      "[I 2025-02-07 00:24:40,751] Trial 82 finished with value: 79.6818039 and parameters: {'learning_rate': 0.003991633225314078, 'n_steps': 70, 'gamma': 0.92, 'ent_coef': 6.669885205489677e-06, 'gae_lambda': 0.8408921213129656, 'vf_coef': 0.7000000000000001, 'max_grad_norm': 1.5000000000000002}. Best is trial 33 with value: 100.89581374999999.\n",
      "[I 2025-02-07 00:25:33,150] Trial 83 finished with value: 79.71324789999998 and parameters: {'learning_rate': 0.003221598848776163, 'n_steps': 65, 'gamma': 0.91, 'ent_coef': 2.8784788494143608e-06, 'gae_lambda': 0.86880609857661, 'vf_coef': 0.6, 'max_grad_norm': 0.8}. Best is trial 33 with value: 100.89581374999999.\n",
      "[I 2025-02-07 00:26:24,451] Trial 84 finished with value: 76.69301199999998 and parameters: {'learning_rate': 0.0054282246545448954, 'n_steps': 75, 'gamma': 0.92, 'ent_coef': 3.09126737450596e-05, 'gae_lambda': 0.8229080368910578, 'vf_coef': 0.7000000000000001, 'max_grad_norm': 1.0}. Best is trial 33 with value: 100.89581374999999.\n",
      "[I 2025-02-07 00:27:16,038] Trial 85 finished with value: 59.15551700000001 and parameters: {'learning_rate': 0.003565040111498967, 'n_steps': 60, 'gamma': 0.9, 'ent_coef': 1.5409782289948865e-05, 'gae_lambda': 0.8600465280844246, 'vf_coef': 1.0, 'max_grad_norm': 1.2}. Best is trial 33 with value: 100.89581374999999.\n",
      "[I 2025-02-07 00:28:04,515] Trial 86 finished with value: 88.65330085000001 and parameters: {'learning_rate': 0.009676758915140748, 'n_steps': 55, 'gamma': 0.91, 'ent_coef': 6.760323041445863e-05, 'gae_lambda': 0.8819251284787932, 'vf_coef': 0.8, 'max_grad_norm': 0.7}. Best is trial 33 with value: 100.89581374999999.\n",
      "[I 2025-02-07 00:28:53,625] Trial 87 finished with value: 53.5493874 and parameters: {'learning_rate': 0.007463625585617603, 'n_steps': 55, 'gamma': 0.9400000000000001, 'ent_coef': 0.00015962249281730812, 'gae_lambda': 0.8807239776632511, 'vf_coef': 0.8, 'max_grad_norm': 3.3}. Best is trial 33 with value: 100.89581374999999.\n",
      "[I 2025-02-07 00:29:45,246] Trial 88 finished with value: 69.4195354 and parameters: {'learning_rate': 0.0010024292599489278, 'n_steps': 40, 'gamma': 0.93, 'ent_coef': 6.55090555711217e-05, 'gae_lambda': 0.8860789457543798, 'vf_coef': 0.8, 'max_grad_norm': 0.8}. Best is trial 33 with value: 100.89581374999999.\n",
      "[I 2025-02-07 00:30:34,962] Trial 89 finished with value: 93.58639675 and parameters: {'learning_rate': 0.001262559480346231, 'n_steps': 50, 'gamma': 0.9, 'ent_coef': 0.0002950881117207619, 'gae_lambda': 0.9517407417427227, 'vf_coef': 0.9, 'max_grad_norm': 2.8}. Best is trial 33 with value: 100.89581374999999.\n",
      "[I 2025-02-07 00:31:24,568] Trial 90 finished with value: 60.64897175000001 and parameters: {'learning_rate': 0.0013249773314297053, 'n_steps': 50, 'gamma': 0.9, 'ent_coef': 0.0005902553036621419, 'gae_lambda': 0.8950004786161075, 'vf_coef': 0.9, 'max_grad_norm': 0.6000000000000001}. Best is trial 33 with value: 100.89581374999999.\n",
      "[I 2025-02-07 00:32:15,023] Trial 91 finished with value: 68.2596257 and parameters: {'learning_rate': 0.0017489600986854404, 'n_steps': 45, 'gamma': 0.9, 'ent_coef': 0.00024887270594620094, 'gae_lambda': 0.9521543038911947, 'vf_coef': 0.9, 'max_grad_norm': 2.8}. Best is trial 33 with value: 100.89581374999999.\n",
      "[I 2025-02-07 00:33:04,754] Trial 92 finished with value: 85.35644505 and parameters: {'learning_rate': 0.0020480584038043786, 'n_steps': 35, 'gamma': 0.91, 'ent_coef': 0.00033606024285853944, 'gae_lambda': 0.9341183721992742, 'vf_coef': 0.9, 'max_grad_norm': 2.9}. Best is trial 33 with value: 100.89581374999999.\n",
      "[I 2025-02-07 00:33:53,101] Trial 93 finished with value: 72.2794621 and parameters: {'learning_rate': 0.009892480947822285, 'n_steps': 50, 'gamma': 0.9, 'ent_coef': 4.745924359116666e-05, 'gae_lambda': 0.9424229246138782, 'vf_coef': 1.0, 'max_grad_norm': 2.6}. Best is trial 33 with value: 100.89581374999999.\n",
      "[I 2025-02-07 00:34:40,045] Trial 94 finished with value: 67.45023195 and parameters: {'learning_rate': 0.0026117257367913116, 'n_steps': 130, 'gamma': 0.91, 'ent_coef': 0.0008102827349112153, 'gae_lambda': 0.892387203092325, 'vf_coef': 0.8, 'max_grad_norm': 3.1}. Best is trial 33 with value: 100.89581374999999.\n",
      "[I 2025-02-07 00:35:29,855] Trial 95 finished with value: 86.51364095 and parameters: {'learning_rate': 0.0007664366766600682, 'n_steps': 55, 'gamma': 0.93, 'ent_coef': 0.00012839264080575282, 'gae_lambda': 0.953608334609769, 'vf_coef': 0.9, 'max_grad_norm': 2.4}. Best is trial 33 with value: 100.89581374999999.\n",
      "[I 2025-02-07 00:36:18,547] Trial 96 finished with value: 39.70255949999999 and parameters: {'learning_rate': 0.0003413668101637178, 'n_steps': 55, 'gamma': 0.93, 'ent_coef': 0.00013214105745026948, 'gae_lambda': 0.969160214453316, 'vf_coef': 0.9, 'max_grad_norm': 2.3}. Best is trial 33 with value: 100.89581374999999.\n",
      "[I 2025-02-07 00:37:07,016] Trial 97 finished with value: 46.89061995 and parameters: {'learning_rate': 0.0009045314153783109, 'n_steps': 60, 'gamma': 0.93, 'ent_coef': 8.204286432089393e-05, 'gae_lambda': 0.9652128273289401, 'vf_coef': 1.0, 'max_grad_norm': 2.0}. Best is trial 33 with value: 100.89581374999999.\n",
      "[I 2025-02-07 00:37:56,186] Trial 98 finished with value: 87.78255200000001 and parameters: {'learning_rate': 0.0007251474071899611, 'n_steps': 50, 'gamma': 0.92, 'ent_coef': 1.8350331481778995e-05, 'gae_lambda': 0.8758034652811763, 'vf_coef': 0.9, 'max_grad_norm': 2.5}. Best is trial 33 with value: 100.89581374999999.\n",
      "[I 2025-02-07 00:38:45,979] Trial 99 finished with value: 88.1692339 and parameters: {'learning_rate': 0.0012848757252249127, 'n_steps': 45, 'gamma': 0.92, 'ent_coef': 0.0029206771367773494, 'gae_lambda': 0.8767103785998811, 'vf_coef': 0.8, 'max_grad_norm': 2.7}. Best is trial 33 with value: 100.89581374999999.\n",
      "[I 2025-02-07 00:39:35,613] Trial 100 finished with value: 78.33397735 and parameters: {'learning_rate': 0.0011733688361024528, 'n_steps': 45, 'gamma': 0.92, 'ent_coef': 0.0032417394040493578, 'gae_lambda': 0.8771417067319052, 'vf_coef': 0.7000000000000001, 'max_grad_norm': 2.7}. Best is trial 33 with value: 100.89581374999999.\n",
      "[I 2025-02-07 00:40:25,900] Trial 101 finished with value: 79.55139885 and parameters: {'learning_rate': 0.0014173425152852598, 'n_steps': 40, 'gamma': 0.92, 'ent_coef': 2.6422072921148326e-05, 'gae_lambda': 0.8816912043743477, 'vf_coef': 0.8, 'max_grad_norm': 1.8}. Best is trial 33 with value: 100.89581374999999.\n",
      "[I 2025-02-07 00:41:14,817] Trial 102 finished with value: 78.27301479999998 and parameters: {'learning_rate': 0.000968333028872424, 'n_steps': 50, 'gamma': 0.91, 'ent_coef': 0.00939326572800945, 'gae_lambda': 0.8717147501644459, 'vf_coef': 0.8, 'max_grad_norm': 2.5}. Best is trial 33 with value: 100.89581374999999.\n",
      "[I 2025-02-07 00:42:04,361] Trial 103 finished with value: 67.35416610000001 and parameters: {'learning_rate': 0.0012591387695961296, 'n_steps': 45, 'gamma': 0.92, 'ent_coef': 0.0020688649660505105, 'gae_lambda': 0.8883629097162359, 'vf_coef': 0.8, 'max_grad_norm': 2.2}. Best is trial 33 with value: 100.89581374999999.\n",
      "[I 2025-02-07 00:42:52,832] Trial 104 finished with value: 84.37166275 and parameters: {'learning_rate': 0.0015847945348892419, 'n_steps': 60, 'gamma': 0.98, 'ent_coef': 1.7886423317303033e-05, 'gae_lambda': 0.8668482651929685, 'vf_coef': 0.9, 'max_grad_norm': 2.7}. Best is trial 33 with value: 100.89581374999999.\n",
      "[I 2025-02-07 00:43:39,766] Trial 105 finished with value: 82.60694275 and parameters: {'learning_rate': 0.002242736902096674, 'n_steps': 120, 'gamma': 0.92, 'ent_coef': 1.4097128585234028e-05, 'gae_lambda': 0.8757506581336573, 'vf_coef': 1.0, 'max_grad_norm': 3.0}. Best is trial 33 with value: 100.89581374999999.\n",
      "[I 2025-02-07 00:44:28,712] Trial 106 finished with value: 76.36499484999999 and parameters: {'learning_rate': 0.0018992941152376904, 'n_steps': 50, 'gamma': 0.9, 'ent_coef': 0.001098947251546372, 'gae_lambda': 0.9013321044239518, 'vf_coef': 0.8, 'max_grad_norm': 2.1}. Best is trial 33 with value: 100.89581374999999.\n",
      "[I 2025-02-07 00:45:18,202] Trial 107 finished with value: 30.625884500000005 and parameters: {'learning_rate': 0.00047407277827317797, 'n_steps': 40, 'gamma': 0.91, 'ent_coef': 3.397323319807353e-05, 'gae_lambda': 0.9067894818583655, 'vf_coef': 0.9, 'max_grad_norm': 3.0}. Best is trial 33 with value: 100.89581374999999.\n",
      "[I 2025-02-07 00:46:07,291] Trial 108 finished with value: 88.28755110000002 and parameters: {'learning_rate': 0.0006309603956233971, 'n_steps': 55, 'gamma': 0.91, 'ent_coef': 8.503663567422272e-06, 'gae_lambda': 0.8010884571792274, 'vf_coef': 0.9, 'max_grad_norm': 2.6}. Best is trial 33 with value: 100.89581374999999.\n",
      "[I 2025-02-07 00:46:56,477] Trial 109 finished with value: 86.2756514 and parameters: {'learning_rate': 0.0006347511555707647, 'n_steps': 55, 'gamma': 0.91, 'ent_coef': 9.486176829485853e-06, 'gae_lambda': 0.8638088853471477, 'vf_coef': 1.0, 'max_grad_norm': 2.5}. Best is trial 33 with value: 100.89581374999999.\n",
      "[I 2025-02-07 00:47:46,715] Trial 110 finished with value: 57.782316949999995 and parameters: {'learning_rate': 0.0007648742874463901, 'n_steps': 35, 'gamma': 0.91, 'ent_coef': 4.156114855449821e-06, 'gae_lambda': 0.8101829582621866, 'vf_coef': 0.8, 'max_grad_norm': 2.8}. Best is trial 33 with value: 100.89581374999999.\n",
      "[I 2025-02-07 00:48:36,282] Trial 111 finished with value: 68.09891594999999 and parameters: {'learning_rate': 0.0005340590496442342, 'n_steps': 50, 'gamma': 0.92, 'ent_coef': 2.1991962096187864e-05, 'gae_lambda': 0.8836666591386543, 'vf_coef': 0.9, 'max_grad_norm': 2.6}. Best is trial 33 with value: 100.89581374999999.\n",
      "[I 2025-02-07 00:49:24,972] Trial 112 finished with value: 74.22779055000001 and parameters: {'learning_rate': 0.001079041526461772, 'n_steps': 65, 'gamma': 0.9, 'ent_coef': 1.2199268938127982e-05, 'gae_lambda': 0.8699932048813012, 'vf_coef': 0.9, 'max_grad_norm': 2.3}. Best is trial 33 with value: 100.89581374999999.\n",
      "[I 2025-02-07 00:50:14,951] Trial 113 finished with value: 63.721793149999996 and parameters: {'learning_rate': 0.0008290122578997886, 'n_steps': 45, 'gamma': 0.9400000000000001, 'ent_coef': 5.167254463330791e-06, 'gae_lambda': 0.8043172232123491, 'vf_coef': 0.7000000000000001, 'max_grad_norm': 4.7}. Best is trial 33 with value: 100.89581374999999.\n",
      "[I 2025-02-07 00:51:04,584] Trial 114 finished with value: 57.48606755000001 and parameters: {'learning_rate': 0.0016833281549106399, 'n_steps': 40, 'gamma': 0.91, 'ent_coef': 0.0028282331874789285, 'gae_lambda': 0.8958481299031661, 'vf_coef': 0.9, 'max_grad_norm': 2.4}. Best is trial 33 with value: 100.89581374999999.\n",
      "[I 2025-02-07 00:51:53,166] Trial 115 finished with value: 44.811812100000004 and parameters: {'learning_rate': 0.0002882826907799485, 'n_steps': 60, 'gamma': 0.92, 'ent_coef': 8.28113161928743e-06, 'gae_lambda': 0.8563715190977813, 'vf_coef': 1.0, 'max_grad_norm': 1.8}. Best is trial 33 with value: 100.89581374999999.\n",
      "[I 2025-02-07 00:52:41,426] Trial 116 finished with value: 76.8505772 and parameters: {'learning_rate': 0.0024245250360881766, 'n_steps': 70, 'gamma': 0.9, 'ent_coef': 5.5408128386235774e-05, 'gae_lambda': 0.8902180418686232, 'vf_coef': 0.9, 'max_grad_norm': 2.8}. Best is trial 33 with value: 100.89581374999999.\n",
      "[I 2025-02-07 00:53:29,999] Trial 117 finished with value: 63.321453500000004 and parameters: {'learning_rate': 0.0004392709358079891, 'n_steps': 55, 'gamma': 0.91, 'ent_coef': 1.696428537601823e-05, 'gae_lambda': 0.8799544925656776, 'vf_coef': 1.0, 'max_grad_norm': 2.5}. Best is trial 33 with value: 100.89581374999999.\n",
      "[I 2025-02-07 00:54:18,865] Trial 118 finished with value: 50.28088245 and parameters: {'learning_rate': 0.0031035794496805896, 'n_steps': 50, 'gamma': 0.9400000000000001, 'ent_coef': 0.018803690732777517, 'gae_lambda': 0.8157612083992419, 'vf_coef': 0.8, 'max_grad_norm': 2.1}. Best is trial 33 with value: 100.89581374999999.\n",
      "[I 2025-02-07 00:55:07,265] Trial 119 finished with value: 75.70344999999999 and parameters: {'learning_rate': 0.0014635983395329933, 'n_steps': 65, 'gamma': 0.92, 'ent_coef': 2.0525028843362155e-06, 'gae_lambda': 0.838197503524883, 'vf_coef': 0.8, 'max_grad_norm': 2.9}. Best is trial 33 with value: 100.89581374999999.\n",
      "[I 2025-02-07 00:55:56,692] Trial 120 finished with value: 75.9160406 and parameters: {'learning_rate': 0.002807151519126102, 'n_steps': 45, 'gamma': 0.93, 'ent_coef': 3.8503084142691526e-05, 'gae_lambda': 0.8876788462605156, 'vf_coef': 0.9, 'max_grad_norm': 2.7}. Best is trial 33 with value: 100.89581374999999.\n",
      "[I 2025-02-07 00:56:45,960] Trial 121 finished with value: 87.61254324999999 and parameters: {'learning_rate': 0.0008764530947963403, 'n_steps': 55, 'gamma': 0.93, 'ent_coef': 0.004974921749827237, 'gae_lambda': 0.8296102821977978, 'vf_coef': 0.9, 'max_grad_norm': 2.4}. Best is trial 33 with value: 100.89581374999999.\n",
      "[I 2025-02-07 00:57:35,360] Trial 122 finished with value: 59.82562080000001 and parameters: {'learning_rate': 0.000605768837731083, 'n_steps': 55, 'gamma': 0.93, 'ent_coef': 0.0023745194436592914, 'gae_lambda': 0.8286600925091188, 'vf_coef': 0.9, 'max_grad_norm': 1.6}. Best is trial 33 with value: 100.89581374999999.\n",
      "[I 2025-02-07 00:58:24,831] Trial 123 finished with value: 71.98572709999999 and parameters: {'learning_rate': 0.0006909818967932173, 'n_steps': 50, 'gamma': 0.93, 'ent_coef': 0.003668100363445716, 'gae_lambda': 0.8000833839408681, 'vf_coef': 0.9, 'max_grad_norm': 2.3}. Best is trial 33 with value: 100.89581374999999.\n",
      "[I 2025-02-07 00:59:14,171] Trial 124 finished with value: 47.85581955 and parameters: {'learning_rate': 0.000930366919609448, 'n_steps': 55, 'gamma': 0.9400000000000001, 'ent_coef': 0.008455242085537579, 'gae_lambda': 0.9124438621627372, 'vf_coef': 1.0, 'max_grad_norm': 1.9000000000000001}. Best is trial 33 with value: 100.89581374999999.\n",
      "[I 2025-02-07 01:00:03,549] Trial 125 finished with value: 79.19845525000001 and parameters: {'learning_rate': 0.001143391479477457, 'n_steps': 60, 'gamma': 0.92, 'ent_coef': 0.013812274313897257, 'gae_lambda': 0.8504738160633225, 'vf_coef': 0.8, 'max_grad_norm': 2.2}. Best is trial 33 with value: 100.89581374999999.\n",
      "[I 2025-02-07 01:00:53,740] Trial 126 finished with value: 56.01729389999999 and parameters: {'learning_rate': 0.0013240213469398526, 'n_steps': 40, 'gamma': 0.9500000000000001, 'ent_coef': 0.005271938510680084, 'gae_lambda': 0.885180393071811, 'vf_coef': 0.9, 'max_grad_norm': 2.6}. Best is trial 33 with value: 100.89581374999999.\n",
      "[I 2025-02-07 01:01:41,584] Trial 127 finished with value: 73.91861395000001 and parameters: {'learning_rate': 0.002073133076120689, 'n_steps': 85, 'gamma': 0.9, 'ent_coef': 0.0014251170459270376, 'gae_lambda': 0.8185026241482248, 'vf_coef': 0.9, 'max_grad_norm': 2.4}. Best is trial 33 with value: 100.89581374999999.\n",
      "[I 2025-02-07 01:02:31,000] Trial 128 finished with value: 80.7451969 and parameters: {'learning_rate': 0.0008347396968783243, 'n_steps': 45, 'gamma': 0.9400000000000001, 'ent_coef': 0.006027464703388805, 'gae_lambda': 0.8293073644965996, 'vf_coef': 0.8, 'max_grad_norm': 2.6}. Best is trial 33 with value: 100.89581374999999.\n",
      "[I 2025-02-07 01:03:19,423] Trial 129 finished with value: 42.990562749999995 and parameters: {'learning_rate': 0.0010400342833545133, 'n_steps': 60, 'gamma': 0.91, 'ent_coef': 2.57092270522506e-05, 'gae_lambda': 0.8719903305113227, 'vf_coef': 1.0, 'max_grad_norm': 3.2}. Best is trial 33 with value: 100.89581374999999.\n",
      "[I 2025-02-07 01:04:08,350] Trial 130 finished with value: 38.787267400000005 and parameters: {'learning_rate': 0.00022403936433849081, 'n_steps': 50, 'gamma': 0.92, 'ent_coef': 1.2226096458561868e-05, 'gae_lambda': 0.8750328815131253, 'vf_coef': 0.7000000000000001, 'max_grad_norm': 2.0}. Best is trial 33 with value: 100.89581374999999.\n",
      "[I 2025-02-07 01:04:57,054] Trial 131 finished with value: 46.20411985 and parameters: {'learning_rate': 0.0007251532892197536, 'n_steps': 55, 'gamma': 0.93, 'ent_coef': 0.0002010072157627836, 'gae_lambda': 0.9625468860284707, 'vf_coef': 0.9, 'max_grad_norm': 2.4}. Best is trial 33 with value: 100.89581374999999.\n",
      "[I 2025-02-07 01:05:45,873] Trial 132 finished with value: 62.60048219999999 and parameters: {'learning_rate': 0.0005535849993268514, 'n_steps': 50, 'gamma': 0.93, 'ent_coef': 7.935917930928561e-05, 'gae_lambda': 0.9577545269579425, 'vf_coef': 0.9, 'max_grad_norm': 2.4}. Best is trial 33 with value: 100.89581374999999.\n",
      "[I 2025-02-07 01:06:34,375] Trial 133 finished with value: 82.33114255000001 and parameters: {'learning_rate': 0.001564329492079286, 'n_steps': 65, 'gamma': 0.9400000000000001, 'ent_coef': 0.00010255203117267863, 'gae_lambda': 0.8087368509932271, 'vf_coef': 0.9, 'max_grad_norm': 2.5}. Best is trial 33 with value: 100.89581374999999.\n",
      "[I 2025-02-07 01:07:24,750] Trial 134 finished with value: 75.12365435000001 and parameters: {'learning_rate': 0.0008294485033463808, 'n_steps': 35, 'gamma': 0.93, 'ent_coef': 0.000339257698395889, 'gae_lambda': 0.950717375889388, 'vf_coef': 0.9, 'max_grad_norm': 2.8}. Best is trial 33 with value: 100.89581374999999.\n",
      "[I 2025-02-07 01:08:13,454] Trial 135 finished with value: 39.8813493 and parameters: {'learning_rate': 0.0036125073009089423, 'n_steps': 55, 'gamma': 0.92, 'ent_coef': 5.627744835735637e-05, 'gae_lambda': 0.8985889983265871, 'vf_coef': 1.0, 'max_grad_norm': 2.3}. Best is trial 33 with value: 100.89581374999999.\n",
      "[I 2025-02-07 01:09:05,233] Trial 136 finished with value: 58.27726779999999 and parameters: {'learning_rate': 0.0018528307960777673, 'n_steps': 25, 'gamma': 0.91, 'ent_coef': 0.0004999747004865349, 'gae_lambda': 0.9542249524286536, 'vf_coef': 0.8, 'max_grad_norm': 2.9}. Best is trial 33 with value: 100.89581374999999.\n",
      "[I 2025-02-07 01:09:55,817] Trial 137 finished with value: 74.85117965 and parameters: {'learning_rate': 0.0012544649401563517, 'n_steps': 45, 'gamma': 0.9500000000000001, 'ent_coef': 0.0001427276376955817, 'gae_lambda': 0.844239383304637, 'vf_coef': 0.9, 'max_grad_norm': 2.7}. Best is trial 33 with value: 100.89581374999999.\n",
      "[I 2025-02-07 01:10:45,062] Trial 138 finished with value: 66.54947170000001 and parameters: {'learning_rate': 0.0006639226830823179, 'n_steps': 70, 'gamma': 0.92, 'ent_coef': 4.143411305919228e-05, 'gae_lambda': 0.9451368256975236, 'vf_coef': 0.8, 'max_grad_norm': 2.1}. Best is trial 33 with value: 100.89581374999999.\n",
      "[I 2025-02-07 01:11:33,407] Trial 139 finished with value: 49.893209399999996 and parameters: {'learning_rate': 0.0004399036411932517, 'n_steps': 60, 'gamma': 0.9400000000000001, 'ent_coef': 2.0514781389808252e-05, 'gae_lambda': 0.9474849050573513, 'vf_coef': 1.0, 'max_grad_norm': 2.2}. Best is trial 33 with value: 100.89581374999999.\n",
      "[I 2025-02-07 01:12:22,926] Trial 140 finished with value: 46.0459608 and parameters: {'learning_rate': 0.00037507792714806245, 'n_steps': 55, 'gamma': 0.93, 'ent_coef': 7.518262489092679e-06, 'gae_lambda': 0.8607151494294325, 'vf_coef': 0.5, 'max_grad_norm': 4.4}. Best is trial 33 with value: 100.89581374999999.\n",
      "[I 2025-02-07 01:13:10,744] Trial 141 finished with value: 44.5870023 and parameters: {'learning_rate': 0.0006482608242233468, 'n_steps': 100, 'gamma': 0.91, 'ent_coef': 3.1441693243255707e-06, 'gae_lambda': 0.866175156294465, 'vf_coef': 1.0, 'max_grad_norm': 2.5}. Best is trial 33 with value: 100.89581374999999.\n",
      "[I 2025-02-07 01:13:59,436] Trial 142 finished with value: 70.64867480000001 and parameters: {'learning_rate': 0.0007473071026210927, 'n_steps': 55, 'gamma': 0.91, 'ent_coef': 9.840511860653046e-06, 'gae_lambda': 0.8634295786377538, 'vf_coef': 1.0, 'max_grad_norm': 2.5}. Best is trial 33 with value: 100.89581374999999.\n",
      "[I 2025-02-07 01:14:48,511] Trial 143 finished with value: 75.08331065 and parameters: {'learning_rate': 0.0006151706269179364, 'n_steps': 50, 'gamma': 0.9, 'ent_coef': 4.934029085675866e-06, 'gae_lambda': 0.8783216862313918, 'vf_coef': 1.0, 'max_grad_norm': 2.6}. Best is trial 33 with value: 100.89581374999999.\n",
      "[I 2025-02-07 01:15:37,373] Trial 144 finished with value: 80.0877152 and parameters: {'learning_rate': 0.000930350033713143, 'n_steps': 55, 'gamma': 0.91, 'ent_coef': 3.113668316048822e-05, 'gae_lambda': 0.8831174407047284, 'vf_coef': 1.0, 'max_grad_norm': 2.3}. Best is trial 33 with value: 100.89581374999999.\n",
      "[I 2025-02-07 01:16:26,815] Trial 145 finished with value: 59.9857114 and parameters: {'learning_rate': 0.0024731089884720446, 'n_steps': 45, 'gamma': 0.91, 'ent_coef': 1.4546252706370926e-05, 'gae_lambda': 0.8691420313219866, 'vf_coef': 0.9, 'max_grad_norm': 2.7}. Best is trial 33 with value: 100.89581374999999.\n",
      "[I 2025-02-07 01:17:15,781] Trial 146 finished with value: 74.15339940000001 and parameters: {'learning_rate': 0.0010051640675461466, 'n_steps': 60, 'gamma': 0.92, 'ent_coef': 6.306081464897821e-06, 'gae_lambda': 0.875457119609069, 'vf_coef': 0.9, 'max_grad_norm': 3.7}. Best is trial 33 with value: 100.89581374999999.\n",
      "[I 2025-02-07 01:18:04,444] Trial 147 finished with value: 63.922950900000004 and parameters: {'learning_rate': 0.008144652146049019, 'n_steps': 65, 'gamma': 0.9, 'ent_coef': 9.987452048907e-06, 'gae_lambda': 0.8932926858562478, 'vf_coef': 1.0, 'max_grad_norm': 3.0}. Best is trial 33 with value: 100.89581374999999.\n",
      "[I 2025-02-07 01:18:53,083] Trial 148 finished with value: 43.1581191 and parameters: {'learning_rate': 3.831074180920375e-05, 'n_steps': 55, 'gamma': 0.92, 'ent_coef': 2.453625510291284e-05, 'gae_lambda': 0.8545292557635907, 'vf_coef': 0.9, 'max_grad_norm': 1.8}. Best is trial 33 with value: 100.89581374999999.\n",
      "[I 2025-02-07 01:19:41,985] Trial 149 finished with value: 66.618328 and parameters: {'learning_rate': 0.002845190682969767, 'n_steps': 50, 'gamma': 0.91, 'ent_coef': 0.00022733166846412004, 'gae_lambda': 0.8373245478369109, 'vf_coef': 0.8, 'max_grad_norm': 2.0}. Best is trial 33 with value: 100.89581374999999.\n",
      "[I 2025-02-07 01:20:31,448] Trial 150 finished with value: 84.6991093 and parameters: {'learning_rate': 0.00589660952646134, 'n_steps': 40, 'gamma': 0.9400000000000001, 'ent_coef': 0.004536908752261039, 'gae_lambda': 0.8639556136284211, 'vf_coef': 1.0, 'max_grad_norm': 2.5}. Best is trial 33 with value: 100.89581374999999.\n",
      "[I 2025-02-07 01:21:22,088] Trial 151 finished with value: 80.46312115 and parameters: {'learning_rate': 0.009653206010971756, 'n_steps': 30, 'gamma': 0.97, 'ent_coef': 0.000662207639115061, 'gae_lambda': 0.9548481339535054, 'vf_coef': 0.5, 'max_grad_norm': 4.0}. Best is trial 33 with value: 100.89581374999999.\n",
      "[I 2025-02-07 01:22:12,057] Trial 152 finished with value: 81.79820665 and parameters: {'learning_rate': 0.008245367623845534, 'n_steps': 35, 'gamma': 0.96, 'ent_coef': 0.0002699139267857446, 'gae_lambda': 0.9042833904715577, 'vf_coef': 0.5, 'max_grad_norm': 3.4}. Best is trial 33 with value: 100.89581374999999.\n",
      "[I 2025-02-07 01:23:02,774] Trial 153 finished with value: 73.5206977 and parameters: {'learning_rate': 0.007146900460087563, 'n_steps': 30, 'gamma': 0.9500000000000001, 'ent_coef': 0.00017021303931317785, 'gae_lambda': 0.9596699068616279, 'vf_coef': 0.5, 'max_grad_norm': 3.5}. Best is trial 33 with value: 100.89581374999999.\n",
      "[I 2025-02-07 01:23:56,361] Trial 154 finished with value: 78.84196969999999 and parameters: {'learning_rate': 0.009988688163695953, 'n_steps': 20, 'gamma': 0.96, 'ent_coef': 0.000403925862880894, 'gae_lambda': 0.946642733494932, 'vf_coef': 0.9, 'max_grad_norm': 2.8}. Best is trial 33 with value: 100.89581374999999.\n",
      "[I 2025-02-07 01:24:46,510] Trial 155 finished with value: 87.02768645 and parameters: {'learning_rate': 0.0005040667263659407, 'n_steps': 45, 'gamma': 0.96, 'ent_coef': 0.00010142561881809893, 'gae_lambda': 0.8251247246642658, 'vf_coef': 0.4, 'max_grad_norm': 2.4}. Best is trial 33 with value: 100.89581374999999.\n",
      "[I 2025-02-07 01:25:35,337] Trial 156 finished with value: 85.59364115000001 and parameters: {'learning_rate': 0.0005011167265999446, 'n_steps': 50, 'gamma': 0.96, 'ent_coef': 6.124379342942581e-05, 'gae_lambda': 0.8192509360459934, 'vf_coef': 0.4, 'max_grad_norm': 2.4}. Best is trial 33 with value: 100.89581374999999.\n",
      "[I 2025-02-07 01:26:24,726] Trial 157 finished with value: 72.73992754999999 and parameters: {'learning_rate': 0.0007442559081980328, 'n_steps': 45, 'gamma': 0.9, 'ent_coef': 0.00011845369577418887, 'gae_lambda': 0.8723807427456893, 'vf_coef': 0.4, 'max_grad_norm': 2.6}. Best is trial 33 with value: 100.89581374999999.\n",
      "[I 2025-02-07 01:27:13,642] Trial 158 finished with value: 49.08057695 and parameters: {'learning_rate': 0.0004324333493576473, 'n_steps': 50, 'gamma': 0.91, 'ent_coef': 3.775017355641771e-05, 'gae_lambda': 0.9397939158671291, 'vf_coef': 0.30000000000000004, 'max_grad_norm': 0.3}. Best is trial 33 with value: 100.89581374999999.\n",
      "[I 2025-02-07 01:28:02,148] Trial 159 finished with value: 51.38398535 and parameters: {'learning_rate': 0.0005820908691108262, 'n_steps': 75, 'gamma': 0.9500000000000001, 'ent_coef': 1.77804535868414e-05, 'gae_lambda': 0.8811867079952664, 'vf_coef': 0.9, 'max_grad_norm': 2.2}. Best is trial 33 with value: 100.89581374999999.\n",
      "[I 2025-02-07 01:28:52,189] Trial 160 finished with value: 35.68529065 and parameters: {'learning_rate': 0.000867454463778899, 'n_steps': 45, 'gamma': 0.92, 'ent_coef': 0.03821290627041895, 'gae_lambda': 0.8261690741212568, 'vf_coef': 0.2, 'max_grad_norm': 2.7}. Best is trial 33 with value: 100.89581374999999.\n",
      "[I 2025-02-07 01:29:43,166] Trial 161 finished with value: 44.68920895 and parameters: {'learning_rate': 0.0005066831493494102, 'n_steps': 55, 'gamma': 0.96, 'ent_coef': 7.171324691849164e-05, 'gae_lambda': 0.8223948634566909, 'vf_coef': 0.30000000000000004, 'max_grad_norm': 2.4}. Best is trial 33 with value: 100.89581374999999.\n",
      "[I 2025-02-07 01:30:33,548] Trial 162 finished with value: 55.641781050000006 and parameters: {'learning_rate': 0.0005448802886802018, 'n_steps': 50, 'gamma': 0.96, 'ent_coef': 5.9241194120396573e-05, 'gae_lambda': 0.8193218493411807, 'vf_coef': 0.4, 'max_grad_norm': 2.6}. Best is trial 33 with value: 100.89581374999999.\n",
      "[I 2025-02-07 01:31:24,018] Trial 163 finished with value: 77.22560039999999 and parameters: {'learning_rate': 0.0003952652572160417, 'n_steps': 40, 'gamma': 0.96, 'ent_coef': 1.3007847641813304e-07, 'gae_lambda': 0.8329101070150388, 'vf_coef': 0.4, 'max_grad_norm': 2.4}. Best is trial 33 with value: 100.89581374999999.\n",
      "[I 2025-02-07 01:32:12,695] Trial 164 finished with value: 68.8045297 and parameters: {'learning_rate': 0.0006350942966070986, 'n_steps': 60, 'gamma': 0.96, 'ent_coef': 0.00010397559926848349, 'gae_lambda': 0.8080565357004824, 'vf_coef': 0.4, 'max_grad_norm': 2.5}. Best is trial 33 with value: 100.89581374999999.\n",
      "[I 2025-02-07 01:33:02,257] Trial 165 finished with value: 71.5692531 and parameters: {'learning_rate': 0.0007756157807616508, 'n_steps': 50, 'gamma': 0.97, 'ent_coef': 4.4347924759009095e-05, 'gae_lambda': 0.8177919710144489, 'vf_coef': 0.4, 'max_grad_norm': 2.3}. Best is trial 33 with value: 100.89581374999999.\n",
      "[I 2025-02-07 01:33:52,170] Trial 166 finished with value: 79.24461955 and parameters: {'learning_rate': 0.001083740992689759, 'n_steps': 45, 'gamma': 0.9400000000000001, 'ent_coef': 2.720174451970802e-05, 'gae_lambda': 0.8122448594744859, 'vf_coef': 0.30000000000000004, 'max_grad_norm': 1.6}. Best is trial 33 with value: 100.89581374999999.\n",
      "[I 2025-02-07 01:34:41,281] Trial 167 finished with value: 62.50757000000001 and parameters: {'learning_rate': 0.00030453156123425577, 'n_steps': 55, 'gamma': 0.91, 'ent_coef': 1.3065016642466349e-05, 'gae_lambda': 0.8271049372958207, 'vf_coef': 0.8, 'max_grad_norm': 1.3}. Best is trial 33 with value: 100.89581374999999.\n",
      "[I 2025-02-07 01:35:30,447] Trial 168 finished with value: 53.59393255 and parameters: {'learning_rate': 0.0005130838804051626, 'n_steps': 45, 'gamma': 0.96, 'ent_coef': 8.184587601137715e-06, 'gae_lambda': 0.8138855155982442, 'vf_coef': 0.9, 'max_grad_norm': 2.1}. Best is trial 33 with value: 100.89581374999999.\n",
      "[I 2025-02-07 01:36:19,380] Trial 169 finished with value: 103.09610405000001 and parameters: {'learning_rate': 0.0006867545672003906, 'n_steps': 50, 'gamma': 0.93, 'ent_coef': 7.90957139789345e-05, 'gae_lambda': 0.8511425851533388, 'vf_coef': 0.8, 'max_grad_norm': 2.4}. Best is trial 169 with value: 103.09610405000001.\n",
      "[I 2025-02-07 01:37:07,978] Trial 170 finished with value: 86.96192225 and parameters: {'learning_rate': 0.0021166969561829817, 'n_steps': 60, 'gamma': 0.93, 'ent_coef': 7.968601445036168e-05, 'gae_lambda': 0.847095509488926, 'vf_coef': 0.8, 'max_grad_norm': 2.8}. Best is trial 169 with value: 103.09610405000001.\n",
      "[I 2025-02-07 01:37:56,201] Trial 171 finished with value: 68.68048110000001 and parameters: {'learning_rate': 0.0022098030330015943, 'n_steps': 60, 'gamma': 0.93, 'ent_coef': 8.114968233512736e-05, 'gae_lambda': 0.8519832136492662, 'vf_coef': 0.8, 'max_grad_norm': 2.8}. Best is trial 169 with value: 103.09610405000001.\n",
      "[I 2025-02-07 01:38:44,608] Trial 172 finished with value: 62.74719315 and parameters: {'learning_rate': 0.0019378373832174573, 'n_steps': 65, 'gamma': 0.93, 'ent_coef': 0.00015275896781064268, 'gae_lambda': 0.8465178894438489, 'vf_coef': 0.7000000000000001, 'max_grad_norm': 2.6}. Best is trial 169 with value: 103.09610405000001.\n",
      "[I 2025-02-07 01:39:34,334] Trial 173 finished with value: 73.75811175 and parameters: {'learning_rate': 0.0014684161628693037, 'n_steps': 55, 'gamma': 0.93, 'ent_coef': 1.9236349991229093e-05, 'gae_lambda': 0.8597887137338525, 'vf_coef': 0.8, 'max_grad_norm': 2.5}. Best is trial 169 with value: 103.09610405000001.\n",
      "[I 2025-02-07 01:40:25,168] Trial 174 finished with value: 89.77720495 and parameters: {'learning_rate': 0.0032097497302006252, 'n_steps': 60, 'gamma': 0.93, 'ent_coef': 0.00010411285132632546, 'gae_lambda': 0.8410951106196436, 'vf_coef': 0.8, 'max_grad_norm': 2.9}. Best is trial 169 with value: 103.09610405000001.\n",
      "[I 2025-02-07 01:41:13,950] Trial 175 finished with value: 52.85773555000001 and parameters: {'learning_rate': 0.002860034636671042, 'n_steps': 60, 'gamma': 0.93, 'ent_coef': 0.00014240388863471278, 'gae_lambda': 0.8435066564968111, 'vf_coef': 0.8, 'max_grad_norm': 2.9}. Best is trial 169 with value: 103.09610405000001.\n",
      "[I 2025-02-07 01:42:00,939] Trial 176 finished with value: 78.28181800000002 and parameters: {'learning_rate': 0.0032837929125326803, 'n_steps': 115, 'gamma': 0.93, 'ent_coef': 9.233039620292556e-05, 'gae_lambda': 0.8372289343986018, 'vf_coef': 0.8, 'max_grad_norm': 3.0}. Best is trial 169 with value: 103.09610405000001.\n",
      "[I 2025-02-07 01:42:49,229] Trial 177 finished with value: 64.07409 and parameters: {'learning_rate': 0.0039129082283959745, 'n_steps': 65, 'gamma': 0.93, 'ent_coef': 4.998286434605944e-05, 'gae_lambda': 0.8397864232255049, 'vf_coef': 0.8, 'max_grad_norm': 3.1}. Best is trial 169 with value: 103.09610405000001.\n",
      "[I 2025-02-07 01:43:39,228] Trial 178 finished with value: 67.77868435 and parameters: {'learning_rate': 0.0023941946643888812, 'n_steps': 70, 'gamma': 0.93, 'ent_coef': 0.00183557464706699, 'gae_lambda': 0.846132514562308, 'vf_coef': 0.8, 'max_grad_norm': 2.7}. Best is trial 169 with value: 103.09610405000001.\n",
      "[I 2025-02-07 01:44:31,379] Trial 179 finished with value: 61.14545670000001 and parameters: {'learning_rate': 0.0017435547778516167, 'n_steps': 60, 'gamma': 0.93, 'ent_coef': 3.2030707334603646e-05, 'gae_lambda': 0.8489498062399193, 'vf_coef': 0.7000000000000001, 'max_grad_norm': 2.9}. Best is trial 169 with value: 103.09610405000001.\n",
      "[I 2025-02-07 01:45:19,831] Trial 180 finished with value: 67.9172638 and parameters: {'learning_rate': 0.004749387057869783, 'n_steps': 55, 'gamma': 0.9400000000000001, 'ent_coef': 0.0009483590888072574, 'gae_lambda': 0.8357349030456201, 'vf_coef': 0.8, 'max_grad_norm': 2.7}. Best is trial 169 with value: 103.09610405000001.\n",
      "[I 2025-02-07 01:46:08,854] Trial 181 finished with value: 67.49359855 and parameters: {'learning_rate': 0.002662372336041511, 'n_steps': 50, 'gamma': 0.93, 'ent_coef': 9.651778113648394e-05, 'gae_lambda': 0.8573496202876055, 'vf_coef': 0.8, 'max_grad_norm': 2.4}. Best is trial 169 with value: 103.09610405000001.\n",
      "[I 2025-02-07 01:46:57,852] Trial 182 finished with value: 67.10580635 and parameters: {'learning_rate': 0.0006995984939589154, 'n_steps': 50, 'gamma': 0.92, 'ent_coef': 0.00672515781034282, 'gae_lambda': 0.8425120499402579, 'vf_coef': 0.9, 'max_grad_norm': 0.5}. Best is trial 169 with value: 103.09610405000001.\n",
      "[I 2025-02-07 01:47:46,551] Trial 183 finished with value: 65.6267277 and parameters: {'learning_rate': 0.000920380120275666, 'n_steps': 55, 'gamma': 0.92, 'ent_coef': 0.00021008743257831344, 'gae_lambda': 0.8534933130934691, 'vf_coef': 0.8, 'max_grad_norm': 2.5}. Best is trial 169 with value: 103.09610405000001.\n",
      "[I 2025-02-07 01:48:35,242] Trial 184 finished with value: 58.99168794999999 and parameters: {'learning_rate': 0.002061062562400209, 'n_steps': 60, 'gamma': 0.93, 'ent_coef': 1.1607827081560455e-05, 'gae_lambda': 0.868383487409412, 'vf_coef': 0.9, 'max_grad_norm': 2.8}. Best is trial 169 with value: 103.09610405000001.\n",
      "[I 2025-02-07 01:49:24,750] Trial 185 finished with value: 48.458465350000004 and parameters: {'learning_rate': 0.003394470893333193, 'n_steps': 55, 'gamma': 0.9400000000000001, 'ent_coef': 6.338338836485814e-05, 'gae_lambda': 0.8791659144998786, 'vf_coef': 0.9, 'max_grad_norm': 1.9000000000000001}. Best is trial 169 with value: 103.09610405000001.\n",
      "[I 2025-02-07 01:50:13,319] Trial 186 finished with value: 86.21742174999999 and parameters: {'learning_rate': 0.000826267537091584, 'n_steps': 65, 'gamma': 0.9, 'ent_coef': 0.0026806832726624613, 'gae_lambda': 0.8728355192390841, 'vf_coef': 0.8, 'max_grad_norm': 2.6}. Best is trial 169 with value: 103.09610405000001.\n",
      "[I 2025-02-07 01:51:03,682] Trial 187 finished with value: 74.98229755000001 and parameters: {'learning_rate': 0.001255944170147205, 'n_steps': 40, 'gamma': 0.91, 'ent_coef': 4.177176532387132e-05, 'gae_lambda': 0.8403165472078212, 'vf_coef': 0.9, 'max_grad_norm': 2.3}. Best is trial 169 with value: 103.09610405000001.\n",
      "[I 2025-02-07 01:51:53,359] Trial 188 finished with value: 47.94789304999999 and parameters: {'learning_rate': 0.0006103193391536667, 'n_steps': 45, 'gamma': 0.92, 'ent_coef': 2.2612953423215076e-05, 'gae_lambda': 0.832257994450846, 'vf_coef': 0.7000000000000001, 'max_grad_norm': 2.7}. Best is trial 169 with value: 103.09610405000001.\n",
      "[I 2025-02-07 01:52:42,478] Trial 189 finished with value: 72.79337274999999 and parameters: {'learning_rate': 0.0023235572221258654, 'n_steps': 55, 'gamma': 0.93, 'ent_coef': 0.00012268508765660345, 'gae_lambda': 0.8768321696472744, 'vf_coef': 0.8, 'max_grad_norm': 2.6}. Best is trial 169 with value: 103.09610405000001.\n",
      "[I 2025-02-07 01:53:31,880] Trial 190 finished with value: 57.13453415 and parameters: {'learning_rate': 0.0007171444447222874, 'n_steps': 50, 'gamma': 0.9400000000000001, 'ent_coef': 0.0003171670206014184, 'gae_lambda': 0.8497608943856936, 'vf_coef': 1.0, 'max_grad_norm': 2.8}. Best is trial 169 with value: 103.09610405000001.\n",
      "[I 2025-02-07 01:54:20,536] Trial 191 finished with value: 78.270925 and parameters: {'learning_rate': 0.0007882331955775798, 'n_steps': 65, 'gamma': 0.9, 'ent_coef': 1.3946051717934334e-05, 'gae_lambda': 0.8741353071687117, 'vf_coef': 0.8, 'max_grad_norm': 2.6}. Best is trial 169 with value: 103.09610405000001.\n",
      "[I 2025-02-07 01:55:08,813] Trial 192 finished with value: 70.40971859999999 and parameters: {'learning_rate': 0.0008619095048984957, 'n_steps': 65, 'gamma': 0.9, 'ent_coef': 0.00305766905459394, 'gae_lambda': 0.8713287956614407, 'vf_coef': 0.8, 'max_grad_norm': 2.4}. Best is trial 169 with value: 103.09610405000001.\n",
      "[I 2025-02-07 01:55:57,458] Trial 193 finished with value: 67.2759729 and parameters: {'learning_rate': 0.0010166332085623847, 'n_steps': 60, 'gamma': 0.9, 'ent_coef': 0.0025082672716086447, 'gae_lambda': 0.9008016163423896, 'vf_coef': 0.8, 'max_grad_norm': 2.2}. Best is trial 169 with value: 103.09610405000001.\n",
      "[I 2025-02-07 01:56:45,487] Trial 194 finished with value: 84.4710774 and parameters: {'learning_rate': 0.001162549148873131, 'n_steps': 70, 'gamma': 0.9, 'ent_coef': 0.0038459208426263576, 'gae_lambda': 0.8901781223117565, 'vf_coef': 0.9, 'max_grad_norm': 2.5}. Best is trial 169 with value: 103.09610405000001.\n",
      "[I 2025-02-07 01:57:34,045] Trial 195 finished with value: 71.0801214 and parameters: {'learning_rate': 0.0006661805501659856, 'n_steps': 60, 'gamma': 0.91, 'ent_coef': 5.8375898007356465e-06, 'gae_lambda': 0.8645723471066852, 'vf_coef': 0.8, 'max_grad_norm': 0.9000000000000001}. Best is trial 169 with value: 103.09610405000001.\n",
      "[I 2025-02-07 01:58:22,770] Trial 196 finished with value: 57.973100349999996 and parameters: {'learning_rate': 0.0015654648197184302, 'n_steps': 50, 'gamma': 0.9, 'ent_coef': 0.013090865376712163, 'gae_lambda': 0.8841697767019836, 'vf_coef': 0.9, 'max_grad_norm': 2.6}. Best is trial 169 with value: 103.09610405000001.\n",
      "[I 2025-02-07 01:59:10,364] Trial 197 finished with value: 58.59061700000001 and parameters: {'learning_rate': 0.002846004153366313, 'n_steps': 90, 'gamma': 0.92, 'ent_coef': 7.342225154915861e-05, 'gae_lambda': 0.8970131878751477, 'vf_coef': 0.8, 'max_grad_norm': 2.7}. Best is trial 169 with value: 103.09610405000001.\n",
      "[I 2025-02-07 01:59:59,915] Trial 198 finished with value: 45.79884855 and parameters: {'learning_rate': 0.0005985980908188873, 'n_steps': 45, 'gamma': 0.91, 'ent_coef': 0.0016143370729984681, 'gae_lambda': 0.951670718304491, 'vf_coef': 1.0, 'max_grad_norm': 2.3}. Best is trial 169 with value: 103.09610405000001.\n",
      "[I 2025-02-07 02:00:48,640] Trial 199 finished with value: 48.43912625 and parameters: {'learning_rate': 0.0008071669999928193, 'n_steps': 55, 'gamma': 0.93, 'ent_coef': 2.738462696104436e-05, 'gae_lambda': 0.868871595147829, 'vf_coef': 0.8, 'max_grad_norm': 2.5}. Best is trial 169 with value: 103.09610405000001.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters: {'learning_rate': 0.0006867545672003906, 'n_steps': 50, 'gamma': 0.93, 'ent_coef': 7.90957139789345e-05, 'gae_lambda': 0.8511425851533388, 'vf_coef': 0.8, 'max_grad_norm': 2.4}\n",
      "Best value (objective): 103.09610405000001\n"
     ]
    }
   ],
   "source": [
    "#Run fine tuning\n",
    "a2c_study = run_a2c_optimization()\n",
    "\n",
    "with open(\"a2c_best_params.pkl\", \"wb\") as f:\n",
    "    pickle.dump(a2c_study.best_params, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc5598be-e04c-4c4d-9d91-bf0d8ad20288",
   "metadata": {},
   "source": [
    "## b. Training final model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7e0fe065-1609-4637-adef-1403ee573429",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Eval num_timesteps=500, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 0        |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 500      |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.673   |\n",
      "|    explained_variance | 0.236    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 9        |\n",
      "|    policy_loss        | 0.17     |\n",
      "|    value_loss         | 2.6      |\n",
      "------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=1000, episode_reward=24.86 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 24.9     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.687   |\n",
      "|    explained_variance | -0.302   |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 19       |\n",
      "|    policy_loss        | 0.0131   |\n",
      "|    value_loss         | 0.0888   |\n",
      "------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=1500, episode_reward=-0.08 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | -0.0771  |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.684   |\n",
      "|    explained_variance | -0.16    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 29       |\n",
      "|    policy_loss        | -0.196   |\n",
      "|    value_loss         | 0.705    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=2000, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 0        |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 2000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.676   |\n",
      "|    explained_variance | 0.198    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 39       |\n",
      "|    policy_loss        | -0.113   |\n",
      "|    value_loss         | 0.223    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=2500, episode_reward=0.93 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 0.929    |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 2500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.623   |\n",
      "|    explained_variance | 0.472    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 49       |\n",
      "|    policy_loss        | -0.181   |\n",
      "|    value_loss         | 0.357    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=3000, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 0        |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 3000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.673   |\n",
      "|    explained_variance | -0.0839  |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 59       |\n",
      "|    policy_loss        | -0.121   |\n",
      "|    value_loss         | 0.392    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=3500, episode_reward=34.33 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 34.3     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 3500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.632   |\n",
      "|    explained_variance | 0.248    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 69       |\n",
      "|    policy_loss        | 0.438    |\n",
      "|    value_loss         | 0.941    |\n",
      "------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=4000, episode_reward=20.74 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 20.7     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 4000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.655   |\n",
      "|    explained_variance | 0.196    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 79       |\n",
      "|    policy_loss        | -0.171   |\n",
      "|    value_loss         | 0.187    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=4500, episode_reward=63.63 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 63.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 4500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.641   |\n",
      "|    explained_variance | 0.0626   |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 89       |\n",
      "|    policy_loss        | 0.048    |\n",
      "|    value_loss         | 1.74     |\n",
      "------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=5000, episode_reward=21.00 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 21       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 5000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.614   |\n",
      "|    explained_variance | 0.222    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 99       |\n",
      "|    policy_loss        | -0.255   |\n",
      "|    value_loss         | 1.65     |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 22.8     |\n",
      "| time/              |          |\n",
      "|    fps             | 2036     |\n",
      "|    iterations      | 100      |\n",
      "|    time_elapsed    | 2        |\n",
      "|    total_timesteps | 5000     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=5500, episode_reward=66.30 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 66.3     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 5500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.644   |\n",
      "|    explained_variance | 0.138    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 109      |\n",
      "|    policy_loss        | 0.213    |\n",
      "|    value_loss         | 0.178    |\n",
      "------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=6000, episode_reward=64.27 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 64.3     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 6000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.651   |\n",
      "|    explained_variance | 0.432    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 119      |\n",
      "|    policy_loss        | 0.22     |\n",
      "|    value_loss         | 0.28     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=6500, episode_reward=66.57 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 66.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 6500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.682   |\n",
      "|    explained_variance | 0.206    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 129      |\n",
      "|    policy_loss        | 0.00728  |\n",
      "|    value_loss         | 0.0595   |\n",
      "------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=7000, episode_reward=65.42 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 65.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 7000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.668   |\n",
      "|    explained_variance | 0.495    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 139      |\n",
      "|    policy_loss        | -0.00289 |\n",
      "|    value_loss         | 0.328    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=7500, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 0        |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 7500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.681   |\n",
      "|    explained_variance | 0.514    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 149      |\n",
      "|    policy_loss        | 0.108    |\n",
      "|    value_loss         | 0.232    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=8000, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 0        |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 8000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.658   |\n",
      "|    explained_variance | 0.0497   |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 159      |\n",
      "|    policy_loss        | 0.201    |\n",
      "|    value_loss         | 1.44     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=8500, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 0        |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 8500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.679   |\n",
      "|    explained_variance | 0.181    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 169      |\n",
      "|    policy_loss        | 0.121    |\n",
      "|    value_loss         | 0.944    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=9000, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 0        |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 9000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.673   |\n",
      "|    explained_variance | -1.14    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 179      |\n",
      "|    policy_loss        | 0.113    |\n",
      "|    value_loss         | 0.0791   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=9500, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 0        |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 9500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.663   |\n",
      "|    explained_variance | 0.499    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 189      |\n",
      "|    policy_loss        | -0.283   |\n",
      "|    value_loss         | 0.963    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=10000, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 0        |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 10000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.685   |\n",
      "|    explained_variance | -0.962   |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 199      |\n",
      "|    policy_loss        | -0.1     |\n",
      "|    value_loss         | 0.229    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 19.6     |\n",
      "| time/              |          |\n",
      "|    fps             | 2053     |\n",
      "|    iterations      | 200      |\n",
      "|    time_elapsed    | 4        |\n",
      "|    total_timesteps | 10000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=10500, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 0        |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 10500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.659   |\n",
      "|    explained_variance | 0.468    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 209      |\n",
      "|    policy_loss        | -0.0385  |\n",
      "|    value_loss         | 0.397    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=11000, episode_reward=77.15 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 77.1     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 11000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.679   |\n",
      "|    explained_variance | 0.294    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 219      |\n",
      "|    policy_loss        | -0.0412  |\n",
      "|    value_loss         | 0.0864   |\n",
      "------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=11500, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 0        |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 11500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.672   |\n",
      "|    explained_variance | 0.301    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 229      |\n",
      "|    policy_loss        | -0.00106 |\n",
      "|    value_loss         | 0.202    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=12000, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 0        |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 12000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.678   |\n",
      "|    explained_variance | 0.65     |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 239      |\n",
      "|    policy_loss        | -0.0434  |\n",
      "|    value_loss         | 0.102    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=12500, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 0        |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 12500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.61    |\n",
      "|    explained_variance | 0.236    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 249      |\n",
      "|    policy_loss        | -0.0678  |\n",
      "|    value_loss         | 1.75     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=13000, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 0        |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 13000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.608   |\n",
      "|    explained_variance | 0.182    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 259      |\n",
      "|    policy_loss        | 0.174    |\n",
      "|    value_loss         | 1.78     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=13500, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 0        |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 13500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.663   |\n",
      "|    explained_variance | 0.114    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 269      |\n",
      "|    policy_loss        | 0.121    |\n",
      "|    value_loss         | 0.0661   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=14000, episode_reward=86.92 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 86.9     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 14000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.641   |\n",
      "|    explained_variance | 0.399    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 279      |\n",
      "|    policy_loss        | 0.141    |\n",
      "|    value_loss         | 0.389    |\n",
      "------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=14500, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 0        |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 14500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.658   |\n",
      "|    explained_variance | 0.282    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 289      |\n",
      "|    policy_loss        | -0.141   |\n",
      "|    value_loss         | 0.0655   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=15000, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 0        |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 15000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.643   |\n",
      "|    explained_variance | 0.193    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 299      |\n",
      "|    policy_loss        | 0.507    |\n",
      "|    value_loss         | 1.96     |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 20.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 2048     |\n",
      "|    iterations      | 300      |\n",
      "|    time_elapsed    | 7        |\n",
      "|    total_timesteps | 15000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=15500, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 0        |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 15500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.672   |\n",
      "|    explained_variance | 0.669    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 309      |\n",
      "|    policy_loss        | -0.0411  |\n",
      "|    value_loss         | 0.0548   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=16000, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 0        |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 16000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.671   |\n",
      "|    explained_variance | 0.364    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 319      |\n",
      "|    policy_loss        | -0.112   |\n",
      "|    value_loss         | 0.505    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=16500, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 0        |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 16500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.65    |\n",
      "|    explained_variance | 0.265    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 329      |\n",
      "|    policy_loss        | 0.482    |\n",
      "|    value_loss         | 3.17     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=17000, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 0        |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 17000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.66    |\n",
      "|    explained_variance | -0.453   |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 339      |\n",
      "|    policy_loss        | 0.157    |\n",
      "|    value_loss         | 0.111    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=17500, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 0        |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 17500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.652   |\n",
      "|    explained_variance | 0.472    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 349      |\n",
      "|    policy_loss        | -0.326   |\n",
      "|    value_loss         | 1.25     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=18000, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 0        |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 18000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.654   |\n",
      "|    explained_variance | -0.571   |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 359      |\n",
      "|    policy_loss        | 0.11     |\n",
      "|    value_loss         | 0.221    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=18500, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 0        |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 18500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.649   |\n",
      "|    explained_variance | 0.363    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 369      |\n",
      "|    policy_loss        | -0.0785  |\n",
      "|    value_loss         | 0.326    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=19000, episode_reward=69.47 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 69.5     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 19000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.635   |\n",
      "|    explained_variance | 0.129    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 379      |\n",
      "|    policy_loss        | 0.00592  |\n",
      "|    value_loss         | 0.0894   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=19500, episode_reward=13.60 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 13.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 19500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.63    |\n",
      "|    explained_variance | 0.498    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 389      |\n",
      "|    policy_loss        | 0.0775   |\n",
      "|    value_loss         | 0.244    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=20000, episode_reward=42.10 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 42.1     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 20000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.614   |\n",
      "|    explained_variance | 0.741    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 399      |\n",
      "|    policy_loss        | -0.109   |\n",
      "|    value_loss         | 0.0764   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 22.3     |\n",
      "| time/              |          |\n",
      "|    fps             | 2047     |\n",
      "|    iterations      | 400      |\n",
      "|    time_elapsed    | 9        |\n",
      "|    total_timesteps | 20000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=20500, episode_reward=17.10 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 17.1     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 20500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.645   |\n",
      "|    explained_variance | -0.0114  |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 409      |\n",
      "|    policy_loss        | -0.211   |\n",
      "|    value_loss         | 1.27     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=21000, episode_reward=-4.10 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | -4.1     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 21000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.612   |\n",
      "|    explained_variance | 0.18     |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 419      |\n",
      "|    policy_loss        | -0.118   |\n",
      "|    value_loss         | 1.01     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=21500, episode_reward=82.99 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 83       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 21500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.656   |\n",
      "|    explained_variance | 0.216    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 429      |\n",
      "|    policy_loss        | 0.096    |\n",
      "|    value_loss         | 0.0569   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=22000, episode_reward=9.16 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 9.16     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 22000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.502   |\n",
      "|    explained_variance | 0.448    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 439      |\n",
      "|    policy_loss        | 0.0406   |\n",
      "|    value_loss         | 0.288    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=22500, episode_reward=83.18 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 83.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 22500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.661   |\n",
      "|    explained_variance | 0.588    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 449      |\n",
      "|    policy_loss        | -0.0166  |\n",
      "|    value_loss         | 0.0366   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=23000, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 0        |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 23000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.501   |\n",
      "|    explained_variance | 0.662    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 459      |\n",
      "|    policy_loss        | 0.0718   |\n",
      "|    value_loss         | 0.393    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=23500, episode_reward=16.23 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 16.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 23500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.485   |\n",
      "|    explained_variance | 0.277    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 469      |\n",
      "|    policy_loss        | 0.0361   |\n",
      "|    value_loss         | 0.274    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=24000, episode_reward=2.22 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 2.22     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 24000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.594   |\n",
      "|    explained_variance | 0.386    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 479      |\n",
      "|    policy_loss        | 0.449    |\n",
      "|    value_loss         | 2.77     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=24500, episode_reward=63.01 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 63       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 24500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.521   |\n",
      "|    explained_variance | 0.737    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 489      |\n",
      "|    policy_loss        | -0.174   |\n",
      "|    value_loss         | 0.557    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=25000, episode_reward=70.97 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 71       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 25000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.645   |\n",
      "|    explained_variance | 0.249    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 499      |\n",
      "|    policy_loss        | 0.0225   |\n",
      "|    value_loss         | 0.0242   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 24.4     |\n",
      "| time/              |          |\n",
      "|    fps             | 2053     |\n",
      "|    iterations      | 500      |\n",
      "|    time_elapsed    | 12       |\n",
      "|    total_timesteps | 25000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=25500, episode_reward=77.77 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 77.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 25500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.282   |\n",
      "|    explained_variance | 0.306    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 509      |\n",
      "|    policy_loss        | -0.409   |\n",
      "|    value_loss         | 0.75     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=26000, episode_reward=72.40 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 72.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 26000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.526   |\n",
      "|    explained_variance | 0.0931   |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 519      |\n",
      "|    policy_loss        | 0.0273   |\n",
      "|    value_loss         | 0.0847   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=26500, episode_reward=20.39 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 20.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 26500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.198   |\n",
      "|    explained_variance | -1.08    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 529      |\n",
      "|    policy_loss        | -0.112   |\n",
      "|    value_loss         | 0.367    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=27000, episode_reward=21.59 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 21.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 27000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.373   |\n",
      "|    explained_variance | 0.235    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 539      |\n",
      "|    policy_loss        | 0.072    |\n",
      "|    value_loss         | 0.1      |\n",
      "------------------------------------\n",
      "Eval num_timesteps=27500, episode_reward=17.77 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 17.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 27500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.429   |\n",
      "|    explained_variance | -0.745   |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 549      |\n",
      "|    policy_loss        | -0.0232  |\n",
      "|    value_loss         | 0.299    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=28000, episode_reward=36.91 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 36.9     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 28000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.411   |\n",
      "|    explained_variance | 0.756    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 559      |\n",
      "|    policy_loss        | 0.0876   |\n",
      "|    value_loss         | 0.206    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=28500, episode_reward=9.16 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 9.16     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 28500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.481   |\n",
      "|    explained_variance | 0.0775   |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 569      |\n",
      "|    policy_loss        | -0.117   |\n",
      "|    value_loss         | 0.924    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=29000, episode_reward=9.16 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 9.16     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 29000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0708  |\n",
      "|    explained_variance | 0.461    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 579      |\n",
      "|    policy_loss        | -0.324   |\n",
      "|    value_loss         | 0.538    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=29500, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 0        |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 29500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0931  |\n",
      "|    explained_variance | -5.19    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 589      |\n",
      "|    policy_loss        | -0.00127 |\n",
      "|    value_loss         | 0.00916  |\n",
      "------------------------------------\n",
      "Eval num_timesteps=30000, episode_reward=9.16 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 9.16     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 30000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.261   |\n",
      "|    explained_variance | -0.264   |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 599      |\n",
      "|    policy_loss        | -0.272   |\n",
      "|    value_loss         | 2.57     |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 25.1     |\n",
      "| time/              |          |\n",
      "|    fps             | 2049     |\n",
      "|    iterations      | 600      |\n",
      "|    time_elapsed    | 14       |\n",
      "|    total_timesteps | 30000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=30500, episode_reward=78.23 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 78.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 30500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.537   |\n",
      "|    explained_variance | 0.218    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 609      |\n",
      "|    policy_loss        | 0.0746   |\n",
      "|    value_loss         | 0.137    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=31000, episode_reward=72.99 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 73       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 31000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.393   |\n",
      "|    explained_variance | 0.344    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 619      |\n",
      "|    policy_loss        | -0.0688  |\n",
      "|    value_loss         | 0.319    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=31500, episode_reward=45.52 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 45.5     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 31500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.333   |\n",
      "|    explained_variance | 0.667    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 629      |\n",
      "|    policy_loss        | -0.0137  |\n",
      "|    value_loss         | 0.0416   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=32000, episode_reward=7.60 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 7.6      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 32000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.512   |\n",
      "|    explained_variance | 0.461    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 639      |\n",
      "|    policy_loss        | 0.523    |\n",
      "|    value_loss         | 2.81     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=32500, episode_reward=70.45 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 70.5     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 32500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.477   |\n",
      "|    explained_variance | 0.698    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 649      |\n",
      "|    policy_loss        | 0.117    |\n",
      "|    value_loss         | 0.341    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=33000, episode_reward=7.55 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 7.55     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 33000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.347   |\n",
      "|    explained_variance | -1.32    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 659      |\n",
      "|    policy_loss        | 0.00143  |\n",
      "|    value_loss         | 0.0327   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=33500, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 0        |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 33500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.114   |\n",
      "|    explained_variance | 0.422    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 669      |\n",
      "|    policy_loss        | -0.0752  |\n",
      "|    value_loss         | 0.187    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=34000, episode_reward=25.92 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 25.9     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 34000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.395   |\n",
      "|    explained_variance | 0.384    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 679      |\n",
      "|    policy_loss        | -0.0691  |\n",
      "|    value_loss         | 0.0653   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=34500, episode_reward=70.68 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 70.7     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 34500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.023   |\n",
      "|    explained_variance | 0.76     |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 689      |\n",
      "|    policy_loss        | 0.000911 |\n",
      "|    value_loss         | 0.0674   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=35000, episode_reward=53.09 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 53.1     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 35000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.337   |\n",
      "|    explained_variance | 0.0558   |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 699      |\n",
      "|    policy_loss        | -0.0253  |\n",
      "|    value_loss         | 0.183    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 26.2     |\n",
      "| time/              |          |\n",
      "|    fps             | 2053     |\n",
      "|    iterations      | 700      |\n",
      "|    time_elapsed    | 17       |\n",
      "|    total_timesteps | 35000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=35500, episode_reward=49.67 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 49.7     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 35500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.354   |\n",
      "|    explained_variance | 0.489    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 709      |\n",
      "|    policy_loss        | -0.078   |\n",
      "|    value_loss         | 0.239    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=36000, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 0        |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 36000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.252   |\n",
      "|    explained_variance | 0.83     |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 719      |\n",
      "|    policy_loss        | 0.0159   |\n",
      "|    value_loss         | 0.106    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=36500, episode_reward=27.44 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 27.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 36500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.276   |\n",
      "|    explained_variance | -0.256   |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 729      |\n",
      "|    policy_loss        | -0.0126  |\n",
      "|    value_loss         | 0.0647   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=37000, episode_reward=34.12 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 34.1     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 37000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0545  |\n",
      "|    explained_variance | 0.265    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 739      |\n",
      "|    policy_loss        | -0.0399  |\n",
      "|    value_loss         | 0.0882   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=37500, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 0        |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 37500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0973  |\n",
      "|    explained_variance | -1.05    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 749      |\n",
      "|    policy_loss        | -0.0261  |\n",
      "|    value_loss         | 0.0284   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=38000, episode_reward=83.98 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 84       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 38000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.103   |\n",
      "|    explained_variance | -0.068   |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 759      |\n",
      "|    policy_loss        | -0.18    |\n",
      "|    value_loss         | 4.48     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=38500, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 0        |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 38500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.396   |\n",
      "|    explained_variance | 0.179    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 769      |\n",
      "|    policy_loss        | 0.108    |\n",
      "|    value_loss         | 0.135    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=39000, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 0        |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 39000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.327   |\n",
      "|    explained_variance | 0.514    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 779      |\n",
      "|    policy_loss        | 0.0514   |\n",
      "|    value_loss         | 0.409    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=39500, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 0        |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 39500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.255   |\n",
      "|    explained_variance | -0.0546  |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 789      |\n",
      "|    policy_loss        | -0.0444  |\n",
      "|    value_loss         | 0.0771   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=40000, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 0        |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 40000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.394   |\n",
      "|    explained_variance | 0.211    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 799      |\n",
      "|    policy_loss        | 0.273    |\n",
      "|    value_loss         | 3.29     |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 27.5     |\n",
      "| time/              |          |\n",
      "|    fps             | 2052     |\n",
      "|    iterations      | 800      |\n",
      "|    time_elapsed    | 19       |\n",
      "|    total_timesteps | 40000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=40500, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 0        |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 40500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.411   |\n",
      "|    explained_variance | 0.359    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 809      |\n",
      "|    policy_loss        | 0.132    |\n",
      "|    value_loss         | 0.906    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=41000, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 0        |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 41000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.106   |\n",
      "|    explained_variance | -0.754   |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 819      |\n",
      "|    policy_loss        | 0.018    |\n",
      "|    value_loss         | 0.0191   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=41500, episode_reward=85.05 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 85       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 41500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.045   |\n",
      "|    explained_variance | -0.132   |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 829      |\n",
      "|    policy_loss        | 0.0026   |\n",
      "|    value_loss         | 0.0614   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=42000, episode_reward=38.30 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 38.3     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 42000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.371   |\n",
      "|    explained_variance | 0.144    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 839      |\n",
      "|    policy_loss        | 0.173    |\n",
      "|    value_loss         | 0.139    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=42500, episode_reward=38.30 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 38.3     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 42500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0165  |\n",
      "|    explained_variance | 0.585    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 849      |\n",
      "|    policy_loss        | 4.09e-05 |\n",
      "|    value_loss         | 0.0223   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=43000, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 0        |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 43000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.35    |\n",
      "|    explained_variance | 0.474    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 859      |\n",
      "|    policy_loss        | -0.0823  |\n",
      "|    value_loss         | 0.135    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=43500, episode_reward=38.30 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 38.3     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 43500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.356   |\n",
      "|    explained_variance | 0.295    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 869      |\n",
      "|    policy_loss        | -0.0256  |\n",
      "|    value_loss         | 0.391    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=44000, episode_reward=53.30 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 53.3     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 44000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.187   |\n",
      "|    explained_variance | 0.892    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 879      |\n",
      "|    policy_loss        | 0.02     |\n",
      "|    value_loss         | 0.0763   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=44500, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 0        |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 44500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.17    |\n",
      "|    explained_variance | -0.513   |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 889      |\n",
      "|    policy_loss        | -0.0894  |\n",
      "|    value_loss         | 0.158    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=45000, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 0        |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 45000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.014   |\n",
      "|    explained_variance | -0.789   |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 899      |\n",
      "|    policy_loss        | 0.000347 |\n",
      "|    value_loss         | 0.00979  |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 29.6     |\n",
      "| time/              |          |\n",
      "|    fps             | 2051     |\n",
      "|    iterations      | 900      |\n",
      "|    time_elapsed    | 21       |\n",
      "|    total_timesteps | 45000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=45500, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 0        |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 45500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0506  |\n",
      "|    explained_variance | -2.82    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 909      |\n",
      "|    policy_loss        | 0.000617 |\n",
      "|    value_loss         | 0.00502  |\n",
      "------------------------------------\n",
      "Eval num_timesteps=46000, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 0        |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 46000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0842  |\n",
      "|    explained_variance | 0.338    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 919      |\n",
      "|    policy_loss        | -0.133   |\n",
      "|    value_loss         | 3.4      |\n",
      "------------------------------------\n",
      "Eval num_timesteps=46500, episode_reward=45.35 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 45.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 46500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.402   |\n",
      "|    explained_variance | 0.0414   |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 929      |\n",
      "|    policy_loss        | 0.0458   |\n",
      "|    value_loss         | 0.0345   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=47000, episode_reward=61.86 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 61.9     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 47000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.295   |\n",
      "|    explained_variance | 0.712    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 939      |\n",
      "|    policy_loss        | -0.00618 |\n",
      "|    value_loss         | 0.104    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=47500, episode_reward=8.64 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 8.64     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 47500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.291   |\n",
      "|    explained_variance | 0.256    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 949      |\n",
      "|    policy_loss        | -0.0975  |\n",
      "|    value_loss         | 0.0986   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=48000, episode_reward=55.45 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 55.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 48000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.393   |\n",
      "|    explained_variance | 0.383    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 959      |\n",
      "|    policy_loss        | 0.357    |\n",
      "|    value_loss         | 2.75     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=48500, episode_reward=59.92 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 59.9     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 48500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.345   |\n",
      "|    explained_variance | 0.375    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 969      |\n",
      "|    policy_loss        | 0.0959   |\n",
      "|    value_loss         | 1.84     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=49000, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 0        |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 49000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.134   |\n",
      "|    explained_variance | -0.664   |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 979      |\n",
      "|    policy_loss        | -0.0217  |\n",
      "|    value_loss         | 0.0182   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=49500, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 0        |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 49500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0245  |\n",
      "|    explained_variance | -1.7     |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 989      |\n",
      "|    policy_loss        | 0.0342   |\n",
      "|    value_loss         | 0.0518   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=50000, episode_reward=42.69 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 42.7     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 50000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.34    |\n",
      "|    explained_variance | -1.84    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 999      |\n",
      "|    policy_loss        | 0.0375   |\n",
      "|    value_loss         | 0.0613   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 31.5     |\n",
      "| time/              |          |\n",
      "|    fps             | 2054     |\n",
      "|    iterations      | 1000     |\n",
      "|    time_elapsed    | 24       |\n",
      "|    total_timesteps | 50000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=50500, episode_reward=43.40 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 230       |\n",
      "|    mean_reward        | 43.4      |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 50500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.0219   |\n",
      "|    explained_variance | 0.803     |\n",
      "|    learning_rate      | 0.000687  |\n",
      "|    n_updates          | 1009      |\n",
      "|    policy_loss        | -0.000112 |\n",
      "|    value_loss         | 0.0307    |\n",
      "-------------------------------------\n",
      "Eval num_timesteps=51000, episode_reward=91.24 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 91.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 51000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.262   |\n",
      "|    explained_variance | 0.387    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 1019     |\n",
      "|    policy_loss        | -0.0296  |\n",
      "|    value_loss         | 0.0803   |\n",
      "------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=51500, episode_reward=47.37 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 47.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 51500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.308   |\n",
      "|    explained_variance | 0.572    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 1029     |\n",
      "|    policy_loss        | 0.0126   |\n",
      "|    value_loss         | 0.161    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=52000, episode_reward=53.83 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 53.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 52000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.156   |\n",
      "|    explained_variance | 0.927    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 1039     |\n",
      "|    policy_loss        | -0.00494 |\n",
      "|    value_loss         | 0.0462   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=52500, episode_reward=38.07 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 38.1     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 52500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.315   |\n",
      "|    explained_variance | 0.354    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 1049     |\n",
      "|    policy_loss        | 0.00795  |\n",
      "|    value_loss         | 0.259    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=53000, episode_reward=46.89 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 46.9     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 53000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0437  |\n",
      "|    explained_variance | 0.0198   |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 1059     |\n",
      "|    policy_loss        | -0.23    |\n",
      "|    value_loss         | 0.585    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=53500, episode_reward=29.73 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 29.7     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 53500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.115   |\n",
      "|    explained_variance | 0.106    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 1069     |\n",
      "|    policy_loss        | 0.00874  |\n",
      "|    value_loss         | 0.00442  |\n",
      "------------------------------------\n",
      "Eval num_timesteps=54000, episode_reward=54.96 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 55       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 54000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0453  |\n",
      "|    explained_variance | 0.5      |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 1079     |\n",
      "|    policy_loss        | -0.04    |\n",
      "|    value_loss         | 3.52     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=54500, episode_reward=38.82 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 38.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 54500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.352   |\n",
      "|    explained_variance | 0.645    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 1089     |\n",
      "|    policy_loss        | -0.0404  |\n",
      "|    value_loss         | 0.0385   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=55000, episode_reward=60.20 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 60.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 55000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.222   |\n",
      "|    explained_variance | 0.458    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 1099     |\n",
      "|    policy_loss        | 0.00394  |\n",
      "|    value_loss         | 0.203    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 32.8     |\n",
      "| time/              |          |\n",
      "|    fps             | 2045     |\n",
      "|    iterations      | 1100     |\n",
      "|    time_elapsed    | 26       |\n",
      "|    total_timesteps | 55000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=55500, episode_reward=30.36 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 30.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 55500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.242   |\n",
      "|    explained_variance | 0.739    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 1109     |\n",
      "|    policy_loss        | 0.0434   |\n",
      "|    value_loss         | 0.0755   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=56000, episode_reward=84.22 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 84.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 56000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.509   |\n",
      "|    explained_variance | 0.341    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 1119     |\n",
      "|    policy_loss        | 0.154    |\n",
      "|    value_loss         | 2.44     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=56500, episode_reward=19.98 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 20       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 56500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.379   |\n",
      "|    explained_variance | 0.727    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 1129     |\n",
      "|    policy_loss        | -0.00154 |\n",
      "|    value_loss         | 0.353    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=57000, episode_reward=23.42 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 23.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 57000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.483   |\n",
      "|    explained_variance | -0.224   |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 1139     |\n",
      "|    policy_loss        | 0.0608   |\n",
      "|    value_loss         | 0.172    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=57500, episode_reward=1.73 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 1.73     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 57500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0804  |\n",
      "|    explained_variance | -0.59    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 1149     |\n",
      "|    policy_loss        | 0.0189   |\n",
      "|    value_loss         | 0.0499   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=58000, episode_reward=57.52 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 57.5     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 58000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.216   |\n",
      "|    explained_variance | 0.784    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 1159     |\n",
      "|    policy_loss        | 0.00531  |\n",
      "|    value_loss         | 0.0257   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=58500, episode_reward=38.02 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 38       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 58500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0219  |\n",
      "|    explained_variance | -0.59    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 1169     |\n",
      "|    policy_loss        | -0.00273 |\n",
      "|    value_loss         | 0.0138   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=59000, episode_reward=56.94 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 56.9     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 59000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.249   |\n",
      "|    explained_variance | 0.0594   |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 1179     |\n",
      "|    policy_loss        | -0.0872  |\n",
      "|    value_loss         | 0.196    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=59500, episode_reward=51.46 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 51.5     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 59500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.214   |\n",
      "|    explained_variance | 0.534    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 1189     |\n",
      "|    policy_loss        | -0.0182  |\n",
      "|    value_loss         | 0.257    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=60000, episode_reward=15.85 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 15.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 60000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.146   |\n",
      "|    explained_variance | 0.902    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 1199     |\n",
      "|    policy_loss        | -0.0111  |\n",
      "|    value_loss         | 0.0478   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 33.7     |\n",
      "| time/              |          |\n",
      "|    fps             | 2047     |\n",
      "|    iterations      | 1200     |\n",
      "|    time_elapsed    | 29       |\n",
      "|    total_timesteps | 60000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=60500, episode_reward=85.05 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 85.1     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 60500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.29    |\n",
      "|    explained_variance | 0.373    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 1209     |\n",
      "|    policy_loss        | 0.0336   |\n",
      "|    value_loss         | 0.217    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=61000, episode_reward=63.28 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 63.3     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 61000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0999  |\n",
      "|    explained_variance | -0.535   |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 1219     |\n",
      "|    policy_loss        | -0.0252  |\n",
      "|    value_loss         | 0.027    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=61500, episode_reward=60.44 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 60.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 61500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.443   |\n",
      "|    explained_variance | -0.0463  |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 1229     |\n",
      "|    policy_loss        | -0.00462 |\n",
      "|    value_loss         | 0.0414   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=62000, episode_reward=61.24 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 61.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 62000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0516  |\n",
      "|    explained_variance | 0.691    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 1239     |\n",
      "|    policy_loss        | -0.016   |\n",
      "|    value_loss         | 0.741    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=62500, episode_reward=5.80 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 5.8      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 62500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.233   |\n",
      "|    explained_variance | 0.111    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 1249     |\n",
      "|    policy_loss        | -0.0108  |\n",
      "|    value_loss         | 0.0487   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=63000, episode_reward=48.56 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 48.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 63000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.249   |\n",
      "|    explained_variance | 0.638    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 1259     |\n",
      "|    policy_loss        | 0.0815   |\n",
      "|    value_loss         | 0.157    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=63500, episode_reward=48.06 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 48.1     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 63500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.268   |\n",
      "|    explained_variance | 0.536    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 1269     |\n",
      "|    policy_loss        | -0.0102  |\n",
      "|    value_loss         | 0.0845   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=64000, episode_reward=77.72 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 77.7     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 64000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.267   |\n",
      "|    explained_variance | 0.512    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 1279     |\n",
      "|    policy_loss        | 0.136    |\n",
      "|    value_loss         | 2.25     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=64500, episode_reward=38.92 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 38.9     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 64500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.312   |\n",
      "|    explained_variance | 0.565    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 1289     |\n",
      "|    policy_loss        | 0.271    |\n",
      "|    value_loss         | 1.27     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=65000, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 0        |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 65000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.316   |\n",
      "|    explained_variance | 0.154    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 1299     |\n",
      "|    policy_loss        | 0.000918 |\n",
      "|    value_loss         | 0.0186   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 35.1     |\n",
      "| time/              |          |\n",
      "|    fps             | 2048     |\n",
      "|    iterations      | 1300     |\n",
      "|    time_elapsed    | 31       |\n",
      "|    total_timesteps | 65000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=65500, episode_reward=7.46 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 7.46     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 65500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0355  |\n",
      "|    explained_variance | -0.984   |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 1309     |\n",
      "|    policy_loss        | 0.000873 |\n",
      "|    value_loss         | 0.00743  |\n",
      "------------------------------------\n",
      "Eval num_timesteps=66000, episode_reward=8.64 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 8.64     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 66000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.232   |\n",
      "|    explained_variance | 0.325    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 1319     |\n",
      "|    policy_loss        | 0.0322   |\n",
      "|    value_loss         | 0.0698   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=66500, episode_reward=44.81 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 44.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 66500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0166  |\n",
      "|    explained_variance | 0.568    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 1329     |\n",
      "|    policy_loss        | 0.000691 |\n",
      "|    value_loss         | 0.0348   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=67000, episode_reward=45.85 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 45.9     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 67000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.188   |\n",
      "|    explained_variance | 0.763    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 1339     |\n",
      "|    policy_loss        | 0.0207   |\n",
      "|    value_loss         | 0.0421   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=67500, episode_reward=38.03 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 38       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 67500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.134   |\n",
      "|    explained_variance | 0.68     |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 1349     |\n",
      "|    policy_loss        | 0.0214   |\n",
      "|    value_loss         | 0.154    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=68000, episode_reward=49.00 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 49       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 68000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0861  |\n",
      "|    explained_variance | 0.935    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 1359     |\n",
      "|    policy_loss        | -0.00317 |\n",
      "|    value_loss         | 0.0379   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=68500, episode_reward=77.12 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 77.1     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 68500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.238   |\n",
      "|    explained_variance | -0.53    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 1369     |\n",
      "|    policy_loss        | -0.06    |\n",
      "|    value_loss         | 0.113    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=69000, episode_reward=62.91 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 62.9     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 69000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.036   |\n",
      "|    explained_variance | -3.13    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 1379     |\n",
      "|    policy_loss        | 0.00402  |\n",
      "|    value_loss         | 0.0113   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=69500, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 0        |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 69500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.102   |\n",
      "|    explained_variance | 0.544    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 1389     |\n",
      "|    policy_loss        | 0.00764  |\n",
      "|    value_loss         | 0.00381  |\n",
      "------------------------------------\n",
      "Eval num_timesteps=70000, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 0        |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 70000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0624  |\n",
      "|    explained_variance | 0.513    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 1399     |\n",
      "|    policy_loss        | -0.625   |\n",
      "|    value_loss         | 3.78     |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 36.2     |\n",
      "| time/              |          |\n",
      "|    fps             | 2050     |\n",
      "|    iterations      | 1400     |\n",
      "|    time_elapsed    | 34       |\n",
      "|    total_timesteps | 70000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=70500, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 0        |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 70500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.178   |\n",
      "|    explained_variance | 0.358    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 1409     |\n",
      "|    policy_loss        | 0.00386  |\n",
      "|    value_loss         | 0.0437   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=71000, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 0        |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 71000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.261   |\n",
      "|    explained_variance | 0.796    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 1419     |\n",
      "|    policy_loss        | -0.00269 |\n",
      "|    value_loss         | 0.0773   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=71500, episode_reward=39.31 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 39.3     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 71500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.232   |\n",
      "|    explained_variance | 0.637    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 1429     |\n",
      "|    policy_loss        | 0.035    |\n",
      "|    value_loss         | 0.0443   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=72000, episode_reward=45.25 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 45.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 72000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.355   |\n",
      "|    explained_variance | 0.616    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 1439     |\n",
      "|    policy_loss        | 0.161    |\n",
      "|    value_loss         | 1.4      |\n",
      "------------------------------------\n",
      "Eval num_timesteps=72500, episode_reward=49.04 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 49       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 72500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.258   |\n",
      "|    explained_variance | 0.627    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 1449     |\n",
      "|    policy_loss        | 0.133    |\n",
      "|    value_loss         | 1.07     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=73000, episode_reward=60.01 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 60       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 73000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.234   |\n",
      "|    explained_variance | -0.0635  |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 1459     |\n",
      "|    policy_loss        | 0.0424   |\n",
      "|    value_loss         | 0.0433   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=73500, episode_reward=35.78 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 35.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 73500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0266  |\n",
      "|    explained_variance | 0.648    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 1469     |\n",
      "|    policy_loss        | -0.0105  |\n",
      "|    value_loss         | 0.0147   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=74000, episode_reward=48.84 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 48.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 74000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.271   |\n",
      "|    explained_variance | 0.351    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 1479     |\n",
      "|    policy_loss        | 0.0813   |\n",
      "|    value_loss         | 0.0905   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=74500, episode_reward=58.61 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 58.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 74500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.022   |\n",
      "|    explained_variance | 0.832    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 1489     |\n",
      "|    policy_loss        | 0.000817 |\n",
      "|    value_loss         | 0.0274   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=75000, episode_reward=65.09 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 65.1     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 75000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.195   |\n",
      "|    explained_variance | 0.698    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 1499     |\n",
      "|    policy_loss        | 0.0057   |\n",
      "|    value_loss         | 0.0415   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 37.6     |\n",
      "| time/              |          |\n",
      "|    fps             | 2052     |\n",
      "|    iterations      | 1500     |\n",
      "|    time_elapsed    | 36       |\n",
      "|    total_timesteps | 75000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=75500, episode_reward=56.97 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 57       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 75500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.197   |\n",
      "|    explained_variance | 0.627    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 1509     |\n",
      "|    policy_loss        | -0.0438  |\n",
      "|    value_loss         | 0.266    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=76000, episode_reward=46.59 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 46.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 76000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.137   |\n",
      "|    explained_variance | 0.907    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 1519     |\n",
      "|    policy_loss        | 0.0326   |\n",
      "|    value_loss         | 0.0673   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=76500, episode_reward=32.17 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 32.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 76500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.231   |\n",
      "|    explained_variance | 0.107    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 1529     |\n",
      "|    policy_loss        | -0.108   |\n",
      "|    value_loss         | 1.23     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=77000, episode_reward=22.83 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 22.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 77000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0584  |\n",
      "|    explained_variance | -0.579   |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 1539     |\n",
      "|    policy_loss        | -0.023   |\n",
      "|    value_loss         | 0.0771   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=77500, episode_reward=77.20 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 77.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 77500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.347   |\n",
      "|    explained_variance | 0.0026   |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 1549     |\n",
      "|    policy_loss        | -0.00698 |\n",
      "|    value_loss         | 0.0149   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=78000, episode_reward=39.33 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 39.3     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 78000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0419  |\n",
      "|    explained_variance | 0.415    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 1559     |\n",
      "|    policy_loss        | -0.164   |\n",
      "|    value_loss         | 5.57     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=78500, episode_reward=69.17 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 69.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 78500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.238   |\n",
      "|    explained_variance | 0.154    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 1569     |\n",
      "|    policy_loss        | -0.0604  |\n",
      "|    value_loss         | 0.0297   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=79000, episode_reward=62.35 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 62.3     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 79000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.215   |\n",
      "|    explained_variance | 0.762    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 1579     |\n",
      "|    policy_loss        | -0.00932 |\n",
      "|    value_loss         | 0.114    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=79500, episode_reward=44.51 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 44.5     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 79500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.238   |\n",
      "|    explained_variance | 0.727    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 1589     |\n",
      "|    policy_loss        | 0.019    |\n",
      "|    value_loss         | 0.0406   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=80000, episode_reward=65.58 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 65.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 80000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.312   |\n",
      "|    explained_variance | 0.702    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 1599     |\n",
      "|    policy_loss        | 0.0683   |\n",
      "|    value_loss         | 1.14     |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 38.5     |\n",
      "| time/              |          |\n",
      "|    fps             | 2052     |\n",
      "|    iterations      | 1600     |\n",
      "|    time_elapsed    | 38       |\n",
      "|    total_timesteps | 80000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=80500, episode_reward=77.11 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 77.1     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 80500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.33    |\n",
      "|    explained_variance | -0.339   |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 1609     |\n",
      "|    policy_loss        | -0.308   |\n",
      "|    value_loss         | 1.54     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=81000, episode_reward=53.57 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 53.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 81000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.273   |\n",
      "|    explained_variance | 0.426    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 1619     |\n",
      "|    policy_loss        | 0.0413   |\n",
      "|    value_loss         | 0.0588   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=81500, episode_reward=88.71 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 88.7     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 81500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0126  |\n",
      "|    explained_variance | 0.199    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 1629     |\n",
      "|    policy_loss        | -0.106   |\n",
      "|    value_loss         | 0.225    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=82000, episode_reward=86.25 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 230       |\n",
      "|    mean_reward        | 86.2      |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 82000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.246    |\n",
      "|    explained_variance | 0.464     |\n",
      "|    learning_rate      | 0.000687  |\n",
      "|    n_updates          | 1639      |\n",
      "|    policy_loss        | -0.000816 |\n",
      "|    value_loss         | 0.0518    |\n",
      "-------------------------------------\n",
      "Eval num_timesteps=82500, episode_reward=46.26 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 46.3     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 82500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0234  |\n",
      "|    explained_variance | 0.455    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 1649     |\n",
      "|    policy_loss        | -0.224   |\n",
      "|    value_loss         | 0.245    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=83000, episode_reward=49.26 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 49.3     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 83000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.169   |\n",
      "|    explained_variance | 0.489    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 1659     |\n",
      "|    policy_loss        | 0.0513   |\n",
      "|    value_loss         | 0.0473   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=83500, episode_reward=55.14 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 230       |\n",
      "|    mean_reward        | 55.1      |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 83500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.158    |\n",
      "|    explained_variance | 0.534     |\n",
      "|    learning_rate      | 0.000687  |\n",
      "|    n_updates          | 1669      |\n",
      "|    policy_loss        | -0.000471 |\n",
      "|    value_loss         | 0.504     |\n",
      "-------------------------------------\n",
      "Eval num_timesteps=84000, episode_reward=47.94 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 47.9     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 84000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0998  |\n",
      "|    explained_variance | 0.892    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 1679     |\n",
      "|    policy_loss        | 0.00556  |\n",
      "|    value_loss         | 0.0844   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=84500, episode_reward=63.68 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 63.7     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 84500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.172   |\n",
      "|    explained_variance | 0.523    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 1689     |\n",
      "|    policy_loss        | 0.0914   |\n",
      "|    value_loss         | 0.448    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=85000, episode_reward=67.70 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 230       |\n",
      "|    mean_reward        | 67.7      |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 85000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.0209   |\n",
      "|    explained_variance | -6.69     |\n",
      "|    learning_rate      | 0.000687  |\n",
      "|    n_updates          | 1699      |\n",
      "|    policy_loss        | -0.000139 |\n",
      "|    value_loss         | 0.0129    |\n",
      "-------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 39.5     |\n",
      "| time/              |          |\n",
      "|    fps             | 2052     |\n",
      "|    iterations      | 1700     |\n",
      "|    time_elapsed    | 41       |\n",
      "|    total_timesteps | 85000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=85500, episode_reward=51.28 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 51.3     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 85500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.145   |\n",
      "|    explained_variance | 0.284    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 1709     |\n",
      "|    policy_loss        | -0.00906 |\n",
      "|    value_loss         | 0.0139   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=86000, episode_reward=55.58 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 55.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 86000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0165  |\n",
      "|    explained_variance | 0.634    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 1719     |\n",
      "|    policy_loss        | -0.0319  |\n",
      "|    value_loss         | 2.29     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=86500, episode_reward=28.30 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 28.3     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 86500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.201   |\n",
      "|    explained_variance | 0.365    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 1729     |\n",
      "|    policy_loss        | -0.00339 |\n",
      "|    value_loss         | 0.0158   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=87000, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 0        |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 87000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.203   |\n",
      "|    explained_variance | 0.685    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 1739     |\n",
      "|    policy_loss        | -0.0387  |\n",
      "|    value_loss         | 0.119    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=87500, episode_reward=88.24 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 88.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 87500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.201   |\n",
      "|    explained_variance | 0.438    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 1749     |\n",
      "|    policy_loss        | 0.0849   |\n",
      "|    value_loss         | 0.0941   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=88000, episode_reward=38.71 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 38.7     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 88000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.289   |\n",
      "|    explained_variance | 0.563    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 1759     |\n",
      "|    policy_loss        | 0.0622   |\n",
      "|    value_loss         | 1.82     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=88500, episode_reward=42.98 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 43       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 88500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.274   |\n",
      "|    explained_variance | 0.414    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 1769     |\n",
      "|    policy_loss        | 0.238    |\n",
      "|    value_loss         | 1.12     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=89000, episode_reward=59.84 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 59.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 89000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.387   |\n",
      "|    explained_variance | 0.56     |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 1779     |\n",
      "|    policy_loss        | -0.00455 |\n",
      "|    value_loss         | 0.0399   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=89500, episode_reward=68.72 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 68.7     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 89500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.00241 |\n",
      "|    explained_variance | -1.14    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 1789     |\n",
      "|    policy_loss        | 6.08e-05 |\n",
      "|    value_loss         | 0.0233   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=90000, episode_reward=47.69 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 47.7     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 90000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.246   |\n",
      "|    explained_variance | 0.663    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 1799     |\n",
      "|    policy_loss        | -0.00122 |\n",
      "|    value_loss         | 0.0442   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 40.8     |\n",
      "| time/              |          |\n",
      "|    fps             | 2051     |\n",
      "|    iterations      | 1800     |\n",
      "|    time_elapsed    | 43       |\n",
      "|    total_timesteps | 90000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=90500, episode_reward=73.17 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 73.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 90500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.028   |\n",
      "|    explained_variance | -0.245   |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 1809     |\n",
      "|    policy_loss        | 0.00599  |\n",
      "|    value_loss         | 0.107    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=91000, episode_reward=43.89 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 43.9     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 91000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.182   |\n",
      "|    explained_variance | -0.803   |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 1819     |\n",
      "|    policy_loss        | -0.0629  |\n",
      "|    value_loss         | 0.0857   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=91500, episode_reward=54.27 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 54.3     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 91500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.177   |\n",
      "|    explained_variance | 0.488    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 1829     |\n",
      "|    policy_loss        | 0.0833   |\n",
      "|    value_loss         | 0.645    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=92000, episode_reward=61.21 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 61.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 92000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.118   |\n",
      "|    explained_variance | 0.912    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 1839     |\n",
      "|    policy_loss        | -0.0317  |\n",
      "|    value_loss         | 0.0596   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=92500, episode_reward=59.86 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 59.9     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 92500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.202   |\n",
      "|    explained_variance | 0.609    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 1849     |\n",
      "|    policy_loss        | 0.138    |\n",
      "|    value_loss         | 0.852    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=93000, episode_reward=64.04 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 64       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 93000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0495  |\n",
      "|    explained_variance | 0.493    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 1859     |\n",
      "|    policy_loss        | 0.000448 |\n",
      "|    value_loss         | 0.0361   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=93500, episode_reward=7.17 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 7.17     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 93500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.201   |\n",
      "|    explained_variance | 0.367    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 1869     |\n",
      "|    policy_loss        | -0.00304 |\n",
      "|    value_loss         | 0.0122   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=94000, episode_reward=45.84 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 45.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 94000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0516  |\n",
      "|    explained_variance | -0.0816  |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 1879     |\n",
      "|    policy_loss        | -0.167   |\n",
      "|    value_loss         | 9.64     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=94500, episode_reward=46.50 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 46.5     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 94500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.147   |\n",
      "|    explained_variance | 0.764    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 1889     |\n",
      "|    policy_loss        | -0.00683 |\n",
      "|    value_loss         | 0.00823  |\n",
      "------------------------------------\n",
      "Eval num_timesteps=95000, episode_reward=51.75 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 51.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 95000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.193   |\n",
      "|    explained_variance | 0.814    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 1899     |\n",
      "|    policy_loss        | -0.0175  |\n",
      "|    value_loss         | 0.146    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 42.3     |\n",
      "| time/              |          |\n",
      "|    fps             | 2051     |\n",
      "|    iterations      | 1900     |\n",
      "|    time_elapsed    | 46       |\n",
      "|    total_timesteps | 95000    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=95500, episode_reward=69.01 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 69       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 95500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.175   |\n",
      "|    explained_variance | 0.788    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 1909     |\n",
      "|    policy_loss        | -0.00164 |\n",
      "|    value_loss         | 0.053    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=96000, episode_reward=79.73 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 79.7     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 96000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.297   |\n",
      "|    explained_variance | 0.809    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 1919     |\n",
      "|    policy_loss        | -0.0456  |\n",
      "|    value_loss         | 0.74     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=96500, episode_reward=40.28 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 40.3     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 96500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.231   |\n",
      "|    explained_variance | 0.378    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 1929     |\n",
      "|    policy_loss        | 0.043    |\n",
      "|    value_loss         | 0.564    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=97000, episode_reward=6.91 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 6.91     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 97000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.237   |\n",
      "|    explained_variance | 0.472    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 1939     |\n",
      "|    policy_loss        | 0.0604   |\n",
      "|    value_loss         | 0.04     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=97500, episode_reward=7.12 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 7.12     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 97500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0031  |\n",
      "|    explained_variance | -4.76    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 1949     |\n",
      "|    policy_loss        | 1.56e-05 |\n",
      "|    value_loss         | 0.0138   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=98000, episode_reward=29.75 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 29.7     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 98000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.192   |\n",
      "|    explained_variance | 0.359    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 1959     |\n",
      "|    policy_loss        | 0.0212   |\n",
      "|    value_loss         | 0.0545   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=98500, episode_reward=34.39 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 34.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 98500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0376  |\n",
      "|    explained_variance | -0.0152  |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 1969     |\n",
      "|    policy_loss        | 0.0257   |\n",
      "|    value_loss         | 0.146    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=99000, episode_reward=67.32 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 67.3     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 99000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.187   |\n",
      "|    explained_variance | 0.614    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 1979     |\n",
      "|    policy_loss        | 0.0256   |\n",
      "|    value_loss         | 0.0327   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=99500, episode_reward=52.28 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 52.3     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 99500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.145   |\n",
      "|    explained_variance | 0.874    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 1989     |\n",
      "|    policy_loss        | 0.0347   |\n",
      "|    value_loss         | 0.127    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=100000, episode_reward=55.28 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 55.3     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 100000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.131   |\n",
      "|    explained_variance | 0.948    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 1999     |\n",
      "|    policy_loss        | -0.0161  |\n",
      "|    value_loss         | 0.041    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 45       |\n",
      "| time/              |          |\n",
      "|    fps             | 2050     |\n",
      "|    iterations      | 2000     |\n",
      "|    time_elapsed    | 48       |\n",
      "|    total_timesteps | 100000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=100500, episode_reward=43.87 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 43.9     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 100500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.188   |\n",
      "|    explained_variance | 0.564    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 2009     |\n",
      "|    policy_loss        | 0.104    |\n",
      "|    value_loss         | 0.176    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=101000, episode_reward=34.37 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 34.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 101000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0265  |\n",
      "|    explained_variance | 0.654    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 2019     |\n",
      "|    policy_loss        | 0.00113  |\n",
      "|    value_loss         | 0.0395   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=101500, episode_reward=55.49 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 55.5     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 101500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0869  |\n",
      "|    explained_variance | 0.172    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 2029     |\n",
      "|    policy_loss        | -0.00498 |\n",
      "|    value_loss         | 0.00597  |\n",
      "------------------------------------\n",
      "Eval num_timesteps=102000, episode_reward=43.04 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 43       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 102000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0169  |\n",
      "|    explained_variance | 0.761    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 2039     |\n",
      "|    policy_loss        | -0.135   |\n",
      "|    value_loss         | 4.4      |\n",
      "------------------------------------\n",
      "Eval num_timesteps=102500, episode_reward=38.56 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 38.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 102500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.182   |\n",
      "|    explained_variance | 0.0989   |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 2049     |\n",
      "|    policy_loss        | -0.0104  |\n",
      "|    value_loss         | 0.0259   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=103000, episode_reward=44.28 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 44.3     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 103000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.14    |\n",
      "|    explained_variance | 0.833    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 2059     |\n",
      "|    policy_loss        | -0.0154  |\n",
      "|    value_loss         | 0.095    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=103500, episode_reward=40.65 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 40.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 103500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.199   |\n",
      "|    explained_variance | 0.323    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 2069     |\n",
      "|    policy_loss        | 0.0627   |\n",
      "|    value_loss         | 0.165    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=104000, episode_reward=24.03 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 24       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 104000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.199   |\n",
      "|    explained_variance | 0.876    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 2079     |\n",
      "|    policy_loss        | 0.0784   |\n",
      "|    value_loss         | 0.558    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=104500, episode_reward=-7.11 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | -7.11    |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 104500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.177   |\n",
      "|    explained_variance | 0.301    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 2089     |\n",
      "|    policy_loss        | -0.0552  |\n",
      "|    value_loss         | 0.689    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=105000, episode_reward=32.15 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 32.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 105000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.147   |\n",
      "|    explained_variance | 0.271    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 2099     |\n",
      "|    policy_loss        | 0.03     |\n",
      "|    value_loss         | 0.157    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 46.8     |\n",
      "| time/              |          |\n",
      "|    fps             | 2050     |\n",
      "|    iterations      | 2100     |\n",
      "|    time_elapsed    | 51       |\n",
      "|    total_timesteps | 105000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=105500, episode_reward=-5.93 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | -5.93    |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 105500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0136  |\n",
      "|    explained_variance | -2.27    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 2109     |\n",
      "|    policy_loss        | 0.000264 |\n",
      "|    value_loss         | 0.0204   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=106000, episode_reward=58.39 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 58.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 106000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.126   |\n",
      "|    explained_variance | 0.671    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 2119     |\n",
      "|    policy_loss        | -0.00867 |\n",
      "|    value_loss         | 0.0462   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=106500, episode_reward=57.18 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 57.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 106500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0188  |\n",
      "|    explained_variance | 0.562    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 2129     |\n",
      "|    policy_loss        | -0.0272  |\n",
      "|    value_loss         | 0.167    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=107000, episode_reward=37.36 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 37.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 107000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.175   |\n",
      "|    explained_variance | 0.255    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 2139     |\n",
      "|    policy_loss        | -0.00714 |\n",
      "|    value_loss         | 0.052    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=107500, episode_reward=52.97 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 53       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 107500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.133   |\n",
      "|    explained_variance | 0.0904   |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 2149     |\n",
      "|    policy_loss        | -0.0415  |\n",
      "|    value_loss         | 0.251    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=108000, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 0        |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 108000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.154   |\n",
      "|    explained_variance | 0.894    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 2159     |\n",
      "|    policy_loss        | -0.00649 |\n",
      "|    value_loss         | 0.0551   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=108500, episode_reward=92.83 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 92.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 108500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0631  |\n",
      "|    explained_variance | -0.102   |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 2169     |\n",
      "|    policy_loss        | -0.0131  |\n",
      "|    value_loss         | 0.427    |\n",
      "------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=109000, episode_reward=63.41 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 63.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 109000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0592  |\n",
      "|    explained_variance | 0.475    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 2179     |\n",
      "|    policy_loss        | 0.0311   |\n",
      "|    value_loss         | 0.323    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=109500, episode_reward=58.72 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 58.7     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 109500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.351   |\n",
      "|    explained_variance | -0.0071  |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 2189     |\n",
      "|    policy_loss        | 0.01     |\n",
      "|    value_loss         | 0.0315   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=110000, episode_reward=59.69 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 59.7     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 110000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0045  |\n",
      "|    explained_variance | 0.983    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 2199     |\n",
      "|    policy_loss        | 0.000169 |\n",
      "|    value_loss         | 0.0506   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 48.4     |\n",
      "| time/              |          |\n",
      "|    fps             | 2052     |\n",
      "|    iterations      | 2200     |\n",
      "|    time_elapsed    | 53       |\n",
      "|    total_timesteps | 110000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=110500, episode_reward=47.59 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 47.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 110500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.196   |\n",
      "|    explained_variance | -0.2     |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 2209     |\n",
      "|    policy_loss        | 0.00958  |\n",
      "|    value_loss         | 0.0293   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=111000, episode_reward=48.85 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 48.9     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 111000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.162   |\n",
      "|    explained_variance | -0.0433  |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 2219     |\n",
      "|    policy_loss        | 0.0236   |\n",
      "|    value_loss         | 0.526    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=111500, episode_reward=58.38 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 58.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 111500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.181   |\n",
      "|    explained_variance | 0.285    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 2229     |\n",
      "|    policy_loss        | 0.00832  |\n",
      "|    value_loss         | 0.138    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=112000, episode_reward=70.78 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 70.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 112000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.16    |\n",
      "|    explained_variance | 0.829    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 2239     |\n",
      "|    policy_loss        | -0.0224  |\n",
      "|    value_loss         | 0.577    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=112500, episode_reward=34.67 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 34.7     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 112500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.261   |\n",
      "|    explained_variance | 0.512    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 2249     |\n",
      "|    policy_loss        | 0.16     |\n",
      "|    value_loss         | 0.892    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=113000, episode_reward=54.52 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 54.5     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 113000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.261   |\n",
      "|    explained_variance | 0.253    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 2259     |\n",
      "|    policy_loss        | 0.0268   |\n",
      "|    value_loss         | 0.0428   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=113500, episode_reward=58.66 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 58.7     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 113500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.00297 |\n",
      "|    explained_variance | -2.43    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 2269     |\n",
      "|    policy_loss        | 4.35e-05 |\n",
      "|    value_loss         | 0.0124   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=114000, episode_reward=52.36 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 52.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 114000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.174   |\n",
      "|    explained_variance | 0.71     |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 2279     |\n",
      "|    policy_loss        | 0.0158   |\n",
      "|    value_loss         | 0.0328   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=114500, episode_reward=53.65 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 53.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 114500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0303  |\n",
      "|    explained_variance | -2.64    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 2289     |\n",
      "|    policy_loss        | 0.118    |\n",
      "|    value_loss         | 1.07     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=115000, episode_reward=42.55 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 42.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 115000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.186   |\n",
      "|    explained_variance | 0.791    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 2299     |\n",
      "|    policy_loss        | 0.00609  |\n",
      "|    value_loss         | 0.03     |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 49.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 2051     |\n",
      "|    iterations      | 2300     |\n",
      "|    time_elapsed    | 56       |\n",
      "|    total_timesteps | 115000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=115500, episode_reward=39.28 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 39.3     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 115500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.159   |\n",
      "|    explained_variance | 0.774    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 2309     |\n",
      "|    policy_loss        | -0.013   |\n",
      "|    value_loss         | 0.136    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=116000, episode_reward=74.10 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 74.1     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 116000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.106   |\n",
      "|    explained_variance | 0.948    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 2319     |\n",
      "|    policy_loss        | -0.00396 |\n",
      "|    value_loss         | 0.0282   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=116500, episode_reward=51.19 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 51.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 116500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.167   |\n",
      "|    explained_variance | 0.715    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 2329     |\n",
      "|    policy_loss        | 0.0197   |\n",
      "|    value_loss         | 0.395    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=117000, episode_reward=41.19 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 41.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 117000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0683  |\n",
      "|    explained_variance | 0.895    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 2339     |\n",
      "|    policy_loss        | 0.0268   |\n",
      "|    value_loss         | 0.0341   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=117500, episode_reward=43.64 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 43.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 117500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.257   |\n",
      "|    explained_variance | -0.0726  |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 2349     |\n",
      "|    policy_loss        | -0.0506  |\n",
      "|    value_loss         | 0.0346   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=118000, episode_reward=55.59 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 55.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 118000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.00181 |\n",
      "|    explained_variance | 0.979    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 2359     |\n",
      "|    policy_loss        | 0.000116 |\n",
      "|    value_loss         | 0.0694   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=118500, episode_reward=41.31 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 41.3     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 118500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.187   |\n",
      "|    explained_variance | 0.614    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 2369     |\n",
      "|    policy_loss        | 0.0222   |\n",
      "|    value_loss         | 0.0252   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=119000, episode_reward=30.50 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 30.5     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 119000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.136   |\n",
      "|    explained_variance | 0.755    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 2379     |\n",
      "|    policy_loss        | 0.0207   |\n",
      "|    value_loss         | 0.203    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=119500, episode_reward=33.87 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 33.9     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 119500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.279   |\n",
      "|    explained_variance | 0.554    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 2389     |\n",
      "|    policy_loss        | -0.0638  |\n",
      "|    value_loss         | 0.0917   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=120000, episode_reward=49.85 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 49.9     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 120000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.2     |\n",
      "|    explained_variance | 0.921    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 2399     |\n",
      "|    policy_loss        | 0.012    |\n",
      "|    value_loss         | 0.42     |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 51.4     |\n",
      "| time/              |          |\n",
      "|    fps             | 2050     |\n",
      "|    iterations      | 2400     |\n",
      "|    time_elapsed    | 58       |\n",
      "|    total_timesteps | 120000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=120500, episode_reward=86.72 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 86.7     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 120500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.241   |\n",
      "|    explained_variance | 0.377    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 2409     |\n",
      "|    policy_loss        | -0.134   |\n",
      "|    value_loss         | 0.345    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=121000, episode_reward=8.64 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 8.64     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 121000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.159   |\n",
      "|    explained_variance | 0.582    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 2419     |\n",
      "|    policy_loss        | 0.00488  |\n",
      "|    value_loss         | 0.0225   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=121500, episode_reward=33.55 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 33.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 121500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.00124 |\n",
      "|    explained_variance | -2.16    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 2429     |\n",
      "|    policy_loss        | 1.24e-06 |\n",
      "|    value_loss         | 0.0461   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=122000, episode_reward=43.18 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 43.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 122000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.152   |\n",
      "|    explained_variance | 0.621    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 2439     |\n",
      "|    policy_loss        | 0.021    |\n",
      "|    value_loss         | 0.0402   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=122500, episode_reward=42.12 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 42.1     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 122500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0165  |\n",
      "|    explained_variance | 0.508    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 2449     |\n",
      "|    policy_loss        | -0.00618 |\n",
      "|    value_loss         | 4.28     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=123000, episode_reward=23.53 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 23.5     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 123000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.181   |\n",
      "|    explained_variance | 0.451    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 2459     |\n",
      "|    policy_loss        | -0.0465  |\n",
      "|    value_loss         | 0.0796   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=123500, episode_reward=65.42 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 65.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 123500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.182   |\n",
      "|    explained_variance | 0.685    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 2469     |\n",
      "|    policy_loss        | 0.0678   |\n",
      "|    value_loss         | 0.237    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=124000, episode_reward=60.41 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 60.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 124000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0937  |\n",
      "|    explained_variance | 0.973    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 2479     |\n",
      "|    policy_loss        | -0.00205 |\n",
      "|    value_loss         | 0.0164   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=124500, episode_reward=16.76 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 16.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 124500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.197   |\n",
      "|    explained_variance | 0.726    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 2489     |\n",
      "|    policy_loss        | 0.0118   |\n",
      "|    value_loss         | 0.333    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=125000, episode_reward=59.15 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 59.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 125000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0598  |\n",
      "|    explained_variance | -0.662   |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 2499     |\n",
      "|    policy_loss        | -0.0179  |\n",
      "|    value_loss         | 0.1      |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 53.2     |\n",
      "| time/              |          |\n",
      "|    fps             | 2048     |\n",
      "|    iterations      | 2500     |\n",
      "|    time_elapsed    | 61       |\n",
      "|    total_timesteps | 125000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=125500, episode_reward=49.40 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 49.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 125500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.293   |\n",
      "|    explained_variance | 0.354    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 2509     |\n",
      "|    policy_loss        | 0.0683   |\n",
      "|    value_loss         | 0.0397   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=126000, episode_reward=82.18 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 82.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 126000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.00179 |\n",
      "|    explained_variance | 0.966    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 2519     |\n",
      "|    policy_loss        | -8.7e-05 |\n",
      "|    value_loss         | 0.0514   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=126500, episode_reward=12.12 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 12.1     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 126500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.159   |\n",
      "|    explained_variance | 0.184    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 2529     |\n",
      "|    policy_loss        | 0.0179   |\n",
      "|    value_loss         | 0.0274   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=127000, episode_reward=85.06 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 85.1     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 127000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.121   |\n",
      "|    explained_variance | 0.917    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 2539     |\n",
      "|    policy_loss        | 0.00562  |\n",
      "|    value_loss         | 0.054    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=127500, episode_reward=60.98 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 61       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 127500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.148   |\n",
      "|    explained_variance | 0.739    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 2549     |\n",
      "|    policy_loss        | -0.0224  |\n",
      "|    value_loss         | 0.0532   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=128000, episode_reward=46.59 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 46.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 128000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.192   |\n",
      "|    explained_variance | 0.787    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 2559     |\n",
      "|    policy_loss        | 0.0743   |\n",
      "|    value_loss         | 0.951    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=128500, episode_reward=42.32 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 42.3     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 128500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.217   |\n",
      "|    explained_variance | 0.347    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 2569     |\n",
      "|    policy_loss        | -0.093   |\n",
      "|    value_loss         | 0.304    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=129000, episode_reward=95.79 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 95.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 129000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.308   |\n",
      "|    explained_variance | 0.165    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 2579     |\n",
      "|    policy_loss        | 0.0341   |\n",
      "|    value_loss         | 0.0868   |\n",
      "------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=129500, episode_reward=92.78 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 230       |\n",
      "|    mean_reward        | 92.8      |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 129500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.000878 |\n",
      "|    explained_variance | -0.296    |\n",
      "|    learning_rate      | 0.000687  |\n",
      "|    n_updates          | 2589      |\n",
      "|    policy_loss        | 7.64e-06  |\n",
      "|    value_loss         | 0.00947   |\n",
      "-------------------------------------\n",
      "Eval num_timesteps=130000, episode_reward=36.50 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 36.5     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 130000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.187   |\n",
      "|    explained_variance | 0.657    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 2599     |\n",
      "|    policy_loss        | -0.0123  |\n",
      "|    value_loss         | 0.0352   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 54.4     |\n",
      "| time/              |          |\n",
      "|    fps             | 2046     |\n",
      "|    iterations      | 2600     |\n",
      "|    time_elapsed    | 63       |\n",
      "|    total_timesteps | 130000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=130500, episode_reward=74.78 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 74.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 130500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0174  |\n",
      "|    explained_variance | 0.753    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 2609     |\n",
      "|    policy_loss        | -0.116   |\n",
      "|    value_loss         | 2.01     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=131000, episode_reward=86.42 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 86.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 131000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.147   |\n",
      "|    explained_variance | 0.581    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 2619     |\n",
      "|    policy_loss        | 0.031    |\n",
      "|    value_loss         | 0.0321   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=131500, episode_reward=76.06 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 76.1     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 131500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.165   |\n",
      "|    explained_variance | 0.656    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 2629     |\n",
      "|    policy_loss        | 0.0198   |\n",
      "|    value_loss         | 0.234    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=132000, episode_reward=14.58 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 14.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 132000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.116   |\n",
      "|    explained_variance | 0.95     |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 2639     |\n",
      "|    policy_loss        | -0.0031  |\n",
      "|    value_loss         | 0.0328   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=132500, episode_reward=58.44 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 58.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 132500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.178   |\n",
      "|    explained_variance | 0.219    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 2649     |\n",
      "|    policy_loss        | -0.0173  |\n",
      "|    value_loss         | 0.625    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=133000, episode_reward=57.12 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 57.1     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 133000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0322  |\n",
      "|    explained_variance | 0.696    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 2659     |\n",
      "|    policy_loss        | -0.0227  |\n",
      "|    value_loss         | 0.0621   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=133500, episode_reward=41.22 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 41.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 133500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.17    |\n",
      "|    explained_variance | -0.245   |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 2669     |\n",
      "|    policy_loss        | -0.0249  |\n",
      "|    value_loss         | 0.0167   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=134000, episode_reward=43.10 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 230       |\n",
      "|    mean_reward        | 43.1      |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 134000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.000374 |\n",
      "|    explained_variance | 0.957     |\n",
      "|    learning_rate      | 0.000687  |\n",
      "|    n_updates          | 2679      |\n",
      "|    policy_loss        | 9.62e-06  |\n",
      "|    value_loss         | 0.0391    |\n",
      "-------------------------------------\n",
      "Eval num_timesteps=134500, episode_reward=61.42 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 61.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 134500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.177   |\n",
      "|    explained_variance | 0.551    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 2689     |\n",
      "|    policy_loss        | -0.0196  |\n",
      "|    value_loss         | 0.0336   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=135000, episode_reward=56.68 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 56.7     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 135000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0769  |\n",
      "|    explained_variance | 0.114    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 2699     |\n",
      "|    policy_loss        | -0.0257  |\n",
      "|    value_loss         | 7.04     |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 55.2     |\n",
      "| time/              |          |\n",
      "|    fps             | 2045     |\n",
      "|    iterations      | 2700     |\n",
      "|    time_elapsed    | 66       |\n",
      "|    total_timesteps | 135000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=135500, episode_reward=83.68 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 83.7     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 135500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.164   |\n",
      "|    explained_variance | 0.267    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 2709     |\n",
      "|    policy_loss        | -0.0119  |\n",
      "|    value_loss         | 0.0513   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=136000, episode_reward=56.06 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 56.1     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 136000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.13    |\n",
      "|    explained_variance | 0.642    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 2719     |\n",
      "|    policy_loss        | -0.129   |\n",
      "|    value_loss         | 1.91     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=136500, episode_reward=52.41 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 52.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 136500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.166   |\n",
      "|    explained_variance | 0.333    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 2729     |\n",
      "|    policy_loss        | -0.00834 |\n",
      "|    value_loss         | 0.331    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=137000, episode_reward=81.55 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 81.5     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 137000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.253   |\n",
      "|    explained_variance | 0.168    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 2739     |\n",
      "|    policy_loss        | 0.0475   |\n",
      "|    value_loss         | 0.0812   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=137500, episode_reward=84.46 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 230       |\n",
      "|    mean_reward        | 84.5      |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 137500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.000868 |\n",
      "|    explained_variance | -0.403    |\n",
      "|    learning_rate      | 0.000687  |\n",
      "|    n_updates          | 2749      |\n",
      "|    policy_loss        | 3.01e-07  |\n",
      "|    value_loss         | 0.0116    |\n",
      "-------------------------------------\n",
      "Eval num_timesteps=138000, episode_reward=87.58 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 87.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 138000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.175   |\n",
      "|    explained_variance | 0.355    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 2759     |\n",
      "|    policy_loss        | -0.0381  |\n",
      "|    value_loss         | 0.0468   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=138500, episode_reward=79.15 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 230       |\n",
      "|    mean_reward        | 79.1      |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 138500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.00438  |\n",
      "|    explained_variance | -5.19     |\n",
      "|    learning_rate      | 0.000687  |\n",
      "|    n_updates          | 2769      |\n",
      "|    policy_loss        | -9.23e-05 |\n",
      "|    value_loss         | 0.0975    |\n",
      "-------------------------------------\n",
      "Eval num_timesteps=139000, episode_reward=77.46 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 230       |\n",
      "|    mean_reward        | 77.5      |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 139000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.129    |\n",
      "|    explained_variance | 0.587     |\n",
      "|    learning_rate      | 0.000687  |\n",
      "|    n_updates          | 2779      |\n",
      "|    policy_loss        | -0.000866 |\n",
      "|    value_loss         | 0.0337    |\n",
      "-------------------------------------\n",
      "Eval num_timesteps=139500, episode_reward=88.06 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 88.1     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 139500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.131   |\n",
      "|    explained_variance | 0.783    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 2789     |\n",
      "|    policy_loss        | -0.0103  |\n",
      "|    value_loss         | 0.176    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=140000, episode_reward=97.10 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 97.1     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 140000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.108   |\n",
      "|    explained_variance | 0.965    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 2799     |\n",
      "|    policy_loss        | 0.0122   |\n",
      "|    value_loss         | 0.0224   |\n",
      "------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 55.8     |\n",
      "| time/              |          |\n",
      "|    fps             | 2045     |\n",
      "|    iterations      | 2800     |\n",
      "|    time_elapsed    | 68       |\n",
      "|    total_timesteps | 140000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=140500, episode_reward=65.42 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 65.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 140500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.292   |\n",
      "|    explained_variance | 0.705    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 2809     |\n",
      "|    policy_loss        | -0.0529  |\n",
      "|    value_loss         | 0.249    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=141000, episode_reward=52.35 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 52.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 141000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0595  |\n",
      "|    explained_variance | 0.871    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 2819     |\n",
      "|    policy_loss        | -0.00208 |\n",
      "|    value_loss         | 0.0611   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=141500, episode_reward=53.69 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 53.7     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 141500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.32    |\n",
      "|    explained_variance | -0.144   |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 2829     |\n",
      "|    policy_loss        | 0.00689  |\n",
      "|    value_loss         | 0.0283   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=142000, episode_reward=56.80 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 230       |\n",
      "|    mean_reward        | 56.8      |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 142000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.000656 |\n",
      "|    explained_variance | -1.87     |\n",
      "|    learning_rate      | 0.000687  |\n",
      "|    n_updates          | 2839      |\n",
      "|    policy_loss        | 1.62e-05  |\n",
      "|    value_loss         | 0.0492    |\n",
      "-------------------------------------\n",
      "Eval num_timesteps=142500, episode_reward=96.35 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 96.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 142500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.122   |\n",
      "|    explained_variance | 0.707    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 2849     |\n",
      "|    policy_loss        | 0.00777  |\n",
      "|    value_loss         | 0.0314   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=143000, episode_reward=91.85 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 91.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 143000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0698  |\n",
      "|    explained_variance | 0.224    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 2859     |\n",
      "|    policy_loss        | -0.286   |\n",
      "|    value_loss         | 6.71     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=143500, episode_reward=37.88 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 37.9     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 143500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.107   |\n",
      "|    explained_variance | 0.518    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 2869     |\n",
      "|    policy_loss        | -0.0147  |\n",
      "|    value_loss         | 0.0173   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=144000, episode_reward=32.42 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 32.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 144000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.169   |\n",
      "|    explained_variance | 0.95     |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 2879     |\n",
      "|    policy_loss        | 0.0182   |\n",
      "|    value_loss         | 0.325    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=144500, episode_reward=1.89 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 1.89     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 144500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.154   |\n",
      "|    explained_variance | 0.473    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 2889     |\n",
      "|    policy_loss        | -0.00126 |\n",
      "|    value_loss         | 0.497    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=145000, episode_reward=16.06 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 16.1     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 145000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.194   |\n",
      "|    explained_variance | 0.485    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 2899     |\n",
      "|    policy_loss        | 0.0607   |\n",
      "|    value_loss         | 0.0898   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 56.5     |\n",
      "| time/              |          |\n",
      "|    fps             | 2043     |\n",
      "|    iterations      | 2900     |\n",
      "|    time_elapsed    | 70       |\n",
      "|    total_timesteps | 145000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=145500, episode_reward=0.43 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 0.433    |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 145500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.00436 |\n",
      "|    explained_variance | -4.98    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 2909     |\n",
      "|    policy_loss        | 1.11e-06 |\n",
      "|    value_loss         | 0.00928  |\n",
      "------------------------------------\n",
      "Eval num_timesteps=146000, episode_reward=22.09 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 22.1     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 146000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.224   |\n",
      "|    explained_variance | 0.638    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 2919     |\n",
      "|    policy_loss        | 0.0154   |\n",
      "|    value_loss         | 0.0326   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=146500, episode_reward=54.72 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 230       |\n",
      "|    mean_reward        | 54.7      |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 146500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.0017   |\n",
      "|    explained_variance | -1.12     |\n",
      "|    learning_rate      | 0.000687  |\n",
      "|    n_updates          | 2929      |\n",
      "|    policy_loss        | -4.23e-05 |\n",
      "|    value_loss         | 0.0327    |\n",
      "-------------------------------------\n",
      "Eval num_timesteps=147000, episode_reward=27.53 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 27.5     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 147000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.114   |\n",
      "|    explained_variance | 0.087    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 2939     |\n",
      "|    policy_loss        | -0.00561 |\n",
      "|    value_loss         | 0.0413   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=147500, episode_reward=27.49 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 27.5     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 147500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.131   |\n",
      "|    explained_variance | 0.913    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 2949     |\n",
      "|    policy_loss        | -0.0108  |\n",
      "|    value_loss         | 0.0938   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=148000, episode_reward=46.52 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 46.5     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 148000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.11    |\n",
      "|    explained_variance | 0.966    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 2959     |\n",
      "|    policy_loss        | -0.00614 |\n",
      "|    value_loss         | 0.0244   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=148500, episode_reward=40.74 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 40.7     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 148500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.148   |\n",
      "|    explained_variance | 0.453    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 2969     |\n",
      "|    policy_loss        | -0.186   |\n",
      "|    value_loss         | 0.491    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=149000, episode_reward=40.33 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 40.3     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 149000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0713  |\n",
      "|    explained_variance | 0.94     |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 2979     |\n",
      "|    policy_loss        | 3.81e-05 |\n",
      "|    value_loss         | 0.0422   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=149500, episode_reward=34.83 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 34.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 149500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.221   |\n",
      "|    explained_variance | -0.254   |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 2989     |\n",
      "|    policy_loss        | -0.0149  |\n",
      "|    value_loss         | 0.023    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=150000, episode_reward=33.23 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 230       |\n",
      "|    mean_reward        | 33.2      |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 150000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.000413 |\n",
      "|    explained_variance | -0.684    |\n",
      "|    learning_rate      | 0.000687  |\n",
      "|    n_updates          | 2999      |\n",
      "|    policy_loss        | -4.74e-06 |\n",
      "|    value_loss         | 0.0302    |\n",
      "-------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 57.3     |\n",
      "| time/              |          |\n",
      "|    fps             | 2042     |\n",
      "|    iterations      | 3000     |\n",
      "|    time_elapsed    | 73       |\n",
      "|    total_timesteps | 150000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=150500, episode_reward=13.57 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 13.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 150500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.189   |\n",
      "|    explained_variance | 0.721    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 3009     |\n",
      "|    policy_loss        | 0.0167   |\n",
      "|    value_loss         | 0.0263   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=151000, episode_reward=5.56 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 5.56     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 151000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0539  |\n",
      "|    explained_variance | 0.541    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 3019     |\n",
      "|    policy_loss        | 0.00414  |\n",
      "|    value_loss         | 4.53     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=151500, episode_reward=86.58 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 86.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 151500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0906  |\n",
      "|    explained_variance | 0.606    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 3029     |\n",
      "|    policy_loss        | -0.0429  |\n",
      "|    value_loss         | 0.0525   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=152000, episode_reward=15.87 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 15.9     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 152000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.158   |\n",
      "|    explained_variance | 0.937    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 3039     |\n",
      "|    policy_loss        | -0.0119  |\n",
      "|    value_loss         | 0.253    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=152500, episode_reward=27.00 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 27       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 152500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.153   |\n",
      "|    explained_variance | 0.586    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 3049     |\n",
      "|    policy_loss        | -0.0355  |\n",
      "|    value_loss         | 0.437    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=153000, episode_reward=19.20 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 19.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 153000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.169   |\n",
      "|    explained_variance | 0.424    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 3059     |\n",
      "|    policy_loss        | 0.0367   |\n",
      "|    value_loss         | 0.164    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=153500, episode_reward=36.53 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 36.5     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 153500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0206  |\n",
      "|    explained_variance | 0.334    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 3069     |\n",
      "|    policy_loss        | 0.00402  |\n",
      "|    value_loss         | 0.0397   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=154000, episode_reward=84.04 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 84       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 154000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.18    |\n",
      "|    explained_variance | -0.668   |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 3079     |\n",
      "|    policy_loss        | -0.0145  |\n",
      "|    value_loss         | 0.0548   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=154500, episode_reward=46.43 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 230       |\n",
      "|    mean_reward        | 46.4      |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 154500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.00717  |\n",
      "|    explained_variance | 0.179     |\n",
      "|    learning_rate      | 0.000687  |\n",
      "|    n_updates          | 3089      |\n",
      "|    policy_loss        | -0.000536 |\n",
      "|    value_loss         | 0.0315    |\n",
      "-------------------------------------\n",
      "Eval num_timesteps=155000, episode_reward=61.85 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 61.9     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 155000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.134   |\n",
      "|    explained_variance | 0.401    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 3099     |\n",
      "|    policy_loss        | -0.0496  |\n",
      "|    value_loss         | 0.131    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 57.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 2039     |\n",
      "|    iterations      | 3100     |\n",
      "|    time_elapsed    | 76       |\n",
      "|    total_timesteps | 155000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=155500, episode_reward=70.10 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 70.1     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 155500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0999  |\n",
      "|    explained_variance | 0.672    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 3109     |\n",
      "|    policy_loss        | 0.0512   |\n",
      "|    value_loss         | 0.273    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=156000, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 0        |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 156000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0864  |\n",
      "|    explained_variance | 0.952    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 3119     |\n",
      "|    policy_loss        | 0.0029   |\n",
      "|    value_loss         | 0.0371   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=156500, episode_reward=84.96 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 85       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 156500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.238   |\n",
      "|    explained_variance | -0.208   |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 3129     |\n",
      "|    policy_loss        | 0.00292  |\n",
      "|    value_loss         | 0.403    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=157000, episode_reward=75.25 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 75.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 157000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0942  |\n",
      "|    explained_variance | 0.695    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 3139     |\n",
      "|    policy_loss        | 0.048    |\n",
      "|    value_loss         | 0.267    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=157500, episode_reward=36.81 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 36.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 157500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.424   |\n",
      "|    explained_variance | 0.288    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 3149     |\n",
      "|    policy_loss        | 0.0586   |\n",
      "|    value_loss         | 0.0494   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=158000, episode_reward=57.96 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 230       |\n",
      "|    mean_reward        | 58        |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 158000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.00257  |\n",
      "|    explained_variance | -2.22     |\n",
      "|    learning_rate      | 0.000687  |\n",
      "|    n_updates          | 3159      |\n",
      "|    policy_loss        | -7.05e-05 |\n",
      "|    value_loss         | 0.0161    |\n",
      "-------------------------------------\n",
      "Eval num_timesteps=158500, episode_reward=78.14 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 78.1     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 158500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.115   |\n",
      "|    explained_variance | 0.724    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 3169     |\n",
      "|    policy_loss        | 0.0236   |\n",
      "|    value_loss         | 0.0416   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=159000, episode_reward=85.18 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 85.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 159000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.032   |\n",
      "|    explained_variance | 0.728    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 3179     |\n",
      "|    policy_loss        | 0.00526  |\n",
      "|    value_loss         | 2.82     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=159500, episode_reward=44.32 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 44.3     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 159500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.109   |\n",
      "|    explained_variance | -0.221   |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 3189     |\n",
      "|    policy_loss        | 0.0204   |\n",
      "|    value_loss         | 0.103    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=160000, episode_reward=67.62 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 67.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 160000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.157   |\n",
      "|    explained_variance | 0.286    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 3199     |\n",
      "|    policy_loss        | -0.389   |\n",
      "|    value_loss         | 2.55     |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 58.3     |\n",
      "| time/              |          |\n",
      "|    fps             | 2039     |\n",
      "|    iterations      | 3200     |\n",
      "|    time_elapsed    | 78       |\n",
      "|    total_timesteps | 160000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=160500, episode_reward=79.37 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 79.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 160500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.138   |\n",
      "|    explained_variance | 0.748    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 3209     |\n",
      "|    policy_loss        | 0.039    |\n",
      "|    value_loss         | 0.34     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=161000, episode_reward=89.77 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 89.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 161000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.185   |\n",
      "|    explained_variance | 0.51     |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 3219     |\n",
      "|    policy_loss        | 0.0401   |\n",
      "|    value_loss         | 0.087    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=161500, episode_reward=88.59 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 88.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 161500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0234  |\n",
      "|    explained_variance | -0.633   |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 3229     |\n",
      "|    policy_loss        | 0.00577  |\n",
      "|    value_loss         | 0.0665   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=162000, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 0        |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 162000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.117   |\n",
      "|    explained_variance | 0.273    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 3239     |\n",
      "|    policy_loss        | -0.0184  |\n",
      "|    value_loss         | 0.0286   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=162500, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 0        |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 162500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.00461 |\n",
      "|    explained_variance | -0.744   |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 3249     |\n",
      "|    policy_loss        | 0.000241 |\n",
      "|    value_loss         | 0.102    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=163000, episode_reward=81.88 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 81.9     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 163000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.132   |\n",
      "|    explained_variance | -0.161   |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 3259     |\n",
      "|    policy_loss        | -0.0181  |\n",
      "|    value_loss         | 0.0602   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=163500, episode_reward=36.89 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 36.9     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 163500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.125   |\n",
      "|    explained_variance | 0.892    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 3269     |\n",
      "|    policy_loss        | 0.0196   |\n",
      "|    value_loss         | 0.146    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=164000, episode_reward=45.33 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 45.3     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 164000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0848  |\n",
      "|    explained_variance | 0.954    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 3279     |\n",
      "|    policy_loss        | -0.0238  |\n",
      "|    value_loss         | 0.0366   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=164500, episode_reward=14.82 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 14.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 164500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.137   |\n",
      "|    explained_variance | 0.106    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 3289     |\n",
      "|    policy_loss        | 0.0552   |\n",
      "|    value_loss         | 0.704    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=165000, episode_reward=36.20 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 36.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 165000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.064   |\n",
      "|    explained_variance | 0.12     |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 3299     |\n",
      "|    policy_loss        | -0.034   |\n",
      "|    value_loss         | 0.112    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 58.8     |\n",
      "| time/              |          |\n",
      "|    fps             | 2040     |\n",
      "|    iterations      | 3300     |\n",
      "|    time_elapsed    | 80       |\n",
      "|    total_timesteps | 165000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=165500, episode_reward=54.19 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 54.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 165500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.346   |\n",
      "|    explained_variance | 0.541    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 3309     |\n",
      "|    policy_loss        | -0.0114  |\n",
      "|    value_loss         | 0.0224   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=166000, episode_reward=79.51 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 230       |\n",
      "|    mean_reward        | 79.5      |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 166000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.000267 |\n",
      "|    explained_variance | 0.253     |\n",
      "|    learning_rate      | 0.000687  |\n",
      "|    n_updates          | 3319      |\n",
      "|    policy_loss        | 2.22e-06  |\n",
      "|    value_loss         | 0.0153    |\n",
      "-------------------------------------\n",
      "Eval num_timesteps=166500, episode_reward=23.82 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 23.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 166500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.135   |\n",
      "|    explained_variance | 0.671    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 3329     |\n",
      "|    policy_loss        | 0.00995  |\n",
      "|    value_loss         | 0.0486   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=167000, episode_reward=14.83 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 14.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 167000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0464  |\n",
      "|    explained_variance | 0.782    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 3339     |\n",
      "|    policy_loss        | -0.052   |\n",
      "|    value_loss         | 2.45     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=167500, episode_reward=78.06 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 78.1     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 167500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.215   |\n",
      "|    explained_variance | 0.607    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 3349     |\n",
      "|    policy_loss        | 0.00302  |\n",
      "|    value_loss         | 0.139    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=168000, episode_reward=46.86 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 46.9     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 168000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.125   |\n",
      "|    explained_variance | 0.713    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 3359     |\n",
      "|    policy_loss        | 0.0963   |\n",
      "|    value_loss         | 1.5      |\n",
      "------------------------------------\n",
      "Eval num_timesteps=168500, episode_reward=87.32 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 87.3     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 168500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.141   |\n",
      "|    explained_variance | 0.722    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 3369     |\n",
      "|    policy_loss        | 0.00995  |\n",
      "|    value_loss         | 0.438    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=169000, episode_reward=53.13 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 53.1     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 169000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.187   |\n",
      "|    explained_variance | 0.421    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 3379     |\n",
      "|    policy_loss        | 0.0135   |\n",
      "|    value_loss         | 0.0621   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=169500, episode_reward=44.27 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 44.3     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 169500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0198  |\n",
      "|    explained_variance | 0.716    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 3389     |\n",
      "|    policy_loss        | -0.00199 |\n",
      "|    value_loss         | 0.0194   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=170000, episode_reward=46.77 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 46.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 170000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.195   |\n",
      "|    explained_variance | 0.326    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 3399     |\n",
      "|    policy_loss        | 0.0186   |\n",
      "|    value_loss         | 0.0346   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 59.1     |\n",
      "| time/              |          |\n",
      "|    fps             | 2042     |\n",
      "|    iterations      | 3400     |\n",
      "|    time_elapsed    | 83       |\n",
      "|    total_timesteps | 170000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=170500, episode_reward=77.24 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 230       |\n",
      "|    mean_reward        | 77.2      |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 170500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.00122  |\n",
      "|    explained_variance | 0.511     |\n",
      "|    learning_rate      | 0.000687  |\n",
      "|    n_updates          | 3409      |\n",
      "|    policy_loss        | -1.82e-05 |\n",
      "|    value_loss         | 0.0475    |\n",
      "-------------------------------------\n",
      "Eval num_timesteps=171000, episode_reward=36.37 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 36.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 171000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.131   |\n",
      "|    explained_variance | -0.26    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 3419     |\n",
      "|    policy_loss        | -0.0136  |\n",
      "|    value_loss         | 0.0503   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=171500, episode_reward=20.92 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 20.9     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 171500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.168   |\n",
      "|    explained_variance | 0.852    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 3429     |\n",
      "|    policy_loss        | -0.00909 |\n",
      "|    value_loss         | 0.223    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=172000, episode_reward=59.51 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 59.5     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 172000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0787  |\n",
      "|    explained_variance | 0.935    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 3439     |\n",
      "|    policy_loss        | -0.0165  |\n",
      "|    value_loss         | 0.0426   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=172500, episode_reward=73.01 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 73       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 172500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.211   |\n",
      "|    explained_variance | 0.522    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 3449     |\n",
      "|    policy_loss        | 0.222    |\n",
      "|    value_loss         | 0.754    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=173000, episode_reward=75.33 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 75.3     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 173000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0563  |\n",
      "|    explained_variance | 0.951    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 3459     |\n",
      "|    policy_loss        | 0.00408  |\n",
      "|    value_loss         | 0.0523   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=173500, episode_reward=3.51 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 3.51     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 173500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.267   |\n",
      "|    explained_variance | -0.16    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 3469     |\n",
      "|    policy_loss        | 0.0126   |\n",
      "|    value_loss         | 0.0304   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=174000, episode_reward=59.78 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 230       |\n",
      "|    mean_reward        | 59.8      |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 174000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.43e-05 |\n",
      "|    explained_variance | -1.95     |\n",
      "|    learning_rate      | 0.000687  |\n",
      "|    n_updates          | 3479      |\n",
      "|    policy_loss        | 1.76e-09  |\n",
      "|    value_loss         | 0.0122    |\n",
      "-------------------------------------\n",
      "Eval num_timesteps=174500, episode_reward=88.49 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 88.5     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 174500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.139   |\n",
      "|    explained_variance | 0.737    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 3489     |\n",
      "|    policy_loss        | 0.0119   |\n",
      "|    value_loss         | 0.0339   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=175000, episode_reward=67.00 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 67       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 175000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0438  |\n",
      "|    explained_variance | 0.388    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 3499     |\n",
      "|    policy_loss        | -0.0235  |\n",
      "|    value_loss         | 4.79     |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 59.6     |\n",
      "| time/              |          |\n",
      "|    fps             | 2040     |\n",
      "|    iterations      | 3500     |\n",
      "|    time_elapsed    | 85       |\n",
      "|    total_timesteps | 175000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=175500, episode_reward=83.34 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 83.3     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 175500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.125   |\n",
      "|    explained_variance | 0.701    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 3509     |\n",
      "|    policy_loss        | -0.007   |\n",
      "|    value_loss         | 0.0245   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=176000, episode_reward=8.72 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 8.72     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 176000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.17    |\n",
      "|    explained_variance | 0.928    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 3519     |\n",
      "|    policy_loss        | -0.0399  |\n",
      "|    value_loss         | 0.189    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=176500, episode_reward=59.95 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 230       |\n",
      "|    mean_reward        | 60        |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 176500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.109    |\n",
      "|    explained_variance | -0.221    |\n",
      "|    learning_rate      | 0.000687  |\n",
      "|    n_updates          | 3529      |\n",
      "|    policy_loss        | -7.73e-05 |\n",
      "|    value_loss         | 0.461     |\n",
      "-------------------------------------\n",
      "Eval num_timesteps=177000, episode_reward=71.07 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 71.1     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 177000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.215   |\n",
      "|    explained_variance | 0.605    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 3539     |\n",
      "|    policy_loss        | 0.00121  |\n",
      "|    value_loss         | 0.0406   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=177500, episode_reward=76.11 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 230       |\n",
      "|    mean_reward        | 76.1      |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 177500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.0139   |\n",
      "|    explained_variance | 0.72      |\n",
      "|    learning_rate      | 0.000687  |\n",
      "|    n_updates          | 3549      |\n",
      "|    policy_loss        | -2.55e-05 |\n",
      "|    value_loss         | 0.0175    |\n",
      "-------------------------------------\n",
      "Eval num_timesteps=178000, episode_reward=55.38 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 55.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 178000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.118   |\n",
      "|    explained_variance | 0.647    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 3559     |\n",
      "|    policy_loss        | -0.015   |\n",
      "|    value_loss         | 0.0256   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=178500, episode_reward=56.23 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 56.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 178500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0111  |\n",
      "|    explained_variance | 0.545    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 3569     |\n",
      "|    policy_loss        | 0.000801 |\n",
      "|    value_loss         | 0.0164   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=179000, episode_reward=77.27 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 77.3     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 179000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.164   |\n",
      "|    explained_variance | 0.21     |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 3579     |\n",
      "|    policy_loss        | -0.0153  |\n",
      "|    value_loss         | 0.0587   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=179500, episode_reward=74.87 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 74.9     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 179500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.147   |\n",
      "|    explained_variance | 0.792    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 3589     |\n",
      "|    policy_loss        | -0.0308  |\n",
      "|    value_loss         | 0.193    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=180000, episode_reward=70.61 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 70.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 180000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0883  |\n",
      "|    explained_variance | 0.959    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 3599     |\n",
      "|    policy_loss        | -0.00556 |\n",
      "|    value_loss         | 0.0264   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 59.8     |\n",
      "| time/              |          |\n",
      "|    fps             | 2040     |\n",
      "|    iterations      | 3600     |\n",
      "|    time_elapsed    | 88       |\n",
      "|    total_timesteps | 180000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=180500, episode_reward=72.10 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 72.1     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 180500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.282   |\n",
      "|    explained_variance | 0.214    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 3609     |\n",
      "|    policy_loss        | -0.0332  |\n",
      "|    value_loss         | 1.13     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=181000, episode_reward=52.82 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 52.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 181000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0629  |\n",
      "|    explained_variance | 0.944    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 3619     |\n",
      "|    policy_loss        | -0.0499  |\n",
      "|    value_loss         | 0.101    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=181500, episode_reward=69.25 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 69.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 181500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.3     |\n",
      "|    explained_variance | 0.225    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 3629     |\n",
      "|    policy_loss        | 0.00791  |\n",
      "|    value_loss         | 0.0168   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=182000, episode_reward=60.17 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 230       |\n",
      "|    mean_reward        | 60.2      |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 182000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7e-05    |\n",
      "|    explained_variance | 0.0556    |\n",
      "|    learning_rate      | 0.000687  |\n",
      "|    n_updates          | 3639      |\n",
      "|    policy_loss        | -3.59e-07 |\n",
      "|    value_loss         | 0.0153    |\n",
      "-------------------------------------\n",
      "Eval num_timesteps=182500, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 0        |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 182500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.126   |\n",
      "|    explained_variance | 0.56     |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 3649     |\n",
      "|    policy_loss        | -0.00219 |\n",
      "|    value_loss         | 0.0315   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=183000, episode_reward=0.00 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 0        |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 183000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0528  |\n",
      "|    explained_variance | 0.544    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 3659     |\n",
      "|    policy_loss        | -0.233   |\n",
      "|    value_loss         | 4.93     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=183500, episode_reward=64.61 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 64.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 183500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.142   |\n",
      "|    explained_variance | 0.857    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 3669     |\n",
      "|    policy_loss        | 0.0151   |\n",
      "|    value_loss         | 0.0238   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=184000, episode_reward=83.49 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 83.5     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 184000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.135   |\n",
      "|    explained_variance | 0.655    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 3679     |\n",
      "|    policy_loss        | -0.0279  |\n",
      "|    value_loss         | 1.1      |\n",
      "------------------------------------\n",
      "Eval num_timesteps=184500, episode_reward=68.70 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 68.7     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 184500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.169   |\n",
      "|    explained_variance | 0.849    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 3689     |\n",
      "|    policy_loss        | 0.041    |\n",
      "|    value_loss         | 0.291    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=185000, episode_reward=-0.47 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | -0.474   |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 185000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.137   |\n",
      "|    explained_variance | 0.483    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 3699     |\n",
      "|    policy_loss        | -0.0114  |\n",
      "|    value_loss         | 0.0787   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 60.4     |\n",
      "| time/              |          |\n",
      "|    fps             | 2041     |\n",
      "|    iterations      | 3700     |\n",
      "|    time_elapsed    | 90       |\n",
      "|    total_timesteps | 185000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=185500, episode_reward=42.17 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 42.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 185500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0231  |\n",
      "|    explained_variance | -0.058   |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 3709     |\n",
      "|    policy_loss        | -0.03    |\n",
      "|    value_loss         | 0.06     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=186000, episode_reward=56.90 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 56.9     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 186000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.168   |\n",
      "|    explained_variance | 0.419    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 3719     |\n",
      "|    policy_loss        | -0.0436  |\n",
      "|    value_loss         | 0.0422   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=186500, episode_reward=39.02 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 230       |\n",
      "|    mean_reward        | 39        |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 186500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.000343 |\n",
      "|    explained_variance | 0.0568    |\n",
      "|    learning_rate      | 0.000687  |\n",
      "|    n_updates          | 3729      |\n",
      "|    policy_loss        | -3.2e-06  |\n",
      "|    value_loss         | 0.0336    |\n",
      "-------------------------------------\n",
      "Eval num_timesteps=187000, episode_reward=68.57 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 68.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 187000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.121   |\n",
      "|    explained_variance | 0.773    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 3739     |\n",
      "|    policy_loss        | -0.011   |\n",
      "|    value_loss         | 0.0288   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=187500, episode_reward=79.60 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 79.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 187500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.138   |\n",
      "|    explained_variance | 0.85     |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 3749     |\n",
      "|    policy_loss        | 0.0569   |\n",
      "|    value_loss         | 0.27     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=188000, episode_reward=64.00 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 64       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 188000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0685  |\n",
      "|    explained_variance | 0.92     |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 3759     |\n",
      "|    policy_loss        | -0.027   |\n",
      "|    value_loss         | 0.0681   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=188500, episode_reward=64.12 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 64.1     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 188500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.235   |\n",
      "|    explained_variance | 0.487    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 3769     |\n",
      "|    policy_loss        | 0.00232  |\n",
      "|    value_loss         | 0.545    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=189000, episode_reward=62.98 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 63       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 189000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0712  |\n",
      "|    explained_variance | 0.956    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 3779     |\n",
      "|    policy_loss        | -0.00378 |\n",
      "|    value_loss         | 0.0633   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=189500, episode_reward=54.35 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 54.3     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 189500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.478   |\n",
      "|    explained_variance | 0.578    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 3789     |\n",
      "|    policy_loss        | -0.0235  |\n",
      "|    value_loss         | 0.0201   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=190000, episode_reward=36.75 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 230       |\n",
      "|    mean_reward        | 36.7      |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 190000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.16e-05 |\n",
      "|    explained_variance | -5        |\n",
      "|    learning_rate      | 0.000687  |\n",
      "|    n_updates          | 3799      |\n",
      "|    policy_loss        | 8.9e-08   |\n",
      "|    value_loss         | 0.015     |\n",
      "-------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 60.7     |\n",
      "| time/              |          |\n",
      "|    fps             | 2041     |\n",
      "|    iterations      | 3800     |\n",
      "|    time_elapsed    | 93       |\n",
      "|    total_timesteps | 190000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=190500, episode_reward=17.31 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 17.3     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 190500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.16    |\n",
      "|    explained_variance | 0.719    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 3809     |\n",
      "|    policy_loss        | -0.00092 |\n",
      "|    value_loss         | 0.0161   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=191000, episode_reward=27.65 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 27.7     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 191000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0114  |\n",
      "|    explained_variance | 0.44     |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 3819     |\n",
      "|    policy_loss        | 0.0561   |\n",
      "|    value_loss         | 1.77     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=191500, episode_reward=82.12 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 230       |\n",
      "|    mean_reward        | 82.1      |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 191500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.162    |\n",
      "|    explained_variance | 0.895     |\n",
      "|    learning_rate      | 0.000687  |\n",
      "|    n_updates          | 3829      |\n",
      "|    policy_loss        | -0.000478 |\n",
      "|    value_loss         | 0.0271    |\n",
      "-------------------------------------\n",
      "Eval num_timesteps=192000, episode_reward=50.86 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 50.9     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 192000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.134   |\n",
      "|    explained_variance | 0.958    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 3839     |\n",
      "|    policy_loss        | 0.0179   |\n",
      "|    value_loss         | 0.181    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=192500, episode_reward=62.77 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 62.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 192500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.15    |\n",
      "|    explained_variance | 0.682    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 3849     |\n",
      "|    policy_loss        | 0.0522   |\n",
      "|    value_loss         | 0.376    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=193000, episode_reward=63.55 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 63.5     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 193000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.108   |\n",
      "|    explained_variance | 0.809    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 3859     |\n",
      "|    policy_loss        | 0.0055   |\n",
      "|    value_loss         | 0.0369   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=193500, episode_reward=66.55 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 66.5     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 193500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0272  |\n",
      "|    explained_variance | 0.89     |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 3869     |\n",
      "|    policy_loss        | -0.0023  |\n",
      "|    value_loss         | 0.0249   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=194000, episode_reward=33.03 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 33       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 194000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.132   |\n",
      "|    explained_variance | -0.122   |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 3879     |\n",
      "|    policy_loss        | 0.0623   |\n",
      "|    value_loss         | 0.0702   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=194500, episode_reward=27.26 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 230       |\n",
      "|    mean_reward        | 27.3      |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 194500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.000182 |\n",
      "|    explained_variance | -2.28     |\n",
      "|    learning_rate      | 0.000687  |\n",
      "|    n_updates          | 3889      |\n",
      "|    policy_loss        | -3.63e-06 |\n",
      "|    value_loss         | 0.0339    |\n",
      "-------------------------------------\n",
      "Eval num_timesteps=195000, episode_reward=34.00 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 34       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 195000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.12    |\n",
      "|    explained_variance | 0.225    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 3899     |\n",
      "|    policy_loss        | -0.0188  |\n",
      "|    value_loss         | 0.0499   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 61.2     |\n",
      "| time/              |          |\n",
      "|    fps             | 2041     |\n",
      "|    iterations      | 3900     |\n",
      "|    time_elapsed    | 95       |\n",
      "|    total_timesteps | 195000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=195500, episode_reward=28.11 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 28.1     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 195500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.124   |\n",
      "|    explained_variance | 0.124    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 3909     |\n",
      "|    policy_loss        | -0.00927 |\n",
      "|    value_loss         | 3.98     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=196000, episode_reward=37.50 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 37.5     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 196000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0591  |\n",
      "|    explained_variance | 0.855    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 3919     |\n",
      "|    policy_loss        | -0.00124 |\n",
      "|    value_loss         | 0.0896   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=196500, episode_reward=70.59 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 70.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 196500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.274   |\n",
      "|    explained_variance | 0.367    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 3929     |\n",
      "|    policy_loss        | 0.0907   |\n",
      "|    value_loss         | 0.822    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=197000, episode_reward=42.64 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 42.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 197000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.132   |\n",
      "|    explained_variance | 0.933    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 3939     |\n",
      "|    policy_loss        | -0.0247  |\n",
      "|    value_loss         | 0.134    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=197500, episode_reward=38.35 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 38.3     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 197500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.372   |\n",
      "|    explained_variance | -0.0965  |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 3949     |\n",
      "|    policy_loss        | 0.0525   |\n",
      "|    value_loss         | 0.0283   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=198000, episode_reward=38.93 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 230       |\n",
      "|    mean_reward        | 38.9      |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 198000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -4.63e-06 |\n",
      "|    explained_variance | -0.268    |\n",
      "|    learning_rate      | 0.000687  |\n",
      "|    n_updates          | 3959      |\n",
      "|    policy_loss        | 1.89e-09  |\n",
      "|    value_loss         | 0.0115    |\n",
      "-------------------------------------\n",
      "Eval num_timesteps=198500, episode_reward=74.35 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 74.3     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 198500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.186   |\n",
      "|    explained_variance | 0.574    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 3969     |\n",
      "|    policy_loss        | 0.00642  |\n",
      "|    value_loss         | 0.0712   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=199000, episode_reward=89.05 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 89       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 199000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.00708 |\n",
      "|    explained_variance | 0.15     |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 3979     |\n",
      "|    policy_loss        | 0.00106  |\n",
      "|    value_loss         | 1.17     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=199500, episode_reward=76.01 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 76       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 199500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.2     |\n",
      "|    explained_variance | 0.858    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 3989     |\n",
      "|    policy_loss        | 0.0374   |\n",
      "|    value_loss         | 0.0389   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=200000, episode_reward=71.18 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 71.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 200000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.147   |\n",
      "|    explained_variance | 0.912    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 3999     |\n",
      "|    policy_loss        | 0.0321   |\n",
      "|    value_loss         | 0.432    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 61.5     |\n",
      "| time/              |          |\n",
      "|    fps             | 2042     |\n",
      "|    iterations      | 4000     |\n",
      "|    time_elapsed    | 97       |\n",
      "|    total_timesteps | 200000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=200500, episode_reward=57.61 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 57.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 200500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0833  |\n",
      "|    explained_variance | 0.606    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 4009     |\n",
      "|    policy_loss        | 0.00685  |\n",
      "|    value_loss         | 0.538    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=201000, episode_reward=6.24 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 6.24     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 201000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.223   |\n",
      "|    explained_variance | 0.375    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 4019     |\n",
      "|    policy_loss        | -0.0877  |\n",
      "|    value_loss         | 0.693    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=201500, episode_reward=44.21 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 44.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 201500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0289  |\n",
      "|    explained_variance | -0.139   |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 4029     |\n",
      "|    policy_loss        | -0.0164  |\n",
      "|    value_loss         | 0.14     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=202000, episode_reward=30.45 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 30.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 202000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.172   |\n",
      "|    explained_variance | 0.649    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 4039     |\n",
      "|    policy_loss        | 0.0335   |\n",
      "|    value_loss         | 0.0498   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=202500, episode_reward=22.24 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 230       |\n",
      "|    mean_reward        | 22.2      |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 202500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.000341 |\n",
      "|    explained_variance | -0.753    |\n",
      "|    learning_rate      | 0.000687  |\n",
      "|    n_updates          | 4049      |\n",
      "|    policy_loss        | -1.29e-05 |\n",
      "|    value_loss         | 0.0585    |\n",
      "-------------------------------------\n",
      "Eval num_timesteps=203000, episode_reward=18.24 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 18.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 203000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.161   |\n",
      "|    explained_variance | 0.523    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 4059     |\n",
      "|    policy_loss        | -0.048   |\n",
      "|    value_loss         | 0.108    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=203500, episode_reward=41.80 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 41.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 203500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.113   |\n",
      "|    explained_variance | 0.875    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 4069     |\n",
      "|    policy_loss        | 0.0141   |\n",
      "|    value_loss         | 0.117    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=204000, episode_reward=52.19 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 52.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 204000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.117   |\n",
      "|    explained_variance | 0.71     |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 4079     |\n",
      "|    policy_loss        | -0.0147  |\n",
      "|    value_loss         | 0.171    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=204500, episode_reward=66.54 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 66.5     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 204500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.265   |\n",
      "|    explained_variance | 0.514    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 4089     |\n",
      "|    policy_loss        | 0.0855   |\n",
      "|    value_loss         | 0.588    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=205000, episode_reward=78.43 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 78.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 205000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.114   |\n",
      "|    explained_variance | 0.658    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 4099     |\n",
      "|    policy_loss        | 0.0447   |\n",
      "|    value_loss         | 0.609    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 61.5     |\n",
      "| time/              |          |\n",
      "|    fps             | 2042     |\n",
      "|    iterations      | 4100     |\n",
      "|    time_elapsed    | 100      |\n",
      "|    total_timesteps | 205000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=205500, episode_reward=67.98 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 68       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 205500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.204   |\n",
      "|    explained_variance | -0.0454  |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 4109     |\n",
      "|    policy_loss        | 0.018    |\n",
      "|    value_loss         | 0.0246   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=206000, episode_reward=82.51 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 230       |\n",
      "|    mean_reward        | 82.5      |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 206000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.31e-06 |\n",
      "|    explained_variance | -0.701    |\n",
      "|    learning_rate      | 0.000687  |\n",
      "|    n_updates          | 4119      |\n",
      "|    policy_loss        | 2.08e-10  |\n",
      "|    value_loss         | 0.00397   |\n",
      "-------------------------------------\n",
      "Eval num_timesteps=206500, episode_reward=53.87 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 53.9     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 206500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.187   |\n",
      "|    explained_variance | 0.438    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 4129     |\n",
      "|    policy_loss        | -0.0247  |\n",
      "|    value_loss         | 0.0539   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=207000, episode_reward=92.00 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 92       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 207000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0077  |\n",
      "|    explained_variance | 0.561    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 4139     |\n",
      "|    policy_loss        | 0.00102  |\n",
      "|    value_loss         | 0.107    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=207500, episode_reward=85.14 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 85.1     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 207500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.167   |\n",
      "|    explained_variance | 0.867    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 4149     |\n",
      "|    policy_loss        | -0.00383 |\n",
      "|    value_loss         | 0.042    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=208000, episode_reward=79.58 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 79.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 208000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0872  |\n",
      "|    explained_variance | 0.94     |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 4159     |\n",
      "|    policy_loss        | -0.0203  |\n",
      "|    value_loss         | 0.239    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=208500, episode_reward=53.09 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 53.1     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 208500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.083   |\n",
      "|    explained_variance | 0.949    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 4169     |\n",
      "|    policy_loss        | 0.00524  |\n",
      "|    value_loss         | 0.104    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=209000, episode_reward=63.00 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 63       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 209000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.203   |\n",
      "|    explained_variance | 0.387    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 4179     |\n",
      "|    policy_loss        | -0.0295  |\n",
      "|    value_loss         | 0.0447   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=209500, episode_reward=48.72 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 48.7     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 209500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0127  |\n",
      "|    explained_variance | 0.768    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 4189     |\n",
      "|    policy_loss        | 0.0012   |\n",
      "|    value_loss         | 0.0725   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=210000, episode_reward=69.28 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 69.3     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 210000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.222   |\n",
      "|    explained_variance | 0.34     |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 4199     |\n",
      "|    policy_loss        | 0.0486   |\n",
      "|    value_loss         | 0.121    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 62.1     |\n",
      "| time/              |          |\n",
      "|    fps             | 2042     |\n",
      "|    iterations      | 4200     |\n",
      "|    time_elapsed    | 102      |\n",
      "|    total_timesteps | 210000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=210500, episode_reward=66.19 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 230       |\n",
      "|    mean_reward        | 66.2      |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 210500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -8.62e-05 |\n",
      "|    explained_variance | -1.39     |\n",
      "|    learning_rate      | 0.000687  |\n",
      "|    n_updates          | 4209      |\n",
      "|    policy_loss        | -8.95e-07 |\n",
      "|    value_loss         | 0.0497    |\n",
      "-------------------------------------\n",
      "Eval num_timesteps=211000, episode_reward=51.88 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 51.9     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 211000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.145   |\n",
      "|    explained_variance | 0.604    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 4219     |\n",
      "|    policy_loss        | 0.0369   |\n",
      "|    value_loss         | 0.0444   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=211500, episode_reward=92.80 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 92.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 211500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0945  |\n",
      "|    explained_variance | 0.692    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 4229     |\n",
      "|    policy_loss        | -0.00614 |\n",
      "|    value_loss         | 1.61     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=212000, episode_reward=54.14 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 54.1     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 212000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.128   |\n",
      "|    explained_variance | 0.349    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 4239     |\n",
      "|    policy_loss        | -0.11    |\n",
      "|    value_loss         | 0.329    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=212500, episode_reward=77.56 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 77.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 212500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.251   |\n",
      "|    explained_variance | 0.181    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 4249     |\n",
      "|    policy_loss        | 0.116    |\n",
      "|    value_loss         | 0.781    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=213000, episode_reward=47.06 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 47.1     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 213000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.114   |\n",
      "|    explained_variance | 0.968    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 4259     |\n",
      "|    policy_loss        | 0.00763  |\n",
      "|    value_loss         | 0.0725   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=213500, episode_reward=58.88 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 58.9     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 213500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.328   |\n",
      "|    explained_variance | 0.141    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 4269     |\n",
      "|    policy_loss        | -0.0119  |\n",
      "|    value_loss         | 0.0518   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=214000, episode_reward=86.21 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 230       |\n",
      "|    mean_reward        | 86.2      |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 214000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.26e-06 |\n",
      "|    explained_variance | -5.05     |\n",
      "|    learning_rate      | 0.000687  |\n",
      "|    n_updates          | 4279      |\n",
      "|    policy_loss        | 6.78e-09  |\n",
      "|    value_loss         | 0.0192    |\n",
      "-------------------------------------\n",
      "Eval num_timesteps=214500, episode_reward=91.13 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 91.1     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 214500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.139   |\n",
      "|    explained_variance | 0.617    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 4289     |\n",
      "|    policy_loss        | 0.00789  |\n",
      "|    value_loss         | 0.0157   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=215000, episode_reward=79.69 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 230       |\n",
      "|    mean_reward        | 79.7      |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 215000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.00809  |\n",
      "|    explained_variance | 0.487     |\n",
      "|    learning_rate      | 0.000687  |\n",
      "|    n_updates          | 4299      |\n",
      "|    policy_loss        | -0.000157 |\n",
      "|    value_loss         | 0.0326    |\n",
      "-------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 62.6     |\n",
      "| time/              |          |\n",
      "|    fps             | 2042     |\n",
      "|    iterations      | 4300     |\n",
      "|    time_elapsed    | 105      |\n",
      "|    total_timesteps | 215000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=215500, episode_reward=71.89 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 71.9     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 215500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.162   |\n",
      "|    explained_variance | 0.774    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 4309     |\n",
      "|    policy_loss        | -0.0175  |\n",
      "|    value_loss         | 0.0514   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=216000, episode_reward=66.37 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 66.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 216000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0705  |\n",
      "|    explained_variance | 0.807    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 4319     |\n",
      "|    policy_loss        | 0.0145   |\n",
      "|    value_loss         | 0.853    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=216500, episode_reward=8.05 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 8.05     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 216500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.135   |\n",
      "|    explained_variance | 0.4      |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 4329     |\n",
      "|    policy_loss        | -0.078   |\n",
      "|    value_loss         | 0.566    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=217000, episode_reward=56.56 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 56.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 217000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0998  |\n",
      "|    explained_variance | 0.086    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 4339     |\n",
      "|    policy_loss        | -0.0188  |\n",
      "|    value_loss         | 0.0314   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=217500, episode_reward=46.67 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 46.7     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 217500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0261  |\n",
      "|    explained_variance | 0.731    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 4349     |\n",
      "|    policy_loss        | 0.0034   |\n",
      "|    value_loss         | 0.0857   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=218000, episode_reward=37.83 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 37.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 218000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.148   |\n",
      "|    explained_variance | 0.62     |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 4359     |\n",
      "|    policy_loss        | 0.022    |\n",
      "|    value_loss         | 0.0438   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=218500, episode_reward=38.78 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 230       |\n",
      "|    mean_reward        | 38.8      |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 218500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.000368 |\n",
      "|    explained_variance | -1.15     |\n",
      "|    learning_rate      | 0.000687  |\n",
      "|    n_updates          | 4369      |\n",
      "|    policy_loss        | 1.75e-05  |\n",
      "|    value_loss         | 0.0997    |\n",
      "-------------------------------------\n",
      "Eval num_timesteps=219000, episode_reward=48.64 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 48.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 219000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.151   |\n",
      "|    explained_variance | 0.496    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 4379     |\n",
      "|    policy_loss        | -0.0314  |\n",
      "|    value_loss         | 0.0309   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=219500, episode_reward=13.00 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 13       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 219500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.112   |\n",
      "|    explained_variance | -0.172   |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 4389     |\n",
      "|    policy_loss        | -0.0147  |\n",
      "|    value_loss         | 3.51     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=220000, episode_reward=58.38 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 58.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 220000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.106   |\n",
      "|    explained_variance | 0.788    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 4399     |\n",
      "|    policy_loss        | 0.0107   |\n",
      "|    value_loss         | 0.164    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 63       |\n",
      "| time/              |          |\n",
      "|    fps             | 2043     |\n",
      "|    iterations      | 4400     |\n",
      "|    time_elapsed    | 107      |\n",
      "|    total_timesteps | 220000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=220500, episode_reward=4.73 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 4.73     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 220500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.181   |\n",
      "|    explained_variance | 0.283    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 4409     |\n",
      "|    policy_loss        | 0.0295   |\n",
      "|    value_loss         | 0.309    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=221000, episode_reward=23.23 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 23.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 221000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.111   |\n",
      "|    explained_variance | 0.936    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 4419     |\n",
      "|    policy_loss        | -0.0999  |\n",
      "|    value_loss         | 0.254    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=221500, episode_reward=21.39 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 21.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 221500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.218   |\n",
      "|    explained_variance | 0.353    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 4429     |\n",
      "|    policy_loss        | 0.0039   |\n",
      "|    value_loss         | 0.0268   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=222000, episode_reward=3.55 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 230       |\n",
      "|    mean_reward        | 3.55      |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 222000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.18e-06 |\n",
      "|    explained_variance | -9.83     |\n",
      "|    learning_rate      | 0.000687  |\n",
      "|    n_updates          | 4439      |\n",
      "|    policy_loss        | 1.01e-09  |\n",
      "|    value_loss         | 0.0119    |\n",
      "-------------------------------------\n",
      "Eval num_timesteps=222500, episode_reward=38.41 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 38.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 222500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.134   |\n",
      "|    explained_variance | 0.624    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 4449     |\n",
      "|    policy_loss        | 0.00663  |\n",
      "|    value_loss         | 0.0374   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=223000, episode_reward=36.41 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 36.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 223000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.00354 |\n",
      "|    explained_variance | -0.555   |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 4459     |\n",
      "|    policy_loss        | 1.48e-05 |\n",
      "|    value_loss         | 0.0192   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=223500, episode_reward=53.60 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 53.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 223500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.121   |\n",
      "|    explained_variance | 0.8      |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 4469     |\n",
      "|    policy_loss        | -0.00652 |\n",
      "|    value_loss         | 0.0573   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=224000, episode_reward=30.07 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 30.1     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 224000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.128   |\n",
      "|    explained_variance | 0.902    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 4479     |\n",
      "|    policy_loss        | 0.0521   |\n",
      "|    value_loss         | 0.599    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=224500, episode_reward=54.37 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 54.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 224500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0988  |\n",
      "|    explained_variance | 0.797    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 4489     |\n",
      "|    policy_loss        | 0.0081   |\n",
      "|    value_loss         | 0.133    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=225000, episode_reward=14.36 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 14.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 225000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.289   |\n",
      "|    explained_variance | -0.211   |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 4499     |\n",
      "|    policy_loss        | 0.0295   |\n",
      "|    value_loss         | 0.0789   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 63.4     |\n",
      "| time/              |          |\n",
      "|    fps             | 2043     |\n",
      "|    iterations      | 4500     |\n",
      "|    time_elapsed    | 110      |\n",
      "|    total_timesteps | 225000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=225500, episode_reward=32.02 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 32       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 225500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.022   |\n",
      "|    explained_variance | 0.542    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 4509     |\n",
      "|    policy_loss        | -0.00167 |\n",
      "|    value_loss         | 0.0657   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=226000, episode_reward=45.47 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 45.5     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 226000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.109   |\n",
      "|    explained_variance | 0.352    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 4519     |\n",
      "|    policy_loss        | -0.0376  |\n",
      "|    value_loss         | 0.0521   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=226500, episode_reward=36.03 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 230       |\n",
      "|    mean_reward        | 36        |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 226500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.54e-05 |\n",
      "|    explained_variance | -2.82     |\n",
      "|    learning_rate      | 0.000687  |\n",
      "|    n_updates          | 4529      |\n",
      "|    policy_loss        | 1.92e-07  |\n",
      "|    value_loss         | 0.0195    |\n",
      "-------------------------------------\n",
      "Eval num_timesteps=227000, episode_reward=61.14 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 61.1     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 227000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.113   |\n",
      "|    explained_variance | 0.587    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 4539     |\n",
      "|    policy_loss        | 0.00433  |\n",
      "|    value_loss         | 0.0454   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=227500, episode_reward=49.60 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 49.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 227500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.109   |\n",
      "|    explained_variance | 0.116    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 4549     |\n",
      "|    policy_loss        | -0.0754  |\n",
      "|    value_loss         | 1.94     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=228000, episode_reward=65.55 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 230       |\n",
      "|    mean_reward        | 65.5      |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 228000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.0652   |\n",
      "|    explained_variance | 0.871     |\n",
      "|    learning_rate      | 0.000687  |\n",
      "|    n_updates          | 4559      |\n",
      "|    policy_loss        | -0.000408 |\n",
      "|    value_loss         | 0.0676    |\n",
      "-------------------------------------\n",
      "Eval num_timesteps=228500, episode_reward=50.45 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 50.5     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 228500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.28    |\n",
      "|    explained_variance | -0.589   |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 4569     |\n",
      "|    policy_loss        | -0.0863  |\n",
      "|    value_loss         | 0.775    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=229000, episode_reward=24.52 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 24.5     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 229000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0763  |\n",
      "|    explained_variance | 0.971    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 4579     |\n",
      "|    policy_loss        | -0.00204 |\n",
      "|    value_loss         | 0.068    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=229500, episode_reward=19.42 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 19.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 229500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.307   |\n",
      "|    explained_variance | 0.352    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 4589     |\n",
      "|    policy_loss        | -0.00528 |\n",
      "|    value_loss         | 0.0407   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=230000, episode_reward=45.35 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 230       |\n",
      "|    mean_reward        | 45.4      |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 230000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.16e-06 |\n",
      "|    explained_variance | -2.25     |\n",
      "|    learning_rate      | 0.000687  |\n",
      "|    n_updates          | 4599      |\n",
      "|    policy_loss        | -1.75e-09 |\n",
      "|    value_loss         | 0.00798   |\n",
      "-------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 64.3     |\n",
      "| time/              |          |\n",
      "|    fps             | 2043     |\n",
      "|    iterations      | 4600     |\n",
      "|    time_elapsed    | 112      |\n",
      "|    total_timesteps | 230000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=230500, episode_reward=55.62 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 55.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 230500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.117   |\n",
      "|    explained_variance | 0.693    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 4609     |\n",
      "|    policy_loss        | 0.00202  |\n",
      "|    value_loss         | 0.0256   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=231000, episode_reward=35.11 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 35.1     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 231000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.00805 |\n",
      "|    explained_variance | -6.64    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 4619     |\n",
      "|    policy_loss        | 0.000203 |\n",
      "|    value_loss         | 0.0178   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=231500, episode_reward=72.34 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 72.3     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 231500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.14    |\n",
      "|    explained_variance | 0.742    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 4629     |\n",
      "|    policy_loss        | 0.0284   |\n",
      "|    value_loss         | 0.0587   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=232000, episode_reward=75.43 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 75.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 232000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0767  |\n",
      "|    explained_variance | 0.932    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 4639     |\n",
      "|    policy_loss        | 0.00312  |\n",
      "|    value_loss         | 0.305    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=232500, episode_reward=80.58 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 80.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 232500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.147   |\n",
      "|    explained_variance | 0.654    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 4649     |\n",
      "|    policy_loss        | -0.0254  |\n",
      "|    value_loss         | 0.211    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=233000, episode_reward=63.93 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 63.9     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 233000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.128   |\n",
      "|    explained_variance | 0.471    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 4659     |\n",
      "|    policy_loss        | 0.0347   |\n",
      "|    value_loss         | 0.0706   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=233500, episode_reward=71.35 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 71.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 233500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.00794 |\n",
      "|    explained_variance | -0.0491  |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 4669     |\n",
      "|    policy_loss        | 0.00011  |\n",
      "|    value_loss         | 0.0313   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=234000, episode_reward=45.26 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 45.3     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 234000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.138   |\n",
      "|    explained_variance | 0.709    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 4679     |\n",
      "|    policy_loss        | 0.0122   |\n",
      "|    value_loss         | 0.0534   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=234500, episode_reward=31.04 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 230       |\n",
      "|    mean_reward        | 31        |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 234500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.000579 |\n",
      "|    explained_variance | -7.09     |\n",
      "|    learning_rate      | 0.000687  |\n",
      "|    n_updates          | 4689      |\n",
      "|    policy_loss        | -1.84e-08 |\n",
      "|    value_loss         | 0.0212    |\n",
      "-------------------------------------\n",
      "Eval num_timesteps=235000, episode_reward=47.98 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 48       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 235000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0867  |\n",
      "|    explained_variance | 0.622    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 4699     |\n",
      "|    policy_loss        | -0.0025  |\n",
      "|    value_loss         | 0.0347   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 65.1     |\n",
      "| time/              |          |\n",
      "|    fps             | 2043     |\n",
      "|    iterations      | 4700     |\n",
      "|    time_elapsed    | 114      |\n",
      "|    total_timesteps | 235000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=235500, episode_reward=54.64 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 54.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 235500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0843  |\n",
      "|    explained_variance | 0.801    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 4709     |\n",
      "|    policy_loss        | -0.0875  |\n",
      "|    value_loss         | 0.869    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=236000, episode_reward=16.70 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 16.7     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 236000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0988  |\n",
      "|    explained_variance | 0.835    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 4719     |\n",
      "|    policy_loss        | -0.0139  |\n",
      "|    value_loss         | 0.156    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=236500, episode_reward=22.30 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 22.3     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 236500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.232   |\n",
      "|    explained_variance | 0.58     |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 4729     |\n",
      "|    policy_loss        | 0.0584   |\n",
      "|    value_loss         | 0.513    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=237000, episode_reward=33.14 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 33.1     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 237000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.116   |\n",
      "|    explained_variance | 0.98     |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 4739     |\n",
      "|    policy_loss        | -0.0116  |\n",
      "|    value_loss         | 0.0542   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=237500, episode_reward=17.12 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 17.1     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 237500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.322   |\n",
      "|    explained_variance | 0.54     |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 4749     |\n",
      "|    policy_loss        | -0.0221  |\n",
      "|    value_loss         | 0.0324   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=238000, episode_reward=11.78 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 230       |\n",
      "|    mean_reward        | 11.8      |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 238000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.42e-06 |\n",
      "|    explained_variance | -0.0166   |\n",
      "|    learning_rate      | 0.000687  |\n",
      "|    n_updates          | 4759      |\n",
      "|    policy_loss        | -3.95e-09 |\n",
      "|    value_loss         | 0.00815   |\n",
      "-------------------------------------\n",
      "Eval num_timesteps=238500, episode_reward=91.34 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 91.3     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 238500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.118   |\n",
      "|    explained_variance | 0.827    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 4769     |\n",
      "|    policy_loss        | -0.00378 |\n",
      "|    value_loss         | 0.0176   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=239000, episode_reward=76.10 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 230       |\n",
      "|    mean_reward        | 76.1      |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 239000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.0132   |\n",
      "|    explained_variance | -1.94     |\n",
      "|    learning_rate      | 0.000687  |\n",
      "|    n_updates          | 4779      |\n",
      "|    policy_loss        | -0.000271 |\n",
      "|    value_loss         | 0.021     |\n",
      "-------------------------------------\n",
      "Eval num_timesteps=239500, episode_reward=47.51 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 47.5     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 239500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.128   |\n",
      "|    explained_variance | 0.8      |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 4789     |\n",
      "|    policy_loss        | 0.00303  |\n",
      "|    value_loss         | 0.0551   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=240000, episode_reward=68.26 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 68.3     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 240000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.11    |\n",
      "|    explained_variance | 0.809    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 4799     |\n",
      "|    policy_loss        | 0.00625  |\n",
      "|    value_loss         | 1.07     |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 65.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 2042     |\n",
      "|    iterations      | 4800     |\n",
      "|    time_elapsed    | 117      |\n",
      "|    total_timesteps | 240000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=240500, episode_reward=42.13 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 42.1     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 240500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.121   |\n",
      "|    explained_variance | 0.891    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 4809     |\n",
      "|    policy_loss        | 0.00252  |\n",
      "|    value_loss         | 0.0874   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=241000, episode_reward=29.61 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 29.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 241000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.138   |\n",
      "|    explained_variance | 0.813    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 4819     |\n",
      "|    policy_loss        | 0.0214   |\n",
      "|    value_loss         | 0.0468   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=241500, episode_reward=50.13 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 50.1     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 241500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.024   |\n",
      "|    explained_variance | 0.8      |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 4829     |\n",
      "|    policy_loss        | -0.00342 |\n",
      "|    value_loss         | 0.0204   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=242000, episode_reward=38.07 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 38.1     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 242000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.137   |\n",
      "|    explained_variance | 0.759    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 4839     |\n",
      "|    policy_loss        | 0.0242   |\n",
      "|    value_loss         | 0.0516   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=242500, episode_reward=30.24 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 30.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 242500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.00404 |\n",
      "|    explained_variance | -0.0559  |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 4849     |\n",
      "|    policy_loss        | -0.559   |\n",
      "|    value_loss         | 3.85     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=243000, episode_reward=34.52 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 34.5     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 243000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.1     |\n",
      "|    explained_variance | 0.69     |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 4859     |\n",
      "|    policy_loss        | -0.0264  |\n",
      "|    value_loss         | 0.0283   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=243500, episode_reward=26.00 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 26       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 243500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0635  |\n",
      "|    explained_variance | 0.202    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 4869     |\n",
      "|    policy_loss        | -0.0312  |\n",
      "|    value_loss         | 1.8      |\n",
      "------------------------------------\n",
      "Eval num_timesteps=244000, episode_reward=11.50 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 11.5     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 244000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.103   |\n",
      "|    explained_variance | 0.851    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 4879     |\n",
      "|    policy_loss        | 0.0046   |\n",
      "|    value_loss         | 0.0896   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=244500, episode_reward=-13.13 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | -13.1    |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 244500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.193   |\n",
      "|    explained_variance | 0.5      |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 4889     |\n",
      "|    policy_loss        | -0.0883  |\n",
      "|    value_loss         | 0.781    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=245000, episode_reward=-9.16 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | -9.16    |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 245000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0824  |\n",
      "|    explained_variance | 0.935    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 4899     |\n",
      "|    policy_loss        | -0.03    |\n",
      "|    value_loss         | 0.158    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 66.4     |\n",
      "| time/              |          |\n",
      "|    fps             | 2043     |\n",
      "|    iterations      | 4900     |\n",
      "|    time_elapsed    | 119      |\n",
      "|    total_timesteps | 245000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=245500, episode_reward=-12.84 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | -12.8    |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 245500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.136   |\n",
      "|    explained_variance | -0.138   |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 4909     |\n",
      "|    policy_loss        | 0.00192  |\n",
      "|    value_loss         | 0.0441   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=246000, episode_reward=-16.24 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 230       |\n",
      "|    mean_reward        | -16.2     |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 246000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.68e-06 |\n",
      "|    explained_variance | -0.826    |\n",
      "|    learning_rate      | 0.000687  |\n",
      "|    n_updates          | 4919      |\n",
      "|    policy_loss        | 2.59e-08  |\n",
      "|    value_loss         | 0.011     |\n",
      "-------------------------------------\n",
      "Eval num_timesteps=246500, episode_reward=-15.81 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | -15.8    |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 246500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.155   |\n",
      "|    explained_variance | 0.807    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 4929     |\n",
      "|    policy_loss        | -0.0251  |\n",
      "|    value_loss         | 0.038    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=247000, episode_reward=3.80 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 3.8      |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 247000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.00272 |\n",
      "|    explained_variance | -4.69    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 4939     |\n",
      "|    policy_loss        | 4.51e-05 |\n",
      "|    value_loss         | 0.0151   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=247500, episode_reward=15.75 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 15.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 247500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.111   |\n",
      "|    explained_variance | 0.851    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 4949     |\n",
      "|    policy_loss        | 0.011    |\n",
      "|    value_loss         | 0.0446   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=248000, episode_reward=38.12 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 38.1     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 248000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0895  |\n",
      "|    explained_variance | 0.941    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 4959     |\n",
      "|    policy_loss        | 0.00087  |\n",
      "|    value_loss         | 0.308    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=248500, episode_reward=23.29 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 23.3     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 248500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.105   |\n",
      "|    explained_variance | 0.864    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 4969     |\n",
      "|    policy_loss        | -0.00292 |\n",
      "|    value_loss         | 0.0896   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=249000, episode_reward=15.38 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 15.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 249000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.143   |\n",
      "|    explained_variance | 0.876    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 4979     |\n",
      "|    policy_loss        | -0.00582 |\n",
      "|    value_loss         | 0.048    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=249500, episode_reward=27.90 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 27.9     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 249500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0124  |\n",
      "|    explained_variance | 0.746    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 4989     |\n",
      "|    policy_loss        | -0.00018 |\n",
      "|    value_loss         | 0.00776  |\n",
      "------------------------------------\n",
      "Eval num_timesteps=250000, episode_reward=47.03 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 47       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 250000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.108   |\n",
      "|    explained_variance | 0.803    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 4999     |\n",
      "|    policy_loss        | 0.0108   |\n",
      "|    value_loss         | 0.0407   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 67.1     |\n",
      "| time/              |          |\n",
      "|    fps             | 2042     |\n",
      "|    iterations      | 5000     |\n",
      "|    time_elapsed    | 122      |\n",
      "|    total_timesteps | 250000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=250500, episode_reward=38.08 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 38.1     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 250500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -9.5e-06 |\n",
      "|    explained_variance | -4.29    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 5009     |\n",
      "|    policy_loss        | 2.19e-08 |\n",
      "|    value_loss         | 0.00883  |\n",
      "------------------------------------\n",
      "Eval num_timesteps=251000, episode_reward=66.00 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 66       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 251000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0823  |\n",
      "|    explained_variance | 0.691    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 5019     |\n",
      "|    policy_loss        | 0.0211   |\n",
      "|    value_loss         | 0.0309   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=251500, episode_reward=47.74 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 47.7     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 251500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0497  |\n",
      "|    explained_variance | 0.397    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 5029     |\n",
      "|    policy_loss        | -0.00686 |\n",
      "|    value_loss         | 1.21     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=252000, episode_reward=22.07 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 22.1     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 252000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0711  |\n",
      "|    explained_variance | 0.957    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 5039     |\n",
      "|    policy_loss        | 0.00189  |\n",
      "|    value_loss         | 0.0304   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=252500, episode_reward=73.26 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 73.3     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 252500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.136   |\n",
      "|    explained_variance | 0.803    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 5049     |\n",
      "|    policy_loss        | -0.0638  |\n",
      "|    value_loss         | 0.313    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=253000, episode_reward=20.87 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 20.9     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 253000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0621  |\n",
      "|    explained_variance | 0.954    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 5059     |\n",
      "|    policy_loss        | 0.048    |\n",
      "|    value_loss         | 0.168    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=253500, episode_reward=59.56 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 59.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 253500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0614  |\n",
      "|    explained_variance | 0.369    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 5069     |\n",
      "|    policy_loss        | 0.00234  |\n",
      "|    value_loss         | 0.0174   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=254000, episode_reward=34.49 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 230       |\n",
      "|    mean_reward        | 34.5      |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 254000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -4.92e-07 |\n",
      "|    explained_variance | -2.1      |\n",
      "|    learning_rate      | 0.000687  |\n",
      "|    n_updates          | 5079      |\n",
      "|    policy_loss        | -4.29e-09 |\n",
      "|    value_loss         | 0.0121    |\n",
      "-------------------------------------\n",
      "Eval num_timesteps=254500, episode_reward=24.85 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 24.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 254500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.157   |\n",
      "|    explained_variance | 0.8      |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 5089     |\n",
      "|    policy_loss        | -0.0218  |\n",
      "|    value_loss         | 0.0452   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=255000, episode_reward=18.85 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 18.9     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 255000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.00324 |\n",
      "|    explained_variance | -4.73    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 5099     |\n",
      "|    policy_loss        | 2.65e-05 |\n",
      "|    value_loss         | 0.00907  |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 67.8     |\n",
      "| time/              |          |\n",
      "|    fps             | 2042     |\n",
      "|    iterations      | 5100     |\n",
      "|    time_elapsed    | 124      |\n",
      "|    total_timesteps | 255000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=255500, episode_reward=49.51 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 49.5     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 255500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.182   |\n",
      "|    explained_variance | 0.777    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 5109     |\n",
      "|    policy_loss        | 0.0211   |\n",
      "|    value_loss         | 0.0603   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=256000, episode_reward=50.12 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 50.1     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 256000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.106   |\n",
      "|    explained_variance | 0.946    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 5119     |\n",
      "|    policy_loss        | 0.0167   |\n",
      "|    value_loss         | 0.243    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=256500, episode_reward=51.89 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 51.9     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 256500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.132   |\n",
      "|    explained_variance | 0.552    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 5129     |\n",
      "|    policy_loss        | 0.147    |\n",
      "|    value_loss         | 1.39     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=257000, episode_reward=12.51 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 12.5     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 257000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.101   |\n",
      "|    explained_variance | 0.339    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 5139     |\n",
      "|    policy_loss        | 0.0164   |\n",
      "|    value_loss         | 0.216    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=257500, episode_reward=21.72 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 21.7     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 257500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0287  |\n",
      "|    explained_variance | 0.579    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 5149     |\n",
      "|    policy_loss        | 0.00902  |\n",
      "|    value_loss         | 0.133    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=258000, episode_reward=85.86 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 85.9     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 258000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.131   |\n",
      "|    explained_variance | 0.714    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 5159     |\n",
      "|    policy_loss        | 0.00304  |\n",
      "|    value_loss         | 0.029    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=258500, episode_reward=68.15 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 230       |\n",
      "|    mean_reward        | 68.2      |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 258500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -6.17e-06 |\n",
      "|    explained_variance | -0.411    |\n",
      "|    learning_rate      | 0.000687  |\n",
      "|    n_updates          | 5169      |\n",
      "|    policy_loss        | 1.43e-08  |\n",
      "|    value_loss         | 0.0209    |\n",
      "-------------------------------------\n",
      "Eval num_timesteps=259000, episode_reward=99.34 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 99.3     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 259000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0786  |\n",
      "|    explained_variance | 0.864    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 5179     |\n",
      "|    policy_loss        | 0.00017  |\n",
      "|    value_loss         | 0.0181   |\n",
      "------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=259500, episode_reward=61.58 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 61.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 259500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0331  |\n",
      "|    explained_variance | 0.668    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 5189     |\n",
      "|    policy_loss        | -0.00207 |\n",
      "|    value_loss         | 0.789    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=260000, episode_reward=92.63 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 92.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 260000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0945  |\n",
      "|    explained_variance | 0.86     |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 5199     |\n",
      "|    policy_loss        | -0.0203  |\n",
      "|    value_loss         | 0.112    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 68.8     |\n",
      "| time/              |          |\n",
      "|    fps             | 2042     |\n",
      "|    iterations      | 5200     |\n",
      "|    time_elapsed    | 127      |\n",
      "|    total_timesteps | 260000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=260500, episode_reward=58.31 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 58.3     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 260500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0567  |\n",
      "|    explained_variance | 0.594    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 5209     |\n",
      "|    policy_loss        | 0.127    |\n",
      "|    value_loss         | 0.716    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=261000, episode_reward=50.24 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 50.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 261000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0403  |\n",
      "|    explained_variance | 0.982    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 5219     |\n",
      "|    policy_loss        | 0.00229  |\n",
      "|    value_loss         | 0.0506   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=261500, episode_reward=64.95 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 65       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 261500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0461  |\n",
      "|    explained_variance | 0.362    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 5229     |\n",
      "|    policy_loss        | -0.00319 |\n",
      "|    value_loss         | 0.0335   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=262000, episode_reward=51.15 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 230       |\n",
      "|    mean_reward        | 51.1      |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 262000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -3.75e-06 |\n",
      "|    explained_variance | -0.577    |\n",
      "|    learning_rate      | 0.000687  |\n",
      "|    n_updates          | 5239      |\n",
      "|    policy_loss        | 1.88e-08  |\n",
      "|    value_loss         | 0.00746   |\n",
      "-------------------------------------\n",
      "Eval num_timesteps=262500, episode_reward=41.62 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 41.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 262500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.106   |\n",
      "|    explained_variance | 0.816    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 5249     |\n",
      "|    policy_loss        | 0.0034   |\n",
      "|    value_loss         | 0.0356   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=263000, episode_reward=35.17 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 230       |\n",
      "|    mean_reward        | 35.2      |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 263000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.00172  |\n",
      "|    explained_variance | 0.259     |\n",
      "|    learning_rate      | 0.000687  |\n",
      "|    n_updates          | 5259      |\n",
      "|    policy_loss        | -4.24e-05 |\n",
      "|    value_loss         | 0.011     |\n",
      "-------------------------------------\n",
      "Eval num_timesteps=263500, episode_reward=56.22 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 56.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 263500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.122   |\n",
      "|    explained_variance | 0.614    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 5269     |\n",
      "|    policy_loss        | -0.0102  |\n",
      "|    value_loss         | 0.0643   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=264000, episode_reward=36.03 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 36       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 264000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.059   |\n",
      "|    explained_variance | 0.961    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 5279     |\n",
      "|    policy_loss        | -0.02    |\n",
      "|    value_loss         | 0.238    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=264500, episode_reward=39.96 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 40       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 264500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0635  |\n",
      "|    explained_variance | 0.844    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 5289     |\n",
      "|    policy_loss        | 0.00248  |\n",
      "|    value_loss         | 0.513    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=265000, episode_reward=19.52 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 19.5     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 265000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.115   |\n",
      "|    explained_variance | 0.673    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 5299     |\n",
      "|    policy_loss        | 0.0279   |\n",
      "|    value_loss         | 0.228    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 70       |\n",
      "| time/              |          |\n",
      "|    fps             | 2041     |\n",
      "|    iterations      | 5300     |\n",
      "|    time_elapsed    | 129      |\n",
      "|    total_timesteps | 265000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=265500, episode_reward=10.94 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 10.9     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 265500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.016   |\n",
      "|    explained_variance | 0.558    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 5309     |\n",
      "|    policy_loss        | -0.00196 |\n",
      "|    value_loss         | 0.0102   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=266000, episode_reward=25.75 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 25.7     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 266000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.127   |\n",
      "|    explained_variance | 0.615    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 5319     |\n",
      "|    policy_loss        | -0.0372  |\n",
      "|    value_loss         | 0.0533   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=266500, episode_reward=54.23 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 230       |\n",
      "|    mean_reward        | 54.2      |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 266500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.88e-05 |\n",
      "|    explained_variance | -1.09     |\n",
      "|    learning_rate      | 0.000687  |\n",
      "|    n_updates          | 5329      |\n",
      "|    policy_loss        | 1.5e-07   |\n",
      "|    value_loss         | 0.0287    |\n",
      "-------------------------------------\n",
      "Eval num_timesteps=267000, episode_reward=28.26 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 28.3     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 267000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0734  |\n",
      "|    explained_variance | 0.677    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 5339     |\n",
      "|    policy_loss        | -0.00892 |\n",
      "|    value_loss         | 0.0273   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=267500, episode_reward=30.58 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 30.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 267500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0492  |\n",
      "|    explained_variance | 0.285    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 5349     |\n",
      "|    policy_loss        | 0.00312  |\n",
      "|    value_loss         | 0.92     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=268000, episode_reward=23.64 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 23.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 268000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.104   |\n",
      "|    explained_variance | 0.808    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 5359     |\n",
      "|    policy_loss        | -0.00428 |\n",
      "|    value_loss         | 0.0904   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=268500, episode_reward=34.58 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 34.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 268500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0968  |\n",
      "|    explained_variance | 0.268    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 5369     |\n",
      "|    policy_loss        | -0.081   |\n",
      "|    value_loss         | 1.97     |\n",
      "------------------------------------\n",
      "Eval num_timesteps=269000, episode_reward=35.01 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 35       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 269000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0548  |\n",
      "|    explained_variance | 0.922    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 5379     |\n",
      "|    policy_loss        | -0.0241  |\n",
      "|    value_loss         | 0.139    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=269500, episode_reward=30.96 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 31       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 269500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.127   |\n",
      "|    explained_variance | 0.582    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 5389     |\n",
      "|    policy_loss        | 0.00144  |\n",
      "|    value_loss         | 0.019    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=270000, episode_reward=31.75 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 230       |\n",
      "|    mean_reward        | 31.8      |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 270000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.07e-06 |\n",
      "|    explained_variance | -0.841    |\n",
      "|    learning_rate      | 0.000687  |\n",
      "|    n_updates          | 5399      |\n",
      "|    policy_loss        | -5.73e-09 |\n",
      "|    value_loss         | 0.00909   |\n",
      "-------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 70.7     |\n",
      "| time/              |          |\n",
      "|    fps             | 2040     |\n",
      "|    iterations      | 5400     |\n",
      "|    time_elapsed    | 132      |\n",
      "|    total_timesteps | 270000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=270500, episode_reward=6.82 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 6.82     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 270500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0592  |\n",
      "|    explained_variance | 0.844    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 5409     |\n",
      "|    policy_loss        | -0.00159 |\n",
      "|    value_loss         | 0.0258   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=271000, episode_reward=8.19 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 230       |\n",
      "|    mean_reward        | 8.19      |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 271000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.74e-05 |\n",
      "|    explained_variance | -1.94     |\n",
      "|    learning_rate      | 0.000687  |\n",
      "|    n_updates          | 5419      |\n",
      "|    policy_loss        | 4.15e-07  |\n",
      "|    value_loss         | 0.0255    |\n",
      "-------------------------------------\n",
      "Eval num_timesteps=271500, episode_reward=18.89 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 18.9     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 271500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0949  |\n",
      "|    explained_variance | 0.857    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 5429     |\n",
      "|    policy_loss        | 0.0142   |\n",
      "|    value_loss         | 0.0446   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=272000, episode_reward=16.31 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 16.3     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 272000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0814  |\n",
      "|    explained_variance | 0.985    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 5439     |\n",
      "|    policy_loss        | 0.00556  |\n",
      "|    value_loss         | 0.0906   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=272500, episode_reward=31.94 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 31.9     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 272500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0657  |\n",
      "|    explained_variance | 0.927    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 5449     |\n",
      "|    policy_loss        | 0.00211  |\n",
      "|    value_loss         | 0.305    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=273000, episode_reward=16.29 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 16.3     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 273000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.131   |\n",
      "|    explained_variance | 0.807    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 5459     |\n",
      "|    policy_loss        | -0.0265  |\n",
      "|    value_loss         | 0.0684   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=273500, episode_reward=20.74 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 230       |\n",
      "|    mean_reward        | 20.7      |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 273500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.0106   |\n",
      "|    explained_variance | 0.87      |\n",
      "|    learning_rate      | 0.000687  |\n",
      "|    n_updates          | 5469      |\n",
      "|    policy_loss        | -0.000151 |\n",
      "|    value_loss         | 0.0347    |\n",
      "-------------------------------------\n",
      "Eval num_timesteps=274000, episode_reward=9.49 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 9.49     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 274000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.15    |\n",
      "|    explained_variance | 0.538    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 5479     |\n",
      "|    policy_loss        | -0.0209  |\n",
      "|    value_loss         | 0.0524   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=274500, episode_reward=40.85 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 230       |\n",
      "|    mean_reward        | 40.9      |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 274500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -4.86e-06 |\n",
      "|    explained_variance | -2.31     |\n",
      "|    learning_rate      | 0.000687  |\n",
      "|    n_updates          | 5489      |\n",
      "|    policy_loss        | -6.28e-08 |\n",
      "|    value_loss         | 0.0339    |\n",
      "-------------------------------------\n",
      "Eval num_timesteps=275000, episode_reward=26.09 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 26.1     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 275000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0676  |\n",
      "|    explained_variance | 0.414    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 5499     |\n",
      "|    policy_loss        | 0.00293  |\n",
      "|    value_loss         | 0.0267   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 71.3     |\n",
      "| time/              |          |\n",
      "|    fps             | 2040     |\n",
      "|    iterations      | 5500     |\n",
      "|    time_elapsed    | 134      |\n",
      "|    total_timesteps | 275000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=275500, episode_reward=21.21 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 21.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 275500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0282  |\n",
      "|    explained_variance | 0.797    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 5509     |\n",
      "|    policy_loss        | 0.0503   |\n",
      "|    value_loss         | 0.324    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=276000, episode_reward=29.07 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 29.1     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 276000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0882  |\n",
      "|    explained_variance | 0.872    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 5519     |\n",
      "|    policy_loss        | -0.00532 |\n",
      "|    value_loss         | 0.0632   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=276500, episode_reward=21.39 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 21.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 276500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0859  |\n",
      "|    explained_variance | 0.934    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 5529     |\n",
      "|    policy_loss        | 0.0185   |\n",
      "|    value_loss         | 0.169    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=277000, episode_reward=22.84 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 230       |\n",
      "|    mean_reward        | 22.8      |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 277000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.111    |\n",
      "|    explained_variance | 0.931     |\n",
      "|    learning_rate      | 0.000687  |\n",
      "|    n_updates          | 5539      |\n",
      "|    policy_loss        | -0.000998 |\n",
      "|    value_loss         | 0.242     |\n",
      "-------------------------------------\n",
      "Eval num_timesteps=277500, episode_reward=21.10 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 21.1     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 277500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.16    |\n",
      "|    explained_variance | 0.27     |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 5549     |\n",
      "|    policy_loss        | -0.00314 |\n",
      "|    value_loss         | 0.0426   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=278000, episode_reward=17.14 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 230       |\n",
      "|    mean_reward        | 17.1      |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 278000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.42e-06 |\n",
      "|    explained_variance | -2.64     |\n",
      "|    learning_rate      | 0.000687  |\n",
      "|    n_updates          | 5559      |\n",
      "|    policy_loss        | 3.81e-09  |\n",
      "|    value_loss         | 0.00785   |\n",
      "-------------------------------------\n",
      "Eval num_timesteps=278500, episode_reward=22.21 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 22.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 278500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0906  |\n",
      "|    explained_variance | 0.782    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 5569     |\n",
      "|    policy_loss        | -0.0223  |\n",
      "|    value_loss         | 0.0406   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=279000, episode_reward=32.00 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 230       |\n",
      "|    mean_reward        | 32        |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 279000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.000102 |\n",
      "|    explained_variance | -2.91     |\n",
      "|    learning_rate      | 0.000687  |\n",
      "|    n_updates          | 5579      |\n",
      "|    policy_loss        | -6.87e-07 |\n",
      "|    value_loss         | 0.0166    |\n",
      "-------------------------------------\n",
      "Eval num_timesteps=279500, episode_reward=24.25 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 24.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 279500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0932  |\n",
      "|    explained_variance | 0.811    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 5589     |\n",
      "|    policy_loss        | -0.0425  |\n",
      "|    value_loss         | 0.0551   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=280000, episode_reward=14.32 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 14.3     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 280000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0695  |\n",
      "|    explained_variance | 0.982    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 5599     |\n",
      "|    policy_loss        | -0.0219  |\n",
      "|    value_loss         | 0.103    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 71.7     |\n",
      "| time/              |          |\n",
      "|    fps             | 2040     |\n",
      "|    iterations      | 5600     |\n",
      "|    time_elapsed    | 137      |\n",
      "|    total_timesteps | 280000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=280500, episode_reward=19.89 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 19.9     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 280500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0852  |\n",
      "|    explained_variance | 0.885    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 5609     |\n",
      "|    policy_loss        | 0.00597  |\n",
      "|    value_loss         | 0.387    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=281000, episode_reward=16.81 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 230       |\n",
      "|    mean_reward        | 16.8      |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 281000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.0922   |\n",
      "|    explained_variance | 0.741     |\n",
      "|    learning_rate      | 0.000687  |\n",
      "|    n_updates          | 5619      |\n",
      "|    policy_loss        | -0.000578 |\n",
      "|    value_loss         | 0.101     |\n",
      "-------------------------------------\n",
      "Eval num_timesteps=281500, episode_reward=14.54 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 14.5     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 281500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.012   |\n",
      "|    explained_variance | 0.0998   |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 5629     |\n",
      "|    policy_loss        | -0.00232 |\n",
      "|    value_loss         | 0.0601   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=282000, episode_reward=14.46 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 14.5     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 282000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.153   |\n",
      "|    explained_variance | 0.825    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 5639     |\n",
      "|    policy_loss        | 0.00771  |\n",
      "|    value_loss         | 0.0292   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=282500, episode_reward=23.79 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 230       |\n",
      "|    mean_reward        | 23.8      |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 282500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.83e-06 |\n",
      "|    explained_variance | -9.84     |\n",
      "|    learning_rate      | 0.000687  |\n",
      "|    n_updates          | 5649      |\n",
      "|    policy_loss        | 6.7e-08   |\n",
      "|    value_loss         | 0.00872   |\n",
      "-------------------------------------\n",
      "Eval num_timesteps=283000, episode_reward=29.44 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 29.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 283000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0554  |\n",
      "|    explained_variance | 0.891    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 5659     |\n",
      "|    policy_loss        | -0.00366 |\n",
      "|    value_loss         | 0.0131   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=283500, episode_reward=26.31 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 26.3     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 283500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0267  |\n",
      "|    explained_variance | 0.966    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 5669     |\n",
      "|    policy_loss        | 0.00293  |\n",
      "|    value_loss         | 0.0592   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=284000, episode_reward=34.75 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 34.7     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 284000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.1     |\n",
      "|    explained_variance | 0.751    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 5679     |\n",
      "|    policy_loss        | -0.035   |\n",
      "|    value_loss         | 0.0848   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=284500, episode_reward=46.58 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 46.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 284500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.109   |\n",
      "|    explained_variance | 0.609    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 5689     |\n",
      "|    policy_loss        | 0.0566   |\n",
      "|    value_loss         | 0.879    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=285000, episode_reward=42.54 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 230       |\n",
      "|    mean_reward        | 42.5      |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 285000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.055    |\n",
      "|    explained_variance | 0.974     |\n",
      "|    learning_rate      | 0.000687  |\n",
      "|    n_updates          | 5699      |\n",
      "|    policy_loss        | -0.000122 |\n",
      "|    value_loss         | 0.0927    |\n",
      "-------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 72.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 2041     |\n",
      "|    iterations      | 5700     |\n",
      "|    time_elapsed    | 139      |\n",
      "|    total_timesteps | 285000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=285500, episode_reward=52.07 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 52.1     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 285500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.153   |\n",
      "|    explained_variance | 0.421    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 5709     |\n",
      "|    policy_loss        | -0.0133  |\n",
      "|    value_loss         | 0.0208   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=286000, episode_reward=37.40 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 230       |\n",
      "|    mean_reward        | 37.4      |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 286000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.57e-05 |\n",
      "|    explained_variance | -1.45     |\n",
      "|    learning_rate      | 0.000687  |\n",
      "|    n_updates          | 5719      |\n",
      "|    policy_loss        | -3.31e-09 |\n",
      "|    value_loss         | 0.0153    |\n",
      "-------------------------------------\n",
      "Eval num_timesteps=286500, episode_reward=20.96 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 21       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 286500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.129   |\n",
      "|    explained_variance | 0.723    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 5729     |\n",
      "|    policy_loss        | -0.0281  |\n",
      "|    value_loss         | 0.0432   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=287000, episode_reward=26.30 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 230       |\n",
      "|    mean_reward        | 26.3      |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 287000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.000339 |\n",
      "|    explained_variance | -2.48     |\n",
      "|    learning_rate      | 0.000687  |\n",
      "|    n_updates          | 5739      |\n",
      "|    policy_loss        | 2.05e-07  |\n",
      "|    value_loss         | 0.00774   |\n",
      "-------------------------------------\n",
      "Eval num_timesteps=287500, episode_reward=54.22 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 54.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 287500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.118   |\n",
      "|    explained_variance | 0.866    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 5749     |\n",
      "|    policy_loss        | 0.0261   |\n",
      "|    value_loss         | 0.0392   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=288000, episode_reward=44.79 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 44.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 288000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0801  |\n",
      "|    explained_variance | 0.989    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 5759     |\n",
      "|    policy_loss        | 0.00906  |\n",
      "|    value_loss         | 0.0808   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=288500, episode_reward=32.28 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 32.3     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 288500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0739  |\n",
      "|    explained_variance | 0.954    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 5769     |\n",
      "|    policy_loss        | 0.00477  |\n",
      "|    value_loss         | 0.194    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=289000, episode_reward=48.27 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 48.3     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 289000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.132   |\n",
      "|    explained_variance | 0.594    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 5779     |\n",
      "|    policy_loss        | -0.0229  |\n",
      "|    value_loss         | 0.241    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=289500, episode_reward=25.96 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 230       |\n",
      "|    mean_reward        | 26        |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 289500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.00438  |\n",
      "|    explained_variance | 0.752     |\n",
      "|    learning_rate      | 0.000687  |\n",
      "|    n_updates          | 5789      |\n",
      "|    policy_loss        | -4.33e-05 |\n",
      "|    value_loss         | 0.0155    |\n",
      "-------------------------------------\n",
      "Eval num_timesteps=290000, episode_reward=3.07 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 3.07     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 290000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.142   |\n",
      "|    explained_variance | -0.335   |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 5799     |\n",
      "|    policy_loss        | 0.0124   |\n",
      "|    value_loss         | 0.175    |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 74       |\n",
      "| time/              |          |\n",
      "|    fps             | 2041     |\n",
      "|    iterations      | 5800     |\n",
      "|    time_elapsed    | 142      |\n",
      "|    total_timesteps | 290000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=290500, episode_reward=2.87 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 230       |\n",
      "|    mean_reward        | 2.87      |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 290500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.32e-05 |\n",
      "|    explained_variance | -0.914    |\n",
      "|    learning_rate      | 0.000687  |\n",
      "|    n_updates          | 5809      |\n",
      "|    policy_loss        | 1.13e-07  |\n",
      "|    value_loss         | 0.0247    |\n",
      "-------------------------------------\n",
      "Eval num_timesteps=291000, episode_reward=27.54 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 27.5     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 291000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0867  |\n",
      "|    explained_variance | -0.184   |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 5819     |\n",
      "|    policy_loss        | -0.00637 |\n",
      "|    value_loss         | 0.0999   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=291500, episode_reward=20.95 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 20.9     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 291500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0278  |\n",
      "|    explained_variance | 0.963    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 5829     |\n",
      "|    policy_loss        | 0.0111   |\n",
      "|    value_loss         | 0.0753   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=292000, episode_reward=7.71 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 7.71     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 292000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0927  |\n",
      "|    explained_variance | 0.791    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 5839     |\n",
      "|    policy_loss        | -0.017   |\n",
      "|    value_loss         | 0.061    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=292500, episode_reward=63.77 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 63.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 292500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.181   |\n",
      "|    explained_variance | 0.929    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 5849     |\n",
      "|    policy_loss        | -0.116   |\n",
      "|    value_loss         | 0.227    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=293000, episode_reward=50.12 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 50.1     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 293000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0794  |\n",
      "|    explained_variance | 0.985    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 5859     |\n",
      "|    policy_loss        | -0.00172 |\n",
      "|    value_loss         | 0.0547   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=293500, episode_reward=18.43 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 18.4     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 293500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.17    |\n",
      "|    explained_variance | 0.336    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 5869     |\n",
      "|    policy_loss        | 0.036    |\n",
      "|    value_loss         | 0.0303   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=294000, episode_reward=18.02 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 230       |\n",
      "|    mean_reward        | 18        |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 294000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.0006   |\n",
      "|    explained_variance | -1.39     |\n",
      "|    learning_rate      | 0.000687  |\n",
      "|    n_updates          | 5879      |\n",
      "|    policy_loss        | -8.92e-06 |\n",
      "|    value_loss         | 0.0136    |\n",
      "-------------------------------------\n",
      "Eval num_timesteps=294500, episode_reward=30.10 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 30.1     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 294500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0845  |\n",
      "|    explained_variance | 0.787    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 5889     |\n",
      "|    policy_loss        | 0.0263   |\n",
      "|    value_loss         | 0.0312   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=295000, episode_reward=34.07 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 230       |\n",
      "|    mean_reward        | 34.1      |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 295000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.000282 |\n",
      "|    explained_variance | -2.33     |\n",
      "|    learning_rate      | 0.000687  |\n",
      "|    n_updates          | 5899      |\n",
      "|    policy_loss        | -2.21e-06 |\n",
      "|    value_loss         | 0.0128    |\n",
      "-------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 74.8     |\n",
      "| time/              |          |\n",
      "|    fps             | 2041     |\n",
      "|    iterations      | 5900     |\n",
      "|    time_elapsed    | 144      |\n",
      "|    total_timesteps | 295000   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=295500, episode_reward=27.81 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 27.8     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 295500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.107   |\n",
      "|    explained_variance | 0.856    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 5909     |\n",
      "|    policy_loss        | 0.00824  |\n",
      "|    value_loss         | 0.0604   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=296000, episode_reward=28.71 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 28.7     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 296000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0729  |\n",
      "|    explained_variance | 0.982    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 5919     |\n",
      "|    policy_loss        | -0.0281  |\n",
      "|    value_loss         | 0.0951   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=296500, episode_reward=42.27 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 42.3     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 296500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0806  |\n",
      "|    explained_variance | 0.975    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 5929     |\n",
      "|    policy_loss        | -0.0124  |\n",
      "|    value_loss         | 0.0859   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=297000, episode_reward=42.23 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 42.2     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 297000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0884  |\n",
      "|    explained_variance | 0.772    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 5939     |\n",
      "|    policy_loss        | 0.0117   |\n",
      "|    value_loss         | 0.142    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=297500, episode_reward=50.01 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 50       |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 297500   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0181  |\n",
      "|    explained_variance | 0.803    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 5949     |\n",
      "|    policy_loss        | 0.00502  |\n",
      "|    value_loss         | 0.0389   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=298000, episode_reward=64.75 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 64.7     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 298000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.09    |\n",
      "|    explained_variance | 0.793    |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 5959     |\n",
      "|    policy_loss        | -0.0339  |\n",
      "|    value_loss         | 0.0337   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=298500, episode_reward=54.70 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 230       |\n",
      "|    mean_reward        | 54.7      |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 298500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.39e-05 |\n",
      "|    explained_variance | -2.15     |\n",
      "|    learning_rate      | 0.000687  |\n",
      "|    n_updates          | 5969      |\n",
      "|    policy_loss        | 9.61e-08  |\n",
      "|    value_loss         | 0.0119    |\n",
      "-------------------------------------\n",
      "Eval num_timesteps=299000, episode_reward=33.56 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 33.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 299000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.0491  |\n",
      "|    explained_variance | 0.82     |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 5979     |\n",
      "|    policy_loss        | -0.00849 |\n",
      "|    value_loss         | 0.0189   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=299500, episode_reward=31.08 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 230       |\n",
      "|    mean_reward        | 31.1      |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 299500    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.0078   |\n",
      "|    explained_variance | 0.986     |\n",
      "|    learning_rate      | 0.000687  |\n",
      "|    n_updates          | 5989      |\n",
      "|    policy_loss        | -0.000114 |\n",
      "|    value_loss         | 0.0233    |\n",
      "-------------------------------------\n",
      "Eval num_timesteps=300000, episode_reward=24.57 +/- 0.00\n",
      "Episode length: 230.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 230      |\n",
      "|    mean_reward        | 24.6     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 300000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -0.113   |\n",
      "|    explained_variance | 0.87     |\n",
      "|    learning_rate      | 0.000687 |\n",
      "|    n_updates          | 5999     |\n",
      "|    policy_loss        | 0.00584  |\n",
      "|    value_loss         | 0.0385   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 75.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 2039     |\n",
      "|    iterations      | 6000     |\n",
      "|    time_elapsed    | 147      |\n",
      "|    total_timesteps | 300000   |\n",
      "---------------------------------\n",
      "Final model evaluation on validation set: Mean Reward = 77.52 ± 15.12\n"
     ]
    }
   ],
   "source": [
    "#Train the final model\n",
    "a2c_best_params = a2c_study.best_params\n",
    "\n",
    "\n",
    "train_a2c_env = CustomStocksEnv(df=train_df, window_size=window_size, frame_bound=(window_size, len(train_df)))\n",
    "val_a2c_env = CustomStocksEnv(df=val_df, window_size=window_size, frame_bound=(window_size, len(val_df)))\n",
    "val_a2c_env = Monitor(val_a2c_env)\n",
    "\n",
    "eval_callback_a2c = EvalCallback(\n",
    "        val_a2c_env,\n",
    "        best_model_save_path='./logs/best_model_a2c/',\n",
    "        log_path='./logs/results/',\n",
    "        eval_freq=500,  # Evaluate every 500 timesteps.\n",
    "        deterministic=True,\n",
    "        render=False\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "final_a2c_model = A2C(\n",
    "        policy=\"MlpPolicy\",\n",
    "        env=train_a2c_env,\n",
    "        verbose=1,\n",
    "        learning_rate=a2c_best_params['learning_rate'],\n",
    "        n_steps=a2c_best_params['n_steps'],\n",
    "        gamma=a2c_best_params['gamma'],\n",
    "        gae_lambda=a2c_best_params['gae_lambda'],\n",
    "        ent_coef=a2c_best_params['ent_coef'],\n",
    "        vf_coef=a2c_best_params['vf_coef'],\n",
    "        max_grad_norm=a2c_best_params['max_grad_norm']\n",
    "    )\n",
    "\n",
    "\n",
    "final_a2c_model.learn(total_timesteps=300_000, callback=eval_callback_a2c)\n",
    "final_a2c_model.save(\"a2c_stocks_model\")\n",
    "\n",
    "#Load best model\n",
    "final_a2c_model = A2C.load(\"./logs/best_model_a2c/best_model.zip\")\n",
    "\n",
    "\n",
    "mean_reward, std_reward = evaluate_policy(final_a2c_model, val_a2c_env, n_eval_episodes=20, deterministic=False)\n",
    "\n",
    "print(f\"Final model evaluation on validation set: Mean Reward = {mean_reward:.2f} ± {std_reward:.2f}\")\n",
    "\n",
    "train_env.close()\n",
    "val_env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b239ca40-4e9a-40e1-99e4-cbe77b9ded1d",
   "metadata": {},
   "source": [
    "## c. Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9e5106e2-63f5-4df6-ad08-269f02569290",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final info: {'total_reward': np.float32(14.699127), 'total_profit': np.float32(0.96534675), 'position': <Positions.Long: 1>}\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_a2c_env = CustomStocksEnv(df=test_df, window_size=window_size, frame_bound=(window_size, len(test_df)))\n",
    "final_a2c_model = A2C.load(\"./logs/best_model_a2c/best_model.zip\")#load(\"a2c_stocks_model\", env=test_a2c_env)\n",
    "\n",
    "obs, info = test_a2c_env.reset()\n",
    "while True:\n",
    "    action, _states = final_a2c_model.predict(obs, deterministic=True)\n",
    "    \n",
    "    obs, reward, terminated, truncated, info = test_a2c_env.step(action)\n",
    "    done = terminated or truncated\n",
    "\n",
    "    if done:\n",
    "        print(\"Final info:\", info)\n",
    "        break\n",
    "\n",
    "plt.figure(figsize=(15, 6))\n",
    "plt.cla()\n",
    "test_a2c_env.unwrapped.render_all()\n",
    "plt.show()\n",
    "\n",
    "# Close the environment to free resources\n",
    "test_a2c_env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a748cc62-2919-48fd-9973-849095463dc8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "124d45d5-4a66-4f9d-b668-d84fee0b286b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
